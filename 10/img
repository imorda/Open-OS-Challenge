.. image:: https://translate.fedoraproject.org/widgets/dnf/-/dnf-master/svg-badge.svg
    :alt: Translation status
    :target: https://translate.fedoraproject.org/engage/dnf/?utm_source=widget
###############
 Dandified YUM
###############

.. image:: https://raw.githubusercontent.com/rpm-software-management/dnf/gh-pages/logos/DNF_logo.png
 
Dandified YUM (DNF) is the next upcoming major version of `YUM <http://yum.baseurl.org/>`_. It does package management using `RPM <http://rpm.org/>`_, `libsolv <https://github.com/openSUSE/libsolv>`_ and `hawkey <https://github.com/rpm-software-management/hawkey>`_ libraries. For metadata handling and package downloads it utilizes `librepo <https://github.com/tojaj/librepo>`_. To process and effectively handle the comps data it uses `libcomps <https://github.com/midnightercz/libcomps>`_.

============
 Installing
============

DNF and all its dependencies are available in Fedora 18 and later, including the
rawhide Fedora.

Optionally you can use repositories with DNF nightly builds for last 2 stable Fedora versions available at copr://rpmsoftwaremanagement/dnf-nightly. You can enable the repository e.g. using:: 

    dnf copr enable rpmsoftwaremanagement/dnf-nightly

Then install DNF typing::

    sudo yum install dnf

In other RPM-based distributions you need to build all the components from their
sources.

======================
 Building from source
======================

All commands should be run from the DNF git checkout directory.

To install the build dependencies::

    sudo dnf builddep dnf.spec

To build DNF::

    mkdir build;
    pushd build;
    cmake ..; # add '-DPYTHON_DESIRED="3"' option for Python 3 build
    make;
    popd;

To run DNF when compiled for Python2::

    PYTHONPATH=`readlink -f .` bin/dnf-2 <arguments>

To run DNF when compiled for Python3::

    PYTHONPATH=`readlink -f .` bin/dnf-3 <arguments>

If you want to build the manpages, use the option ``-DWITH_MAN=0`` with cmake.

Man pages will be located in ``build/doc`` and can be read with ``man -l``, e.g::

    man -l build/doc/dnf.8

=============================
 Building and installing rpm
=============================

From the DNF git checkout directory::

    $ tito build --test --rpm
    # dnf install /tmp/tito/noarch/*

===============
 Running tests
===============

From the DNF git checkout directory::

    mkdir build;
    pushd build;
    cmake .. && make ARGS="-V" test;
    popd;

==============
 Contribution
==============

Here's the most direct way to get your work merged into the project.

1. Fork the project
#. Clone down your fork
#. Implement your feature or bug fix and commit changes
#. If the change fixes a bug at `Red Hat bugzilla <https://bugzilla.redhat.com/>`_, or if it is important to the end user, add the following block to the commit message::

    = changelog =
    msg:           message to be included in the changelog
    type:          one of: bugfix/enhancement/security (this field is required when message is present)
    resolves:      URLs to bugs or issues resolved by this commit (can be specified multiple times)
    related:       URLs to any related bugs or issues (can be specified multiple times)

   * For example::

       = changelog =
       msg: Verify GPG signatures when running dnf-automatic
       type: bugfix
       resolves: https://bugzilla.redhat.com/show_bug.cgi?id=1793298

   * For your convenience, you can also use git commit template by running the following command in the top-level directory of this project::

       git config commit.template ./.git-commit-template

#. In special commit add your name and email under ``DNF CONTRIBUTORS`` section in `authors file <https://github.com/rpm-software-management/dnf/blob/master/AUTHORS>`_ as a reward for your generosity
#. Push the branch up to your fork
#. Send a pull request for your branch

Please, do not create the pull requests with translation (.po) files improvements. Fix the translation on `Fedora Weblate <https://translate.fedoraproject.org/projects/dnf/>`_ instead.

===============
 Documentation
===============

The DNF package distribution contains man pages, dnf(8) and dnf.conf(8). It is also possible to `read the DNF documentation <http://dnf.readthedocs.org>`_ online, the page includes API documentation. There's also a `wiki <https://github.com/rpm-software-management/dnf/wiki>`_ meant for contributors to DNF and related projects.

====================
 Bug reporting etc.
====================

Please report discovered bugs to the `Red Hat bugzilla <https://bugzilla.redhat.com/>`_ following this `guide <https://github.com/rpm-software-management/dnf/wiki/Bug-Reporting>`_. If you planned to propose the patch in the report, consider `Contribution`_ instead.

Freenode's irc channel ``#yum`` is meant for discussions related to both YUM and DNF. Questions should be asked there, issues discussed. Remember: ``#yum`` is not a support channel and prior research is expected from the questioner.
pip
===

The `PyPA recommended
<https://packaging.python.org/en/latest/current/>`_
tool for installing Python packages.

* `Installation <https://pip.pypa.io/en/stable/installing.html>`_
* `Documentation <https://pip.pypa.io/>`_
* `Changelog <https://pip.pypa.io/en/stable/news.html>`_
* `Github Page <https://github.com/pypa/pip>`_
* `Issue Tracking <https://github.com/pypa/pip/issues>`_
* `User mailing list <http://groups.google.com/group/python-virtualenv>`_
* `Dev mailing list <http://groups.google.com/group/pypa-dev>`_
* User IRC: #pypa on Freenode.
* Dev IRC: #pypa-dev on Freenode.


.. image:: https://img.shields.io/pypi/v/pip.svg
   :target: https://pypi.python.org/pypi/pip

.. image:: https://img.shields.io/travis/pypa/pip/master.svg
   :target: http://travis-ci.org/pypa/pip

.. image:: https://img.shields.io/appveyor/ci/pypa/pip.svg
   :target: https://ci.appveyor.com/project/pypa/pip/history

.. image:: https://readthedocs.org/projects/pip/badge/?version=stable
   :target: https://pip.pypa.io/en/stable

Code of Conduct
---------------

Everyone interacting in the pip project's codebases, issue trackers, chat
rooms, and mailing lists is expected to follow the `PyPA Code of Conduct`_.

.. _PyPA Code of Conduct: https://www.pypa.io/en/latest/code-of-conduct/
v39.2.0
-------

* #1359: Support using "file:" to load a PEP 440-compliant package version from
  a text file.
* #1360: Fixed issue with a mismatch between the name of the package and the
  name of the .dist-info file in wheel files
* #1365: Take the package_dir option into account when loading the version from
  a module attribute.
* #1353: Added coverage badge to README.
* #1356: Made small fixes to the developer guide documentation.
* #1357: Fixed warnings in documentation builds and started enforcing that the
  docs build without warnings in tox.
* #1376: Updated release process docs.
* #1343: The ``setuptools`` specific ``long_description_content_type``,
  ``project_urls`` and ``provides_extras`` fields are now set consistently
  after any ``distutils`` ``setup_keywords`` calls, allowing them to override
  values.
* #1352: Added ``tox`` environment for documentation builds.
* #1354: Added ``towncrier`` for changelog managment.
* #1355: Add PR template.
* #1368: Fixed tests which failed without network connectivity.
* #1369: Added unit tests for PEP 425 compatibility tags support.
* #1372: Stop testing Python 3.3 in Travis CI, now that the latest version of
  ``wheel`` no longer installs on it.

v39.1.0
-------

* #1340: Update all PyPI URLs to reflect the switch to the
  new Warehouse codebase.
* #1337: In ``pkg_resources``, now support loading resources
  for modules loaded by the ``SourcelessFileLoader``.
* #1332: Silence spurious wheel related warnings on Windows.

v39.0.1
-------

* #1297: Restore Unicode handling for Maintainer fields in
  metadata.

v39.0.0
-------

* #1296: Setuptools now vendors its own direct dependencies, no
  longer relying on the dependencies as vendored by pkg_resources.

* #296: Removed long-deprecated support for iteration on
  Version objects as returned by ``pkg_resources.parse_version``.
  Removed the ``SetuptoolsVersion`` and
  ``SetuptoolsLegacyVersion`` names as well. They should not
  have been used, but if they were, replace with
  ``Version`` and ``LegacyVersion`` from ``packaging.version``.

v38.7.0
-------

* #1288: Add support for maintainer in PKG-INFO.

v38.6.1
-------

* #1292: Avoid generating ``Provides-Extra`` in metadata when
  no extra is present (but environment markers are).

v38.6.0
-------

* #1286: Add support for Metadata 2.1 (PEP 566).

v38.5.2
-------

* #1285: Fixed RuntimeError in pkg_resources.parse_requirements
  on Python 3.7 (stemming from PEP 479).

v38.5.1
-------

* #1271: Revert to Cython legacy ``build_ext`` behavior for
  compatibility.

v38.5.0
-------

* #1229: Expand imports in ``build_ext`` to refine detection of
  Cython availability.

* #1270: When Cython is available, ``build_ext`` now uses the
  new_build_ext.

v38.4.1
-------

* #1257: In bdist_egg.scan_module, fix ValueError on Python 3.7.

v38.4.0
-------

* #1231: Removed warning when PYTHONDONTWRITEBYTECODE is enabled.

v38.3.0
-------

* #1210: Add support for PEP 345 Project-URL metadata.
* #1207: Add support for ``long_description_type`` to setup.cfg
  declarative config as intended and documented.

v38.2.5
-------

* #1232: Fix trailing slash handling in ``pkg_resources.ZipProvider``.

v38.2.4
-------

* #1220: Fix `data_files` handling when installing from wheel.

v38.2.3
-------

* fix Travis' Python 3.3 job.

v38.2.2
-------

* #1214: fix handling of namespace packages when installing
  from a wheel.

v38.2.1
-------

* #1212: fix encoding handling of metadata when installing
  from a wheel.

v38.2.0
-------

* #1200: easy_install now support installing from wheels:
  they will be installed as standalone unzipped eggs.

v38.1.0
-------

* #1208: Improve error message when failing to locate scripts
  in egg-info metadata.

v38.0.0
-------

* #458: In order to support deterministic builds, Setuptools no
  longer allows packages to declare ``install_requires`` as
  unordered sequences (sets or dicts).

v37.0.0
-------

* #878: Drop support for Python 2.6. Python 2.6 users should
  rely on 'setuptools < 37dev'.

v36.8.0
-------

* #1190: In SSL support for package index operations, use SNI
  where available.

v36.7.3
-------

* #1175: Bug fixes to ``build_meta`` module.

v36.7.2
-------

* #701: Fixed duplicate test discovery on Python 3.

v36.7.1
-------

* #1193: Avoid test failures in bdist_egg when
  PYTHONDONTWRITEBYTECODE is set.

v36.7.0
-------

* #1054: Support ``setup_requires`` in ``setup.cfg`` files.

v36.6.1
-------

* #1132: Removed redundant and costly serialization/parsing step
  in ``EntryPoint.__init__``.

* #844: ``bdist_egg --exclude-source-files`` now tested and works
  on Python 3.

v36.6.0
-------

* #1143: Added ``setuptools.build_meta`` module, an implementation
  of PEP-517 for Setuptools-defined packages.

* #1143: Added ``dist_info`` command for producing dist_info
  metadata.

v36.5.0
-------

* #170: When working with Mercurial checkouts, use Windows-friendly
  syntax for suppressing output.

* Inspired by #1134, performed substantial refactoring of
  ``pkg_resources.find_on_path`` to facilitate an optimization
  for paths with many non-version entries.

v36.4.0
-------

* #1075: Add new ``Description-Content-Type`` metadata field. `See here for
  documentation on how to use this field.
  <https://packaging.python.org/specifications/#description-content-type>`_

* #1068: Sort files and directories when building eggs for
  deterministic order.

* #196: Remove caching of easy_install command in fetch_build_egg.
  Fixes issue where ``pytest-runner-N.N`` would satisfy the installation
  of ``pytest``.

* #1129: Fix working set dependencies handling when replacing conflicting
  distributions (e.g. when using ``setup_requires`` with a conflicting
  transitive dependency, fix #1124).

* #1133: Improved handling of README files extensions and added
  Markdown to the list of searched READMES.

* #1135: Improve performance of pkg_resources import by not invoking
  ``access`` or ``stat`` and using ``os.listdir`` instead.

v36.3.0
-------

* #1131: Make possible using several files within ``file:`` directive
  in metadata.long_description in ``setup.cfg``.

v36.2.7
-------

* fix #1105: Fix handling of requirements with environment
  markers when declared in ``setup.cfg`` (same treatment as
  for #1081).

v36.2.6
-------

* #462: Don't assume a directory is an egg by the ``.egg``
  extension alone.

v36.2.5
-------

* #1093: Fix test command handler with extras_require.
* #1112, #1091, #1115: Now using Trusty containers in
  Travis for CI and CD.

v36.2.4
-------

* #1092: ``pkg_resources`` now uses ``inspect.getmro`` to
  resolve classes in method resolution order.

v36.2.3
-------

* #1102: Restore behavior for empty extras.

v36.2.2
-------

* #1099: Revert commit a3ec721, restoring intended purpose of
  extras as part of a requirement declaration.

v36.2.1
-------

* fix #1086
* fix #1087
* support extras specifiers in install_requires requirements

v36.2.0
-------

* #1081: Environment markers indicated in ``install_requires``
  are now processed and treated as nameless ``extras_require``
  with markers, allowing their metadata in requires.txt to be
  correctly generated.

* #1053: Tagged commits are now released using Travis-CI
  build stages, meaning releases depend on passing tests on
  all supported Python versions (Linux) and not just the latest
  Python version.

v36.1.1
-------

* #1083: Correct ``py31compat.makedirs`` to correctly honor
  ``exist_ok`` parameter.
* #1083: Also use makedirs compatibility throughout setuptools.

v36.1.0
-------

* #1083: Avoid race condition on directory creation in
  ``pkg_resources.ensure_directory``.

* Removed deprecation of and restored support for
  ``upload_docs`` command for sites other than PyPI.
  Only warehouse is dropping support, but services like
  `devpi <http://doc.devpi.net/latest/>`_ continue to
  support docs built by setuptools' plugins. See
  `this comment <https://bitbucket.org/hpk42/devpi/issues/388/support-rtd-model-for-building-uploading#comment-34292423>`_
  for more context on the motivation for this change.

v36.0.1
-------

* #1042: Fix import in py27compat module that still
  referenced six directly, rather than through the externs
  module (vendored packages hook).

v36.0.0
-------

* #980 and others: Once again, Setuptools vendors all
  of its dependencies. It seems to be the case that in
  the Python ecosystem, all build tools must run without
  any dependencies (build, runtime, or otherwise). At
  such a point that a mechanism exists that allows
  build tools to have dependencies, Setuptools will adopt
  it.

v35.0.2
-------

* #1015: Fix test failures on Python 3.7.

* #1024: Add workaround for Jython #2581 in monkey module.

v35.0.1
-------

* #992: Revert change introduced in v34.4.1, now
  considered invalid.

* #1016: Revert change introduced in v35.0.0 per #1014,
  referencing #436. The approach had unintended
  consequences, causing sdist installs to be missing
  files.

v35.0.0
-------

* #436: In egg_info.manifest_maker, no longer read
  the file list from the manifest file, and instead
  re-build it on each build. In this way, files removed
  from the specification will not linger in the manifest.
  As a result, any files manually added to the manifest
  will be removed on subsequent egg_info invocations.
  No projects should be manually adding files to the
  manifest and should instead use MANIFEST.in or SCM
  file finders to force inclusion of files in the manifest.

v34.4.1
-------

* #1008: In MSVC support, use always the last version available for Windows SDK and UCRT SDK.

* #1008: In MSVC support, fix "vcruntime140.dll" returned path with Visual Studio 2017.

* #992: In msvc.msvc9_query_vcvarsall, ensure the
  returned dicts have str values and not Unicode for
  compatibility with os.environ.

v34.4.0
-------

* #995: In MSVC support, add support for "Microsoft Visual Studio 2017" and "Microsoft Visual Studio Build Tools 2017".

* #999 via #1007: Extend support for declarative package
  config in a setup.cfg file to include the options
  ``python_requires`` and ``py_modules``.

v34.3.3
-------

* #967 (and #997): Explicitly import submodules of
  packaging to account for environments where the imports
  of those submodules is not implied by other behavior.

v34.3.2
-------

* #993: Fix documentation upload by correcting
  rendering of content-type in _build_multipart
  on Python 3.

v34.3.1
-------

* #988: Trap ``os.unlink`` same as ``os.remove`` in
  ``auto_chmod`` error handler.

* #983: Fixes to invalid escape sequence deprecations on
  Python 3.6.

v34.3.0
-------

* #941: In the upload command, if the username is blank,
  default to ``getpass.getuser()``.

* #971: Correct distutils findall monkeypatch to match
  appropriate versions (namely Python 3.4.6).

v34.2.0
-------

* #966: Add support for reading dist-info metadata and
  thus locating Distributions from zip files.

* #968: Allow '+' and '!' in egg fragments
  so that it can take package names that contain
  PEP 440 conforming version specifiers.

v34.1.1
-------

* #953: More aggressively employ the compatibility issue
  originally added in #706.

v34.1.0
-------

* #930: ``build_info`` now accepts two new parameters
  to optimize and customize the building of C libraries.

v34.0.3
-------

* #947: Loosen restriction on the version of six required,
  restoring compatibility with environments relying on
  six 1.6.0 and later.

v34.0.2
-------

* #882: Ensure extras are honored when building the
  working set.
* #913: Fix issue in develop if package directory has
  a trailing slash.

v34.0.1
-------

* #935: Fix glob syntax in graft.

v34.0.0
-------

* #581: Instead of vendoring the growing list of
  dependencies that Setuptools requires to function,
  Setuptools now requires these dependencies just like
  any other project. Unlike other projects, however,
  Setuptools cannot rely on ``setup_requires`` to
  demand the dependencies it needs to install because
  its own machinery would be necessary to pull those
  dependencies if not present (a bootstrapping problem).
  As a result, Setuptools no longer supports self upgrade or
  installation in the general case. Instead, users are
  directed to use pip to install and upgrade using the
  ``wheel`` distributions of setuptools.

  Users are welcome to contrive other means to install
  or upgrade Setuptools using other means, such as
  pre-installing the Setuptools dependencies with pip
  or a bespoke bootstrap tool, but such usage is not
  recommended and is not supported.

  As discovered in #940, not all versions of pip will
  successfully install Setuptools from its pre-built
  wheel. If you encounter issues with "No module named
  six" or "No module named packaging", especially
  following a line "Running setup.py egg_info for package
  setuptools", then your pip is not new enough.

  There's an additional issue in pip where setuptools
  is upgraded concurrently with other source packages,
  described in pip #4253. The proposed workaround is to
  always upgrade Setuptools first prior to upgrading
  other packages that would upgrade Setuptools.

v33.1.1
-------

* #921: Correct issue where certifi fallback not being
  reached on Windows.

v33.1.0
-------

Installation via pip, as indicated in the `Python Packaging
User's Guide <https://packaging.python.org/installing/>`_,
is the officially-supported mechanism for installing
Setuptools, and this recommendation is now explicit in the
much more concise README.

Other edits and tweaks were made to the documentation. The
codebase is unchanged.

v33.0.0
-------

* #619: Removed support for the ``tag_svn_revision``
  distribution option. If Subversion tagging support is
  still desired, consider adding the functionality to
  setuptools_svn in setuptools_svn #2.

v32.3.1
-------

* #866: Use ``dis.Bytecode`` on Python 3.4 and later in
  ``setuptools.depends``.

v32.3.0
-------

* #889: Backport proposed fix for disabling interpolation in
  distutils.Distribution.parse_config_files.

v32.2.0
-------

* #884: Restore support for running the tests under
  `pytest-runner <https://github.com/pytest-dev/pytest-runner>`_
  by ensuring that PYTHONPATH is honored in tests invoking
  a subprocess.

v32.1.3
-------

* #706: Add rmtree compatibility shim for environments where
  rmtree fails when passed a unicode string.

v32.1.2
-------

* #893: Only release sdist in zip format as warehouse now
  disallows releasing two different formats.

v32.1.1
-------

* #704: More selectively ensure that 'rmtree' is not invoked with
  a byte string, enabling it to remove files that are non-ascii,
  even on Python 2.

* #712: In 'sandbox.run_setup', ensure that ``__file__`` is
  always a ``str``, modeling the behavior observed by the
  interpreter when invoking scripts and modules.

v32.1.0
-------

* #891: In 'test' command on test failure, raise DistutilsError,
  suppression invocation of subsequent commands.

v32.0.0
-------

* #890: Revert #849. ``global-exclude .foo`` will not match all
  ``*.foo`` files any more. Package authors must add an explicit
  wildcard, such as ``global-exclude *.foo``, to match all
  ``.foo`` files. See #886, #849.

v31.0.1
-------

* #885: Fix regression where 'pkg_resources._rebuild_mod_path'
  would fail when a namespace package's '__path__' was not
  a list with a sort attribute.

v31.0.0
-------

* #250: Install '-nspkg.pth' files for packages installed
  with 'setup.py develop'. These .pth files allow
  namespace packages installed by pip or develop to
  co-mingle. This change required the removal of the
  change for #805 and pip #1924, introduced in 28.3.0 and implicated
  in #870, but means that namespace packages not in a
  site packages directory will no longer work on Python
  earlier than 3.5, whereas before they would work on
  Python not earlier than 3.3.

v30.4.0
-------

* #879: For declarative config:

  - read_configuration() now accepts ignore_option_errors argument. This allows scraping tools to read metadata without a need to download entire packages. E.g. we can gather some stats right from GitHub repos just by downloading setup.cfg.

  - packages find: directive now supports fine tuning from a subsection. The same arguments as for find() are accepted.

v30.3.0
-------

* #394 via #862: Added support for `declarative package
  config in a setup.cfg file
  <https://setuptools.readthedocs.io/en/latest/setuptools.html#configuring-setup-using-setup-cfg-files>`_.

v30.2.1
-------

* #850: In test command, invoke unittest.main with
  indication not to exit the process.

v30.2.0
-------

* #854: Bump to vendored Packaging 16.8.

v30.1.0
-------

* #846: Also trap 'socket.error' when opening URLs in
  package_index.

* #849: Manifest processing now matches the filename
  pattern anywhere in the filename and not just at the
  start. Restores behavior found prior to 28.5.0.

v30.0.0
-------

* #864: Drop support for Python 3.2. Systems requiring
  Python 3.2 support must use 'setuptools < 30'.

* #825: Suppress warnings for single files.

* #830 via #843: Once again restored inclusion of data
  files to sdists, but now trap TypeError caused by
  techniques employed rjsmin and similar.

v29.0.1
-------

* #861: Re-release of v29.0.1 with the executable script
  launchers bundled. Now, launchers are included by default
  and users that want to disable this behavior must set the
  environment variable
  'SETUPTOOLS_INSTALL_WINDOWS_SPECIFIC_FILES' to
  a false value like "false" or "0".

v29.0.0
-------

* #841: Drop special exception for packages invoking
  win32com during the build/install process. See
  Distribute #118 for history.

v28.8.0
-------

* #629: Per the discussion, refine the sorting to use version
  value order for more accurate detection of the latest
  available version when scanning for packages. See also
  #829.

* #837: Rely on the config var "SO" for Python 3.3.0 only
  when determining the ext filename.

v28.7.1
-------

* #827: Update PyPI root for dependency links.

* #833: Backed out changes from #830 as the implementation
  seems to have problems in some cases.

v28.7.0
-------

* #832: Moved much of the namespace package handling
  functionality into a separate module for re-use in something
  like #789.
* #830: ``sdist`` command no longer suppresses the inclusion
  of data files, re-aligning with the expectation of distutils
  and addressing #274 and #521.

v28.6.1
-------

* #816: Fix manifest file list order in tests.

v28.6.0
-------

* #629: When scanning for packages, ``pkg_resources`` now
  ignores empty egg-info directories and gives precedence to
  packages whose versions are lexicographically greatest,
  a rough approximation for preferring the latest available
  version.

v28.5.0
-------

* #810: Tests are now invoked with tox and not setup.py test.
* #249 and #450 via #764: Avoid scanning the whole tree
  when building the manifest. Also fixes a long-standing bug
  where patterns in ``MANIFEST.in`` had implicit wildcard
  matching. This caused ``global-exclude .foo`` to exclude
  all ``*.foo`` files, but also ``global-exclude bar.py`` to
  exclude ``foo_bar.py``.

v28.4.0
-------

* #732: Now extras with a hyphen are honored per PEP 426.
* #811: Update to pyparsing 2.1.10.
* Updated ``setuptools.command.sdist`` to re-use most of
  the functionality directly from ``distutils.command.sdist``
  for the ``add_defaults`` method with strategic overrides.
  See #750 for rationale.
* #760 via #762: Look for certificate bundle where SUSE
  Linux typically presents it. Use ``certifi.where()`` to locate
  the bundle.

v28.3.0
-------

* #809: In ``find_packages()``, restore support for excluding
  a parent package without excluding a child package.

* #805: Disable ``-nspkg.pth`` behavior on Python 3.3+ where
  PEP-420 functionality is adequate. Fixes pip #1924.

v28.1.0
-------

* #803: Bump certifi to 2016.9.26.

v28.0.0
-------

* #733: Do not search excluded directories for packages.
  This introduced a backwards incompatible change in ``find_packages()``
  so that ``find_packages(exclude=['foo']) == []``, excluding subpackages of ``foo``.
  Previously, ``find_packages(exclude=['foo']) == ['foo.bar']``,
  even though the parent ``foo`` package was excluded.

* #795: Bump certifi.

* #719: Suppress decoding errors and instead log a warning
  when metadata cannot be decoded.

v27.3.1
-------

* #790: In MSVC monkeypatching, explicitly patch each
  function by name in the target module instead of inferring
  the module from the function's ``__module__``. Improves
  compatibility with other packages that might have previously
  patched distutils functions (i.e. NumPy).

v27.3.0
-------

* #794: In test command, add installed eggs to PYTHONPATH
  when invoking tests so that subprocesses will also have the
  dependencies available. Fixes `tox 330
  <https://github.com/tox-dev/tox/issues/330>`_.

* #795: Update vendored pyparsing 2.1.9.

v27.2.0
-------

* #520 and #513: Suppress ValueErrors in fixup_namespace_packages
  when lookup fails.

* Nicer, more consistent interfaces for msvc monkeypatching.

v27.1.2
-------

* #779 via #781: Fix circular import.

v27.1.1
-------

* #778: Fix MSVC monkeypatching.

v27.1.0
-------

* Introduce the (private) ``monkey`` module to encapsulate
  the distutils monkeypatching behavior.

v27.0.0
-------

* Now use Warehouse by default for
  ``upload``, patching ``distutils.config.PyPIRCCommand`` to
  affect default behavior.

  Any config in .pypirc should be updated to replace

    https://pypi.python.org/pypi/

  with

    https://upload.pypi.org/legacy/

  Similarly, any passwords stored in the keyring should be
  updated to use this new value for "system".

  The ``upload_docs`` command will continue to use the python.org
  site, but the command is now deprecated. Users are urged to use
  Read The Docs instead.

* #776: Use EXT_SUFFIX for py_limited_api renaming.

* #774 and #775: Use LegacyVersion from packaging when
  detecting numpy versions.

v26.1.1
-------

* Re-release of 26.1.0 with pytest pinned to allow for automated
  deployment and thus proper packaging environment variables,
  fixing issues with missing executable launchers.

v26.1.0
-------

* #763: ``pkg_resources.get_default_cache`` now defers to the
  `appdirs project <https://pypi.org/project/appdirs>`_ to
  resolve the cache directory. Adds a vendored dependency on
  appdirs to pkg_resources.

v26.0.0
-------

* #748: By default, sdists are now produced in gzipped tarfile
  format by default on all platforms, adding forward compatibility
  for the same behavior in Python 3.6 (See Python #27819).

* #459 via #736: On Windows with script launchers,
  sys.argv[0] now reflects
  the name of the entry point, consistent with the behavior in
  distlib and pip wrappers.

* #752 via #753: When indicating ``py_limited_api`` to Extension,
  it must be passed as a keyword argument.

v25.4.0
-------

* Add Extension(py_limited_api=True). When set to a truthy value,
  that extension gets a filename appropriate for code using Py_LIMITED_API.
  When used correctly this allows a single compiled extension to work on
  all future versions of CPython 3.
  The py_limited_api argument only controls the filename. To be
  compatible with multiple versions of Python 3, the C extension
  will also need to set -DPy_LIMITED_API=... and be modified to use
  only the functions in the limited API.

v25.3.0
-------

* #739 Fix unquoted libpaths by fixing compatibility between `numpy.distutils` and `distutils._msvccompiler` for numpy < 1.11.2 (Fix issue #728, error also fixed in Numpy).

* #731: Bump certifi.

* Style updates. See #740, #741, #743, #744, #742, #747.

* #735: include license file.

v25.2.0
-------

* #612 via #730: Add a LICENSE file which needs to be provided by the terms of
  the MIT license.

v25.1.6
-------

* #725: revert `library_dir_option` patch (Error is related to `numpy.distutils` and make errors on non Numpy users).

v25.1.5
-------

* #720
* #723: Improve patch for `library_dir_option`.

v25.1.4
-------

* #717
* #713
* #707: Fix Python 2 compatibility for MSVC by catching errors properly.
* #715: Fix unquoted libpaths by patching `library_dir_option`.

v25.1.3
-------

* #714 and #704: Revert fix as it breaks other components
  downstream that can't handle unicode. See #709, #710,
  and #712.

v25.1.2
-------

* #704: Fix errors when installing a zip sdist that contained
  files named with non-ascii characters on Windows would
  crash the install when it attempted to clean up the build.
* #646: MSVC compatibility - catch errors properly in
  RegistryInfo.lookup.
* #702: Prevent UnboundLocalError when initial working_set
  is empty.

v25.1.1
-------

* #686: Fix issue in sys.path ordering by pkg_resources when
  rewrite technique is "raw".
* #699: Fix typo in msvc support.

v25.1.0
-------

* #609: Setuptools will now try to download a distribution from
  the next possible download location if the first download fails.
  This means you can now specify multiple links as ``dependency_links``
  and all links will be tried until a working download link is encountered.

v25.0.2
-------

* #688: Fix AttributeError in setup.py when invoked not from
  the current directory.

v25.0.1
-------

* Cleanup of setup.py script.

* Fixed documentation builders by allowing setup.py
  to be imported without having bootstrapped the
  metadata.

* More style cleanup. See #677, #678, #679, #681, #685.

v25.0.0
-------

* #674: Default ``sys.path`` manipulation by easy-install.pth
  is now "raw", meaning that when writing easy-install.pth
  during any install operation, the ``sys.path`` will not be
  rewritten and will no longer give preference to easy_installed
  packages.

  To retain the old behavior when using any easy_install
  operation (including ``setup.py install`` when setuptools is
  present), set the environment variable:

    SETUPTOOLS_SYS_PATH_TECHNIQUE=rewrite

  This project hopes that that few if any environments find it
  necessary to retain the old behavior, and intends to drop
  support for it altogether in a future release. Please report
  any relevant concerns in the ticket for this change.

v24.3.1
-------

* #398: Fix shebang handling on Windows in script
  headers where spaces in ``sys.executable`` would
  produce an improperly-formatted shebang header,
  introduced in 12.0 with the fix for #188.

* #663, #670: More style updates.

v24.3.0
-------

* #516: Disable ``os.link`` to avoid hard linking
  in ``sdist.make_distribution``, avoiding errors on
  systems that support hard links but not on the
  file system in which the build is occurring.

v24.2.1
-------

* #667: Update Metadata-Version to 1.2 when
  ``python_requires`` is supplied.

v24.2.0
-------

* #631: Add support for ``python_requires`` keyword.

v24.1.1
-------

* More style updates. See #660, #661, #641.

v24.1.0
-------

* #659: ``setup.py`` now will fail fast and with a helpful
  error message when the necessary metadata is missing.
* More style updates. See #656, #635, #640,
  #644, #650, #652, and #655.

v24.0.3
-------

* Updated style in much of the codebase to match
  community expectations. See #632, #633, #634,
  #637, #639, #638, #642, #648.

v24.0.2
-------

* If MSVC++14 is needed ``setuptools.msvc`` now redirect
  user to Visual C++ Build Tools web page.

v24.0.1
-------

* #625 and #626: Fixes on ``setuptools.msvc`` mainly
  for Python 2 and Linux.

v24.0.0
-------

* Pull Request #174: Add more aggressive support for
  standalone Microsoft Visual C++ compilers in
  msvc9compiler patch.
  Particularly : Windows SDK 6.1 and 7.0
  (MSVC++ 9.0), Windows SDK 7.1 (MSVC++ 10.0),
  Visual C++ Build Tools 2015 (MSVC++14)
* Renamed ``setuptools.msvc9_support`` to
  ``setuptools.msvc``.

v23.2.1
-------

Re-release of v23.2.0, which was missing the intended
commits.

* #623: Remove used of deprecated 'U' flag when reading
  manifests.

v23.1.0
-------

* #619: Deprecated ``tag_svn_revision`` distribution
  option.

v23.0.0
-------

* #611: Removed ARM executables for CLI and GUI script
  launchers on Windows. If this was a feature you cared
  about, please comment in the ticket.
* #604: Removed docs building support. The project
  now relies on documentation hosted at
  https://setuptools.readthedocs.io/.

v22.0.5
-------

* #604: Restore repository for upload_docs command
  to restore publishing of docs during release.

v22.0.4
-------

* #589: Upload releases to pypi.io using the upload
  hostname and legacy path.

v22.0.3
-------

* #589: Releases are now uploaded to pypi.io (Warehouse)
  even when releases are made on Twine via Travis.

v22.0.2
-------

* #589: Releases are now uploaded to pypi.io (Warehouse).

v22.0.1
-------

* #190: On Python 2, if unicode is passed for packages to
  ``build_py`` command, it will be handled just as with
  text on Python 3.

v22.0.0
-------

Intended to be v21.3.0, but jaraco accidentally released as
a major bump.

* #598: Setuptools now lists itself first in the User-Agent
  for web requests, better following the guidelines in
  `RFC 7231
  <https://tools.ietf.org/html/rfc7231#section-5.5.3>`_.

v21.2.2
-------

* Minor fixes to changelog and docs.

v21.2.1
-------

* #261: Exclude directories when resolving globs in
  package_data.

v21.2.0
-------

* #539: In the easy_install get_site_dirs, honor all
  paths found in ``site.getsitepackages``.

v21.1.0
-------

* #572: In build_ext, now always import ``_CONFIG_VARS``
  from ``distutils`` rather than from ``sysconfig``
  to allow ``distutils.sysconfig.customize_compiler``
  configure the OS X compiler for ``-dynamiclib``.

v21.0.0
-------

* Removed ez_setup.py from Setuptools sdist. The
  bootstrap script will be maintained in its own
  branch and should be generally be retrieved from
  its canonical location at
  https://bootstrap.pypa.io/ez_setup.py.

v20.10.0
--------

* #553: egg_info section is now generated in a
  deterministic order, matching the order generated
  by earlier versions of Python. Except on Python 2.6,
  order is preserved when existing settings are present.
* #556: Update to Packaging 16.7, restoring support
  for deprecated ``python_implmentation`` marker.
* #555: Upload command now prompts for a password
  when uploading to PyPI (or other repository) if no
  password is present in .pypirc or in the keyring.

v20.9.0
-------

* #548: Update certify version to 2016.2.28
* #545: Safely handle deletion of non-zip eggs in rotate
  command.

v20.8.1
-------

* Issue #544: Fix issue with extra environment marker
  processing in WorkingSet due to refactor in v20.7.0.

v20.8.0
-------

* Issue #543: Re-release so that latest release doesn't
  cause déjà vu with distribute and setuptools 0.7 in
  older environments.

v20.7.0
-------

* Refactored extra environment marker processing
  in WorkingSet.
* Issue #533: Fixed intermittent test failures.
* Issue #536: In msvc9_support, trap additional exceptions
  that might occur when importing
  ``distutils.msvc9compiler`` in mingw environments.
* Issue #537: Provide better context when package
  metadata fails to decode in UTF-8.

v20.6.8
-------

* Issue #523: Restored support for environment markers,
  now honoring 'extra' environment markers.

v20.6.7
-------

* Issue #523: Disabled support for environment markers
  introduced in v20.5.

v20.6.6
-------

* Issue #503: Restore support for PEP 345 environment
  markers by updating to Packaging 16.6.

v20.6.0
-------

* New release process that relies on
  `bumpversion <https://github.com/peritus/bumpversion>`_
  and Travis CI for continuous deployment.
* Project versioning semantics now follow
  `semver <https://semver.org>`_ precisely.
  The 'v' prefix on version numbers now also allows
  version numbers to be referenced in the changelog,
  e.g. http://setuptools.readthedocs.io/en/latest/history.html#v20-6-0.

20.5
----

* BB Pull Request #185, #470: Add support for environment markers
  in requirements in install_requires, setup_requires,
  tests_require as well as adding a test for the existing
  extra_requires machinery.

20.4
----

* Issue #422: Moved hosting to
  `Github <https://github.com/pypa/setuptools>`_
  from `Bitbucket <https://bitbucket.org/pypa/setuptools>`_.
  Issues have been migrated, though all issues and comments
  are attributed to bb-migration. So if you have a particular
  issue or issues to which you've been subscribed, you will
  want to "watch" the equivalent issue in Github.
  The Bitbucket project will be retained for the indefinite
  future, but Github now hosts the canonical project repository.

20.3.1
------

* Issue #519: Remove import hook when reloading the
  ``pkg_resources`` module.
* BB Pull Request #184: Update documentation in ``pkg_resources``
  around new ``Requirement`` implementation.

20.3
----

* BB Pull Request #179: ``pkg_resources.Requirement`` objects are
  now a subclass of ``packaging.requirements.Requirement``,
  allowing any environment markers and url (if any) to be
  affiliated with the requirement
* BB Pull Request #179: Restore use of RequirementParseError
  exception unintentionally dropped in 20.2.

20.2.2
------

* Issue #502: Correct regression in parsing of multiple
  version specifiers separated by commas and spaces.

20.2.1
------

* Issue #499: Restore compatibility for legacy versions
  by bumping to packaging 16.4.

20.2
----

* Changelog now includes release dates and links to PEPs.
* BB Pull Request #173: Replace dual PEP 345 _markerlib implementation
  and PEP 426 implementation of environment marker support from
  packaging 16.1 and PEP 508. Fixes Issue #122.
  See also BB Pull Request #175, BB Pull Request #168, and
  BB Pull Request #164. Additionally:

   - ``Requirement.parse`` no longer retains the order of extras.
   - ``parse_requirements`` now requires that all versions be
     PEP-440 compliant, as revealed in #499. Packages released
     with invalid local versions should be re-released using
     the proper local version syntax, e.g. ``mypkg-1.0+myorg.1``.

20.1.1
------

* Update ``upload_docs`` command to also honor keyring
  for password resolution.

20.1
----

* Added support for using passwords from keyring in the upload
  command. See `the upload docs
  <https://setuptools.readthedocs.io/en/latest/setuptools.html#upload-upload-source-and-or-egg-distributions-to-pypi>`_
  for details.

20.0
----

* Issue #118: Once again omit the package metadata (egg-info)
  from the list of outputs in ``--record``. This version of setuptools
  can no longer be used to upgrade pip earlier than 6.0.

19.7
----

* `Off-project PR <https://github.com/jaraco/setuptools/pull/32>`_:
  For FreeBSD, also honor root certificates from ca_root_nss.

19.6.2
------

* Issue #491: Correct regression incurred in 19.4 where
  a double-namespace package installed using pip would
  cause a TypeError.

19.6.1
------

* Restore compatibility for PyPy 3 compatibility lost in
  19.4.1 addressing Issue #487.
* ``setuptools.launch`` shim now loads scripts in a new
  namespace, avoiding getting relative imports from
  the setuptools package on Python 2.

19.6
----

* Added a new entry script ``setuptools.launch``,
  implementing the shim found in
  ``pip.util.setuptools_build``. Use this command to launch
  distutils-only packages under setuptools in the same way that
  pip does, causing the setuptools monkeypatching of distutils
  to be invoked prior to invoking a script. Useful for debugging
  or otherwise installing a distutils-only package under
  setuptools when pip isn't available or otherwise does not
  expose the desired functionality. For example::

    $ python -m setuptools.launch setup.py develop

* Issue #488: Fix dual manifestation of Extension class in
  extension packages installed as dependencies when Cython
  is present.

19.5
----

* Issue #486: Correct TypeError when getfilesystemencoding
  returns None.
* Issue #139: Clarified the license as MIT.
* BB Pull Request #169: Removed special handling of command
  spec in scripts for Jython.

19.4.1
------

* Issue #487: Use direct invocation of ``importlib.machinery``
  in ``pkg_resources`` to avoid missing detection on relevant
  platforms.

19.4
----

* Issue #341: Correct error in path handling of package data
  files in ``build_py`` command when package is empty.
* Distribute #323, Issue #141, Issue #207, and
  BB Pull Request #167: Another implementation of
  ``pkg_resources.WorkingSet`` and ``pkg_resources.Distribution``
  that supports replacing an extant package with a new one,
  allowing for setup_requires dependencies to supersede installed
  packages for the session.

19.3
----

* Issue #229: Implement new technique for readily incorporating
  dependencies conditionally from vendored copies or primary
  locations. Adds a new dependency on six.

19.2
----

* BB Pull Request #163: Add get_command_list method to Distribution.
* BB Pull Request #162: Add missing whitespace to multiline string
  literals.

19.1.1
------

* Issue #476: Cast version to string (using default encoding)
  to avoid creating Unicode types on Python 2 clients.
* Issue #477: In Powershell downloader, use explicit rendering
  of strings, rather than rely on ``repr``, which can be
  incorrect (especially on Python 2).

19.1
----

* Issue #215: The bootstrap script ``ez_setup.py`` now
  automatically detects
  the latest version of setuptools (using PyPI JSON API) rather
  than hard-coding a particular value.
* Issue #475: Fix incorrect usage in _translate_metadata2.

19.0
----

* Issue #442: Use RawConfigParser for parsing .pypirc file.
  Interpolated values are no longer honored in .pypirc files.

18.8.1
------

* Issue #440: Prevent infinite recursion when a SandboxViolation
  or other UnpickleableException occurs in a sandbox context
  with setuptools hidden. Fixes regression introduced in Setuptools
  12.0.

18.8
----

* Deprecated ``egg_info.get_pkg_info_revision``.
* Issue #471: Don't rely on repr for an HTML attribute value in
  package_index.
* Issue #419: Avoid errors in FileMetadata when the metadata directory
  is broken.
* Issue #472: Remove deprecated use of 'U' in mode parameter
  when opening files.

18.7.1
------

* Issue #469: Refactored logic for Issue #419 fix to re-use metadata
  loading from Provider.

18.7
----

* Update dependency on certify.
* BB Pull Request #160: Improve detection of gui script in
  ``easy_install._adjust_header``.
* Made ``test.test_args`` a non-data property; alternate fix
  for the issue reported in BB Pull Request #155.
* Issue #453: In ``ez_setup`` bootstrap module, unload all
  ``pkg_resources`` modules following download.
* BB Pull Request #158: Honor PEP-488 when excluding
  files for namespace packages.
* Issue #419 and BB Pull Request #144: Add experimental support for
  reading the version info from distutils-installed metadata rather
  than using the version in the filename.

18.6.1
------

* Issue #464: Correct regression in invocation of superclass on old-style
  class on Python 2.

18.6
----

* Issue #439: When installing entry_point scripts under development,
  omit the version number of the package, allowing any version of the
  package to be used.

18.5
----

* In preparation for dropping support for Python 3.2, a warning is
  now logged when pkg_resources is imported on Python 3.2 or earlier
  Python 3 versions.
* `Add support for python_platform_implementation environment marker
  <https://github.com/jaraco/setuptools/pull/28>`_.
* `Fix dictionary mutation during iteration
  <https://github.com/jaraco/setuptools/pull/29>`_.

18.4
----

* Issue #446: Test command now always invokes unittest, even
  if no test suite is supplied.

18.3.2
------

* Correct another regression in setuptools.findall
  where the fix for Python #12885 was lost.

18.3.1
------

* Issue #425: Correct regression in setuptools.findall.

18.3
----

* BB Pull Request #135: Setuptools now allows disabling of
  the manipulation of the sys.path
  during the processing of the easy-install.pth file. To do so, set
  the environment variable ``SETUPTOOLS_SYS_PATH_TECHNIQUE`` to
  anything but "rewrite" (consider "raw"). During any install operation
  with manipulation disabled, setuptools packages will be appended to
  sys.path naturally.

  Future versions may change the default behavior to disable
  manipulation. If so, the default behavior can be retained by setting
  the variable to "rewrite".

* Issue #257: ``easy_install --version`` now shows more detail
  about the installation location and Python version.

* Refactor setuptools.findall in preparation for re-submission
  back to distutils.

18.2
----

* Issue #412: More efficient directory search in ``find_packages``.

18.1
----

* Upgrade to vendored packaging 15.3.

18.0.1
------

* Issue #401: Fix failure in test suite.

18.0
----

* Dropped support for builds with Pyrex. Only Cython is supported.
* Issue #288: Detect Cython later in the build process, after
  ``setup_requires`` dependencies are resolved.
  Projects backed by Cython can now be readily built
  with a ``setup_requires`` dependency. For example::

    ext = setuptools.Extension('mylib', ['src/CythonStuff.pyx', 'src/CStuff.c'])
    setuptools.setup(
        ...
        ext_modules=[ext],
        setup_requires=['cython'],
    )

  For compatibility with older versions of setuptools, packagers should
  still include ``src/CythonMod.c`` in the source distributions or
  require that Cython be present before building source distributions.
  However, for systems with this build of setuptools, Cython will be
  downloaded on demand.
* Issue #396: Fixed test failure on OS X.
* BB Pull Request #136: Remove excessive quoting from shebang headers
  for Jython.

17.1.1
------

* Backed out unintended changes to pkg_resources, restoring removal of
  deprecated imp module (`ref
  <https://bitbucket.org/pypa/setuptools/commits/f572ec9563d647fa8d4ffc534f2af8070ea07a8b#comment-1881283>`_).

17.1
----

* Issue #380: Add support for range operators on environment
  marker evaluation.

17.0
----

* Issue #378: Do not use internal importlib._bootstrap module.
* Issue #390: Disallow console scripts with path separators in
  the name. Removes unintended functionality and brings behavior
  into parity with pip.

16.0
----

* BB Pull Request #130: Better error messages for errors in
  parsed requirements.
* BB Pull Request #133: Removed ``setuptools.tests`` from the
  installed packages.
* BB Pull Request #129: Address deprecation warning due to usage
  of imp module.

15.2
----

* Issue #373: Provisionally expose
  ``pkg_resources._initialize_master_working_set``, allowing for
  imperative re-initialization of the master working set.

15.1
----

* Updated to Packaging 15.1 to address Packaging #28.
* Fix ``setuptools.sandbox._execfile()`` with Python 3.1.

15.0
----

* BB Pull Request #126: DistributionNotFound message now lists the package or
  packages that required it. E.g.::

      pkg_resources.DistributionNotFound: The 'colorama>=0.3.1' distribution was not found and is required by smlib.log.

  Note that zc.buildout once dependended on the string rendering of this
  message to determine the package that was not found. This expectation
  has since been changed, but older versions of buildout may experience
  problems. See Buildout #242 for details.

14.3.1
------

* Issue #307: Removed PEP-440 warning during parsing of versions
  in ``pkg_resources.Distribution``.
* Issue #364: Replace deprecated usage with recommended usage of
  ``EntryPoint.load``.

14.3
----

* Issue #254: When creating temporary egg cache on Unix, use mode 755
  for creating the directory to avoid the subsequent warning if
  the directory is group writable.

14.2
----

* Issue #137: Update ``Distribution.hashcmp`` so that Distributions with
  None for pyversion or platform can be compared against Distributions
  defining those attributes.

14.1.1
------

* Issue #360: Removed undesirable behavior from test runs, preventing
  write tests and installation to system site packages.

14.1
----

* BB Pull Request #125: Add ``__ne__`` to Requirement class.
* Various refactoring of easy_install.

14.0
----

* Bootstrap script now accepts ``--to-dir`` to customize save directory or
  allow for re-use of existing repository of setuptools versions. See
  BB Pull Request #112 for background.
* Issue #285: ``easy_install`` no longer will default to installing
  packages to the "user site packages" directory if it is itself installed
  there. Instead, the user must pass ``--user`` in all cases to install
  packages to the user site packages.
  This behavior now matches that of "pip install". To configure
  an environment to always install to the user site packages, consider
  using the "install-dir" and "scripts-dir" parameters to easy_install
  through an appropriate distutils config file.

13.0.2
------

* Issue #359: Include pytest.ini in the sdist so invocation of py.test on the
  sdist honors the pytest configuration.

13.0.1
------

Re-release of 13.0. Intermittent connectivity issues caused the release
process to fail and PyPI uploads no longer accept files for 13.0.

13.0
----

* Issue #356: Back out BB Pull Request #119 as it requires Setuptools 10 or later
  as the source during an upgrade.
* Removed build_py class from setup.py. According to 892f439d216e, this
  functionality was added to support upgrades from old Distribute versions,
  0.6.5 and 0.6.6.

12.4
----

* BB Pull Request #119: Restore writing of ``setup_requires`` to metadata
  (previously added in 8.4 and removed in 9.0).

12.3
----

* Documentation is now linked using the rst.linker package.
* Fix ``setuptools.command.easy_install.extract_wininst_cfg()``
  with Python 2.6 and 2.7.
* Issue #354. Added documentation on building setuptools
  documentation.

12.2
----

* Issue #345: Unload all modules under pkg_resources during
  ``ez_setup.use_setuptools()``.
* Issue #336: Removed deprecation from ``ez_setup.use_setuptools``,
  as it is clearly still used by buildout's bootstrap. ``ez_setup``
  remains deprecated for use by individual packages.
* Simplified implementation of ``ez_setup.use_setuptools``.

12.1
----

* BB Pull Request #118: Soften warning for non-normalized versions in
  Distribution.

12.0.5
------

* Issue #339: Correct Attribute reference in ``cant_write_to_target``.
* Issue #336: Deprecated ``ez_setup.use_setuptools``.

12.0.4
------

* Issue #335: Fix script header generation on Windows.

12.0.3
------

* Fixed incorrect class attribute in ``install_scripts``. Tests would be nice.

12.0.2
------

* Issue #331: Fixed ``install_scripts`` command on Windows systems corrupting
  the header.

12.0.1
------

* Restore ``setuptools.command.easy_install.sys_executable`` for pbr
  compatibility. For the future, tools should construct a CommandSpec
  explicitly.

12.0
----

* Issue #188: Setuptools now support multiple entities in the value for
  ``build.executable``, such that an executable of "/usr/bin/env my-python" may
  be specified. This means that systems with a specified executable whose name
  has spaces in the path must be updated to escape or quote that value.
* Deprecated ``easy_install.ScriptWriter.get_writer``, replaced by ``.best()``
  with slightly different semantics (no force_windows flag).

11.3.1
------

* Issue #327: Formalize and restore support for any printable character in an
  entry point name.

11.3
----

* Expose ``EntryPoint.resolve`` in place of EntryPoint._load, implementing the
  simple, non-requiring load. Deprecated all uses of ``EntryPoint._load``
  except for calling with no parameters, which is just a shortcut for
  ``ep.require(); ep.resolve();``.

  Apps currently invoking ``ep.load(require=False)`` should instead do the
  following if wanting to avoid the deprecating warning::

    getattr(ep, "resolve", lambda: ep.load(require=False))()

11.2
----

* Pip #2326: Report deprecation warning at stacklevel 2 for easier diagnosis.

11.1
----

* Issue #281: Since Setuptools 6.1 (Issue #268), a ValueError would be raised
  in certain cases where VersionConflict was raised with two arguments, which
  occurred in ``pkg_resources.WorkingSet.find``. This release adds support
  for indicating the dependent packages while maintaining support for
  a VersionConflict when no dependent package context is known. New unit tests
  now capture the expected interface.

11.0
----

* Interop #3: Upgrade to Packaging 15.0; updates to PEP 440 so that >1.7 does
  not exclude 1.7.1 but does exclude 1.7.0 and 1.7.0.post1.

10.2.1
------

* Issue #323: Fix regression in entry point name parsing.

10.2
----

* Deprecated use of EntryPoint.load(require=False). Passing a boolean to a
  function to select behavior is an anti-pattern. Instead use
  ``Entrypoint._load()``.
* Substantial refactoring of all unit tests. Tests are now much leaner and
  re-use a lot of fixtures and contexts for better clarity of purpose.

10.1
----

* Issue #320: Added a compatibility implementation of
  ``sdist._default_revctrl``
  so that systems relying on that interface do not fail (namely, Ubuntu 12.04
  and similar Debian releases).

10.0.1
------

* Issue #319: Fixed issue installing pure distutils packages.

10.0
----

* Issue #313: Removed built-in support for subversion. Projects wishing to
  retain support for subversion will need to use a third party library. The
  extant implementation is being ported to `setuptools_svn
  <https://pypi.org/project/setuptools_svn/>`_.
* Issue #315: Updated setuptools to hide its own loaded modules during
  installation of another package. This change will enable setuptools to
  upgrade (or downgrade) itself even when its own metadata and implementation
  change.

9.1
---

* Prefer vendored packaging library `as recommended
  <https://github.com/jaraco/setuptools/commit/170657b68f4b92e7e1bf82f5e19a831f5744af67#commitcomment-9109448>`_.

9.0.1
-----

* Issue #312: Restored presence of pkg_resources API tests (doctest) to sdist.

9.0
---

* Issue #314: Disabled support for ``setup_requires`` metadata to avoid issue
  where Setuptools was unable to upgrade over earlier versions.

8.4
---

* BB Pull Request #106: Now write ``setup_requires`` metadata.

8.3
---

* Issue #311: Decoupled pkg_resources from setuptools once again.
  ``pkg_resources`` is now a package instead of a module.

8.2.1
-----

* Issue #306: Suppress warnings about Version format except in select scenarios
  (such as installation).

8.2
---

* BB Pull Request #85: Search egg-base when adding egg-info to manifest.

8.1
---

* Upgrade ``packaging`` to 14.5, giving preference to "rc" as designator for
  release candidates over "c".
* PEP-440 warnings are now raised as their own class,
  ``pkg_resources.PEP440Warning``, instead of RuntimeWarning.
* Disabled warnings on empty versions.

8.0.4
-----

* Upgrade ``packaging`` to 14.4, fixing an error where there is a
  different result for if 2.0.5 is contained within >2.0dev and >2.0.dev even
  though normalization rules should have made them equal.
* Issue #296: Add warning when a version is parsed as legacy. This warning will
  make it easier for developers to recognize deprecated version numbers.

8.0.3
-----

* Issue #296: Restored support for ``__hash__`` on parse_version results.

8.0.2
-----

* Issue #296: Restored support for ``__getitem__`` and sort operations on
  parse_version result.

8.0.1
-----

* Issue #296: Restore support for iteration over parse_version result, but
  deprecated that usage with a warning. Fixes failure with buildout.

8.0
---

* Implement PEP 440 within
  pkg_resources and setuptools. This change
  deprecates some version numbers such that they will no longer be installable
  without using the ``===`` escape hatch. See `the changes to test_resources
  <https://bitbucket.org/pypa/setuptools/commits/dcd552da643c4448056de84c73d56da6d70769d5#chg-setuptools/tests/test_resources.py>`_
  for specific examples of version numbers and specifiers that are no longer
  supported. Setuptools now "vendors" the `packaging
  <https://github.com/pypa/packaging>`_ library.

7.0
---

* Issue #80, Issue #209: Eggs that are downloaded for ``setup_requires``,
  ``test_requires``, etc. are now placed in a ``./.eggs`` directory instead of
  directly in the current directory. This choice of location means the files
  can be readily managed (removed, ignored). Additionally,
  later phases or invocations of setuptools will not detect the package as
  already installed and ignore it for permanent install (See #209).

  This change is indicated as backward-incompatible as installations that
  depend on the installation in the current directory will need to account for
  the new location. Systems that ignore ``*.egg`` will probably need to be
  adapted to ignore ``.eggs``. The files will need to be manually moved or
  will be retrieved again. Most use cases will require no attention.

6.1
---

* Issue #268: When resolving package versions, a VersionConflict now reports
  which package previously required the conflicting version.

6.0.2
-----

* Issue #262: Fixed regression in pip install due to egg-info directories
  being omitted. Re-opens Issue #118.

6.0.1
-----

* Issue #259: Fixed regression with namespace package handling on ``single
  version, externally managed`` installs.

6.0
---

* Issue #100: When building a distribution, Setuptools will no longer match
  default files using platform-dependent case sensitivity, but rather will
  only match the files if their case matches exactly. As a result, on Windows
  and other case-insensitive file systems, files with names such as
  'readme.txt' or 'README.TXT' will be omitted from the distribution and a
  warning will be issued indicating that 'README.txt' was not found. Other
  filenames affected are:

    - README.rst
    - README
    - setup.cfg
    - setup.py (or the script name)
    - test/test*.py

  Any users producing distributions with filenames that match those above
  case-insensitively, but not case-sensitively, should rename those files in
  their repository for better portability.
* BB Pull Request #72: When using ``single_version_externally_managed``, the
  exclusion list now includes Python 3.2 ``__pycache__`` entries.
* BB Pull Request #76 and BB Pull Request #78: lines in top_level.txt are now
  ordered deterministically.
* Issue #118: The egg-info directory is now no longer included in the list
  of outputs.
* Issue #258: Setuptools now patches distutils msvc9compiler to
  recognize the specially-packaged compiler package for easy extension module
  support on Python 2.6, 2.7, and 3.2.

5.8
---

* Issue #237: ``pkg_resources`` now uses explicit detection of Python 2 vs.
  Python 3, supporting environments where builtins have been patched to make
  Python 3 look more like Python 2.

5.7
---

* Issue #240: Based on real-world performance measures against 5.4, zip
  manifests are now cached in all circumstances. The
  ``PKG_RESOURCES_CACHE_ZIP_MANIFESTS`` environment variable is no longer
  relevant. The observed "memory increase" referenced in the 5.4 release
  notes and detailed in Issue #154 was likely not an increase over the status
  quo, but rather only an increase over not storing the zip info at all.

5.6
---

* Issue #242: Use absolute imports in svn_utils to avoid issues if the
  installing package adds an xml module to the path.

5.5.1
-----

* Issue #239: Fix typo in 5.5 such that fix did not take.

5.5
---

* Issue #239: Setuptools now includes the setup_requires directive on
  Distribution objects and validates the syntax just like install_requires
  and tests_require directives.

5.4.2
-----

* Issue #236: Corrected regression in execfile implementation for Python 2.6.

5.4.1
-----

* Python #7776: (ssl_support) Correct usage of host for validation when
  tunneling for HTTPS.

5.4
---

* Issue #154: ``pkg_resources`` will now cache the zip manifests rather than
  re-processing the same file from disk multiple times, but only if the
  environment variable ``PKG_RESOURCES_CACHE_ZIP_MANIFESTS`` is set. Clients
  that package many modules in the same zip file will see some improvement
  in startup time by enabling this feature. This feature is not enabled by
  default because it causes a substantial increase in memory usage.

5.3
---

* Issue #185: Make svn tagging work on the new style SVN metadata.
  Thanks cazabon!
* Prune revision control directories (e.g .svn) from base path
  as well as sub-directories.

5.2
---

* Added a `Developer Guide
  <https://setuptools.readthedocs.io/en/latest/developer-guide.html>`_ to the official
  documentation.
* Some code refactoring and cleanup was done with no intended behavioral
  changes.
* During install_egg_info, the generated lines for namespace package .pth
  files are now processed even during a dry run.

5.1
---

* Issue #202: Implemented more robust cache invalidation for the ZipImporter,
  building on the work in Issue #168. Special thanks to Jurko Gospodnetic and
  PJE.

5.0.2
-----

* Issue #220: Restored script templates.

5.0.1
-----

* Renamed script templates to end with .tmpl now that they no longer need
  to be processed by 2to3. Fixes spurious syntax errors during build/install.

5.0
---

* Issue #218: Re-release of 3.8.1 to signal that it supersedes 4.x.
* Incidentally, script templates were updated not to include the triple-quote
  escaping.

3.7.1 and 3.8.1 and 4.0.1
-------------------------

* Issue #213: Use legacy StringIO behavior for compatibility under pbr.
* Issue #218: Setuptools 3.8.1 superseded 4.0.1, and 4.x was removed
  from the available versions to install.

4.0
---

* Issue #210: ``setup.py develop`` now copies scripts in binary mode rather
  than text mode, matching the behavior of the ``install`` command.

3.8
---

* Extend Issue #197 workaround to include all Python 3 versions prior to
  3.2.2.

3.7
---

* Issue #193: Improved handling of Unicode filenames when building manifests.

3.6
---

* Issue #203: Honor proxy settings for Powershell downloader in the bootstrap
  routine.

3.5.2
-----

* Issue #168: More robust handling of replaced zip files and stale caches.
  Fixes ZipImportError complaining about a 'bad local header'.

3.5.1
-----

* Issue #199: Restored ``install._install`` for compatibility with earlier
  NumPy versions.

3.5
---

* Issue #195: Follow symbolic links in find_packages (restoring behavior
  broken in 3.4).
* Issue #197: On Python 3.1, PKG-INFO is now saved in a UTF-8 encoding instead
  of ``sys.getpreferredencoding`` to match the behavior on Python 2.6-3.4.
* Issue #192: Preferred bootstrap location is now
  https://bootstrap.pypa.io/ez_setup.py (mirrored from former location).

3.4.4
-----

* Issue #184: Correct failure where find_package over-matched packages
  when directory traversal isn't short-circuited.

3.4.3
-----

* Issue #183: Really fix test command with Python 3.1.

3.4.2
-----

* Issue #183: Fix additional regression in test command on Python 3.1.

3.4.1
-----

* Issue #180: Fix regression in test command not caught by py.test-run tests.

3.4
---

* Issue #176: Add parameter to the test command to support a custom test
  runner: --test-runner or -r.
* Issue #177: Now assume most common invocation to install command on
  platforms/environments without stack support (issuing a warning). Setuptools
  now installs naturally on IronPython. Behavior on CPython should be
  unchanged.

3.3
---

* Add ``include`` parameter to ``setuptools.find_packages()``.

3.2
---

* BB Pull Request #39: Add support for C++ targets from Cython ``.pyx`` files.
* Issue #162: Update dependency on certifi to 1.0.1.
* Issue #164: Update dependency on wincertstore to 0.2.

3.1
---

* Issue #161: Restore Features functionality to allow backward compatibility
  (for Features) until the uses of that functionality is sufficiently removed.

3.0.2
-----

* Correct typo in previous bugfix.

3.0.1
-----

* Issue #157: Restore support for Python 2.6 in bootstrap script where
  ``zipfile.ZipFile`` does not yet have support for context managers.

3.0
---

* Issue #125: Prevent Subversion support from creating a ~/.subversion
  directory just for checking the presence of a Subversion repository.
* Issue #12: Namespace packages are now imported lazily. That is, the mere
  declaration of a namespace package in an egg on ``sys.path`` no longer
  causes it to be imported when ``pkg_resources`` is imported. Note that this
  change means that all of a namespace package's ``__init__.py`` files must
  include a ``declare_namespace()`` call in order to ensure that they will be
  handled properly at runtime. In 2.x it was possible to get away without
  including the declaration, but only at the cost of forcing namespace
  packages to be imported early, which 3.0 no longer does.
* Issue #148: When building (bdist_egg), setuptools no longer adds
  ``__init__.py`` files to namespace packages. Any packages that rely on this
  behavior will need to create ``__init__.py`` files and include the
  ``declare_namespace()``.
* Issue #7: Setuptools itself is now distributed as a zip archive in addition to
  tar archive. ez_setup.py now uses zip archive. This approach avoids the potential
  security vulnerabilities presented by use of tar archives in ez_setup.py.
  It also leverages the security features added to ZipFile.extract in Python 2.7.4.
* Issue #65: Removed deprecated Features functionality.
* BB Pull Request #28: Remove backport of ``_bytecode_filenames`` which is
  available in Python 2.6 and later, but also has better compatibility with
  Python 3 environments.
* Issue #156: Fix spelling of __PYVENV_LAUNCHER__ variable.

2.2
---

* Issue #141: Restored fix for allowing setup_requires dependencies to
  override installed dependencies during setup.
* Issue #128: Fixed issue where only the first dependency link was honored
  in a distribution where multiple dependency links were supplied.

2.1.2
-----

* Issue #144: Read long_description using codecs module to avoid errors
  installing on systems where LANG=C.

2.1.1
-----

* Issue #139: Fix regression in re_finder for CVS repos (and maybe Git repos
  as well).

2.1
---

* Issue #129: Suppress inspection of ``*.whl`` files when searching for files
  in a zip-imported file.
* Issue #131: Fix RuntimeError when constructing an egg fetcher.

2.0.2
-----

* Fix NameError during installation with Python implementations (e.g. Jython)
  not containing parser module.
* Fix NameError in ``sdist:re_finder``.

2.0.1
-----

* Issue #124: Fixed error in list detection in upload_docs.

2.0
---

* Issue #121: Exempt lib2to3 pickled grammars from DirectorySandbox.
* Issue #41: Dropped support for Python 2.4 and Python 2.5. Clients requiring
  setuptools for those versions of Python should use setuptools 1.x.
* Removed ``setuptools.command.easy_install.HAS_USER_SITE``. Clients
  expecting this boolean variable should use ``site.ENABLE_USER_SITE``
  instead.
* Removed ``pkg_resources.ImpWrapper``. Clients that expected this class
  should use ``pkgutil.ImpImporter`` instead.

1.4.2
-----

* Issue #116: Correct TypeError when reading a local package index on Python
  3.

1.4.1
-----

* Issue #114: Use ``sys.getfilesystemencoding`` for decoding config in
  ``bdist_wininst`` distributions.

* Issue #105 and Issue #113: Establish a more robust technique for
  determining the terminal encoding::

    1. Try ``getpreferredencoding``
    2. If that returns US_ASCII or None, try the encoding from
       ``getdefaultlocale``. If that encoding was a "fallback" because Python
       could not figure it out from the environment or OS, encoding remains
       unresolved.
    3. If the encoding is resolved, then make sure Python actually implements
       the encoding.
    4. On the event of an error or unknown codec, revert to fallbacks
       (UTF-8 on Darwin, ASCII on everything else).
    5. On the encoding is 'mac-roman' on Darwin, use UTF-8 as 'mac-roman' was
       a bug on older Python releases.

    On a side note, it would seem that the encoding only matters for when SVN
    does not yet support ``--xml`` and when getting repository and svn version
    numbers. The ``--xml`` technique should yield UTF-8 according to some
    messages on the SVN mailing lists. So if the version numbers are always
    7-bit ASCII clean, it may be best to only support the file parsing methods
    for legacy SVN releases and support for SVN without the subprocess command
    would simple go away as support for the older SVNs does.

1.4
---

* Issue #27: ``easy_install`` will now use credentials from .pypirc if
  present for connecting to the package index.
* BB Pull Request #21: Omit unwanted newlines in ``package_index._encode_auth``
  when the username/password pair length indicates wrapping.

1.3.2
-----

* Issue #99: Fix filename encoding issues in SVN support.

1.3.1
-----

* Remove exuberant warning in SVN support when SVN is not used.

1.3
---

* Address security vulnerability in SSL match_hostname check as reported in
  Python #17997.
* Prefer `backports.ssl_match_hostname
  <https://pypi.org/project/backports.ssl_match_hostname/>`_ for backport
  implementation if present.
* Correct NameError in ``ssl_support`` module (``socket.error``).

1.2
---

* Issue #26: Add support for SVN 1.7. Special thanks to Philip Thiem for the
  contribution.
* Issue #93: Wheels are now distributed with every release. Note that as
  reported in Issue #108, as of Pip 1.4, scripts aren't installed properly
  from wheels. Therefore, if using Pip to install setuptools from a wheel,
  the ``easy_install`` command will not be available.
* Setuptools "natural" launcher support, introduced in 1.0, is now officially
  supported.

1.1.7
-----

* Fixed behavior of NameError handling in 'script template (dev).py' (script
  launcher for 'develop' installs).
* ``ez_setup.py`` now ensures partial downloads are cleaned up following
  a failed download.
* Distribute #363 and Issue #55: Skip an sdist test that fails on locales
  other than UTF-8.

1.1.6
-----

* Distribute #349: ``sandbox.execfile`` now opens the target file in binary
  mode, thus honoring a BOM in the file when compiled.

1.1.5
-----

* Issue #69: Second attempt at fix (logic was reversed).

1.1.4
-----

* Issue #77: Fix error in upload command (Python 2.4).

1.1.3
-----

* Fix NameError in previous patch.

1.1.2
-----

* Issue #69: Correct issue where 404 errors are returned for URLs with
  fragments in them (such as #egg=).

1.1.1
-----

* Issue #75: Add ``--insecure`` option to ez_setup.py to accommodate
  environments where a trusted SSL connection cannot be validated.
* Issue #76: Fix AttributeError in upload command with Python 2.4.

1.1
---

* Issue #71 (Distribute #333): EasyInstall now puts less emphasis on the
  condition when a host is blocked via ``--allow-hosts``.
* Issue #72: Restored Python 2.4 compatibility in ``ez_setup.py``.

1.0
---

* Issue #60: On Windows, Setuptools supports deferring to another launcher,
  such as Vinay Sajip's `pylauncher <https://bitbucket.org/pypa/pylauncher>`_
  (included with Python 3.3) to launch console and GUI scripts and not install
  its own launcher executables. This experimental functionality is currently
  only enabled if  the ``SETUPTOOLS_LAUNCHER`` environment variable is set to
  "natural". In the future, this behavior may become default, but only after
  it has matured and seen substantial adoption. The ``SETUPTOOLS_LAUNCHER``
  also accepts "executable" to force the default behavior of creating launcher
  executables.
* Issue #63: Bootstrap script (ez_setup.py) now prefers Powershell, curl, or
  wget for retrieving the Setuptools tarball for improved security of the
  install. The script will still fall back to a simple ``urlopen`` on
  platforms that do not have these tools.
* Issue #65: Deprecated the ``Features`` functionality.
* Issue #52: In ``VerifyingHTTPSConn``, handle a tunnelled (proxied)
  connection.

Backward-Incompatible Changes
=============================

This release includes a couple of backward-incompatible changes, but most if
not all users will find 1.0 a drop-in replacement for 0.9.

* Issue #50: Normalized API of environment marker support. Specifically,
  removed line number and filename from SyntaxErrors when returned from
  `pkg_resources.invalid_marker`. Any clients depending on the specific
  string representation of exceptions returned by that function may need to
  be updated to account for this change.
* Issue #50: SyntaxErrors generated by `pkg_resources.invalid_marker` are
  normalized for cross-implementation consistency.
* Removed ``--ignore-conflicts-at-my-risk`` and ``--delete-conflicting``
  options to easy_install. These options have been deprecated since 0.6a11.

0.9.8
-----

* Issue #53: Fix NameErrors in `_vcs_split_rev_from_url`.

0.9.7
-----

* Issue #49: Correct AttributeError on PyPy where a hashlib.HASH object does
  not have a `.name` attribute.
* Issue #34: Documentation now refers to bootstrap script in code repository
  referenced by bookmark.
* Add underscore-separated keys to environment markers (markerlib).

0.9.6
-----

* Issue #44: Test failure on Python 2.4 when MD5 hash doesn't have a `.name`
  attribute.

0.9.5
-----

* Python #17980: Fix security vulnerability in SSL certificate validation.

0.9.4
-----

* Issue #43: Fix issue (introduced in 0.9.1) with version resolution when
  upgrading over other releases of Setuptools.

0.9.3
-----

* Issue #42: Fix new ``AttributeError`` introduced in last fix.

0.9.2
-----

* Issue #42: Fix regression where blank checksums would trigger an
  ``AttributeError``.

0.9.1
-----

* Distribute #386: Allow other positional and keyword arguments to os.open.
* Corrected dependency on certifi mis-referenced in 0.9.

0.9
---

* `package_index` now validates hashes other than MD5 in download links.

0.8
---

* Code base now runs on Python 2.4 - Python 3.3 without Python 2to3
  conversion.

0.7.8
-----

* Distribute #375: Yet another fix for yet another regression.

0.7.7
-----

* Distribute #375: Repair AttributeError created in last release (redo).
* Issue #30: Added test for get_cache_path.

0.7.6
-----

* Distribute #375: Repair AttributeError created in last release.

0.7.5
-----

* Issue #21: Restore Python 2.4 compatibility in ``test_easy_install``.
* Distribute #375: Merged additional warning from Distribute 0.6.46.
* Now honor the environment variable
  ``SETUPTOOLS_DISABLE_VERSIONED_EASY_INSTALL_SCRIPT`` in addition to the now
  deprecated ``DISTRIBUTE_DISABLE_VERSIONED_EASY_INSTALL_SCRIPT``.

0.7.4
-----

* Issue #20: Fix comparison of parsed SVN version on Python 3.

0.7.3
-----

* Issue #1: Disable installation of Windows-specific files on non-Windows systems.
* Use new sysconfig module with Python 2.7 or >=3.2.

0.7.2
-----

* Issue #14: Use markerlib when the `parser` module is not available.
* Issue #10: ``ez_setup.py`` now uses HTTPS to download setuptools from PyPI.

0.7.1
-----

* Fix NameError (Issue #3) again - broken in bad merge.

0.7
---

* Merged Setuptools and Distribute. See docs/merge.txt for details.

Added several features that were slated for setuptools 0.6c12:

* Index URL now defaults to HTTPS.
* Added experimental environment marker support. Now clients may designate a
  PEP-426 environment marker for "extra" dependencies. Setuptools uses this
  feature in ``setup.py`` for optional SSL and certificate validation support
  on older platforms. Based on Distutils-SIG discussions, the syntax is
  somewhat tentative. There should probably be a PEP with a firmer spec before
  the feature should be considered suitable for use.
* Added support for SSL certificate validation when installing packages from
  an HTTPS service.

0.7b4
-----

* Issue #3: Fixed NameError in SSL support.

0.6.49
------

* Move warning check in ``get_cache_path`` to follow the directory creation
  to avoid errors when the cache path does not yet exist. Fixes the error
  reported in Distribute #375.

0.6.48
------

* Correct AttributeError in ``ResourceManager.get_cache_path`` introduced in
  0.6.46 (redo).

0.6.47
------

* Correct AttributeError in ``ResourceManager.get_cache_path`` introduced in
  0.6.46.

0.6.46
------

* Distribute #375: Issue a warning if the PYTHON_EGG_CACHE or otherwise
  customized egg cache location specifies a directory that's group- or
  world-writable.

0.6.45
------

* Distribute #379: ``distribute_setup.py`` now traps VersionConflict as well,
  restoring ability to upgrade from an older setuptools version.

0.6.44
------

* ``distribute_setup.py`` has been updated to allow Setuptools 0.7 to
  satisfy use_setuptools.

0.6.43
------

* Distribute #378: Restore support for Python 2.4 Syntax (regression in 0.6.42).

0.6.42
------

* External links finder no longer yields duplicate links.
* Distribute #337: Moved site.py to setuptools/site-patch.py (graft of very old
  patch from setuptools trunk which inspired PR #31).

0.6.41
------

* Distribute #27: Use public api for loading resources from zip files rather than
  the private method `_zip_directory_cache`.
* Added a new function ``easy_install.get_win_launcher`` which may be used by
  third-party libraries such as buildout to get a suitable script launcher.

0.6.40
------

* Distribute #376: brought back cli.exe and gui.exe that were deleted in the
  previous release.

0.6.39
------

* Add support for console launchers on ARM platforms.
* Fix possible issue in GUI launchers where the subsystem was not supplied to
  the linker.
* Launcher build script now refactored for robustness.
* Distribute #375: Resources extracted from a zip egg to the file system now also
  check the contents of the file against the zip contents during each
  invocation of get_resource_filename.

0.6.38
------

* Distribute #371: The launcher manifest file is now installed properly.

0.6.37
------

* Distribute #143: Launcher scripts, including easy_install itself, are now
  accompanied by a manifest on 32-bit Windows environments to avoid the
  Installer Detection Technology and thus undesirable UAC elevation described
  in `this Microsoft article
  <http://technet.microsoft.com/en-us/library/cc709628%28WS.10%29.aspx>`_.

0.6.36
------

* BB Pull Request #35: In Buildout #64, it was reported that
  under Python 3, installation of distutils scripts could attempt to copy
  the ``__pycache__`` directory as a file, causing an error, apparently only
  under Windows. Easy_install now skips all directories when processing
  metadata scripts.

0.6.35
------


Note this release is backward-incompatible with distribute 0.6.23-0.6.34 in
how it parses version numbers.

* Distribute #278: Restored compatibility with distribute 0.6.22 and setuptools
  0.6. Updated the documentation to match more closely with the version
  parsing as intended in setuptools 0.6.

0.6.34
------

* Distribute #341: 0.6.33 fails to build under Python 2.4.

0.6.33
------

* Fix 2 errors with Jython 2.5.
* Fix 1 failure with Jython 2.5 and 2.7.
* Disable workaround for Jython scripts on Linux systems.
* Distribute #336: `setup.py` no longer masks failure exit code when tests fail.
* Fix issue in pkg_resources where try/except around a platform-dependent
  import would trigger hook load failures on Mercurial. See pull request 32
  for details.
* Distribute #341: Fix a ResourceWarning.

0.6.32
------

* Fix test suite with Python 2.6.
* Fix some DeprecationWarnings and ResourceWarnings.
* Distribute #335: Backed out `setup_requires` superceding installed requirements
  until regression can be addressed.

0.6.31
------

* Distribute #303: Make sure the manifest only ever contains UTF-8 in Python 3.
* Distribute #329: Properly close files created by tests for compatibility with
  Jython.
* Work around Jython #1980 and Jython #1981.
* Distribute #334: Provide workaround for packages that reference `sys.__stdout__`
  such as numpy does. This change should address
  `virtualenv #359 <https://github.com/pypa/virtualenv/issues/359>`_ as long
  as the system encoding is UTF-8 or the IO encoding is specified in the
  environment, i.e.::

     PYTHONIOENCODING=utf8 pip install numpy

* Fix for encoding issue when installing from Windows executable on Python 3.
* Distribute #323: Allow `setup_requires` requirements to supercede installed
  requirements. Added some new keyword arguments to existing pkg_resources
  methods. Also had to updated how __path__ is handled for namespace packages
  to ensure that when a new egg distribution containing a namespace package is
  placed on sys.path, the entries in __path__ are found in the same order they
  would have been in had that egg been on the path when pkg_resources was
  first imported.

0.6.30
------

* Distribute #328: Clean up temporary directories in distribute_setup.py.
* Fix fatal bug in distribute_setup.py.

0.6.29
------

* BB Pull Request #14: Honor file permissions in zip files.
* Distribute #327: Merged pull request #24 to fix a dependency problem with pip.
* Merged pull request #23 to fix https://github.com/pypa/virtualenv/issues/301.
* If Sphinx is installed, the `upload_docs` command now runs `build_sphinx`
  to produce uploadable documentation.
* Distribute #326: `upload_docs` provided mangled auth credentials under Python 3.
* Distribute #320: Fix check for "createable" in distribute_setup.py.
* Distribute #305: Remove a warning that was triggered during normal operations.
* Distribute #311: Print metadata in UTF-8 independent of platform.
* Distribute #303: Read manifest file with UTF-8 encoding under Python 3.
* Distribute #301: Allow to run tests of namespace packages when using 2to3.
* Distribute #304: Prevent import loop in site.py under Python 3.3.
* Distribute #283: Reenable scanning of `*.pyc` / `*.pyo` files on Python 3.3.
* Distribute #299: The develop command didn't work on Python 3, when using 2to3,
  as the egg link would go to the Python 2 source. Linking to the 2to3'd code
  in build/lib makes it work, although you will have to rebuild the module
  before testing it.
* Distribute #306: Even if 2to3 is used, we build in-place under Python 2.
* Distribute #307: Prints the full path when .svn/entries is broken.
* Distribute #313: Support for sdist subcommands (Python 2.7)
* Distribute #314: test_local_index() would fail an OS X.
* Distribute #310: Non-ascii characters in a namespace __init__.py causes errors.
* Distribute #218: Improved documentation on behavior of `package_data` and
  `include_package_data`. Files indicated by `package_data` are now included
  in the manifest.
* `distribute_setup.py` now allows a `--download-base` argument for retrieving
  distribute from a specified location.

0.6.28
------

* Distribute #294: setup.py can now be invoked from any directory.
* Scripts are now installed honoring the umask.
* Added support for .dist-info directories.
* Distribute #283: Fix and disable scanning of `*.pyc` / `*.pyo` files on
  Python 3.3.

0.6.27
------

* Support current snapshots of CPython 3.3.
* Distribute now recognizes README.rst as a standard, default readme file.
* Exclude 'encodings' modules when removing modules from sys.modules.
  Workaround for #285.
* Distribute #231: Don't fiddle with system python when used with buildout
  (bootstrap.py)

0.6.26
------

* Distribute #183: Symlinked files are now extracted from source distributions.
* Distribute #227: Easy_install fetch parameters are now passed during the
  installation of a source distribution; now fulfillment of setup_requires
  dependencies will honor the parameters passed to easy_install.

0.6.25
------

* Distribute #258: Workaround a cache issue
* Distribute #260: distribute_setup.py now accepts the --user parameter for
  Python 2.6 and later.
* Distribute #262: package_index.open_with_auth no longer throws LookupError
  on Python 3.
* Distribute #269: AttributeError when an exception occurs reading Manifest.in
  on late releases of Python.
* Distribute #272: Prevent TypeError when namespace package names are unicode
  and single-install-externally-managed is used. Also fixes PIP issue
  449.
* Distribute #273: Legacy script launchers now install with Python2/3 support.

0.6.24
------

* Distribute #249: Added options to exclude 2to3 fixers

0.6.23
------

* Distribute #244: Fixed a test
* Distribute #243: Fixed a test
* Distribute #239: Fixed a test
* Distribute #240: Fixed a test
* Distribute #241: Fixed a test
* Distribute #237: Fixed a test
* Distribute #238: easy_install now uses 64bit executable wrappers on 64bit Python
* Distribute #208: Fixed parsed_versions, it now honors post-releases as noted in the documentation
* Distribute #207: Windows cli and gui wrappers pass CTRL-C to child python process
* Distribute #227: easy_install now passes its arguments to setup.py bdist_egg
* Distribute #225: Fixed a NameError on Python 2.5, 2.4

0.6.21
------

* Distribute #225: FIxed a regression on py2.4

0.6.20
------

* Distribute #135: Include url in warning when processing URLs in package_index.
* Distribute #212: Fix issue where easy_instal fails on Python 3 on windows installer.
* Distribute #213: Fix typo in documentation.

0.6.19
------

* Distribute #206: AttributeError: 'HTTPMessage' object has no attribute 'getheaders'

0.6.18
------

* Distribute #210: Fixed a regression introduced by Distribute #204 fix.

0.6.17
------

* Support 'DISTRIBUTE_DISABLE_VERSIONED_EASY_INSTALL_SCRIPT' environment
  variable to allow to disable installation of easy_install-${version} script.
* Support Python >=3.1.4 and >=3.2.1.
* Distribute #204: Don't try to import the parent of a namespace package in
  declare_namespace
* Distribute #196: Tolerate responses with multiple Content-Length headers
* Distribute #205: Sandboxing doesn't preserve working_set. Leads to setup_requires
  problems.

0.6.16
------

* Builds sdist gztar even on Windows (avoiding Distribute #193).
* Distribute #192: Fixed metadata omitted on Windows when package_dir
  specified with forward-slash.
* Distribute #195: Cython build support.
* Distribute #200: Issues with recognizing 64-bit packages on Windows.

0.6.15
------

* Fixed typo in bdist_egg
* Several issues under Python 3 has been solved.
* Distribute #146: Fixed missing DLL files after easy_install of windows exe package.

0.6.14
------

* Distribute #170: Fixed unittest failure. Thanks to Toshio.
* Distribute #171: Fixed race condition in unittests cause deadlocks in test suite.
* Distribute #143: Fixed a lookup issue with easy_install.
  Thanks to David and Zooko.
* Distribute #174: Fixed the edit mode when its used with setuptools itself

0.6.13
------

* Distribute #160: 2.7 gives ValueError("Invalid IPv6 URL")
* Distribute #150: Fixed using ~/.local even in a --no-site-packages virtualenv
* Distribute #163: scan index links before external links, and don't use the md5 when
  comparing two distributions

0.6.12
------

* Distribute #149: Fixed various failures on 2.3/2.4

0.6.11
------

* Found another case of SandboxViolation - fixed
* Distribute #15 and Distribute #48: Introduced a socket timeout of 15 seconds on url openings
* Added indexsidebar.html into MANIFEST.in
* Distribute #108: Fixed TypeError with Python3.1
* Distribute #121: Fixed --help install command trying to actually install.
* Distribute #112: Added an os.makedirs so that Tarek's solution will work.
* Distribute #133: Added --no-find-links to easy_install
* Added easy_install --user
* Distribute #100: Fixed develop --user not taking '.' in PYTHONPATH into account
* Distribute #134: removed spurious UserWarnings. Patch by VanLindberg
* Distribute #138: cant_write_to_target error when setup_requires is used.
* Distribute #147: respect the sys.dont_write_bytecode flag

0.6.10
------

* Reverted change made for the DistributionNotFound exception because
  zc.buildout uses the exception message to get the name of the
  distribution.

0.6.9
-----

* Distribute #90: unknown setuptools version can be added in the working set
* Distribute #87: setupt.py doesn't try to convert distribute_setup.py anymore
  Initial Patch by arfrever.
* Distribute #89: added a side bar with a download link to the doc.
* Distribute #86: fixed missing sentence in pkg_resources doc.
* Added a nicer error message when a DistributionNotFound is raised.
* Distribute #80: test_develop now works with Python 3.1
* Distribute #93: upload_docs now works if there is an empty sub-directory.
* Distribute #70: exec bit on non-exec files
* Distribute #99: now the standalone easy_install command doesn't uses a
  "setup.cfg" if any exists in the working directory. It will use it
  only if triggered by ``install_requires`` from a setup.py call
  (install, develop, etc).
* Distribute #101: Allowing ``os.devnull`` in Sandbox
* Distribute #92: Fixed the "no eggs" found error with MacPort
  (platform.mac_ver() fails)
* Distribute #103: test_get_script_header_jython_workaround not run
  anymore under py3 with C or POSIX local. Contributed by Arfrever.
* Distribute #104: remvoved the assertion when the installation fails,
  with a nicer message for the end user.
* Distribute #100: making sure there's no SandboxViolation when
  the setup script patches setuptools.

0.6.8
-----

* Added "check_packages" in dist. (added in Setuptools 0.6c11)
* Fixed the DONT_PATCH_SETUPTOOLS state.

0.6.7
-----

* Distribute #58: Added --user support to the develop command
* Distribute #11: Generated scripts now wrap their call to the script entry point
  in the standard "if name == 'main'"
* Added the 'DONT_PATCH_SETUPTOOLS' environment variable, so virtualenv
  can drive an installation that doesn't patch a global setuptools.
* Reviewed unladen-swallow specific change from
  http://code.google.com/p/unladen-swallow/source/detail?spec=svn875&r=719
  and determined that it no longer applies. Distribute should work fine with
  Unladen Swallow 2009Q3.
* Distribute #21: Allow PackageIndex.open_url to gracefully handle all cases of a
  httplib.HTTPException instead of just InvalidURL and BadStatusLine.
* Removed virtual-python.py from this distribution and updated documentation
  to point to the actively maintained virtualenv instead.
* Distribute #64: use_setuptools no longer rebuilds the distribute egg every
  time it is run
* use_setuptools now properly respects the requested version
* use_setuptools will no longer try to import a distribute egg for the
  wrong Python version
* Distribute #74: no_fake should be True by default.
* Distribute #72: avoid a bootstrapping issue with easy_install -U

0.6.6
-----

* Unified the bootstrap file so it works on both py2.x and py3k without 2to3
  (patch by Holger Krekel)

0.6.5
-----

* Distribute #65: cli.exe and gui.exe are now generated at build time,
  depending on the platform in use.

* Distribute #67: Fixed doc typo (PEP 381/PEP 382).

* Distribute no longer shadows setuptools if we require a 0.7-series
  setuptools. And an error is raised when installing a 0.7 setuptools with
  distribute.

* When run from within buildout, no attempt is made to modify an existing
  setuptools egg, whether in a shared egg directory or a system setuptools.

* Fixed a hole in sandboxing allowing builtin file to write outside of
  the sandbox.

0.6.4
-----

* Added the generation of `distribute_setup_3k.py` during the release.
  This closes Distribute #52.

* Added an upload_docs command to easily upload project documentation to
  PyPI's https://pythonhosted.org. This close issue Distribute #56.

* Fixed a bootstrap bug on the use_setuptools() API.

0.6.3
-----

setuptools
==========

* Fixed a bunch of calls to file() that caused crashes on Python 3.

bootstrapping
=============

* Fixed a bug in sorting that caused bootstrap to fail on Python 3.

0.6.2
-----

setuptools
==========

* Added Python 3 support; see docs/python3.txt.
  This closes Old Setuptools #39.

* Added option to run 2to3 automatically when installing on Python 3.
  This closes issue Distribute #31.

* Fixed invalid usage of requirement.parse, that broke develop -d.
  This closes Old Setuptools #44.

* Fixed script launcher for 64-bit Windows.
  This closes Old Setuptools #2.

* KeyError when compiling extensions.
  This closes Old Setuptools #41.

bootstrapping
=============

* Fixed bootstrap not working on Windows. This closes issue Distribute #49.

* Fixed 2.6 dependencies. This closes issue Distribute #50.

* Make sure setuptools is patched when running through easy_install
  This closes Old Setuptools #40.

0.6.1
-----

setuptools
==========

* package_index.urlopen now catches BadStatusLine and malformed url errors.
  This closes Distribute #16 and Distribute #18.

* zip_ok is now False by default. This closes Old Setuptools #33.

* Fixed invalid URL error catching. Old Setuptools #20.

* Fixed invalid bootstraping with easy_install installation (Distribute #40).
  Thanks to Florian Schulze for the help.

* Removed buildout/bootstrap.py. A new repository will create a specific
  bootstrap.py script.


bootstrapping
=============

* The boostrap process leave setuptools alone if detected in the system
  and --root or --prefix is provided, but is not in the same location.
  This closes Distribute #10.

0.6
---

setuptools
==========

* Packages required at build time where not fully present at install time.
  This closes Distribute #12.

* Protected against failures in tarfile extraction. This closes Distribute #10.

* Made Jython api_tests.txt doctest compatible. This closes Distribute #7.

* sandbox.py replaced builtin type file with builtin function open. This
  closes Distribute #6.

* Immediately close all file handles. This closes Distribute #3.

* Added compatibility with Subversion 1.6. This references Distribute #1.

pkg_resources
=============

* Avoid a call to /usr/bin/sw_vers on OSX and use the official platform API
  instead. Based on a patch from ronaldoussoren. This closes issue #5.

* Fixed a SandboxViolation for mkdir that could occur in certain cases.
  This closes Distribute #13.

* Allow to find_on_path on systems with tight permissions to fail gracefully.
  This closes Distribute #9.

* Corrected inconsistency between documentation and code of add_entry.
  This closes Distribute #8.

* Immediately close all file handles. This closes Distribute #3.

easy_install
============

* Immediately close all file handles. This closes Distribute #3.

0.6c9
-----

 * Fixed a missing files problem when using Windows source distributions on
   non-Windows platforms, due to distutils not handling manifest file line
   endings correctly.

 * Updated Pyrex support to work with Pyrex 0.9.6 and higher.

 * Minor changes for Jython compatibility, including skipping tests that can't
   work on Jython.

 * Fixed not installing eggs in ``install_requires`` if they were also used for
   ``setup_requires`` or ``tests_require``.

 * Fixed not fetching eggs in ``install_requires`` when running tests.

 * Allow ``ez_setup.use_setuptools()`` to upgrade existing setuptools
   installations when called from a standalone ``setup.py``.

 * Added a warning if a namespace package is declared, but its parent package
   is not also declared as a namespace.

 * Support Subversion 1.5

 * Removed use of deprecated ``md5`` module if ``hashlib`` is available

 * Fixed ``bdist_wininst upload`` trying to upload the ``.exe`` twice

 * Fixed ``bdist_egg`` putting a ``native_libs.txt`` in the source package's
   ``.egg-info``, when it should only be in the built egg's ``EGG-INFO``.

 * Ensure that _full_name is set on all shared libs before extensions are
   checked for shared lib usage.  (Fixes a bug in the experimental shared
   library build support.)

 * Fix to allow unpacked eggs containing native libraries to fail more
   gracefully under Google App Engine (with an ``ImportError`` loading the
   C-based module, instead of getting a ``NameError``).

0.6c7
-----

 * Fixed ``distutils.filelist.findall()`` crashing on broken symlinks, and
   ``egg_info`` command failing on new, uncommitted SVN directories.

 * Fix import problems with nested namespace packages installed via
   ``--root`` or ``--single-version-externally-managed``, due to the
   parent package not having the child package as an attribute.

0.6c6
-----

 * Added ``--egg-path`` option to ``develop`` command, allowing you to force
   ``.egg-link`` files to use relative paths (allowing them to be shared across
   platforms on a networked drive).

 * Fix not building binary RPMs correctly.

 * Fix "eggsecutables" (such as setuptools' own egg) only being runnable with
   bash-compatible shells.

 * Fix ``#!`` parsing problems in Windows ``.exe`` script wrappers, when there
   was whitespace inside a quoted argument or at the end of the ``#!`` line
   (a regression introduced in 0.6c4).

 * Fix ``test`` command possibly failing if an older version of the project
   being tested was installed on ``sys.path`` ahead of the test source
   directory.

 * Fix ``find_packages()`` treating ``ez_setup`` and directories with ``.`` in
   their names as packages.

0.6c5
-----

 * Fix uploaded ``bdist_rpm`` packages being described as ``bdist_egg``
   packages under Python versions less than 2.5.

 * Fix uploaded ``bdist_wininst`` packages being described as suitable for
   "any" version by Python 2.5, even if a ``--target-version`` was specified.

0.6c4
-----

 * Overhauled Windows script wrapping to support ``bdist_wininst`` better.
   Scripts installed with ``bdist_wininst`` will always use ``#!python.exe`` or
   ``#!pythonw.exe`` as the executable name (even when built on non-Windows
   platforms!), and the wrappers will look for the executable in the script's
   parent directory (which should find the right version of Python).

 * Fix ``upload`` command not uploading files built by ``bdist_rpm`` or
   ``bdist_wininst`` under Python 2.3 and 2.4.

 * Add support for "eggsecutable" headers: a ``#!/bin/sh`` script that is
   prepended to an ``.egg`` file to allow it to be run as a script on Unix-ish
   platforms.  (This is mainly so that setuptools itself can have a single-file
   installer on Unix, without doing multiple downloads, dealing with firewalls,
   etc.)

 * Fix problem with empty revision numbers in Subversion 1.4 ``entries`` files

 * Use cross-platform relative paths in ``easy-install.pth`` when doing
   ``develop`` and the source directory is a subdirectory of the installation
   target directory.

 * Fix a problem installing eggs with a system packaging tool if the project
   contained an implicit namespace package; for example if the ``setup()``
   listed a namespace package ``foo.bar`` without explicitly listing ``foo``
   as a namespace package.

0.6c3
-----

 * Fixed breakages caused by Subversion 1.4's new "working copy" format

0.6c2
-----

 * The ``ez_setup`` module displays the conflicting version of setuptools (and
   its installation location) when a script requests a version that's not
   available.

 * Running ``setup.py develop`` on a setuptools-using project will now install
   setuptools if needed, instead of only downloading the egg.

0.6c1
-----

 * Fixed ``AttributeError`` when trying to download a ``setup_requires``
   dependency when a distribution lacks a ``dependency_links`` setting.

 * Made ``zip-safe`` and ``not-zip-safe`` flag files contain a single byte, so
   as to play better with packaging tools that complain about zero-length
   files.

 * Made ``setup.py develop`` respect the ``--no-deps`` option, which it
   previously was ignoring.

 * Support ``extra_path`` option to ``setup()`` when ``install`` is run in
   backward-compatibility mode.

 * Source distributions now always include a ``setup.cfg`` file that explicitly
   sets ``egg_info`` options such that they produce an identical version number
   to the source distribution's version number.  (Previously, the default
   version number could be different due to the use of ``--tag-date``, or if
   the version was overridden on the command line that built the source
   distribution.)

0.6b4
-----

 * Fix ``register`` not obeying name/version set by ``egg_info`` command, if
   ``egg_info`` wasn't explicitly run first on the same command line.

 * Added ``--no-date`` and ``--no-svn-revision`` options to ``egg_info``
   command, to allow suppressing tags configured in ``setup.cfg``.

 * Fixed redundant warnings about missing ``README`` file(s); it should now
   appear only if you are actually a source distribution.

0.6b3
-----

 * Fix ``bdist_egg`` not including files in subdirectories of ``.egg-info``.

 * Allow ``.py`` files found by the ``include_package_data`` option to be
   automatically included. Remove duplicate data file matches if both
   ``include_package_data`` and ``package_data`` are used to refer to the same
   files.

0.6b1
-----

 * Strip ``module`` from the end of compiled extension modules when computing
   the name of a ``.py`` loader/wrapper.  (Python's import machinery ignores
   this suffix when searching for an extension module.)

0.6a11
------

 * Added ``test_loader`` keyword to support custom test loaders

 * Added ``setuptools.file_finders`` entry point group to allow implementing
   revision control plugins.

 * Added ``--identity`` option to ``upload`` command.

 * Added ``dependency_links`` to allow specifying URLs for ``--find-links``.

 * Enhanced test loader to scan packages as well as modules, and call
   ``additional_tests()`` if present to get non-unittest tests.

 * Support namespace packages in conjunction with system packagers, by omitting
   the installation of any ``__init__.py`` files for namespace packages, and
   adding a special ``.pth`` file to create a working package in
   ``sys.modules``.

 * Made ``--single-version-externally-managed`` automatic when ``--root`` is
   used, so that most system packagers won't require special support for
   setuptools.

 * Fixed ``setup_requires``, ``tests_require``, etc. not using ``setup.cfg`` or
   other configuration files for their option defaults when installing, and
   also made the install use ``--multi-version`` mode so that the project
   directory doesn't need to support .pth files.

 * ``MANIFEST.in`` is now forcibly closed when any errors occur while reading
   it. Previously, the file could be left open and the actual error would be
   masked by problems trying to remove the open file on Windows systems.

0.6a10
------

 * Fixed the ``develop`` command ignoring ``--find-links``.

0.6a9
-----

 * The ``sdist`` command no longer uses the traditional ``MANIFEST`` file to
   create source distributions.  ``MANIFEST.in`` is still read and processed,
   as are the standard defaults and pruning. But the manifest is built inside
   the project's ``.egg-info`` directory as ``SOURCES.txt``, and it is rebuilt
   every time the ``egg_info`` command is run.

 * Added the ``include_package_data`` keyword to ``setup()``, allowing you to
   automatically include any package data listed in revision control or
   ``MANIFEST.in``

 * Added the ``exclude_package_data`` keyword to ``setup()``, allowing you to
   trim back files included via the ``package_data`` and
   ``include_package_data`` options.

 * Fixed ``--tag-svn-revision`` not working when run from a source
   distribution.

 * Added warning for namespace packages with missing ``declare_namespace()``

 * Added ``tests_require`` keyword to ``setup()``, so that e.g. packages
   requiring ``nose`` to run unit tests can make this dependency optional
   unless the ``test`` command is run.

 * Made all commands that use ``easy_install`` respect its configuration
   options, as this was causing some problems with ``setup.py install``.

 * Added an ``unpack_directory()`` driver to ``setuptools.archive_util``, so
   that you can process a directory tree through a processing filter as if it
   were a zipfile or tarfile.

 * Added an internal ``install_egg_info`` command to use as part of old-style
   ``install`` operations, that installs an ``.egg-info`` directory with the
   package.

 * Added a ``--single-version-externally-managed`` option to the ``install``
   command so that you can more easily wrap a "flat" egg in a system package.

 * Enhanced ``bdist_rpm`` so that it installs single-version eggs that
   don't rely on a ``.pth`` file. The ``--no-egg`` option has been removed,
   since all RPMs are now built in a more backwards-compatible format.

 * Support full roundtrip translation of eggs to and from ``bdist_wininst``
   format. Running ``bdist_wininst`` on a setuptools-based package wraps the
   egg in an .exe that will safely install it as an egg (i.e., with metadata
   and entry-point wrapper scripts), and ``easy_install`` can turn the .exe
   back into an ``.egg`` file or directory and install it as such.


0.6a8
-----

 * Fixed some problems building extensions when Pyrex was installed, especially
   with Python 2.4 and/or packages using SWIG.

 * Made ``develop`` command accept all the same options as ``easy_install``,
   and use the ``easy_install`` command's configuration settings as defaults.

 * Made ``egg_info --tag-svn-revision`` fall back to extracting the revision
   number from ``PKG-INFO`` in case it is being run on a source distribution of
   a snapshot taken from a Subversion-based project.

 * Automatically detect ``.dll``, ``.so`` and ``.dylib`` files that are being
   installed as data, adding them to ``native_libs.txt`` automatically.

 * Fixed some problems with fresh checkouts of projects that don't include
   ``.egg-info/PKG-INFO`` under revision control and put the project's source
   code directly in the project directory. If such a package had any
   requirements that get processed before the ``egg_info`` command can be run,
   the setup scripts would fail with a "Missing 'Version:' header and/or
   PKG-INFO file" error, because the egg runtime interpreted the unbuilt
   metadata in a directory on ``sys.path`` (i.e. the current directory) as
   being a corrupted egg. Setuptools now monkeypatches the distribution
   metadata cache to pretend that the egg has valid version information, until
   it has a chance to make it actually be so (via the ``egg_info`` command).

0.6a5
-----

 * Fixed missing gui/cli .exe files in distribution. Fixed bugs in tests.

0.6a3
-----

 * Added ``gui_scripts`` entry point group to allow installing GUI scripts
   on Windows and other platforms.  (The special handling is only for Windows;
   other platforms are treated the same as for ``console_scripts``.)

0.6a2
-----

 * Added ``console_scripts`` entry point group to allow installing scripts
   without the need to create separate script files. On Windows, console
   scripts get an ``.exe`` wrapper so you can just type their name. On other
   platforms, the scripts are written without a file extension.

0.6a1
-----

 * Added support for building "old-style" RPMs that don't install an egg for
   the target package, using a ``--no-egg`` option.

 * The ``build_ext`` command now works better when using the ``--inplace``
   option and multiple Python versions. It now makes sure that all extensions
   match the current Python version, even if newer copies were built for a
   different Python version.

 * The ``upload`` command no longer attaches an extra ``.zip`` when uploading
   eggs, as PyPI now supports egg uploads without trickery.

 * The ``ez_setup`` script/module now displays a warning before downloading
   the setuptools egg, and attempts to check the downloaded egg against an
   internal MD5 checksum table.

 * Fixed the ``--tag-svn-revision`` option of ``egg_info`` not finding the
   latest revision number; it was using the revision number of the directory
   containing ``setup.py``, not the highest revision number in the project.

 * Added ``eager_resources`` setup argument

 * The ``sdist`` command now recognizes Subversion "deleted file" entries and
   does not include them in source distributions.

 * ``setuptools`` now embeds itself more thoroughly into the distutils, so that
   other distutils extensions (e.g. py2exe, py2app) will subclass setuptools'
   versions of things, rather than the native distutils ones.

 * Added ``entry_points`` and ``setup_requires`` arguments to ``setup()``;
   ``setup_requires`` allows you to automatically find and download packages
   that are needed in order to *build* your project (as opposed to running it).

 * ``setuptools`` now finds its commands, ``setup()`` argument validators, and
   metadata writers using entry points, so that they can be extended by
   third-party packages. See `Creating distutils Extensions
   <https://setuptools.readthedocs.io/en/latest/setuptools.html#creating-distutils-extensions>`_
   for more details.

 * The vestigial ``depends`` command has been removed. It was never finished
   or documented, and never would have worked without EasyInstall - which it
   pre-dated and was never compatible with.

0.5a12
------

 * The zip-safety scanner now checks for modules that might be used with
   ``python -m``, and marks them as unsafe for zipping, since Python 2.4 can't
   handle ``-m`` on zipped modules.

0.5a11
------

 * Fix breakage of the "develop" command that was caused by the addition of
   ``--always-unzip`` to the ``easy_install`` command.

0.5a9
-----

 * Include ``svn:externals`` directories in source distributions as well as
   normal subversion-controlled files and directories.

 * Added ``exclude=patternlist`` option to ``setuptools.find_packages()``

 * Changed --tag-svn-revision to include an "r" in front of the revision number
   for better readability.

 * Added ability to build eggs without including source files (except for any
   scripts, of course), using the ``--exclude-source-files`` option to
   ``bdist_egg``.

 * ``setup.py install`` now automatically detects when an "unmanaged" package
   or module is going to be on ``sys.path`` ahead of a package being installed,
   thereby preventing the newer version from being imported. If this occurs,
   a warning message is output to ``sys.stderr``, but installation proceeds
   anyway. The warning message informs the user what files or directories
   need deleting, and advises them they can also use EasyInstall (with the
   ``--delete-conflicting`` option) to do it automatically.

 * The ``egg_info`` command now adds a ``top_level.txt`` file to the metadata
   directory that lists all top-level modules and packages in the distribution.
   This is used by the ``easy_install`` command to find possibly-conflicting
   "unmanaged" packages when installing the distribution.

 * Added ``zip_safe`` and ``namespace_packages`` arguments to ``setup()``.
   Added package analysis to determine zip-safety if the ``zip_safe`` flag
   is not given, and advise the author regarding what code might need changing.

 * Fixed the swapped ``-d`` and ``-b`` options of ``bdist_egg``.

0.5a8
-----

 * The "egg_info" command now always sets the distribution metadata to "safe"
   forms of the distribution name and version, so that distribution files will
   be generated with parseable names (i.e., ones that don't include '-' in the
   name or version). Also, this means that if you use the various ``--tag``
   options of "egg_info", any distributions generated will use the tags in the
   version, not just egg distributions.

 * Added support for defining command aliases in distutils configuration files,
   under the "[aliases]" section. To prevent recursion and to allow aliases to
   call the command of the same name, a given alias can be expanded only once
   per command-line invocation. You can define new aliases with the "alias"
   command, either for the local, global, or per-user configuration.

 * Added "rotate" command to delete old distribution files, given a set of
   patterns to match and the number of files to keep.  (Keeps the most
   recently-modified distribution files matching each pattern.)

 * Added "saveopts" command that saves all command-line options for the current
   invocation to the local, global, or per-user configuration file. Useful for
   setting defaults without having to hand-edit a configuration file.

 * Added a "setopt" command that sets a single option in a specified distutils
   configuration file.

0.5a7
-----

 * Added "upload" support for egg and source distributions, including a bug
   fix for "upload" and a temporary workaround for lack of .egg support in
   PyPI.

0.5a6
-----

 * Beefed up the "sdist" command so that if you don't have a MANIFEST.in, it
   will include all files under revision control (CVS or Subversion) in the
   current directory, and it will regenerate the list every time you create a
   source distribution, not just when you tell it to. This should make the
   default "do what you mean" more often than the distutils' default behavior
   did, while still retaining the old behavior in the presence of MANIFEST.in.

 * Fixed the "develop" command always updating .pth files, even if you
   specified ``-n`` or ``--dry-run``.

 * Slightly changed the format of the generated version when you use
   ``--tag-build`` on the "egg_info" command, so that you can make tagged
   revisions compare *lower* than the version specified in setup.py (e.g. by
   using ``--tag-build=dev``).

0.5a5
-----

 * Added ``develop`` command to ``setuptools``-based packages. This command
   installs an ``.egg-link`` pointing to the package's source directory, and
   script wrappers that ``execfile()`` the source versions of the package's
   scripts. This lets you put your development checkout(s) on sys.path without
   having to actually install them.  (To uninstall the link, use
   use ``setup.py develop --uninstall``.)

 * Added ``egg_info`` command to ``setuptools``-based packages. This command
   just creates or updates the "projectname.egg-info" directory, without
   building an egg.  (It's used by the ``bdist_egg``, ``test``, and ``develop``
   commands.)

 * Enhanced the ``test`` command so that it doesn't install the package, but
   instead builds any C extensions in-place, updates the ``.egg-info``
   metadata, adds the source directory to ``sys.path``, and runs the tests
   directly on the source. This avoids an "unmanaged" installation of the
   package to ``site-packages`` or elsewhere.

 * Made ``easy_install`` a standard ``setuptools`` command, moving it from
   the ``easy_install`` module to ``setuptools.command.easy_install``. Note
   that if you were importing or extending it, you must now change your imports
   accordingly.  ``easy_install.py`` is still installed as a script, but not as
   a module.

0.5a4
-----

 * Setup scripts using setuptools can now list their dependencies directly in
   the setup.py file, without having to manually create a ``depends.txt`` file.
   The ``install_requires`` and ``extras_require`` arguments to ``setup()``
   are used to create a dependencies file automatically. If you are manually
   creating ``depends.txt`` right now, please switch to using these setup
   arguments as soon as practical, because ``depends.txt`` support will be
   removed in the 0.6 release cycle. For documentation on the new arguments,
   see the ``setuptools.dist.Distribution`` class.

 * Setup scripts using setuptools now always install using ``easy_install``
   internally, for ease of uninstallation and upgrading.

0.5a1
-----

 * Added support for "self-installation" bootstrapping. Packages can now
   include ``ez_setup.py`` in their source distribution, and add the following
   to their ``setup.py``, in order to automatically bootstrap installation of
   setuptools as part of their setup process::

    from ez_setup import use_setuptools
    use_setuptools()

    from setuptools import setup
    # etc...

0.4a2
-----

 * Added ``ez_setup.py`` installer/bootstrap script to make initial setuptools
   installation easier, and to allow distributions using setuptools to avoid
   having to include setuptools in their source distribution.

 * All downloads are now managed by the ``PackageIndex`` class (which is now
   subclassable and replaceable), so that embedders can more easily override
   download logic, give download progress reports, etc. The class has also
   been moved to the new ``setuptools.package_index`` module.

 * The ``Installer`` class no longer handles downloading, manages a temporary
   directory, or tracks the ``zip_ok`` option. Downloading is now handled
   by ``PackageIndex``, and ``Installer`` has become an ``easy_install``
   command class based on ``setuptools.Command``.

 * There is a new ``setuptools.sandbox.run_setup()`` API to invoke a setup
   script in a directory sandbox, and a new ``setuptools.archive_util`` module
   with an ``unpack_archive()`` API. These were split out of EasyInstall to
   allow reuse by other tools and applications.

 * ``setuptools.Command`` now supports reinitializing commands using keyword
   arguments to set/reset options. Also, ``Command`` subclasses can now set
   their ``command_consumes_arguments`` attribute to ``True`` in order to
   receive an ``args`` option containing the rest of the command line.

0.3a2
-----

 * Added new options to ``bdist_egg`` to allow tagging the egg's version number
   with a subversion revision number, the current date, or an explicit tag
   value. Run ``setup.py bdist_egg --help`` to get more information.

 * Misc. bug fixes

0.3a1
-----

 * Initial release.

.. image:: https://img.shields.io/pypi/v/setuptools.svg
   :target: https://pypi.org/project/setuptools

.. image:: https://readthedocs.org/projects/setuptools/badge/?version=latest
    :target: https://setuptools.readthedocs.io

.. image:: https://img.shields.io/travis/pypa/setuptools/master.svg?label=Linux%20build%20%40%20Travis%20CI
   :target: https://travis-ci.org/pypa/setuptools

.. image:: https://img.shields.io/appveyor/ci/pypa/setuptools/master.svg?label=Windows%20build%20%40%20Appveyor
   :target: https://ci.appveyor.com/project/pypa/setuptools/branch/master

.. image:: https://img.shields.io/codecov/c/github/pypa/setuptools/master.svg
   :target: https://codecov.io/gh/pypa/setuptools

.. image:: https://img.shields.io/pypi/pyversions/setuptools.svg

See the `Installation Instructions
<https://packaging.python.org/installing/>`_ in the Python Packaging
User's Guide for instructions on installing, upgrading, and uninstalling
Setuptools.

The project is `maintained at GitHub <https://github.com/pypa/setuptools>`_.

Questions and comments should be directed to the `distutils-sig
mailing list <http://mail.python.org/pipermail/distutils-sig/>`_.
Bug reports and especially tested patches may be
submitted directly to the `bug tracker
<https://github.com/pypa/setuptools/issues>`_.


Code of Conduct
---------------

Everyone interacting in the setuptools project's codebases, issue trackers,
chat rooms, and mailing lists is expected to follow the
`PyPA Code of Conduct <https://www.pypa.io/en/latest/code-of-conduct/>`_.
This is Python version 3.6.8
============================

.. image:: https://travis-ci.org/python/cpython.svg?branch=3.6
   :alt: CPython build status on Travis CI
   :target: https://travis-ci.org/python/cpython

.. image:: https://ci.appveyor.com/api/projects/status/4mew1a93xdkbf5ua/branch/3.6?svg=true
   :alt: CPython build status on Appveyor
   :target: https://ci.appveyor.com/project/python/cpython/branch/3.6

.. image:: https://dev.azure.com/python/cpython/_apis/build/status/Azure%20Pipelines%20CI?branchName=3.6
   :alt: CPython build status on Azure Pipelines
   :target: https://dev.azure.com/python/cpython/_build/latest?definitionId=4&branchName=3.6

.. image:: https://codecov.io/gh/python/cpython/branch/3.6/graph/badge.svg
   :alt: CPython code coverage on Codecov
   :target: https://codecov.io/gh/python/cpython

Copyright (c) 2001, 2002, 2003, 2004, 2005, 2006, 2007, 2008, 2009, 2010, 2011,
2012, 2013, 2014, 2015, 2016, 2017, 2018 Python Software Foundation.  All rights
reserved.

See the end of this file for further copyright and license information.

General Information
-------------------

- Website: https://www.python.org
- Source code: https://github.com/python/cpython
- Issue tracker: https://bugs.python.org
- Documentation: https://docs.python.org
- Developer's Guide: https://devguide.python.org/

Contributing to CPython
-----------------------

For more complete instructions on contributing to CPython development,
see the `Developer Guide`_.

.. _Developer Guide: https://devguide.python.org/

Using Python
------------

Installable Python kits, and information about using Python, are available at
`python.org`_.

.. _python.org: https://www.python.org/


Build Instructions
------------------

On Unix, Linux, BSD, macOS, and Cygwin::

    ./configure
    make
    make test
    sudo make install

This will install Python as python3.

You can pass many options to the configure script; run ``./configure --help``
to find out more.  On macOS and Cygwin, the executable is called ``python.exe``;
elsewhere it's just ``python``.

If you are running on macOS with the latest updates installed, make sure to install
openSSL or some other SSL software along with Homebrew or another package manager.
If issues persist, see https://devguide.python.org/setup/#macos-and-os-x for more 
information. 

On macOS, if you have configured Python with ``--enable-framework``, you
should use ``make frameworkinstall`` to do the installation.  Note that this
installs the Python executable in a place that is not normally on your PATH,
you may want to set up a symlink in ``/usr/local/bin``.

On Windows, see `PCbuild/readme.txt
<https://github.com/python/cpython/blob/3.6/PCbuild/readme.txt>`_.

If you wish, you can create a subdirectory and invoke configure from there.
For example::

    mkdir debug
    cd debug
    ../configure --with-pydebug
    make
    make test

(This will fail if you *also* built at the top-level directory.  You should do
a ``make clean`` at the toplevel first.)

To get an optimized build of Python, ``configure --enable-optimizations``
before you run ``make``.  This sets the default make targets up to enable
Profile Guided Optimization (PGO) and may be used to auto-enable Link Time
Optimization (LTO) on some platforms.  For more details, see the sections
below.


Profile Guided Optimization
---------------------------

PGO takes advantage of recent versions of the GCC or Clang compilers.  If used,
either via ``configure --enable-optimizations`` above or by manually running
``make profile-opt`` regardless of configure flags it will do several steps.

First, the entire Python directory is cleaned of temporary files that may have
resulted in a previous compilation.

Then, an instrumented version of the interpreter is built, using suitable
compiler flags for each flavour. Note that this is just an intermediary step.
The binary resulting from this step is not good for real life workloads as
it has profiling instructions embedded inside.

After this instrumented version of the interpreter is built, the Makefile will
automatically run a training workload. This is necessary in order to profile
the interpreter execution. Note also that any output, both stdout and stderr,
that may appear at this step is suppressed.

Finally, the last step is to rebuild the interpreter, using the information
collected in the previous one. The end result will be a Python binary that is
optimized and suitable for distribution or production installation.


Link Time Optimization
----------------------

Enabled via configure's ``--with-lto`` flag.  LTO takes advantage of the
ability of recent compiler toolchains to optimize across the otherwise
arbitrary ``.o`` file boundary when building final executables or shared
libraries for additional performance gains.


What's New
----------

We have a comprehensive overview of the changes in the `What's New in Python
3.6 <https://docs.python.org/3.6/whatsnew/3.6.html>`_ document.  For a more
detailed change log, read `Misc/NEWS
<https://github.com/python/cpython/blob/3.6/Misc/NEWS.d>`_, but a full
accounting of changes can only be gleaned from the `commit history
<https://github.com/python/cpython/commits/3.6>`_.

If you want to install multiple versions of Python see the section below
entitled "Installing multiple versions".


Documentation
-------------

`Documentation for Python 3.6 <https://docs.python.org/3.6/>`_ is online,
updated daily.

It can also be downloaded in many formats for faster access.  The documentation
is downloadable in HTML, PDF, and reStructuredText formats; the latter version
is primarily for documentation authors, translators, and people with special
formatting requirements.

For information about building Python's documentation, refer to `Doc/README.rst
<https://github.com/python/cpython/blob/3.6/Doc/README.rst>`_.


Converting From Python 2.x to 3.x
---------------------------------

Significant backward incompatible changes were made for the release of Python
3.0, which may cause programs written for Python 2 to fail when run with Python
3.  For more information about porting your code from Python 2 to Python 3, see
the `Porting HOWTO <https://docs.python.org/3/howto/pyporting.html>`_.


Testing
-------

To test the interpreter, type ``make test`` in the top-level directory.  The
test set produces some output.  You can generally ignore the messages about
skipped tests due to optional features which can't be imported.  If a message
is printed about a failed test or a traceback or core dump is produced,
something is wrong.

By default, tests are prevented from overusing resources like disk space and
memory.  To enable these tests, run ``make testall``.

If any tests fail, you can re-run the failing test(s) in verbose mode.  For
example, if ``test_os`` and ``test_gdb`` failed, you can run::

    make test TESTOPTS="-v test_os test_gdb"

If the failure persists and appears to be a problem with Python rather than
your environment, you can `file a bug report <https://bugs.python.org>`_ and
include relevant output from that command to show the issue.

See `Running & Writing Tests <https://devguide.python.org/runtests/>`_
for more on running tests.

Installing multiple versions
----------------------------

On Unix and Mac systems if you intend to install multiple versions of Python
using the same installation prefix (``--prefix`` argument to the configure
script) you must take care that your primary python executable is not
overwritten by the installation of a different version.  All files and
directories installed using ``make altinstall`` contain the major and minor
version and can thus live side-by-side.  ``make install`` also creates
``${prefix}/bin/python3`` which refers to ``${prefix}/bin/pythonX.Y``.  If you
intend to install multiple versions using the same prefix you must decide which
version (if any) is your "primary" version.  Install that version using ``make
install``.  Install all other versions using ``make altinstall``.

For example, if you want to install Python 2.7, 3.5, and 3.6 with 3.6 being the
primary version, you would execute ``make install`` in your 3.6 build directory
and ``make altinstall`` in the others.


Issue Tracker and Mailing List
------------------------------

Bug reports are welcome!  You can use the `issue tracker
<https://bugs.python.org>`_ to report bugs, and/or submit pull requests `on
GitHub <https://github.com/python/cpython>`_.

You can also follow development discussion on the `python-dev mailing list
<https://mail.python.org/mailman/listinfo/python-dev/>`_.


Proposals for enhancement
-------------------------

If you have a proposal to change Python, you may want to send an email to the
comp.lang.python or `python-ideas`_ mailing lists for initial feedback.  A
Python Enhancement Proposal (PEP) may be submitted if your idea gains ground.
All current PEPs, as well as guidelines for submitting a new PEP, are listed at
`python.org/dev/peps/ <https://www.python.org/dev/peps/>`_.

.. _python-ideas: https://mail.python.org/mailman/listinfo/python-ideas/


Release Schedule
----------------

See :pep:`494` for Python 3.6 release details.


Copyright and License Information
---------------------------------

Copyright (c) 2001, 2002, 2003, 2004, 2005, 2006, 2007, 2008, 2009, 2010, 2011,
2012, 2013, 2014, 2015, 2016, 2017, 2018 Python Software Foundation.  All rights
reserved.

Copyright (c) 2000 BeOpen.com.  All rights reserved.

Copyright (c) 1995-2001 Corporation for National Research Initiatives.  All
rights reserved.

Copyright (c) 1991-1995 Stichting Mathematisch Centrum.  All rights reserved.

See the file "LICENSE" for information on the history of this software, terms &
conditions for usage, and a DISCLAIMER OF ALL WARRANTIES.

This Python distribution contains *no* GNU General Public License (GPL) code,
so it may be used in proprietary projects.  There are interfaces to some GNU
code but these are entirely optional.

All trademarks referenced herein are property of their respective holders.
This is Python version 3.6.8
============================

.. image:: https://travis-ci.org/python/cpython.svg?branch=3.6
   :alt: CPython build status on Travis CI
   :target: https://travis-ci.org/python/cpython

.. image:: https://ci.appveyor.com/api/projects/status/4mew1a93xdkbf5ua/branch/3.6?svg=true
   :alt: CPython build status on Appveyor
   :target: https://ci.appveyor.com/project/python/cpython/branch/3.6

.. image:: https://dev.azure.com/python/cpython/_apis/build/status/Azure%20Pipelines%20CI?branchName=3.6
   :alt: CPython build status on Azure Pipelines
   :target: https://dev.azure.com/python/cpython/_build/latest?definitionId=4&branchName=3.6

.. image:: https://codecov.io/gh/python/cpython/branch/3.6/graph/badge.svg
   :alt: CPython code coverage on Codecov
   :target: https://codecov.io/gh/python/cpython

Copyright (c) 2001, 2002, 2003, 2004, 2005, 2006, 2007, 2008, 2009, 2010, 2011,
2012, 2013, 2014, 2015, 2016, 2017, 2018 Python Software Foundation.  All rights
reserved.

See the end of this file for further copyright and license information.

General Information
-------------------

- Website: https://www.python.org
- Source code: https://github.com/python/cpython
- Issue tracker: https://bugs.python.org
- Documentation: https://docs.python.org
- Developer's Guide: https://devguide.python.org/

Contributing to CPython
-----------------------

For more complete instructions on contributing to CPython development,
see the `Developer Guide`_.

.. _Developer Guide: https://devguide.python.org/

Using Python
------------

Installable Python kits, and information about using Python, are available at
`python.org`_.

.. _python.org: https://www.python.org/


Build Instructions
------------------

On Unix, Linux, BSD, macOS, and Cygwin::

    ./configure
    make
    make test
    sudo make install

This will install Python as python3.

You can pass many options to the configure script; run ``./configure --help``
to find out more.  On macOS and Cygwin, the executable is called ``python.exe``;
elsewhere it's just ``python``.

If you are running on macOS with the latest updates installed, make sure to install
openSSL or some other SSL software along with Homebrew or another package manager.
If issues persist, see https://devguide.python.org/setup/#macos-and-os-x for more 
information. 

On macOS, if you have configured Python with ``--enable-framework``, you
should use ``make frameworkinstall`` to do the installation.  Note that this
installs the Python executable in a place that is not normally on your PATH,
you may want to set up a symlink in ``/usr/local/bin``.

On Windows, see `PCbuild/readme.txt
<https://github.com/python/cpython/blob/3.6/PCbuild/readme.txt>`_.

If you wish, you can create a subdirectory and invoke configure from there.
For example::

    mkdir debug
    cd debug
    ../configure --with-pydebug
    make
    make test

(This will fail if you *also* built at the top-level directory.  You should do
a ``make clean`` at the toplevel first.)

To get an optimized build of Python, ``configure --enable-optimizations``
before you run ``make``.  This sets the default make targets up to enable
Profile Guided Optimization (PGO) and may be used to auto-enable Link Time
Optimization (LTO) on some platforms.  For more details, see the sections
below.


Profile Guided Optimization
---------------------------

PGO takes advantage of recent versions of the GCC or Clang compilers.  If used,
either via ``configure --enable-optimizations`` above or by manually running
``make profile-opt`` regardless of configure flags it will do several steps.

First, the entire Python directory is cleaned of temporary files that may have
resulted in a previous compilation.

Then, an instrumented version of the interpreter is built, using suitable
compiler flags for each flavour. Note that this is just an intermediary step.
The binary resulting from this step is not good for real life workloads as
it has profiling instructions embedded inside.

After this instrumented version of the interpreter is built, the Makefile will
automatically run a training workload. This is necessary in order to profile
the interpreter execution. Note also that any output, both stdout and stderr,
that may appear at this step is suppressed.

Finally, the last step is to rebuild the interpreter, using the information
collected in the previous one. The end result will be a Python binary that is
optimized and suitable for distribution or production installation.


Link Time Optimization
----------------------

Enabled via configure's ``--with-lto`` flag.  LTO takes advantage of the
ability of recent compiler toolchains to optimize across the otherwise
arbitrary ``.o`` file boundary when building final executables or shared
libraries for additional performance gains.


What's New
----------

We have a comprehensive overview of the changes in the `What's New in Python
3.6 <https://docs.python.org/3.6/whatsnew/3.6.html>`_ document.  For a more
detailed change log, read `Misc/NEWS
<https://github.com/python/cpython/blob/3.6/Misc/NEWS.d>`_, but a full
accounting of changes can only be gleaned from the `commit history
<https://github.com/python/cpython/commits/3.6>`_.

If you want to install multiple versions of Python see the section below
entitled "Installing multiple versions".


Documentation
-------------

`Documentation for Python 3.6 <https://docs.python.org/3.6/>`_ is online,
updated daily.

It can also be downloaded in many formats for faster access.  The documentation
is downloadable in HTML, PDF, and reStructuredText formats; the latter version
is primarily for documentation authors, translators, and people with special
formatting requirements.

For information about building Python's documentation, refer to `Doc/README.rst
<https://github.com/python/cpython/blob/3.6/Doc/README.rst>`_.


Converting From Python 2.x to 3.x
---------------------------------

Significant backward incompatible changes were made for the release of Python
3.0, which may cause programs written for Python 2 to fail when run with Python
3.  For more information about porting your code from Python 2 to Python 3, see
the `Porting HOWTO <https://docs.python.org/3/howto/pyporting.html>`_.


Testing
-------

To test the interpreter, type ``make test`` in the top-level directory.  The
test set produces some output.  You can generally ignore the messages about
skipped tests due to optional features which can't be imported.  If a message
is printed about a failed test or a traceback or core dump is produced,
something is wrong.

By default, tests are prevented from overusing resources like disk space and
memory.  To enable these tests, run ``make testall``.

If any tests fail, you can re-run the failing test(s) in verbose mode.  For
example, if ``test_os`` and ``test_gdb`` failed, you can run::

    make test TESTOPTS="-v test_os test_gdb"

If the failure persists and appears to be a problem with Python rather than
your environment, you can `file a bug report <https://bugs.python.org>`_ and
include relevant output from that command to show the issue.

See `Running & Writing Tests <https://devguide.python.org/runtests/>`_
for more on running tests.

Installing multiple versions
----------------------------

On Unix and Mac systems if you intend to install multiple versions of Python
using the same installation prefix (``--prefix`` argument to the configure
script) you must take care that your primary python executable is not
overwritten by the installation of a different version.  All files and
directories installed using ``make altinstall`` contain the major and minor
version and can thus live side-by-side.  ``make install`` also creates
``${prefix}/bin/python3`` which refers to ``${prefix}/bin/pythonX.Y``.  If you
intend to install multiple versions using the same prefix you must decide which
version (if any) is your "primary" version.  Install that version using ``make
install``.  Install all other versions using ``make altinstall``.

For example, if you want to install Python 2.7, 3.5, and 3.6 with 3.6 being the
primary version, you would execute ``make install`` in your 3.6 build directory
and ``make altinstall`` in the others.


Issue Tracker and Mailing List
------------------------------

Bug reports are welcome!  You can use the `issue tracker
<https://bugs.python.org>`_ to report bugs, and/or submit pull requests `on
GitHub <https://github.com/python/cpython>`_.

You can also follow development discussion on the `python-dev mailing list
<https://mail.python.org/mailman/listinfo/python-dev/>`_.


Proposals for enhancement
-------------------------

If you have a proposal to change Python, you may want to send an email to the
comp.lang.python or `python-ideas`_ mailing lists for initial feedback.  A
Python Enhancement Proposal (PEP) may be submitted if your idea gains ground.
All current PEPs, as well as guidelines for submitting a new PEP, are listed at
`python.org/dev/peps/ <https://www.python.org/dev/peps/>`_.

.. _python-ideas: https://mail.python.org/mailman/listinfo/python-ideas/


Release Schedule
----------------

See :pep:`494` for Python 3.6 release details.


Copyright and License Information
---------------------------------

Copyright (c) 2001, 2002, 2003, 2004, 2005, 2006, 2007, 2008, 2009, 2010, 2011,
2012, 2013, 2014, 2015, 2016, 2017, 2018 Python Software Foundation.  All rights
reserved.

Copyright (c) 2000 BeOpen.com.  All rights reserved.

Copyright (c) 1995-2001 Corporation for National Research Initiatives.  All
rights reserved.

Copyright (c) 1991-1995 Stichting Mathematisch Centrum.  All rights reserved.

See the file "LICENSE" for information on the history of this software, terms &
conditions for usage, and a DISCLAIMER OF ALL WARRANTIES.

This Python distribution contains *no* GNU General Public License (GPL) code,
so it may be used in proprietary projects.  There are interfaces to some GNU
code but these are entirely optional.

All trademarks referenced herein are property of their respective holders.
.. image:: http://img.shields.io/pypi/v/six.svg
   :target: https://pypi.python.org/pypi/six

.. image:: https://travis-ci.org/benjaminp/six.svg?branch=master
    :target: https://travis-ci.org/benjaminp/six

.. image:: http://img.shields.io/badge/license-MIT-green.svg
   :target: https://github.com/benjaminp/six/blob/master/LICENSE

Six is a Python 2 and 3 compatibility library.  It provides utility functions
for smoothing over the differences between the Python versions with the goal of
writing Python code that is compatible on both Python versions.  See the
documentation for more information on what is provided.

Six supports every Python version since 2.6.  It is contained in only one Python
file, so it can be easily copied into your project. (The copyright and license
notice must be retained.)

Online documentation is at http://six.rtfd.org.

Bugs can be reported to https://github.com/benjaminp/six.  The code can also
be found there.

For questions about six or porting in general, email the python-porting mailing
list: https://mail.python.org/mailman/listinfo/python-porting
Six: Python 2 and 3 Compatibility Library
=========================================

.. module:: six
   :synopsis: Python 2 and 3 compatibility

.. moduleauthor:: Benjamin Peterson <benjamin@python.org>
.. sectionauthor:: Benjamin Peterson <benjamin@python.org>


Six provides simple utilities for wrapping over differences between Python 2 and
Python 3.  It is intended to support codebases that work on both Python 2 and 3
without modification.  six consists of only one Python file, so it is painless
to copy into a project.

Six can be downloaded on `PyPi <https://pypi.python.org/pypi/six/>`_.  Its bug
tracker and code hosting is on `GitHub <https://github.com/benjaminp/six>`_.

The name, "six", comes from the fact that 2*3 equals 6.  Why not addition?
Multiplication is more powerful, and, anyway, "five" has already been snatched
away by the (admittedly now moribund) Zope Five project.


Indices and tables
------------------

* :ref:`genindex`
* :ref:`search`


Package contents
----------------

.. data:: PY2

   A boolean indicating if the code is running on Python 2.

.. data:: PY3

   A boolean indicating if the code is running on Python 3.


Constants
>>>>>>>>>

Six provides constants that may differ between Python versions.  Ones ending
``_types`` are mostly useful as the second argument to ``isinstance`` or
``issubclass``.


.. data:: class_types

   Possible class types.  In Python 2, this encompasses old-style and new-style
   classes.  In Python 3, this is just new-styles.


.. data:: integer_types

   Possible integer types.  In Python 2, this is :func:`py2:long` and
   :func:`py2:int`, and in Python 3, just :func:`py3:int`.


.. data:: string_types

   Possible types for text data.  This is :func:`py2:basestring` in Python 2 and
   :func:`py3:str` in Python 3.


.. data:: text_type

   Type for representing (Unicode) textual data.  This is :func:`py2:unicode` in
   Python 2 and :func:`py3:str` in Python 3.


.. data:: binary_type

   Type for representing binary data.  This is :func:`py2:str` in Python 2 and
   :func:`py3:bytes` in Python 3.


.. data:: MAXSIZE

   The maximum  size of a  container like :func:`py3:list`  or :func:`py3:dict`.
   This  is  equivalent  to  :data:`py3:sys.maxsize` in  Python  2.6  and  later
   (including 3.x).   Note, this is temptingly  similar to, but not  the same as
   :data:`py2:sys.maxint`  in  Python  2.   There is  no  direct  equivalent  to
   :data:`py2:sys.maxint` in  Python 3  because its integer  type has  no limits
   aside from memory.


Here's example usage of the module::

   import six

   def dispatch_types(value):
       if isinstance(value, six.integer_types):
           handle_integer(value)
       elif isinstance(value, six.class_types):
           handle_class(value)
       elif isinstance(value, six.string_types):
           handle_string(value)


Object model compatibility
>>>>>>>>>>>>>>>>>>>>>>>>>>

Python 3 renamed the attributes of several interpreter data structures.  The
following accessors are available.  Note that the recommended way to inspect
functions and methods is the stdlib :mod:`py3:inspect` module.


.. function:: get_unbound_function(meth)

   Get the function out of unbound method *meth*.  In Python 3, unbound methods
   don't exist, so this function just returns *meth* unchanged.  Example
   usage::

      from six import get_unbound_function

      class X(object):
          def method(self):
              pass
      method_function = get_unbound_function(X.method)


.. function:: get_method_function(meth)

   Get the function out of method object *meth*.


.. function:: get_method_self(meth)

   Get the ``self`` of bound method *meth*.


.. function:: get_function_closure(func)

   Get the closure (list of cells) associated with *func*.  This is equivalent
   to ``func.__closure__`` on Python 2.6+ and ``func.func_closure`` on Python
   2.5.


.. function:: get_function_code(func)

   Get the code object associated with *func*.  This is equivalent to
   ``func.__code__`` on Python 2.6+ and ``func.func_code`` on Python 2.5.


.. function:: get_function_defaults(func)

   Get the defaults tuple associated with *func*.  This is equivalent to
   ``func.__defaults__`` on Python 2.6+ and ``func.func_defaults`` on Python
   2.5.


.. function:: get_function_globals(func)

   Get the globals of *func*.  This is equivalent to ``func.__globals__`` on
   Python 2.6+ and ``func.func_globals`` on Python 2.5.


.. function:: next(it)
              advance_iterator(it)

   Get the next item of iterator *it*.  :exc:`py3:StopIteration` is raised if
   the iterator is exhausted.  This is a replacement for calling ``it.next()``
   in Python 2 and ``next(it)`` in Python 3.  Python 2.6 and above have a
   builtin ``next`` function, so six's version is only necessary for Python 2.5
   compatibility.


.. function:: callable(obj)

   Check if *obj* can be called.  Note ``callable`` has returned in Python 3.2,
   so using six's version is only necessary when supporting Python 3.0 or 3.1.


.. function:: iterkeys(dictionary, **kwargs)

   Returns an iterator over *dictionary*\'s keys. This replaces
   ``dictionary.iterkeys()`` on Python 2 and ``dictionary.keys()`` on
   Python 3.  *kwargs* are passed through to the underlying method.


.. function:: itervalues(dictionary, **kwargs)

   Returns an iterator over *dictionary*\'s values. This replaces
   ``dictionary.itervalues()`` on Python 2 and ``dictionary.values()`` on
   Python 3.  *kwargs* are passed through to the underlying method.


.. function:: iteritems(dictionary, **kwargs)

   Returns an iterator over *dictionary*\'s items. This replaces
   ``dictionary.iteritems()`` on Python 2 and ``dictionary.items()`` on
   Python 3.  *kwargs* are passed through to the underlying method.


.. function:: iterlists(dictionary, **kwargs)

   Calls ``dictionary.iterlists()`` on Python 2 and ``dictionary.lists()`` on
   Python 3.  No builtin Python mapping type has such a method; this method is
   intended for use with multi-valued dictionaries like `Werkzeug's
   <http://werkzeug.pocoo.org/docs/datastructures/#werkzeug.datastructures.MultiDict>`_.
   *kwargs* are passed through to the underlying method.


.. function:: viewkeys(dictionary)

   Return a view over *dictionary*\'s keys. This replaces
   :meth:`py2:dict.viewkeys` on Python 2.7 and :meth:`py3:dict.keys` on
   Python 3.


.. function:: viewvalues(dictionary)

   Return a view over *dictionary*\'s values. This replaces
   :meth:`py2:dict.viewvalues` on Python 2.7 and :meth:`py3:dict.values` on
   Python 3.


.. function:: viewitems(dictionary)

   Return a view over *dictionary*\'s items. This replaces
   :meth:`py2:dict.viewitems` on Python 2.7 and :meth:`py3:dict.items` on
   Python 3.


.. function:: create_bound_method(func, obj)

   Return a method object wrapping *func* and bound to *obj*.  On both Python 2
   and 3, this will return a :func:`py3:types.MethodType` object.  The reason
   this wrapper exists is that on Python 2, the ``MethodType`` constructor
   requires the *obj*'s class to be passed.


.. function:: create_unbound_method(func, cls)

   Return an unbound method object wrapping *func*.  In Python 2, this will
   return a :func:`py2:types.MethodType` object.  In Python 3, unbound methods
   do not exist and this wrapper will simply return *func*.


.. class:: Iterator

   A class for making portable iterators. The intention is that it be subclassed
   and subclasses provide a ``__next__`` method. In Python 2, :class:`Iterator`
   has one method: ``next``. It simply delegates to ``__next__``. An alternate
   way to do this would be to simply alias ``next`` to ``__next__``. However,
   this interacts badly with subclasses that override
   ``__next__``. :class:`Iterator` is empty on Python 3. (In fact, it is just
   aliased to :class:`py3:object`.)


.. decorator:: wraps(wrapped, assigned=functools.WRAPPER_ASSIGNMENTS, updated=functools.WRAPPER_UPDATES)

   This is exactly the :func:`py3:functools.wraps` decorator, but it sets the
   ``__wrapped__`` attribute on what it decorates as :func:`py3:functools.wraps`
   does on Python versions after 3.2.


Syntax compatibility
>>>>>>>>>>>>>>>>>>>>

These functions smooth over operations which have different syntaxes between
Python 2 and 3.


.. function:: exec_(code, globals=None, locals=None)

   Execute *code* in the scope of *globals* and *locals*.  *code* can be a
   string or a code object.  If *globals* or *locals* are not given, they will
   default to the scope of the caller.  If just *globals* is given, it will also
   be used as *locals*.

   .. note::

      Python 3's :func:`py3:exec` doesn't take keyword arguments, so calling
      :func:`exec` with them should be avoided.


.. function:: print_(*args, *, file=sys.stdout, end="\\n", sep=" ", flush=False)

   Print *args* into *file*.  Each argument will be separated with *sep* and
   *end* will be written to the file after the last argument is printed.  If
   *flush* is true, ``file.flush()`` will be called after all data is written.

   .. note::

      In Python 2, this function imitates Python 3's :func:`py3:print` by not
      having softspace support.  If you don't know what that is, you're probably
      ok. :)


.. function:: raise_from(exc_value, exc_value_from)

   Raise an exception from a context.  On Python 3, this is equivalent to
   ``raise exc_value from exc_value_from``.  On Python 2, which does not support
   exception chaining, it is equivalent to ``raise exc_value``.


.. function:: reraise(exc_type, exc_value, exc_traceback=None)

   Reraise an exception, possibly with a different traceback.  In the simple
   case, ``reraise(*sys.exc_info())`` with an active exception (in an except
   block) reraises the current exception with the last traceback.  A different
   traceback can be specified with the *exc_traceback* parameter.  Note that
   since the exception reraising is done within the :func:`reraise` function,
   Python will attach the call frame of :func:`reraise` to whatever traceback is
   raised.


.. function:: with_metaclass(metaclass, *bases)

   Create a new class with base classes *bases* and metaclass *metaclass*.  This
   is designed to be used in class declarations like this: ::

      from six import with_metaclass

      class Meta(type):
          pass

      class Base(object):
          pass

      class MyClass(with_metaclass(Meta, Base)):
          pass

   Another way to set a metaclass on a class is with the :func:`add_metaclass`
   decorator.


.. decorator:: add_metaclass(metaclass)

   Class decorator that replaces a normally-constructed class with a
   metaclass-constructed one.  Example usage: ::

       @add_metaclass(Meta)
       class MyClass(object):
           pass

   That code produces a class equivalent to ::

       class MyClass(object, metaclass=Meta):
           pass

   on Python 3 or ::

       class MyClass(object):
           __metaclass__ = Meta

   on Python 2.

   Note that class decorators require Python 2.6. However, the effect of the
   decorator can be emulated on Python 2.5 like so::

       class MyClass(object):
           pass
       MyClass = add_metaclass(Meta)(MyClass)


Binary and text data
>>>>>>>>>>>>>>>>>>>>

Python 3 enforces the distinction between byte strings and text strings far more
rigorously than Python 2 does; binary data cannot be automatically coerced to
or from text data.  six provides several functions to assist in classifying
string data in all Python versions.


.. function:: b(data)

   A "fake" bytes literal.  *data* should always be a normal string literal.  In
   Python 2, :func:`b` returns a 8-bit string.  In Python 3, *data* is encoded
   with the latin-1 encoding to bytes.


   .. note::

      Since all Python versions 2.6 and after support the ``b`` prefix,
      code without 2.5 support doesn't need :func:`b`.


.. function:: u(text)

   A "fake" unicode literal.  *text* should always be a normal string literal.
   In Python 2, :func:`u` returns unicode, and in Python 3, a string.  Also, in
   Python 2, the string is decoded with the ``unicode-escape`` codec, which
   allows unicode escapes to be used in it.


   .. note::

      In Python 3.3, the ``u`` prefix has been reintroduced. Code that only
      supports Python 3 versions of 3.3 and higher thus does not need
      :func:`u`.

   .. note::

      On Python 2, :func:`u` doesn't know what the encoding of the literal
      is. Each byte is converted directly to the unicode codepoint of the same
      value. Because of this, it's only safe to use :func:`u` with strings of
      ASCII data.


.. function:: unichr(c)

   Return the (Unicode) string representing the codepoint *c*.  This is
   equivalent to :func:`py2:unichr` on Python 2 and :func:`py3:chr` on Python 3.


.. function:: int2byte(i)

   Converts *i* to a byte.  *i* must be in ``range(0, 256)``.  This is
   equivalent to :func:`py2:chr` in Python 2 and ``bytes((i,))`` in Python 3.


.. function:: byte2int(bs)

   Converts the first byte of *bs* to an integer.  This is equivalent to
   ``ord(bs[0])`` on Python 2 and ``bs[0]`` on Python 3.


.. function:: indexbytes(buf, i)

   Return the byte at index *i* of *buf* as an integer.  This is equivalent to
   indexing a bytes object in Python 3.


.. function:: iterbytes(buf)

   Return an iterator over bytes in *buf* as integers.  This is equivalent to
   a bytes object iterator in Python 3.


.. data:: StringIO

   This is a fake file object for textual data.  It's an alias for
   :class:`py2:StringIO.StringIO` in Python 2 and :class:`py3:io.StringIO` in
   Python 3.


.. data:: BytesIO

   This is a fake file object for binary data.  In Python 2, it's an alias for
   :class:`py2:StringIO.StringIO`, but in Python 3, it's an alias for
   :class:`py3:io.BytesIO`.


.. decorator:: python_2_unicode_compatible

   A class decorator that takes a class defining a ``__str__`` method.  On
   Python 3, the decorator does nothing.  On Python 2, it aliases the
   ``__str__`` method to ``__unicode__`` and creates a new ``__str__`` method
   that returns the result of ``__unicode__()`` encoded with UTF-8.


unittest assertions
>>>>>>>>>>>>>>>>>>>

Six contains compatibility shims for unittest assertions that have been renamed.
The parameters are the same as their aliases, but you must pass the test method
as the first argument. For example::

    import six
    import unittest

    class TestAssertCountEqual(unittest.TestCase):
        def test(self):
            six.assertCountEqual(self, (1, 2), [2, 1])

Note these functions are only available on Python 2.7 or later.

.. function:: assertCountEqual()

   Alias for :meth:`~py3:unittest.TestCase.assertCountEqual` on Python 3 and
   :meth:`~py2:unittest.TestCase.assertItemsEqual` on Python 2.


.. function:: assertRaisesRegex()

   Alias for :meth:`~py3:unittest.TestCase.assertRaisesRegex` on Python 3 and
   :meth:`~py2:unittest.TestCase.assertRaisesRegexp` on Python 2.


.. function:: assertRegex()

   Alias for :meth:`~py3:unittest.TestCase.assertRegex` on Python 3 and
   :meth:`~py2:unittest.TestCase.assertRegexpMatches` on Python 2.


Renamed modules and attributes compatibility
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>

.. module:: six.moves
   :synopsis: Renamed modules and attributes compatibility

Python 3 reorganized the standard library and moved several functions to
different modules.  Six provides a consistent interface to them through the fake
:mod:`six.moves` module.  For example, to load the module for parsing HTML on
Python 2 or 3, write::

   from six.moves import html_parser

Similarly, to get the function to reload modules, which was moved from the
builtin module to the ``imp`` module, use::

   from six.moves import reload_module

For the most part, :mod:`six.moves` aliases are the names of the modules in
Python 3.  When the new Python 3 name is a package, the components of the name
are separated by underscores.  For example, ``html.parser`` becomes
``html_parser``.  In some cases where several modules have been combined, the
Python 2 name is retained.  This is so the appropriate modules can be found when
running on Python 2.  For example, ``BaseHTTPServer`` which is in
``http.server`` in Python 3 is aliased as ``BaseHTTPServer``.

Some modules which had two implementations have been merged in Python 3.  For
example, ``cPickle`` no longer exists in Python 3; it was merged with
``pickle``.  In these cases, fetching the fast version will load the fast one on
Python 2 and the merged module in Python 3.

The :mod:`py2:urllib`, :mod:`py2:urllib2`, and :mod:`py2:urlparse` modules have
been combined in the :mod:`py3:urllib` package in Python 3.  The
:mod:`six.moves.urllib` package is a version-independent location for this
functionality; its structure mimics the structure of the Python 3
:mod:`py3:urllib` package.

.. note::

   In order to make imports of the form::

     from six.moves.cPickle import loads

   work, six places special proxy objects in :data:`py3:sys.modules`. These
   proxies lazily load the underlying module when an attribute is fetched. This
   will fail if the underlying module is not available in the Python
   interpreter. For example, ``sys.modules["six.moves.winreg"].LoadKey`` would
   fail on any non-Windows platform. Unfortunately, some applications try to
   load attributes on every module in :data:`py3:sys.modules`. six mitigates
   this problem for some applications by pretending attributes on unimportable
   modules do not exist. This hack does not work in every case, though. If you are
   encountering problems with the lazy modules and don't use any from imports
   directly from ``six.moves`` modules, you can workaround the issue by removing
   the six proxy modules::

     d = [name for name in sys.modules if name.startswith("six.moves.")]
     for name in d:
         del sys.modules[name]

Supported renames:

+------------------------------+-------------------------------------+---------------------------------------+
| Name                         | Python 2 name                       | Python 3 name                         |
+==============================+=====================================+=======================================+
| ``builtins``                 | :mod:`py2:__builtin__`              | :mod:`py3:builtins`                   |
+------------------------------+-------------------------------------+---------------------------------------+
| ``configparser``             | :mod:`py2:ConfigParser`             | :mod:`py3:configparser`               |
+------------------------------+-------------------------------------+---------------------------------------+
| ``copyreg``                  | :mod:`py2:copy_reg`                 | :mod:`py3:copyreg`                    |
+------------------------------+-------------------------------------+---------------------------------------+
| ``cPickle``                  | :mod:`py2:cPickle`                  | :mod:`py3:pickle`                     |
+------------------------------+-------------------------------------+---------------------------------------+
| ``cStringIO``                | :func:`py2:cStringIO.StringIO`      | :class:`py3:io.StringIO`              |
+------------------------------+-------------------------------------+---------------------------------------+
| ``dbm_gnu``                  | :func:`py2:gdbm`                    | :class:`py3:dbm.gnu`                  |
+------------------------------+-------------------------------------+---------------------------------------+
| ``_dummy_thread``            | :mod:`py2:dummy_thread`             | :mod:`py3:_dummy_thread`              |
+------------------------------+-------------------------------------+---------------------------------------+
| ``email_mime_base``          | :mod:`py2:email.MIMEBase`           | :mod:`py3:email.mime.base`            |
+------------------------------+-------------------------------------+---------------------------------------+
| ``email_mime_image``         | :mod:`py2:email.MIMEImage`          | :mod:`py3:email.mime.image`           |
+------------------------------+-------------------------------------+---------------------------------------+
| ``email_mime_multipart``     | :mod:`py2:email.MIMEMultipart`      | :mod:`py3:email.mime.multipart`       |
+------------------------------+-------------------------------------+---------------------------------------+
| ``email_mime_nonmultipart``  | :mod:`py2:email.MIMENonMultipart`   | :mod:`py3:email.mime.nonmultipart`    |
+------------------------------+-------------------------------------+---------------------------------------+
| ``email_mime_text``          | :mod:`py2:email.MIMEText`           | :mod:`py3:email.mime.text`            |
+------------------------------+-------------------------------------+---------------------------------------+
| ``filter``                   | :func:`py2:itertools.ifilter`       | :func:`py3:filter`                    |
+------------------------------+-------------------------------------+---------------------------------------+
| ``filterfalse``              | :func:`py2:itertools.ifilterfalse`  | :func:`py3:itertools.filterfalse`     |
+------------------------------+-------------------------------------+---------------------------------------+
| ``getcwd``                   | :func:`py2:os.getcwdu`              | :func:`py3:os.getcwd`                 |
+------------------------------+-------------------------------------+---------------------------------------+
| ``getcwdb``                  | :func:`py2:os.getcwd`               | :func:`py3:os.getcwdb`                |
+------------------------------+-------------------------------------+---------------------------------------+
| ``getoutput``                | :func:`py2:commands.getoutput`      | :func:`py3:subprocess.getoutput`      |
+------------------------------+-------------------------------------+---------------------------------------+
| ``http_cookiejar``           | :mod:`py2:cookielib`                | :mod:`py3:http.cookiejar`             |
+------------------------------+-------------------------------------+---------------------------------------+
| ``http_cookies``             | :mod:`py2:Cookie`                   | :mod:`py3:http.cookies`               |
+------------------------------+-------------------------------------+---------------------------------------+
| ``html_entities``            | :mod:`py2:htmlentitydefs`           | :mod:`py3:html.entities`              |
+------------------------------+-------------------------------------+---------------------------------------+
| ``html_parser``              | :mod:`py2:HTMLParser`               | :mod:`py3:html.parser`                |
+------------------------------+-------------------------------------+---------------------------------------+
| ``http_client``              | :mod:`py2:httplib`                  | :mod:`py3:http.client`                |
+------------------------------+-------------------------------------+---------------------------------------+
| ``BaseHTTPServer``           | :mod:`py2:BaseHTTPServer`           | :mod:`py3:http.server`                |
+------------------------------+-------------------------------------+---------------------------------------+
| ``CGIHTTPServer``            | :mod:`py2:CGIHTTPServer`            | :mod:`py3:http.server`                |
+------------------------------+-------------------------------------+---------------------------------------+
| ``SimpleHTTPServer``         | :mod:`py2:SimpleHTTPServer`         | :mod:`py3:http.server`                |
+------------------------------+-------------------------------------+---------------------------------------+
| ``input``                    | :func:`py2:raw_input`               | :func:`py3:input`                     |
+------------------------------+-------------------------------------+---------------------------------------+
| ``intern``                   | :func:`py2:intern`                  | :func:`py3:sys.intern`                |
+------------------------------+-------------------------------------+---------------------------------------+
| ``map``                      | :func:`py2:itertools.imap`          | :func:`py3:map`                       |
+------------------------------+-------------------------------------+---------------------------------------+
| ``queue``                    | :mod:`py2:Queue`                    | :mod:`py3:queue`                      |
+------------------------------+-------------------------------------+---------------------------------------+
| ``range``                    | :func:`py2:xrange`                  | :func:`py3:range`                     |
+------------------------------+-------------------------------------+---------------------------------------+
| ``reduce``                   | :func:`py2:reduce`                  | :func:`py3:functools.reduce`          |
+------------------------------+-------------------------------------+---------------------------------------+
| ``reload_module``            | :func:`py2:reload`                  | :func:`py3:imp.reload`,               |
|                              |                                     | :func:`py3:importlib.reload`          |
|                              |                                     | on Python 3.4+                        |
+------------------------------+-------------------------------------+---------------------------------------+
| ``reprlib``                  | :mod:`py2:repr`                     | :mod:`py3:reprlib`                    |
+------------------------------+-------------------------------------+---------------------------------------+
| ``shlex_quote``              | :mod:`py2:pipes.quote`              | :mod:`py3:shlex.quote`                |
+------------------------------+-------------------------------------+---------------------------------------+
| ``socketserver``             | :mod:`py2:SocketServer`             | :mod:`py3:socketserver`               |
+------------------------------+-------------------------------------+---------------------------------------+
| ``_thread``                  | :mod:`py2:thread`                   | :mod:`py3:_thread`                    |
+------------------------------+-------------------------------------+---------------------------------------+
| ``tkinter``                  | :mod:`py2:Tkinter`                  | :mod:`py3:tkinter`                    |
+------------------------------+-------------------------------------+---------------------------------------+
| ``tkinter_dialog``           | :mod:`py2:Dialog`                   | :mod:`py3:tkinter.dialog`             |
+------------------------------+-------------------------------------+---------------------------------------+
| ``tkinter_filedialog``       | :mod:`py2:FileDialog`               | :mod:`py3:tkinter.FileDialog`         |
+------------------------------+-------------------------------------+---------------------------------------+
| ``tkinter_scrolledtext``     | :mod:`py2:ScrolledText`             | :mod:`py3:tkinter.scrolledtext`       |
+------------------------------+-------------------------------------+---------------------------------------+
| ``tkinter_simpledialog``     | :mod:`py2:SimpleDialog`             | :mod:`py3:tkinter.simpledialog`       |
+------------------------------+-------------------------------------+---------------------------------------+
| ``tkinter_ttk``              | :mod:`py2:ttk`                      | :mod:`py3:tkinter.ttk`                |
+------------------------------+-------------------------------------+---------------------------------------+
| ``tkinter_tix``              | :mod:`py2:Tix`                      | :mod:`py3:tkinter.tix`                |
+------------------------------+-------------------------------------+---------------------------------------+
| ``tkinter_constants``        | :mod:`py2:Tkconstants`              | :mod:`py3:tkinter.constants`          |
+------------------------------+-------------------------------------+---------------------------------------+
| ``tkinter_dnd``              | :mod:`py2:Tkdnd`                    | :mod:`py3:tkinter.dnd`                |
+------------------------------+-------------------------------------+---------------------------------------+
| ``tkinter_colorchooser``     | :mod:`py2:tkColorChooser`           | :mod:`py3:tkinter.colorchooser`       |
+------------------------------+-------------------------------------+---------------------------------------+
| ``tkinter_commondialog``     | :mod:`py2:tkCommonDialog`           | :mod:`py3:tkinter.commondialog`       |
+------------------------------+-------------------------------------+---------------------------------------+
| ``tkinter_tkfiledialog``     | :mod:`py2:tkFileDialog`             | :mod:`py3:tkinter.filedialog`         |
+------------------------------+-------------------------------------+---------------------------------------+
| ``tkinter_font``             | :mod:`py2:tkFont`                   | :mod:`py3:tkinter.font`               |
+------------------------------+-------------------------------------+---------------------------------------+
| ``tkinter_messagebox``       | :mod:`py2:tkMessageBox`             | :mod:`py3:tkinter.messagebox`         |
+------------------------------+-------------------------------------+---------------------------------------+
| ``tkinter_tksimpledialog``   | :mod:`py2:tkSimpleDialog`           | :mod:`py3:tkinter.simpledialog`       |
+------------------------------+-------------------------------------+---------------------------------------+
| ``urllib.parse``             | See :mod:`six.moves.urllib.parse`   | :mod:`py3:urllib.parse`               |
+------------------------------+-------------------------------------+---------------------------------------+
| ``urllib.error``             | See :mod:`six.moves.urllib.error`   | :mod:`py3:urllib.error`               |
+------------------------------+-------------------------------------+---------------------------------------+
| ``urllib.request``           | See :mod:`six.moves.urllib.request` | :mod:`py3:urllib.request`             |
+------------------------------+-------------------------------------+---------------------------------------+
| ``urllib.response``          | See :mod:`six.moves.urllib.response`| :mod:`py3:urllib.response`            |
+------------------------------+-------------------------------------+---------------------------------------+
| ``urllib.robotparser``       | :mod:`py2:robotparser`              | :mod:`py3:urllib.robotparser`         |
+------------------------------+-------------------------------------+---------------------------------------+
| ``urllib_robotparser``       | :mod:`py2:robotparser`              | :mod:`py3:urllib.robotparser`         |
+------------------------------+-------------------------------------+---------------------------------------+
| ``UserDict``                 | :class:`py2:UserDict.UserDict`      | :class:`py3:collections.UserDict`     |
+------------------------------+-------------------------------------+---------------------------------------+
| ``UserList``                 | :class:`py2:UserList.UserList`      | :class:`py3:collections.UserList`     |
+------------------------------+-------------------------------------+---------------------------------------+
| ``UserString``               | :class:`py2:UserString.UserString`  | :class:`py3:collections.UserString`   |
+------------------------------+-------------------------------------+---------------------------------------+
| ``winreg``                   | :mod:`py2:_winreg`                  | :mod:`py3:winreg`                     |
+------------------------------+-------------------------------------+---------------------------------------+
| ``xmlrpc_client``            | :mod:`py2:xmlrpclib`                | :mod:`py3:xmlrpc.client`              |
+------------------------------+-------------------------------------+---------------------------------------+
| ``xmlrpc_server``            | :mod:`py2:SimpleXMLRPCServer`       | :mod:`py3:xmlrpc.server`              |
+------------------------------+-------------------------------------+---------------------------------------+
| ``xrange``                   | :func:`py2:xrange`                  | :func:`py3:range`                     |
+------------------------------+-------------------------------------+---------------------------------------+
| ``zip``                      | :func:`py2:itertools.izip`          | :func:`py3:zip`                       |
+------------------------------+-------------------------------------+---------------------------------------+
| ``zip_longest``              | :func:`py2:itertools.izip_longest`  | :func:`py3:itertools.zip_longest`     |
+------------------------------+-------------------------------------+---------------------------------------+

urllib parse
<<<<<<<<<<<<

.. module:: six.moves.urllib.parse
   :synopsis: Stuff from :mod:`py2:urlparse` and :mod:`py2:urllib` in Python 2 and :mod:`py3:urllib.parse` in Python 3

Contains functions from Python 3's :mod:`py3:urllib.parse` and Python 2's:

:mod:`py2:urlparse`:

* :func:`py2:urlparse.ParseResult`
* :func:`py2:urlparse.SplitResult`
* :func:`py2:urlparse.urlparse`
* :func:`py2:urlparse.urlunparse`
* :func:`py2:urlparse.parse_qs`
* :func:`py2:urlparse.parse_qsl`
* :func:`py2:urlparse.urljoin`
* :func:`py2:urlparse.urldefrag`
* :func:`py2:urlparse.urlsplit`
* :func:`py2:urlparse.urlunsplit`
* :func:`py2:urlparse.splitquery`
* :func:`py2:urlparse.uses_fragment`
* :func:`py2:urlparse.uses_netloc`
* :func:`py2:urlparse.uses_params`
* :func:`py2:urlparse.uses_query`
* :func:`py2:urlparse.uses_relative`

and :mod:`py2:urllib`:

* :func:`py2:urllib.quote`
* :func:`py2:urllib.quote_plus`
* :func:`py2:urllib.splittag`
* :func:`py2:urllib.splituser`
* :func:`py2:urllib.splitvalue`
* :func:`py2:urllib.unquote` (also exposed as :func:`py3:urllib.parse.unquote_to_bytes`)
* :func:`py2:urllib.unquote_plus`
* :func:`py2:urllib.urlencode`


urllib error
<<<<<<<<<<<<

.. module:: six.moves.urllib.error
   :synopsis: Stuff from :mod:`py2:urllib` and :mod:`py2:urllib2` in Python 2 and :mod:`py3:urllib.error` in Python 3

Contains exceptions from Python 3's :mod:`py3:urllib.error` and Python 2's:

:mod:`py2:urllib`:

* :exc:`py2:urllib.ContentTooShortError`

and :mod:`py2:urllib2`:

* :exc:`py2:urllib2.URLError`
* :exc:`py2:urllib2.HTTPError`


urllib request
<<<<<<<<<<<<<<

.. module:: six.moves.urllib.request
   :synopsis: Stuff from :mod:`py2:urllib` and :mod:`py2:urllib2` in Python 2 and :mod:`py3:urllib.request` in Python 3

Contains items from Python 3's :mod:`py3:urllib.request` and Python 2's:

:mod:`py2:urllib`:

* :func:`py2:urllib.pathname2url`
* :func:`py2:urllib.url2pathname`
* :func:`py2:urllib.getproxies`
* :func:`py2:urllib.urlretrieve`
* :func:`py2:urllib.urlcleanup`
* :class:`py2:urllib.URLopener`
* :class:`py2:urllib.FancyURLopener`
* :func:`py2:urllib.proxy_bypass`

and :mod:`py2:urllib2`:

* :func:`py2:urllib2.urlopen`
* :func:`py2:urllib2.install_opener`
* :func:`py2:urllib2.build_opener`
* :func:`py2:urllib2.parse_http_list`
* :func:`py2:urllib2.parse_keqv_list`
* :class:`py2:urllib2.Request`
* :class:`py2:urllib2.OpenerDirector`
* :class:`py2:urllib2.HTTPDefaultErrorHandler`
* :class:`py2:urllib2.HTTPRedirectHandler`
* :class:`py2:urllib2.HTTPCookieProcessor`
* :class:`py2:urllib2.ProxyHandler`
* :class:`py2:urllib2.BaseHandler`
* :class:`py2:urllib2.HTTPPasswordMgr`
* :class:`py2:urllib2.HTTPPasswordMgrWithDefaultRealm`
* :class:`py2:urllib2.AbstractBasicAuthHandler`
* :class:`py2:urllib2.HTTPBasicAuthHandler`
* :class:`py2:urllib2.ProxyBasicAuthHandler`
* :class:`py2:urllib2.AbstractDigestAuthHandler`
* :class:`py2:urllib2.HTTPDigestAuthHandler`
* :class:`py2:urllib2.ProxyDigestAuthHandler`
* :class:`py2:urllib2.HTTPHandler`
* :class:`py2:urllib2.HTTPSHandler`
* :class:`py2:urllib2.FileHandler`
* :class:`py2:urllib2.FTPHandler`
* :class:`py2:urllib2.CacheFTPHandler`
* :class:`py2:urllib2.UnknownHandler`
* :class:`py2:urllib2.HTTPErrorProcessor`


urllib response
<<<<<<<<<<<<<<<

.. module:: six.moves.urllib.response
   :synopsis: Stuff from :mod:`py2:urllib` in Python 2 and :mod:`py3:urllib.response` in Python 3

Contains classes from Python 3's :mod:`py3:urllib.response` and Python 2's:

:mod:`py2:urllib`:

* :class:`py2:urllib.addbase`
* :class:`py2:urllib.addclosehook`
* :class:`py2:urllib.addinfo`
* :class:`py2:urllib.addinfourl`


Advanced - Customizing renames
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

.. currentmodule:: six

It is possible to add additional names to the :mod:`six.moves` namespace.


.. function:: add_move(item)

   Add *item* to the :mod:`six.moves` mapping.  *item* should be a
   :class:`MovedAttribute` or :class:`MovedModule` instance.


.. function:: remove_move(name)

   Remove the :mod:`six.moves` mapping called *name*.  *name* should be a
   string.


Instances of the following classes can be passed to :func:`add_move`.  Neither
have any public members.


.. class:: MovedModule(name, old_mod, new_mod)

   Create a mapping for :mod:`six.moves` called *name* that references different
   modules in Python 2 and 3.  *old_mod* is the name of the Python 2 module.
   *new_mod* is the name of the Python 3 module.


.. class:: MovedAttribute(name, old_mod, new_mod, old_attr=None, new_attr=None)

   Create a mapping for :mod:`six.moves` called *name* that references different
   attributes in Python 2 and 3.  *old_mod* is the name of the Python 2 module.
   *new_mod* is the name of the Python 3 module.  If *new_attr* is not given, it
   defaults to *old_attr*.  If neither is given, they both default to *name*.
dateutil - powerful extensions to datetime
==========================================

.. image:: https://img.shields.io/travis/dateutil/dateutil/master.svg?style=flat-square
    :target: https://travis-ci.org/dateutil/dateutil
    :alt: travis build status

.. image:: https://img.shields.io/appveyor/ci/dateutil/dateutil/master.svg?style=flat-square
    :target: https://ci.appveyor.com/project/dateutil/dateutil
    :alt: appveyor build status

.. image:: https://codecov.io/github/dateutil/dateutil/coverage.svg?branch=master
    :target: https://codecov.io/github/dateutil/dateutil?branch=master
    :alt: Code coverage

.. image:: https://img.shields.io/pypi/dd/python-dateutil.svg?style=flat-square
    :target: https://pypi.python.org/pypi/python-dateutil/
    :alt: pypi downloads per day

.. image:: https://img.shields.io/pypi/v/python-dateutil.svg?style=flat-square
    :target: https://pypi.python.org/pypi/python-dateutil/
    :alt: pypi version


The `dateutil` module provides powerful extensions to
the standard `datetime` module, available in Python.


Download
========
dateutil is available on PyPI
https://pypi.python.org/pypi/python-dateutil/

The documentation is hosted at:
https://dateutil.readthedocs.io/

Code
====
https://github.com/dateutil/dateutil/

Features
========

* Computing of relative deltas (next month, next year,
  next monday, last week of month, etc);
* Computing of relative deltas between two given
  date and/or datetime objects;
* Computing of dates based on very flexible recurrence rules,
  using a superset of the `iCalendar <https://www.ietf.org/rfc/rfc2445.txt>`_
  specification. Parsing of RFC strings is supported as well.
* Generic parsing of dates in almost any string format;
* Timezone (tzinfo) implementations for tzfile(5) format
  files (/etc/localtime, /usr/share/zoneinfo, etc), TZ
  environment string (in all known formats), iCalendar
  format files, given ranges (with help from relative deltas),
  local machine timezone, fixed offset timezone, UTC timezone,
  and Windows registry-based time zones.
* Internal up-to-date world timezone information based on
  Olson's database.
* Computing of Easter Sunday dates for any given year,
  using Western, Orthodox or Julian algorithms;
* A comprehensive test suite.

Quick example
=============
Here's a snapshot, just to give an idea about the power of the
package. For more examples, look at the documentation.

Suppose you want to know how much time is left, in
years/months/days/etc, before the next easter happening on a
year with a Friday 13th in August, and you want to get today's
date out of the "date" unix system command. Here is the code:

.. doctest:: readmeexample

    >>> from dateutil.relativedelta import *
    >>> from dateutil.easter import *
    >>> from dateutil.rrule import *
    >>> from dateutil.parser import *
    >>> from datetime import *
    >>> now = parse("Sat Oct 11 17:13:46 UTC 2003")
    >>> today = now.date()
    >>> year = rrule(YEARLY,dtstart=now,bymonth=8,bymonthday=13,byweekday=FR)[0].year
    >>> rdelta = relativedelta(easter(year), today)
    >>> print("Today is: %s" % today)
    Today is: 2003-10-11
    >>> print("Year with next Aug 13th on a Friday is: %s" % year)
    Year with next Aug 13th on a Friday is: 2004
    >>> print("How far is the Easter of that year: %s" % rdelta)
    How far is the Easter of that year: relativedelta(months=+6)
    >>> print("And the Easter of that year is: %s" % (today+rdelta))
    And the Easter of that year is: 2004-04-11

Being exactly 6 months ahead was **really** a coincidence :)


Author
======
The dateutil module was written by Gustavo Niemeyer <gustavo@niemeyer.net>
in 2003.

It is maintained by:

* Gustavo Niemeyer <gustavo@niemeyer.net> 2003-2011
* Tomi Pieviläinen <tomi.pievilainen@iki.fi> 2012-2014
* Yaron de Leeuw <me@jarondl.net> 2014-2016
* Paul Ganssle <paul@ganssle.io> 2015-

Our mailing list is available at `dateutil@python.org <https://mail.python.org/mailman/listinfo/dateutil>`_. As it is hosted by the PSF, it is subject to the `PSF code of
conduct <https://www.python.org/psf/codeofconduct/>`_.

Building and releasing
======================
When you get the source, it does not contain the internal zoneinfo
database. To get (and update) the database, run the updatezinfo.py script. Make sure
that the zic command is in your path, and that you have network connectivity
to get the latest timezone information from IANA, or from `our mirror of the
IANA database <https://dateutil.github.io/tzdata/>`_.

Starting with version 2.4.1, all source and binary distributions will be signed
by a PGP key that has, at the very least, been signed by the key which made the
previous release. A table of release signing keys can be found below:

===========  ============================
Releases     Signing key fingerprint
===========  ============================
2.4.1-       `6B49 ACBA DCF6 BD1C A206 67AB CD54 FCE3 D964 BEFB`_
===========  ============================

Testing
=======
dateutil has a comprehensive test suite, which can be run simply by running
`python setup.py test [-q]` in the project root. Note that if you don't have the internal
zoneinfo database, some tests will fail. Apart from that, all tests should pass.

To easily test dateutil against all supported Python versions, you can use
`tox <https://tox.readthedocs.io/en/latest/>`_.

All github pull requests are automatically tested using travis and appveyor.


.. _6B49 ACBA DCF6 BD1C A206 67AB CD54 FCE3 D964 BEFB:
   https://pgp.mit.edu/pks/lookup?op=vindex&search=0xCD54FCE3D964BEFB
0.21.0 (July 20, 2016)
======================

- Deprecate use of Device object as mapping from udev property names to values.
- Add a Properties class and a Device.properties() method for udev properties.
- Fix places where Device object was incorrectly used in a boolean context.
- Return an empty string, not None, if the property value is an empty string.
- Exceptions re-raised from libudev calls now have a message string.
- Insert a warning about using a Device in a boolean context in docs.
- Infrastructure for vagrant tests is removed.
- Various internal refactorings.
- Extensive test improvements.
- Numerous documentation fixes.

0.20.0 (April 29, 2016)
=======================

- Remove parsing code added in previous release.
- No longer do CI for Python 2.6.
- Eliminate all wildcard imports and __all__ statements.
- No longer use deprecated Device.from_sys_path() method.
- Minor pylint induced changes.
- Documentation fixes.

0.19.0 (Feb 3, 2016)
==================

- Restore raising KeyError by Attributes.as* methods when attribute not found.
- Explicitly require six module.
- Never raise a DeviceNotFoundError when iterating over a device enumeration.
- Device.subsystem() now returns None if device has no subsystem.
- Add DeprecationWarnings to deprecated Device methods.
- Replace "/" with "!" in Device.from_name() sys_name parameter.
- Add some unstable classes for parsing some kinds of values.
- Make version info more like Python's including micro numbers and levels.
- Refactor some internal modules into subdirectories.
- Work on tests and reproducers.

0.18 (Dec 1, 2015)
===================

- DeviceNotFoundError is no longer a subtype of LookupError
- Added support for pyqt5 monitor observer
- Added discover module, which looks up a device on limited information
- Attributes class no longer extends Mapping, extends object instead
- Attributes class no longer inherits [] operator, Mapping methods
- Attributes class objects are no longer iterable
- Attributes.available_attributes property added
- Attributes.get() method, with usual semantics, defined
- Device.from_* methods are deprecated, uses Devices.from_* methods instead
- Device.from_device_file() now raises DeviceNotFoundByFileError
- Device.from_device_number() now raises DeviceNotFoundByNumberError
- Devices.from_interface_index() method added
- Devices.from_kernel_device() method added
- Numerous testing infrastructure changes

0.17 (Aug 26, 2015)
=====================

- #52: Remove global libudev object
- #57: Really start the monitor on :meth:`pyudev.Monitor.poll()`
- #60: Do not use :meth:`select.select` to avoid hitting its file descriptor
  limit
- #58: Force non-blocking IO in :class:`pyudev.Monitor` to avoid blocking on
  receiving the device
- #63: Set proper flags on pipe fds.
- #65: Handle irregular polling events properly.
- #50: Add :class:`pyudev.wx.MonitorObserver` and deprecate
  :class:`pyudev.wx.WxUDevMonitorObserver`
- #50: Add :class:`pyudev.glib.MonitorObserver` and deprecate
  :class:`pyudev.glib.GUDevMonitorObserver`
- #50: Add :class:`pyudev.pyqt4.MonitorObserver` and deprecate
  :class:`pyudev.pyqt4.QUDevMonitorObserver`
- #50: Add :class:`pyudev.pyside.MonitorObserver` and deprecate
  :class:`pyudev.pyside.QUDevMonitorObserver`
- Add a wrapper function to retry interruptible system calls.


0.16.1 (Aug 02, 2012)
=====================

- #53: Fix source distribution
- #54: Fix typo in test


0.16 (Jul 25, 2012)
===================

- Remove :meth:`pyudev.Monitor.from_socket`.
- Deprecate :meth:`pyudev.Device.traverse()` in favor of
  :attr:`pyudev.Device.ancestors`.
- #47: Deprecate :meth:`pyudev.Monitor.receive_device` in favor of
  :attr:`pyudev.Monitor.poll`.
- #47: Deprecate :attr:`pyudev.Monitor.enable_receiving` in favor of
  :attr:`pyudev.Monitor.start`.
- #47: Deprecate :attr:`pyudev.Monitor.__iter__` in favor of explicit looping or
  :class:`pyudev.MonitorObserver`.
- #49: Deprecate ``event_handler`` to :class:`pyudev.MonitorObserver` in favour
  of ``callback`` argument.
- #46: Continuously test pyudev on Travis-CI.
- Add :attr:`pyudev.Device.ancestors`.
- Add :attr:`pyudev.Device.action`.
- #10: Add :attr:`pyudev.Device.sequence_number`.
- #47: Add :meth:`pyudev.Monitor.poll`.
- #47: Add :attr:`pyudev.Monitor.started`.
- #49: Add ``callback`` argument to :class:`pyudev.Monitor`.
- :meth:`pyudev.Monitor.start` can be called repeatedly.
- #45: Get rid of 2to3
- #43: Fix test failures on Python 2.6
- Fix signature in declaration of ``udev_monitor_set_receive_buffer_size``.
- #44: Test wrapped signatures with help of ``gccxml``.
- Fix compatibility with udev 183 and newer in :class:`pyudev.Context`.
- :meth:`pyudev.MonitorObserver.stop` can be called from the observer thread.


0.15 (Mar 1, 2012)
==================

- #20: Add :meth:`~pyudev.Monitor.remove_filter()`.
- #40: Add user guide to the documentation.
- #39: Add :meth:`pyudev.Device.from_device_file()`.
- :data:`errno.EINVAL` from underlying libudev functions causes
  :exc:`~exceptions.ValueError` instead of :exc:`~exceptions.EnvironmentError`.
- :class:`pyudev.MonitorObserver` calls
  :meth:`pyudev.Monitor.enable_receiving()` when started.
- #20: :meth:`pyudev.Monitor.filter_by()` and
  :meth:`pyudev.Monitor.filter_by_tag()` can be called after
  :meth:`pyudev.Monitor.enable_receiving()`.


0.14 (Feb 10, 2012)
===================

- Host documentation at http://pyudev.readthedocs.org (thanks to the
  readthedocs.org team for this service)
- #37: Add :class:`pyudev.wx.WxUDevMonitorObserver` for wxPython (thanks to
  Tobias Eberle).
- Add :class:`pyudev.MonitorObserver`.
- Add :attr:`pyudev.glib.GUDevMonitorObserver.enabled`,
  :attr:`pyudev.pyqt4.QUDevMonitorObserver.enabled` and
  :attr:`pyudev.pyside.QUDevMonitorObserver.enabled`.


0.13 (Nov 4, 2011)
==================

- #36: Add :meth:`pyudev.Monitor.set_receive_buffer_size` (thanks to Rémi
  Rérolle).
- Add :meth:`pyudev.Enumerator.match_parent`.
- Add ``parent`` keyword argument to :meth:`pyudev.Enumerator.match()`.
- #31: Add :meth:`pyudev.Enumerator.match_attribute`.
- Add ``nomatch`` argument to :meth:`pyudev.Enumerator.match_subsystem` and
  :meth:`pyudev.Enumerator.match_attribute`.
- Remove :meth:`pyudev.Enumerator.match_children` in favour of
  :meth:`pyudev.Enumerator.match_parent`.
- #34: :class:`pyudev.Device.tags` returns a :class:`pyudev.Tags` object.
- :attr:`pyudev.Device.children` requires udev version 172 now


0.12 (Aug 31, 2011)
===================

- #32: Fix memory leak.
- #33: Fix Python 3 support for :mod:`pyudev.glib`.
- Fix license header in :mod:`pyudev._compat`.


0.11 (Jun 26, 2011)
===================

- #30: Add :attr:`pyudev.Device.sys_number`.
- #29: Add :meth:`pyudev.Device.from_device_number`
- #29: Add :attr:`pyudev.Device.device_number`.
- Support PyPy.


0.10 (Apr 20, 2011)
===================

- Add :attr:`pyudev.__version_info__`
- Add :attr:`pyudev.Device.device_type`
- :class:`pyudev.Context`, :class:`pyudev.Enumerator`, :class:`pyudev.Device`
  and :class:`pyudev.Monitor` can directly be passed to
  :mod:`ctypes`-wrapped functions.
- #24: Add :attr:`pyudev.Context.run_path`.


0.9 (Mar 09, 2011)
==================

- #21: Add :meth:`pyudev.Device.find_parent`.
- #22: Add :meth:`pyudev.Monitor.filter_by_tag`.
- Add :attr:`pyudev.Context.log_priority`.
- Improve error reporting, if libudev is missing.


0.8 (Jan 08, 2011)
==================

- #16: Add :meth:`pyudev.Enumerator.match`.
- Add keyword arguments to :meth:`pyudev.Context.list_devices()`.
- #19: Add :meth:`pyudev.Enumerator.match_sys_name`.
- #18: Add :func:`pyudev.udev_version()`.
- #17: Add :attr:`pyudev.Device.is_initialized`.
- #17: Add :attr:`pyudev.Device.time_since_initialized`.
- #17: Add :meth:`pyudev.Enumerator.match_is_initialized`
- Fix support for earlier releases of udev.
- Document minimum udev version for all affected attributes.


0.7 (Nov 15, 2010)
==================

- #15: Add :mod:`pyudev.glib.GUDevMonitorObserver`.


0.6 (Oct 03, 2010)
==================

- #8: Add :attr:`pyudev.Device.tags`.
- #8: Add :meth:`pyudev.Enumerator.match_tag`.
- #11: Add :meth:`pyudev.Device.from_environment`
- #5: Add :mod:`pyudev.pyside`
- #14: Remove apipkg_ dependency.
- #14: Require explicit import of :mod:`pyudev.pyqt4`.
- Fix licence headers in source files.

.. _apipkg: http://pypi.python.org/pypi/apipkg/


0.5 (Sep 06, 2010)
==================

- Support Python 3.
- #6: Add :attr:`pyudev.Device.attributes` (thanks to Daniel Lazzari).
- #6: Add :class:`pyudev.Attributes` (thanks to Daniel Lazzari).
- #7: :attr:`pyudev.Device.context` and :attr:`pyudev.Monitor.context` are
  part of the public API.
- #9: Add :attr:`pyudev.Device.driver`.
- #12: Add :meth:`pyudev.Device.from_name`.
- Rename :exc:`pyudev.NoSuchDeviceError` to :exc:`pyudev.DeviceNotFoundError`.
- :meth:`pyudev.Device.from_sys_path` raises
  :exc:`pyudev.DeviceNotFoundAtPathError`.
- #13: Fix :exc:`~exceptions.AttributeError` in
  :attr:`pyudev.Device.device_node`.
- Improve and extend documentation.
- Add more tests.


0.4 (Aug 23, 2010)
==================

API changes
-----------

- #3: Rename :mod:`udev` to :mod:`pyudev`.
- #3: Rename :mod:`qudev` to :mod:`pyudev.pyqt4`.
- Add :meth:`pyudev.Device.from_path`.
- :meth:`pyudev.Device.from_sys_path` raises :exc:`pyudev.NoSuchDeviceError`.
- :meth:`pyudev.Monitor.receive_device` raises
  :exc:`~exceptions.EnvironmentError`.
- ``errno``, ``strerror`` and ``filename`` attributes of
  :class:`~exceptions.EnvironmentError` exceptions have meaningful content.
- Fix :exc:`~exceptions.NameError` in :meth:`pyudev.Monitor.from_socket`
- ``subsystem`` argument to :meth:`pyudev.Monitor.filter_by` is mandatory.
- Delete underlying C objects if :class:`pyudev.Device` is garbage-collected.
- Fix broken signal emitting in :class:`pyudev.pyqt4.QUDevMonitorObserver`.


0.3 (Jul 28, 2010)
==================

- #1: Fix documentation to reflect the actual behaviour of the underlying
  API
- Raise :exc:`~exceptions.TypeError` if :class:`udev.Device` are compared with
  ``>``, ``>=``, ``<`` or ``<=``.
- Add :meth:`udev.Enumerator.match_children`.
- Add :attr:`udev.Device.children`.
- Add :meth:`qudev.QUDevMonitorObserver.deviceChanged`.
- Add :meth:`qudev.QUDevMonitorObserver.deviceMoved`.


0.2 (Jun 28, 2010)
==================

- Add :class:`udev.Monitor`.
- Add :meth:`udev.Device.asbool`.
- Add :meth:`udev.Device.asint`.
- Remove type magic in :meth:`udev.Device.__getitem__`.
- Add :mod:`qudev`.


0.1 (May 03, 2010)
==================

- Initial release.
- Add :class:`udev.Context`.
- Add :class:`udev.Device`.
- Add :class:`udev.Enumerator`.
######
pyudev
######

.. image:: https://secure.travis-ci.org/pyudev/pyudev.png?branch=develop
   :target: http://travis-ci.org/pyudev/pyudev

http://pyudev.readthedocs.org

pyudev is a LGPL_ licensed, pure Python_ binding for libudev_, the device and
hardware management and information library for Linux.  It supports almost all
libudev_ functionality. You can enumerate devices, query device properties and
attributes or monitor devices, including asynchronous monitoring with threads,
or within the event loops of Qt, Glib or wxPython.

The binding supports CPython_ 2 (2.6 or newer) and 3 (3.1 or newer), and PyPy_
1.5 or newer.  It is tested against udev 151 or newer, earlier versions of udev
as found on dated Linux systems may work, but are not officially supported.


Usage
-----

Usage of pyudev is quite simply thanks to the power of the underlying udev
library. Getting the labels of all partitions just takes a few lines:

>>> import pyudev
>>> context = pyudev.Context()
>>> for device in context.list_devices(subsystem='block', DEVTYPE='partition'):
...     print(device.get('ID_FS_LABEL', 'unlabeled partition'))
...
boot
swap
system

The website_ provides a detailed `user guide`_ and a complete `API reference`_.


Support
-------

Please report issues and questions to the issue tracker, but respect the
following guidelines:

- Check that the issue has not already been reported.
- Check that the issue is not already fixed in the ``master`` branch.
- Open issues with clear title and a detailed description in grammatically
  correct, complete sentences.
- Include the Python version and the udev version (see ``udevadm --version``) in
  the description of your issue.


Development
-----------

The source code is hosted on GitHub_::

   git clone git://github.com/pyudev/pyudev.git

Please fork the repository and send pull requests with your fixes or new
features, but respect the following guidelines:

- Read `how to properly contribute to open source projects on GitHub
  <http://gun.io/blog/how-to-github-fork-branch-and-pull-request/>`_.
- Understand the `branching model
  <http://nvie.com/posts/a-successful-git-branching-model/>`_.
- Use a topic branch based on the ``develop`` branch to easily amend a pull
  request later, if necessary.
- Write `good commit messages
  <http://tbaggery.com/2008/04/19/a-note-about-git-commit-messages.html>`_.
- Squash commits on the topic branch before opening a pull request.
- Respect :pep:`8` (use pep8_ to check your coding style compliance).
- Add unit tests if possible (refer to the `testsuite documentation
  <http://pyudev.readthedocs.org/en/latest/tests/index.html>`_).
- Add API documentation in docstrings.
- Open a `pull request <https://help.github.com/articles/using-pull-requests>`_
  that relates to but one subject with a clear title and description in
  grammatically correct, complete sentences.


.. _LGPL: http://www.gnu.org/licenses/old-licenses/lgpl-2.1.html
.. _Python: http://www.python.org/
.. _CPython: http://www.python.org/
.. _PyPy: http://www.pypy.org/
.. _libudev: http://www.kernel.org/pub/linux/utils/kernel/hotplug/libudev/
.. _website: http://pyudev.readthedocs.org
.. _user guide: http://pyudev.readthedocs.org/en/latest/guide.html
.. _api reference: http://pyudev.readthedocs.org/en/latest/api/index.html
.. _issue tracker: http://github.com/lunaryorn/pyudev/issues
.. _GitHub: http://github.com/lunaryorn/pyudev
.. _git: http://www.git-scm.com/
.. _pep8: http://pypi.python.org/pypi/pep8/
##################
 Core DNF Plugins
##################

Core plugins to use with `DNF package manager <https://github.com/rpm-software-management/dnf>`_.

==============
 Installation
==============

RPM packages are available in official Fedora repositories::

   dnf install dnf-plugins-core

Nigthly builds can be installed from `copr repository <https://copr.fedorainfracloud.org/coprs/rpmsoftwaremanagement/dnf-nightly/>`_.


======================
 Building from source
======================

From the DNF git checkout directory::

    mkdir build;
    pushd build;
    cmake .. && make;
    popd;

Then to run DNF::

    PYTHONPATH=`readlink -f .` bin/dnf <arguments>

===============
 Running tests
===============

From the DNF git checkout directory::

    mkdir build;
    pushd build;
    cmake .. && make ARGS="-V" test;
    popd;

==============
 Contribution
==============

Here's the most direct way to get your work merged into the project.

1. Fork the project
#. Clone down your fork
#. Implement your feature or bug fix and commit changes
#. If the change fixes a bug at `Red Hat bugzilla <https://bugzilla.redhat.com/>`_, or if it is important to the end user, add the following block to the commit message::

    = changelog =
    msg:           message to be included in the changelog
    type:          one of: bugfix/enhancement/security (this field is required when message is present)
    resolves:      URLs to bugs or issues resolved by this commit (can be specified multiple times)
    related:       URLs to any related bugs or issues (can be specified multiple times)

   * For example::

       = changelog =
       msg: [download] Respect repo priority
       type: bugfix
       resolves: https://bugzilla.redhat.com/show_bug.cgi?id=1800342

   * For your convenience, you can also use git commit template by running the following command in the top-level directory of this project::

       git config commit.template ./.git-commit-template

#. In a separate commit, add your name and email under ``DNF-PLUGINS-CORE CONTRIBUTORS`` section in the `authors file <https://github.com/rpm-software-management/dnf-plugins-core/blob/master/AUTHORS>`_ as a reward for your generosity
#. Push the branch up to your fork
#. Send a pull request for your branch

Please do not create pull requests with translation (.po) file improvements. Fix the translation on `Fedora Weblate <https://translate.fedoraproject.org/projects/dnf/>`_ instead.

===============
 Documentation
===============

The DNF-PLUGINS-CORE package distribution contains man pages ``dnf.plugin.*(8)``. It is also possible to read the `DNF-PLUGINS-CORE documentation <http://dnf-plugins-core.readthedocs.org>`_ online.

====================
 Bug reporting etc.
====================

Please report discovered bugs to the `Red Hat bugzilla <https://bugzilla.redhat.com/>`_ following this `guide <https://github.com/rpm-software-management/dnf/wiki/Bug-Reporting>`_. If you planed to propose the patch in the report, consider `contribution`_ instead.

Freenode's irc channel ``#yum`` is meant for discussions related to both Yum and DNF. Questions should be asked there, issues discussed. Remember: ``#yum`` is not a support channel and prior research is expected from the questioner.
===========
QEMU README
===========

QEMU is a generic and open source machine & userspace emulator and
virtualizer.

QEMU is capable of emulating a complete machine in software without any
need for hardware virtualization support. By using dynamic translation,
it achieves very good performance. QEMU can also integrate with the Xen
and KVM hypervisors to provide emulated hardware while allowing the
hypervisor to manage the CPU. With hypervisor support, QEMU can achieve
near native performance for CPUs. When QEMU emulates CPUs directly it is
capable of running operating systems made for one machine (e.g. an ARMv7
board) on a different machine (e.g. an x86_64 PC board).

QEMU is also capable of providing userspace API virtualization for Linux
and BSD kernel interfaces. This allows binaries compiled against one
architecture ABI (e.g. the Linux PPC64 ABI) to be run on a host using a
different architecture ABI (e.g. the Linux x86_64 ABI). This does not
involve any hardware emulation, simply CPU and syscall emulation.

QEMU aims to fit into a variety of use cases. It can be invoked directly
by users wishing to have full control over its behaviour and settings.
It also aims to facilitate integration into higher level management
layers, by providing a stable command line interface and monitor API.
It is commonly invoked indirectly via the libvirt library when using
open source applications such as oVirt, OpenStack and virt-manager.

QEMU as a whole is released under the GNU General Public License,
version 2. For full licensing details, consult the LICENSE file.


Documentation
=============

Documentation can be found hosted online at
`<https://www.qemu.org/documentation/>`_. The documentation for the
current development version that is available at
`<https://www.qemu.org/docs/master/>`_ is generated from the ``docs/``
folder in the source tree, and is built by `Sphinx
<https://www.sphinx-doc.org/en/master/>_`.


Building
========

QEMU is multi-platform software intended to be buildable on all modern
Linux platforms, OS-X, Win32 (via the Mingw64 toolchain) and a variety
of other UNIX targets. The simple steps to build QEMU are:


.. code-block:: shell

  mkdir build
  cd build
  ../configure
  make

Additional information can also be found online via the QEMU website:

* `<https://wiki.qemu.org/Hosts/Linux>`_
* `<https://wiki.qemu.org/Hosts/Mac>`_
* `<https://wiki.qemu.org/Hosts/W32>`_


Submitting patches
==================

The QEMU source code is maintained under the GIT version control system.

.. code-block:: shell

   git clone https://gitlab.com/qemu-project/qemu.git

When submitting patches, one common approach is to use 'git
format-patch' and/or 'git send-email' to format & send the mail to the
qemu-devel@nongnu.org mailing list. All patches submitted must contain
a 'Signed-off-by' line from the author. Patches should follow the
guidelines set out in the `style section
<https://www.qemu.org/docs/master/devel/style.html>` of
the Developers Guide.

Additional information on submitting patches can be found online via
the QEMU website

* `<https://wiki.qemu.org/Contribute/SubmitAPatch>`_
* `<https://wiki.qemu.org/Contribute/TrivialPatches>`_

The QEMU website is also maintained under source control.

.. code-block:: shell

  git clone https://gitlab.com/qemu-project/qemu-web.git

* `<https://www.qemu.org/2017/02/04/the-new-qemu-website-is-up/>`_

A 'git-publish' utility was created to make above process less
cumbersome, and is highly recommended for making regular contributions,
or even just for sending consecutive patch series revisions. It also
requires a working 'git send-email' setup, and by default doesn't
automate everything, so you may want to go through the above steps
manually for once.

For installation instructions, please go to

*  `<https://github.com/stefanha/git-publish>`_

The workflow with 'git-publish' is:

.. code-block:: shell

  $ git checkout master -b my-feature
  $ # work on new commits, add your 'Signed-off-by' lines to each
  $ git publish

Your patch series will be sent and tagged as my-feature-v1 if you need to refer
back to it in the future.

Sending v2:

.. code-block:: shell

  $ git checkout my-feature # same topic branch
  $ # making changes to the commits (using 'git rebase', for example)
  $ git publish

Your patch series will be sent with 'v2' tag in the subject and the git tip
will be tagged as my-feature-v2.

Bug reporting
=============

The QEMU project uses GitLab issues to track bugs. Bugs
found when running code built from QEMU git or upstream released sources
should be reported via:

* `<https://gitlab.com/qemu-project/qemu/-/issues>`_

If using QEMU via an operating system vendor pre-built binary package, it
is preferable to report bugs to the vendor's own bug tracker first. If
the bug is also known to affect latest upstream code, it can also be
reported via GitLab.

For additional information on bug reporting consult:

* `<https://wiki.qemu.org/Contribute/ReportABug>`_


ChangeLog
=========

For version history and release notes, please visit
`<https://wiki.qemu.org/ChangeLog/>`_ or look at the git history for
more detailed information.


Contact
=======

The QEMU community can be contacted in a number of ways, with the two
main methods being email and IRC

* `<mailto:qemu-devel@nongnu.org>`_
* `<https://lists.nongnu.org/mailman/listinfo/qemu-devel>`_
* #qemu on irc.oftc.net

Information on additional methods of contacting the community can be
found online via the QEMU website:

* `<https://wiki.qemu.org/Contribute/StartHere>`_
Chardet: The Universal Character Encoding Detector
--------------------------------------------------

.. image:: https://img.shields.io/travis/chardet/chardet/stable.svg
   :alt: Build status
   :target: https://travis-ci.org/chardet/chardet

.. image:: https://img.shields.io/coveralls/chardet/chardet/stable.svg
   :target: https://coveralls.io/r/chardet/chardet

.. image:: https://img.shields.io/pypi/v/chardet.svg
   :target: https://warehouse.python.org/project/chardet/
   :alt: Latest version on PyPI

.. image:: https://img.shields.io/pypi/l/chardet.svg
   :alt: License


Detects
 - ASCII, UTF-8, UTF-16 (2 variants), UTF-32 (4 variants)
 - Big5, GB2312, EUC-TW, HZ-GB-2312, ISO-2022-CN (Traditional and Simplified Chinese)
 - EUC-JP, SHIFT_JIS, CP932, ISO-2022-JP (Japanese)
 - EUC-KR, ISO-2022-KR (Korean)
 - KOI8-R, MacCyrillic, IBM855, IBM866, ISO-8859-5, windows-1251 (Cyrillic)
 - ISO-8859-5, windows-1251 (Bulgarian)
 - ISO-8859-1, windows-1252 (Western European languages)
 - ISO-8859-7, windows-1253 (Greek)
 - ISO-8859-8, windows-1255 (Visual and Logical Hebrew)
 - TIS-620 (Thai)

.. note::
   Our ISO-8859-2 and windows-1250 (Hungarian) probers have been temporarily
   disabled until we can retrain the models.

Requires Python 2.6, 2.7, or 3.3+.

Installation
------------

Install from `PyPI <https://pypi.python.org/pypi/chardet>`_::

    pip install chardet

Documentation
-------------

For users, docs are now available at https://chardet.readthedocs.io/.

Command-line Tool
-----------------

chardet comes with a command-line script which reports on the encodings of one
or more files::

    % chardetect somefile someotherfile
    somefile: windows-1252 with confidence 0.5
    someotherfile: ascii with confidence 1.0

About
-----

This is a continuation of Mark Pilgrim's excellent chardet. Previously, two
versions needed to be maintained: one that supported python 2.x and one that
supported python 3.x.  We've recently merged with `Ian Cordasco <https://github.com/sigmavirus24>`_'s
`charade <https://github.com/sigmavirus24/charade>`_ fork, so now we have one
coherent version that works for Python 2.6+.

:maintainer: Dan Blanchard
What is lxml?
=============

lxml is the most feature-rich and easy-to-use library for processing XML and HTML in the Python language.
It's also very fast and memory friendly, just so you know.

For an introduction and further documentation, see `doc/main.txt`_.

For installation information, see `INSTALL.txt`_.

For issue tracker, see https://bugs.launchpad.net/lxml

Support the project
-------------------

lxml has been downloaded from the `Python Package Index`_
millions of times and is also available directly in many package
distributions, e.g. for Linux or macOS.

.. _`Python Package Index`: https://pypi.python.org/pypi/lxml

Most people who use lxml do so because they like using it.
You can show us that you like it by blogging about your experience
with it and linking to the project website.

If you are using lxml for your work and feel like giving a bit of
your own benefit back to support the project, consider sending us
money through GitHub Sponsors, Tidelift or PayPal that we can use
to buy us free time for the maintenance of this great library, to
fix bugs in the software, review and integrate code contributions,
to improve its features and documentation, or to just take a deep
breath and have a cup of tea every once in a while.
Please read the Legal Notice below, at the bottom of this page.
Thank you for your support.

.. class:: center

  Support lxml through `GitHub Sponsors <https://github.com/users/scoder/sponsorship>`_

  via a `Tidelift subscription <https://tidelift.com/subscription/pkg/pypi-lxml>`_

  or via PayPal:

  |Donate|_

.. _`Donate`: https://www.paypal.com/cgi-bin/webscr?cmd=_s-xclick&hosted_button_id=R56JE3VCPDA9N

Please `contact Stefan Behnel <http://consulting.behnel.de/>`_
for other ways to support the lxml project,
as well as commercial consulting, customisations and trainings on lxml and
fast Python XML processing.

Note that we are not accepting donations in crypto currencies.
Much of the development and hosting for lxml is done in a carbon-neutral way
or with compensated and very low emissions.
Crypto currencies do not fit into that ambition.

.. |Donate| image:: https://lxml.de/paypal_btn_donateCC_LG.png
            :width: 160
            :height: 47
            :alt: Donate to the lxml project

.. _`doc/main.txt`: https://github.com/lxml/lxml/blob/master/doc/main.txt
.. _`INSTALL.txt`: http://lxml.de/installation.html

`Travis-CI <https://travis-ci.org/>`_ and `AppVeyor <https://www.appveyor.com/>`_
support the lxml project with their build and CI servers.
Jetbrains supports the lxml project by donating free licenses of their
`PyCharm IDE <https://www.jetbrains.com/pycharm/>`_.
Another supporter of the lxml project is
`COLOGNE Webdesign <https://www.colognewebdesign.de/>`_.


Project income report
---------------------

* Total project income in 2020: EUR 6065,86  (506.49 € / month)

  - Tidelift: EUR 4064.77
  - Paypal: EUR 1401.09
  - other: EUR 600.00

* Total project income in 2019: EUR 717.52  (59.79 € / month)

  - Tidelift: EUR 360.30
  - Paypal: EUR 157.22
  - other: EUR 200.00


Legal Notice for Donations
--------------------------

Any donation that you make to the lxml project is voluntary and
is not a fee for any services, goods, or advantages.  By making
a donation to the lxml project, you acknowledge that we have the
right to use the money you donate in any lawful way and for any
lawful purpose we see fit and we are not obligated to disclose
the way and purpose to any party unless required by applicable
law.  Although lxml is free software, to the best of our knowledge
the lxml project does not have any tax exempt status.  The lxml
project is neither a registered non-profit corporation nor a
registered charity in any country.  Your donation may or may not
be tax-deductible; please consult your tax advisor in this matter.
We will not publish or disclose your name and/or e-mail address
without your consent, unless required by applicable law.  Your
donation is non-refundable.
v39.2.0
-------

* #1359: Support using "file:" to load a PEP 440-compliant package version from
  a text file.
* #1360: Fixed issue with a mismatch between the name of the package and the
  name of the .dist-info file in wheel files
* #1365: Take the package_dir option into account when loading the version from
  a module attribute.
* #1353: Added coverage badge to README.
* #1356: Made small fixes to the developer guide documentation.
* #1357: Fixed warnings in documentation builds and started enforcing that the
  docs build without warnings in tox.
* #1376: Updated release process docs.
* #1343: The ``setuptools`` specific ``long_description_content_type``,
  ``project_urls`` and ``provides_extras`` fields are now set consistently
  after any ``distutils`` ``setup_keywords`` calls, allowing them to override
  values.
* #1352: Added ``tox`` environment for documentation builds.
* #1354: Added ``towncrier`` for changelog managment.
* #1355: Add PR template.
* #1368: Fixed tests which failed without network connectivity.
* #1369: Added unit tests for PEP 425 compatibility tags support.
* #1372: Stop testing Python 3.3 in Travis CI, now that the latest version of
  ``wheel`` no longer installs on it.

v39.1.0
-------

* #1340: Update all PyPI URLs to reflect the switch to the
  new Warehouse codebase.
* #1337: In ``pkg_resources``, now support loading resources
  for modules loaded by the ``SourcelessFileLoader``.
* #1332: Silence spurious wheel related warnings on Windows.

v39.0.1
-------

* #1297: Restore Unicode handling for Maintainer fields in
  metadata.

v39.0.0
-------

* #1296: Setuptools now vendors its own direct dependencies, no
  longer relying on the dependencies as vendored by pkg_resources.

* #296: Removed long-deprecated support for iteration on
  Version objects as returned by ``pkg_resources.parse_version``.
  Removed the ``SetuptoolsVersion`` and
  ``SetuptoolsLegacyVersion`` names as well. They should not
  have been used, but if they were, replace with
  ``Version`` and ``LegacyVersion`` from ``packaging.version``.

v38.7.0
-------

* #1288: Add support for maintainer in PKG-INFO.

v38.6.1
-------

* #1292: Avoid generating ``Provides-Extra`` in metadata when
  no extra is present (but environment markers are).

v38.6.0
-------

* #1286: Add support for Metadata 2.1 (PEP 566).

v38.5.2
-------

* #1285: Fixed RuntimeError in pkg_resources.parse_requirements
  on Python 3.7 (stemming from PEP 479).

v38.5.1
-------

* #1271: Revert to Cython legacy ``build_ext`` behavior for
  compatibility.

v38.5.0
-------

* #1229: Expand imports in ``build_ext`` to refine detection of
  Cython availability.

* #1270: When Cython is available, ``build_ext`` now uses the
  new_build_ext.

v38.4.1
-------

* #1257: In bdist_egg.scan_module, fix ValueError on Python 3.7.

v38.4.0
-------

* #1231: Removed warning when PYTHONDONTWRITEBYTECODE is enabled.

v38.3.0
-------

* #1210: Add support for PEP 345 Project-URL metadata.
* #1207: Add support for ``long_description_type`` to setup.cfg
  declarative config as intended and documented.

v38.2.5
-------

* #1232: Fix trailing slash handling in ``pkg_resources.ZipProvider``.

v38.2.4
-------

* #1220: Fix `data_files` handling when installing from wheel.

v38.2.3
-------

* fix Travis' Python 3.3 job.

v38.2.2
-------

* #1214: fix handling of namespace packages when installing
  from a wheel.

v38.2.1
-------

* #1212: fix encoding handling of metadata when installing
  from a wheel.

v38.2.0
-------

* #1200: easy_install now support installing from wheels:
  they will be installed as standalone unzipped eggs.

v38.1.0
-------

* #1208: Improve error message when failing to locate scripts
  in egg-info metadata.

v38.0.0
-------

* #458: In order to support deterministic builds, Setuptools no
  longer allows packages to declare ``install_requires`` as
  unordered sequences (sets or dicts).

v37.0.0
-------

* #878: Drop support for Python 2.6. Python 2.6 users should
  rely on 'setuptools < 37dev'.

v36.8.0
-------

* #1190: In SSL support for package index operations, use SNI
  where available.

v36.7.3
-------

* #1175: Bug fixes to ``build_meta`` module.

v36.7.2
-------

* #701: Fixed duplicate test discovery on Python 3.

v36.7.1
-------

* #1193: Avoid test failures in bdist_egg when
  PYTHONDONTWRITEBYTECODE is set.

v36.7.0
-------

* #1054: Support ``setup_requires`` in ``setup.cfg`` files.

v36.6.1
-------

* #1132: Removed redundant and costly serialization/parsing step
  in ``EntryPoint.__init__``.

* #844: ``bdist_egg --exclude-source-files`` now tested and works
  on Python 3.

v36.6.0
-------

* #1143: Added ``setuptools.build_meta`` module, an implementation
  of PEP-517 for Setuptools-defined packages.

* #1143: Added ``dist_info`` command for producing dist_info
  metadata.

v36.5.0
-------

* #170: When working with Mercurial checkouts, use Windows-friendly
  syntax for suppressing output.

* Inspired by #1134, performed substantial refactoring of
  ``pkg_resources.find_on_path`` to facilitate an optimization
  for paths with many non-version entries.

v36.4.0
-------

* #1075: Add new ``Description-Content-Type`` metadata field. `See here for
  documentation on how to use this field.
  <https://packaging.python.org/specifications/#description-content-type>`_

* #1068: Sort files and directories when building eggs for
  deterministic order.

* #196: Remove caching of easy_install command in fetch_build_egg.
  Fixes issue where ``pytest-runner-N.N`` would satisfy the installation
  of ``pytest``.

* #1129: Fix working set dependencies handling when replacing conflicting
  distributions (e.g. when using ``setup_requires`` with a conflicting
  transitive dependency, fix #1124).

* #1133: Improved handling of README files extensions and added
  Markdown to the list of searched READMES.

* #1135: Improve performance of pkg_resources import by not invoking
  ``access`` or ``stat`` and using ``os.listdir`` instead.

v36.3.0
-------

* #1131: Make possible using several files within ``file:`` directive
  in metadata.long_description in ``setup.cfg``.

v36.2.7
-------

* fix #1105: Fix handling of requirements with environment
  markers when declared in ``setup.cfg`` (same treatment as
  for #1081).

v36.2.6
-------

* #462: Don't assume a directory is an egg by the ``.egg``
  extension alone.

v36.2.5
-------

* #1093: Fix test command handler with extras_require.
* #1112, #1091, #1115: Now using Trusty containers in
  Travis for CI and CD.

v36.2.4
-------

* #1092: ``pkg_resources`` now uses ``inspect.getmro`` to
  resolve classes in method resolution order.

v36.2.3
-------

* #1102: Restore behavior for empty extras.

v36.2.2
-------

* #1099: Revert commit a3ec721, restoring intended purpose of
  extras as part of a requirement declaration.

v36.2.1
-------

* fix #1086
* fix #1087
* support extras specifiers in install_requires requirements

v36.2.0
-------

* #1081: Environment markers indicated in ``install_requires``
  are now processed and treated as nameless ``extras_require``
  with markers, allowing their metadata in requires.txt to be
  correctly generated.

* #1053: Tagged commits are now released using Travis-CI
  build stages, meaning releases depend on passing tests on
  all supported Python versions (Linux) and not just the latest
  Python version.

v36.1.1
-------

* #1083: Correct ``py31compat.makedirs`` to correctly honor
  ``exist_ok`` parameter.
* #1083: Also use makedirs compatibility throughout setuptools.

v36.1.0
-------

* #1083: Avoid race condition on directory creation in
  ``pkg_resources.ensure_directory``.

* Removed deprecation of and restored support for
  ``upload_docs`` command for sites other than PyPI.
  Only warehouse is dropping support, but services like
  `devpi <http://doc.devpi.net/latest/>`_ continue to
  support docs built by setuptools' plugins. See
  `this comment <https://bitbucket.org/hpk42/devpi/issues/388/support-rtd-model-for-building-uploading#comment-34292423>`_
  for more context on the motivation for this change.

v36.0.1
-------

* #1042: Fix import in py27compat module that still
  referenced six directly, rather than through the externs
  module (vendored packages hook).

v36.0.0
-------

* #980 and others: Once again, Setuptools vendors all
  of its dependencies. It seems to be the case that in
  the Python ecosystem, all build tools must run without
  any dependencies (build, runtime, or otherwise). At
  such a point that a mechanism exists that allows
  build tools to have dependencies, Setuptools will adopt
  it.

v35.0.2
-------

* #1015: Fix test failures on Python 3.7.

* #1024: Add workaround for Jython #2581 in monkey module.

v35.0.1
-------

* #992: Revert change introduced in v34.4.1, now
  considered invalid.

* #1016: Revert change introduced in v35.0.0 per #1014,
  referencing #436. The approach had unintended
  consequences, causing sdist installs to be missing
  files.

v35.0.0
-------

* #436: In egg_info.manifest_maker, no longer read
  the file list from the manifest file, and instead
  re-build it on each build. In this way, files removed
  from the specification will not linger in the manifest.
  As a result, any files manually added to the manifest
  will be removed on subsequent egg_info invocations.
  No projects should be manually adding files to the
  manifest and should instead use MANIFEST.in or SCM
  file finders to force inclusion of files in the manifest.

v34.4.1
-------

* #1008: In MSVC support, use always the last version available for Windows SDK and UCRT SDK.

* #1008: In MSVC support, fix "vcruntime140.dll" returned path with Visual Studio 2017.

* #992: In msvc.msvc9_query_vcvarsall, ensure the
  returned dicts have str values and not Unicode for
  compatibility with os.environ.

v34.4.0
-------

* #995: In MSVC support, add support for "Microsoft Visual Studio 2017" and "Microsoft Visual Studio Build Tools 2017".

* #999 via #1007: Extend support for declarative package
  config in a setup.cfg file to include the options
  ``python_requires`` and ``py_modules``.

v34.3.3
-------

* #967 (and #997): Explicitly import submodules of
  packaging to account for environments where the imports
  of those submodules is not implied by other behavior.

v34.3.2
-------

* #993: Fix documentation upload by correcting
  rendering of content-type in _build_multipart
  on Python 3.

v34.3.1
-------

* #988: Trap ``os.unlink`` same as ``os.remove`` in
  ``auto_chmod`` error handler.

* #983: Fixes to invalid escape sequence deprecations on
  Python 3.6.

v34.3.0
-------

* #941: In the upload command, if the username is blank,
  default to ``getpass.getuser()``.

* #971: Correct distutils findall monkeypatch to match
  appropriate versions (namely Python 3.4.6).

v34.2.0
-------

* #966: Add support for reading dist-info metadata and
  thus locating Distributions from zip files.

* #968: Allow '+' and '!' in egg fragments
  so that it can take package names that contain
  PEP 440 conforming version specifiers.

v34.1.1
-------

* #953: More aggressively employ the compatibility issue
  originally added in #706.

v34.1.0
-------

* #930: ``build_info`` now accepts two new parameters
  to optimize and customize the building of C libraries.

v34.0.3
-------

* #947: Loosen restriction on the version of six required,
  restoring compatibility with environments relying on
  six 1.6.0 and later.

v34.0.2
-------

* #882: Ensure extras are honored when building the
  working set.
* #913: Fix issue in develop if package directory has
  a trailing slash.

v34.0.1
-------

* #935: Fix glob syntax in graft.

v34.0.0
-------

* #581: Instead of vendoring the growing list of
  dependencies that Setuptools requires to function,
  Setuptools now requires these dependencies just like
  any other project. Unlike other projects, however,
  Setuptools cannot rely on ``setup_requires`` to
  demand the dependencies it needs to install because
  its own machinery would be necessary to pull those
  dependencies if not present (a bootstrapping problem).
  As a result, Setuptools no longer supports self upgrade or
  installation in the general case. Instead, users are
  directed to use pip to install and upgrade using the
  ``wheel`` distributions of setuptools.

  Users are welcome to contrive other means to install
  or upgrade Setuptools using other means, such as
  pre-installing the Setuptools dependencies with pip
  or a bespoke bootstrap tool, but such usage is not
  recommended and is not supported.

  As discovered in #940, not all versions of pip will
  successfully install Setuptools from its pre-built
  wheel. If you encounter issues with "No module named
  six" or "No module named packaging", especially
  following a line "Running setup.py egg_info for package
  setuptools", then your pip is not new enough.

  There's an additional issue in pip where setuptools
  is upgraded concurrently with other source packages,
  described in pip #4253. The proposed workaround is to
  always upgrade Setuptools first prior to upgrading
  other packages that would upgrade Setuptools.

v33.1.1
-------

* #921: Correct issue where certifi fallback not being
  reached on Windows.

v33.1.0
-------

Installation via pip, as indicated in the `Python Packaging
User's Guide <https://packaging.python.org/installing/>`_,
is the officially-supported mechanism for installing
Setuptools, and this recommendation is now explicit in the
much more concise README.

Other edits and tweaks were made to the documentation. The
codebase is unchanged.

v33.0.0
-------

* #619: Removed support for the ``tag_svn_revision``
  distribution option. If Subversion tagging support is
  still desired, consider adding the functionality to
  setuptools_svn in setuptools_svn #2.

v32.3.1
-------

* #866: Use ``dis.Bytecode`` on Python 3.4 and later in
  ``setuptools.depends``.

v32.3.0
-------

* #889: Backport proposed fix for disabling interpolation in
  distutils.Distribution.parse_config_files.

v32.2.0
-------

* #884: Restore support for running the tests under
  `pytest-runner <https://github.com/pytest-dev/pytest-runner>`_
  by ensuring that PYTHONPATH is honored in tests invoking
  a subprocess.

v32.1.3
-------

* #706: Add rmtree compatibility shim for environments where
  rmtree fails when passed a unicode string.

v32.1.2
-------

* #893: Only release sdist in zip format as warehouse now
  disallows releasing two different formats.

v32.1.1
-------

* #704: More selectively ensure that 'rmtree' is not invoked with
  a byte string, enabling it to remove files that are non-ascii,
  even on Python 2.

* #712: In 'sandbox.run_setup', ensure that ``__file__`` is
  always a ``str``, modeling the behavior observed by the
  interpreter when invoking scripts and modules.

v32.1.0
-------

* #891: In 'test' command on test failure, raise DistutilsError,
  suppression invocation of subsequent commands.

v32.0.0
-------

* #890: Revert #849. ``global-exclude .foo`` will not match all
  ``*.foo`` files any more. Package authors must add an explicit
  wildcard, such as ``global-exclude *.foo``, to match all
  ``.foo`` files. See #886, #849.

v31.0.1
-------

* #885: Fix regression where 'pkg_resources._rebuild_mod_path'
  would fail when a namespace package's '__path__' was not
  a list with a sort attribute.

v31.0.0
-------

* #250: Install '-nspkg.pth' files for packages installed
  with 'setup.py develop'. These .pth files allow
  namespace packages installed by pip or develop to
  co-mingle. This change required the removal of the
  change for #805 and pip #1924, introduced in 28.3.0 and implicated
  in #870, but means that namespace packages not in a
  site packages directory will no longer work on Python
  earlier than 3.5, whereas before they would work on
  Python not earlier than 3.3.

v30.4.0
-------

* #879: For declarative config:

  - read_configuration() now accepts ignore_option_errors argument. This allows scraping tools to read metadata without a need to download entire packages. E.g. we can gather some stats right from GitHub repos just by downloading setup.cfg.

  - packages find: directive now supports fine tuning from a subsection. The same arguments as for find() are accepted.

v30.3.0
-------

* #394 via #862: Added support for `declarative package
  config in a setup.cfg file
  <https://setuptools.readthedocs.io/en/latest/setuptools.html#configuring-setup-using-setup-cfg-files>`_.

v30.2.1
-------

* #850: In test command, invoke unittest.main with
  indication not to exit the process.

v30.2.0
-------

* #854: Bump to vendored Packaging 16.8.

v30.1.0
-------

* #846: Also trap 'socket.error' when opening URLs in
  package_index.

* #849: Manifest processing now matches the filename
  pattern anywhere in the filename and not just at the
  start. Restores behavior found prior to 28.5.0.

v30.0.0
-------

* #864: Drop support for Python 3.2. Systems requiring
  Python 3.2 support must use 'setuptools < 30'.

* #825: Suppress warnings for single files.

* #830 via #843: Once again restored inclusion of data
  files to sdists, but now trap TypeError caused by
  techniques employed rjsmin and similar.

v29.0.1
-------

* #861: Re-release of v29.0.1 with the executable script
  launchers bundled. Now, launchers are included by default
  and users that want to disable this behavior must set the
  environment variable
  'SETUPTOOLS_INSTALL_WINDOWS_SPECIFIC_FILES' to
  a false value like "false" or "0".

v29.0.0
-------

* #841: Drop special exception for packages invoking
  win32com during the build/install process. See
  Distribute #118 for history.

v28.8.0
-------

* #629: Per the discussion, refine the sorting to use version
  value order for more accurate detection of the latest
  available version when scanning for packages. See also
  #829.

* #837: Rely on the config var "SO" for Python 3.3.0 only
  when determining the ext filename.

v28.7.1
-------

* #827: Update PyPI root for dependency links.

* #833: Backed out changes from #830 as the implementation
  seems to have problems in some cases.

v28.7.0
-------

* #832: Moved much of the namespace package handling
  functionality into a separate module for re-use in something
  like #789.
* #830: ``sdist`` command no longer suppresses the inclusion
  of data files, re-aligning with the expectation of distutils
  and addressing #274 and #521.

v28.6.1
-------

* #816: Fix manifest file list order in tests.

v28.6.0
-------

* #629: When scanning for packages, ``pkg_resources`` now
  ignores empty egg-info directories and gives precedence to
  packages whose versions are lexicographically greatest,
  a rough approximation for preferring the latest available
  version.

v28.5.0
-------

* #810: Tests are now invoked with tox and not setup.py test.
* #249 and #450 via #764: Avoid scanning the whole tree
  when building the manifest. Also fixes a long-standing bug
  where patterns in ``MANIFEST.in`` had implicit wildcard
  matching. This caused ``global-exclude .foo`` to exclude
  all ``*.foo`` files, but also ``global-exclude bar.py`` to
  exclude ``foo_bar.py``.

v28.4.0
-------

* #732: Now extras with a hyphen are honored per PEP 426.
* #811: Update to pyparsing 2.1.10.
* Updated ``setuptools.command.sdist`` to re-use most of
  the functionality directly from ``distutils.command.sdist``
  for the ``add_defaults`` method with strategic overrides.
  See #750 for rationale.
* #760 via #762: Look for certificate bundle where SUSE
  Linux typically presents it. Use ``certifi.where()`` to locate
  the bundle.

v28.3.0
-------

* #809: In ``find_packages()``, restore support for excluding
  a parent package without excluding a child package.

* #805: Disable ``-nspkg.pth`` behavior on Python 3.3+ where
  PEP-420 functionality is adequate. Fixes pip #1924.

v28.1.0
-------

* #803: Bump certifi to 2016.9.26.

v28.0.0
-------

* #733: Do not search excluded directories for packages.
  This introduced a backwards incompatible change in ``find_packages()``
  so that ``find_packages(exclude=['foo']) == []``, excluding subpackages of ``foo``.
  Previously, ``find_packages(exclude=['foo']) == ['foo.bar']``,
  even though the parent ``foo`` package was excluded.

* #795: Bump certifi.

* #719: Suppress decoding errors and instead log a warning
  when metadata cannot be decoded.

v27.3.1
-------

* #790: In MSVC monkeypatching, explicitly patch each
  function by name in the target module instead of inferring
  the module from the function's ``__module__``. Improves
  compatibility with other packages that might have previously
  patched distutils functions (i.e. NumPy).

v27.3.0
-------

* #794: In test command, add installed eggs to PYTHONPATH
  when invoking tests so that subprocesses will also have the
  dependencies available. Fixes `tox 330
  <https://github.com/tox-dev/tox/issues/330>`_.

* #795: Update vendored pyparsing 2.1.9.

v27.2.0
-------

* #520 and #513: Suppress ValueErrors in fixup_namespace_packages
  when lookup fails.

* Nicer, more consistent interfaces for msvc monkeypatching.

v27.1.2
-------

* #779 via #781: Fix circular import.

v27.1.1
-------

* #778: Fix MSVC monkeypatching.

v27.1.0
-------

* Introduce the (private) ``monkey`` module to encapsulate
  the distutils monkeypatching behavior.

v27.0.0
-------

* Now use Warehouse by default for
  ``upload``, patching ``distutils.config.PyPIRCCommand`` to
  affect default behavior.

  Any config in .pypirc should be updated to replace

    https://pypi.python.org/pypi/

  with

    https://upload.pypi.org/legacy/

  Similarly, any passwords stored in the keyring should be
  updated to use this new value for "system".

  The ``upload_docs`` command will continue to use the python.org
  site, but the command is now deprecated. Users are urged to use
  Read The Docs instead.

* #776: Use EXT_SUFFIX for py_limited_api renaming.

* #774 and #775: Use LegacyVersion from packaging when
  detecting numpy versions.

v26.1.1
-------

* Re-release of 26.1.0 with pytest pinned to allow for automated
  deployment and thus proper packaging environment variables,
  fixing issues with missing executable launchers.

v26.1.0
-------

* #763: ``pkg_resources.get_default_cache`` now defers to the
  `appdirs project <https://pypi.org/project/appdirs>`_ to
  resolve the cache directory. Adds a vendored dependency on
  appdirs to pkg_resources.

v26.0.0
-------

* #748: By default, sdists are now produced in gzipped tarfile
  format by default on all platforms, adding forward compatibility
  for the same behavior in Python 3.6 (See Python #27819).

* #459 via #736: On Windows with script launchers,
  sys.argv[0] now reflects
  the name of the entry point, consistent with the behavior in
  distlib and pip wrappers.

* #752 via #753: When indicating ``py_limited_api`` to Extension,
  it must be passed as a keyword argument.

v25.4.0
-------

* Add Extension(py_limited_api=True). When set to a truthy value,
  that extension gets a filename appropriate for code using Py_LIMITED_API.
  When used correctly this allows a single compiled extension to work on
  all future versions of CPython 3.
  The py_limited_api argument only controls the filename. To be
  compatible with multiple versions of Python 3, the C extension
  will also need to set -DPy_LIMITED_API=... and be modified to use
  only the functions in the limited API.

v25.3.0
-------

* #739 Fix unquoted libpaths by fixing compatibility between `numpy.distutils` and `distutils._msvccompiler` for numpy < 1.11.2 (Fix issue #728, error also fixed in Numpy).

* #731: Bump certifi.

* Style updates. See #740, #741, #743, #744, #742, #747.

* #735: include license file.

v25.2.0
-------

* #612 via #730: Add a LICENSE file which needs to be provided by the terms of
  the MIT license.

v25.1.6
-------

* #725: revert `library_dir_option` patch (Error is related to `numpy.distutils` and make errors on non Numpy users).

v25.1.5
-------

* #720
* #723: Improve patch for `library_dir_option`.

v25.1.4
-------

* #717
* #713
* #707: Fix Python 2 compatibility for MSVC by catching errors properly.
* #715: Fix unquoted libpaths by patching `library_dir_option`.

v25.1.3
-------

* #714 and #704: Revert fix as it breaks other components
  downstream that can't handle unicode. See #709, #710,
  and #712.

v25.1.2
-------

* #704: Fix errors when installing a zip sdist that contained
  files named with non-ascii characters on Windows would
  crash the install when it attempted to clean up the build.
* #646: MSVC compatibility - catch errors properly in
  RegistryInfo.lookup.
* #702: Prevent UnboundLocalError when initial working_set
  is empty.

v25.1.1
-------

* #686: Fix issue in sys.path ordering by pkg_resources when
  rewrite technique is "raw".
* #699: Fix typo in msvc support.

v25.1.0
-------

* #609: Setuptools will now try to download a distribution from
  the next possible download location if the first download fails.
  This means you can now specify multiple links as ``dependency_links``
  and all links will be tried until a working download link is encountered.

v25.0.2
-------

* #688: Fix AttributeError in setup.py when invoked not from
  the current directory.

v25.0.1
-------

* Cleanup of setup.py script.

* Fixed documentation builders by allowing setup.py
  to be imported without having bootstrapped the
  metadata.

* More style cleanup. See #677, #678, #679, #681, #685.

v25.0.0
-------

* #674: Default ``sys.path`` manipulation by easy-install.pth
  is now "raw", meaning that when writing easy-install.pth
  during any install operation, the ``sys.path`` will not be
  rewritten and will no longer give preference to easy_installed
  packages.

  To retain the old behavior when using any easy_install
  operation (including ``setup.py install`` when setuptools is
  present), set the environment variable:

    SETUPTOOLS_SYS_PATH_TECHNIQUE=rewrite

  This project hopes that that few if any environments find it
  necessary to retain the old behavior, and intends to drop
  support for it altogether in a future release. Please report
  any relevant concerns in the ticket for this change.

v24.3.1
-------

* #398: Fix shebang handling on Windows in script
  headers where spaces in ``sys.executable`` would
  produce an improperly-formatted shebang header,
  introduced in 12.0 with the fix for #188.

* #663, #670: More style updates.

v24.3.0
-------

* #516: Disable ``os.link`` to avoid hard linking
  in ``sdist.make_distribution``, avoiding errors on
  systems that support hard links but not on the
  file system in which the build is occurring.

v24.2.1
-------

* #667: Update Metadata-Version to 1.2 when
  ``python_requires`` is supplied.

v24.2.0
-------

* #631: Add support for ``python_requires`` keyword.

v24.1.1
-------

* More style updates. See #660, #661, #641.

v24.1.0
-------

* #659: ``setup.py`` now will fail fast and with a helpful
  error message when the necessary metadata is missing.
* More style updates. See #656, #635, #640,
  #644, #650, #652, and #655.

v24.0.3
-------

* Updated style in much of the codebase to match
  community expectations. See #632, #633, #634,
  #637, #639, #638, #642, #648.

v24.0.2
-------

* If MSVC++14 is needed ``setuptools.msvc`` now redirect
  user to Visual C++ Build Tools web page.

v24.0.1
-------

* #625 and #626: Fixes on ``setuptools.msvc`` mainly
  for Python 2 and Linux.

v24.0.0
-------

* Pull Request #174: Add more aggressive support for
  standalone Microsoft Visual C++ compilers in
  msvc9compiler patch.
  Particularly : Windows SDK 6.1 and 7.0
  (MSVC++ 9.0), Windows SDK 7.1 (MSVC++ 10.0),
  Visual C++ Build Tools 2015 (MSVC++14)
* Renamed ``setuptools.msvc9_support`` to
  ``setuptools.msvc``.

v23.2.1
-------

Re-release of v23.2.0, which was missing the intended
commits.

* #623: Remove used of deprecated 'U' flag when reading
  manifests.

v23.1.0
-------

* #619: Deprecated ``tag_svn_revision`` distribution
  option.

v23.0.0
-------

* #611: Removed ARM executables for CLI and GUI script
  launchers on Windows. If this was a feature you cared
  about, please comment in the ticket.
* #604: Removed docs building support. The project
  now relies on documentation hosted at
  https://setuptools.readthedocs.io/.

v22.0.5
-------

* #604: Restore repository for upload_docs command
  to restore publishing of docs during release.

v22.0.4
-------

* #589: Upload releases to pypi.io using the upload
  hostname and legacy path.

v22.0.3
-------

* #589: Releases are now uploaded to pypi.io (Warehouse)
  even when releases are made on Twine via Travis.

v22.0.2
-------

* #589: Releases are now uploaded to pypi.io (Warehouse).

v22.0.1
-------

* #190: On Python 2, if unicode is passed for packages to
  ``build_py`` command, it will be handled just as with
  text on Python 3.

v22.0.0
-------

Intended to be v21.3.0, but jaraco accidentally released as
a major bump.

* #598: Setuptools now lists itself first in the User-Agent
  for web requests, better following the guidelines in
  `RFC 7231
  <https://tools.ietf.org/html/rfc7231#section-5.5.3>`_.

v21.2.2
-------

* Minor fixes to changelog and docs.

v21.2.1
-------

* #261: Exclude directories when resolving globs in
  package_data.

v21.2.0
-------

* #539: In the easy_install get_site_dirs, honor all
  paths found in ``site.getsitepackages``.

v21.1.0
-------

* #572: In build_ext, now always import ``_CONFIG_VARS``
  from ``distutils`` rather than from ``sysconfig``
  to allow ``distutils.sysconfig.customize_compiler``
  configure the OS X compiler for ``-dynamiclib``.

v21.0.0
-------

* Removed ez_setup.py from Setuptools sdist. The
  bootstrap script will be maintained in its own
  branch and should be generally be retrieved from
  its canonical location at
  https://bootstrap.pypa.io/ez_setup.py.

v20.10.0
--------

* #553: egg_info section is now generated in a
  deterministic order, matching the order generated
  by earlier versions of Python. Except on Python 2.6,
  order is preserved when existing settings are present.
* #556: Update to Packaging 16.7, restoring support
  for deprecated ``python_implmentation`` marker.
* #555: Upload command now prompts for a password
  when uploading to PyPI (or other repository) if no
  password is present in .pypirc or in the keyring.

v20.9.0
-------

* #548: Update certify version to 2016.2.28
* #545: Safely handle deletion of non-zip eggs in rotate
  command.

v20.8.1
-------

* Issue #544: Fix issue with extra environment marker
  processing in WorkingSet due to refactor in v20.7.0.

v20.8.0
-------

* Issue #543: Re-release so that latest release doesn't
  cause déjà vu with distribute and setuptools 0.7 in
  older environments.

v20.7.0
-------

* Refactored extra environment marker processing
  in WorkingSet.
* Issue #533: Fixed intermittent test failures.
* Issue #536: In msvc9_support, trap additional exceptions
  that might occur when importing
  ``distutils.msvc9compiler`` in mingw environments.
* Issue #537: Provide better context when package
  metadata fails to decode in UTF-8.

v20.6.8
-------

* Issue #523: Restored support for environment markers,
  now honoring 'extra' environment markers.

v20.6.7
-------

* Issue #523: Disabled support for environment markers
  introduced in v20.5.

v20.6.6
-------

* Issue #503: Restore support for PEP 345 environment
  markers by updating to Packaging 16.6.

v20.6.0
-------

* New release process that relies on
  `bumpversion <https://github.com/peritus/bumpversion>`_
  and Travis CI for continuous deployment.
* Project versioning semantics now follow
  `semver <https://semver.org>`_ precisely.
  The 'v' prefix on version numbers now also allows
  version numbers to be referenced in the changelog,
  e.g. http://setuptools.readthedocs.io/en/latest/history.html#v20-6-0.

20.5
----

* BB Pull Request #185, #470: Add support for environment markers
  in requirements in install_requires, setup_requires,
  tests_require as well as adding a test for the existing
  extra_requires machinery.

20.4
----

* Issue #422: Moved hosting to
  `Github <https://github.com/pypa/setuptools>`_
  from `Bitbucket <https://bitbucket.org/pypa/setuptools>`_.
  Issues have been migrated, though all issues and comments
  are attributed to bb-migration. So if you have a particular
  issue or issues to which you've been subscribed, you will
  want to "watch" the equivalent issue in Github.
  The Bitbucket project will be retained for the indefinite
  future, but Github now hosts the canonical project repository.

20.3.1
------

* Issue #519: Remove import hook when reloading the
  ``pkg_resources`` module.
* BB Pull Request #184: Update documentation in ``pkg_resources``
  around new ``Requirement`` implementation.

20.3
----

* BB Pull Request #179: ``pkg_resources.Requirement`` objects are
  now a subclass of ``packaging.requirements.Requirement``,
  allowing any environment markers and url (if any) to be
  affiliated with the requirement
* BB Pull Request #179: Restore use of RequirementParseError
  exception unintentionally dropped in 20.2.

20.2.2
------

* Issue #502: Correct regression in parsing of multiple
  version specifiers separated by commas and spaces.

20.2.1
------

* Issue #499: Restore compatibility for legacy versions
  by bumping to packaging 16.4.

20.2
----

* Changelog now includes release dates and links to PEPs.
* BB Pull Request #173: Replace dual PEP 345 _markerlib implementation
  and PEP 426 implementation of environment marker support from
  packaging 16.1 and PEP 508. Fixes Issue #122.
  See also BB Pull Request #175, BB Pull Request #168, and
  BB Pull Request #164. Additionally:

   - ``Requirement.parse`` no longer retains the order of extras.
   - ``parse_requirements`` now requires that all versions be
     PEP-440 compliant, as revealed in #499. Packages released
     with invalid local versions should be re-released using
     the proper local version syntax, e.g. ``mypkg-1.0+myorg.1``.

20.1.1
------

* Update ``upload_docs`` command to also honor keyring
  for password resolution.

20.1
----

* Added support for using passwords from keyring in the upload
  command. See `the upload docs
  <https://setuptools.readthedocs.io/en/latest/setuptools.html#upload-upload-source-and-or-egg-distributions-to-pypi>`_
  for details.

20.0
----

* Issue #118: Once again omit the package metadata (egg-info)
  from the list of outputs in ``--record``. This version of setuptools
  can no longer be used to upgrade pip earlier than 6.0.

19.7
----

* `Off-project PR <https://github.com/jaraco/setuptools/pull/32>`_:
  For FreeBSD, also honor root certificates from ca_root_nss.

19.6.2
------

* Issue #491: Correct regression incurred in 19.4 where
  a double-namespace package installed using pip would
  cause a TypeError.

19.6.1
------

* Restore compatibility for PyPy 3 compatibility lost in
  19.4.1 addressing Issue #487.
* ``setuptools.launch`` shim now loads scripts in a new
  namespace, avoiding getting relative imports from
  the setuptools package on Python 2.

19.6
----

* Added a new entry script ``setuptools.launch``,
  implementing the shim found in
  ``pip.util.setuptools_build``. Use this command to launch
  distutils-only packages under setuptools in the same way that
  pip does, causing the setuptools monkeypatching of distutils
  to be invoked prior to invoking a script. Useful for debugging
  or otherwise installing a distutils-only package under
  setuptools when pip isn't available or otherwise does not
  expose the desired functionality. For example::

    $ python -m setuptools.launch setup.py develop

* Issue #488: Fix dual manifestation of Extension class in
  extension packages installed as dependencies when Cython
  is present.

19.5
----

* Issue #486: Correct TypeError when getfilesystemencoding
  returns None.
* Issue #139: Clarified the license as MIT.
* BB Pull Request #169: Removed special handling of command
  spec in scripts for Jython.

19.4.1
------

* Issue #487: Use direct invocation of ``importlib.machinery``
  in ``pkg_resources`` to avoid missing detection on relevant
  platforms.

19.4
----

* Issue #341: Correct error in path handling of package data
  files in ``build_py`` command when package is empty.
* Distribute #323, Issue #141, Issue #207, and
  BB Pull Request #167: Another implementation of
  ``pkg_resources.WorkingSet`` and ``pkg_resources.Distribution``
  that supports replacing an extant package with a new one,
  allowing for setup_requires dependencies to supersede installed
  packages for the session.

19.3
----

* Issue #229: Implement new technique for readily incorporating
  dependencies conditionally from vendored copies or primary
  locations. Adds a new dependency on six.

19.2
----

* BB Pull Request #163: Add get_command_list method to Distribution.
* BB Pull Request #162: Add missing whitespace to multiline string
  literals.

19.1.1
------

* Issue #476: Cast version to string (using default encoding)
  to avoid creating Unicode types on Python 2 clients.
* Issue #477: In Powershell downloader, use explicit rendering
  of strings, rather than rely on ``repr``, which can be
  incorrect (especially on Python 2).

19.1
----

* Issue #215: The bootstrap script ``ez_setup.py`` now
  automatically detects
  the latest version of setuptools (using PyPI JSON API) rather
  than hard-coding a particular value.
* Issue #475: Fix incorrect usage in _translate_metadata2.

19.0
----

* Issue #442: Use RawConfigParser for parsing .pypirc file.
  Interpolated values are no longer honored in .pypirc files.

18.8.1
------

* Issue #440: Prevent infinite recursion when a SandboxViolation
  or other UnpickleableException occurs in a sandbox context
  with setuptools hidden. Fixes regression introduced in Setuptools
  12.0.

18.8
----

* Deprecated ``egg_info.get_pkg_info_revision``.
* Issue #471: Don't rely on repr for an HTML attribute value in
  package_index.
* Issue #419: Avoid errors in FileMetadata when the metadata directory
  is broken.
* Issue #472: Remove deprecated use of 'U' in mode parameter
  when opening files.

18.7.1
------

* Issue #469: Refactored logic for Issue #419 fix to re-use metadata
  loading from Provider.

18.7
----

* Update dependency on certify.
* BB Pull Request #160: Improve detection of gui script in
  ``easy_install._adjust_header``.
* Made ``test.test_args`` a non-data property; alternate fix
  for the issue reported in BB Pull Request #155.
* Issue #453: In ``ez_setup`` bootstrap module, unload all
  ``pkg_resources`` modules following download.
* BB Pull Request #158: Honor PEP-488 when excluding
  files for namespace packages.
* Issue #419 and BB Pull Request #144: Add experimental support for
  reading the version info from distutils-installed metadata rather
  than using the version in the filename.

18.6.1
------

* Issue #464: Correct regression in invocation of superclass on old-style
  class on Python 2.

18.6
----

* Issue #439: When installing entry_point scripts under development,
  omit the version number of the package, allowing any version of the
  package to be used.

18.5
----

* In preparation for dropping support for Python 3.2, a warning is
  now logged when pkg_resources is imported on Python 3.2 or earlier
  Python 3 versions.
* `Add support for python_platform_implementation environment marker
  <https://github.com/jaraco/setuptools/pull/28>`_.
* `Fix dictionary mutation during iteration
  <https://github.com/jaraco/setuptools/pull/29>`_.

18.4
----

* Issue #446: Test command now always invokes unittest, even
  if no test suite is supplied.

18.3.2
------

* Correct another regression in setuptools.findall
  where the fix for Python #12885 was lost.

18.3.1
------

* Issue #425: Correct regression in setuptools.findall.

18.3
----

* BB Pull Request #135: Setuptools now allows disabling of
  the manipulation of the sys.path
  during the processing of the easy-install.pth file. To do so, set
  the environment variable ``SETUPTOOLS_SYS_PATH_TECHNIQUE`` to
  anything but "rewrite" (consider "raw"). During any install operation
  with manipulation disabled, setuptools packages will be appended to
  sys.path naturally.

  Future versions may change the default behavior to disable
  manipulation. If so, the default behavior can be retained by setting
  the variable to "rewrite".

* Issue #257: ``easy_install --version`` now shows more detail
  about the installation location and Python version.

* Refactor setuptools.findall in preparation for re-submission
  back to distutils.

18.2
----

* Issue #412: More efficient directory search in ``find_packages``.

18.1
----

* Upgrade to vendored packaging 15.3.

18.0.1
------

* Issue #401: Fix failure in test suite.

18.0
----

* Dropped support for builds with Pyrex. Only Cython is supported.
* Issue #288: Detect Cython later in the build process, after
  ``setup_requires`` dependencies are resolved.
  Projects backed by Cython can now be readily built
  with a ``setup_requires`` dependency. For example::

    ext = setuptools.Extension('mylib', ['src/CythonStuff.pyx', 'src/CStuff.c'])
    setuptools.setup(
        ...
        ext_modules=[ext],
        setup_requires=['cython'],
    )

  For compatibility with older versions of setuptools, packagers should
  still include ``src/CythonMod.c`` in the source distributions or
  require that Cython be present before building source distributions.
  However, for systems with this build of setuptools, Cython will be
  downloaded on demand.
* Issue #396: Fixed test failure on OS X.
* BB Pull Request #136: Remove excessive quoting from shebang headers
  for Jython.

17.1.1
------

* Backed out unintended changes to pkg_resources, restoring removal of
  deprecated imp module (`ref
  <https://bitbucket.org/pypa/setuptools/commits/f572ec9563d647fa8d4ffc534f2af8070ea07a8b#comment-1881283>`_).

17.1
----

* Issue #380: Add support for range operators on environment
  marker evaluation.

17.0
----

* Issue #378: Do not use internal importlib._bootstrap module.
* Issue #390: Disallow console scripts with path separators in
  the name. Removes unintended functionality and brings behavior
  into parity with pip.

16.0
----

* BB Pull Request #130: Better error messages for errors in
  parsed requirements.
* BB Pull Request #133: Removed ``setuptools.tests`` from the
  installed packages.
* BB Pull Request #129: Address deprecation warning due to usage
  of imp module.

15.2
----

* Issue #373: Provisionally expose
  ``pkg_resources._initialize_master_working_set``, allowing for
  imperative re-initialization of the master working set.

15.1
----

* Updated to Packaging 15.1 to address Packaging #28.
* Fix ``setuptools.sandbox._execfile()`` with Python 3.1.

15.0
----

* BB Pull Request #126: DistributionNotFound message now lists the package or
  packages that required it. E.g.::

      pkg_resources.DistributionNotFound: The 'colorama>=0.3.1' distribution was not found and is required by smlib.log.

  Note that zc.buildout once dependended on the string rendering of this
  message to determine the package that was not found. This expectation
  has since been changed, but older versions of buildout may experience
  problems. See Buildout #242 for details.

14.3.1
------

* Issue #307: Removed PEP-440 warning during parsing of versions
  in ``pkg_resources.Distribution``.
* Issue #364: Replace deprecated usage with recommended usage of
  ``EntryPoint.load``.

14.3
----

* Issue #254: When creating temporary egg cache on Unix, use mode 755
  for creating the directory to avoid the subsequent warning if
  the directory is group writable.

14.2
----

* Issue #137: Update ``Distribution.hashcmp`` so that Distributions with
  None for pyversion or platform can be compared against Distributions
  defining those attributes.

14.1.1
------

* Issue #360: Removed undesirable behavior from test runs, preventing
  write tests and installation to system site packages.

14.1
----

* BB Pull Request #125: Add ``__ne__`` to Requirement class.
* Various refactoring of easy_install.

14.0
----

* Bootstrap script now accepts ``--to-dir`` to customize save directory or
  allow for re-use of existing repository of setuptools versions. See
  BB Pull Request #112 for background.
* Issue #285: ``easy_install`` no longer will default to installing
  packages to the "user site packages" directory if it is itself installed
  there. Instead, the user must pass ``--user`` in all cases to install
  packages to the user site packages.
  This behavior now matches that of "pip install". To configure
  an environment to always install to the user site packages, consider
  using the "install-dir" and "scripts-dir" parameters to easy_install
  through an appropriate distutils config file.

13.0.2
------

* Issue #359: Include pytest.ini in the sdist so invocation of py.test on the
  sdist honors the pytest configuration.

13.0.1
------

Re-release of 13.0. Intermittent connectivity issues caused the release
process to fail and PyPI uploads no longer accept files for 13.0.

13.0
----

* Issue #356: Back out BB Pull Request #119 as it requires Setuptools 10 or later
  as the source during an upgrade.
* Removed build_py class from setup.py. According to 892f439d216e, this
  functionality was added to support upgrades from old Distribute versions,
  0.6.5 and 0.6.6.

12.4
----

* BB Pull Request #119: Restore writing of ``setup_requires`` to metadata
  (previously added in 8.4 and removed in 9.0).

12.3
----

* Documentation is now linked using the rst.linker package.
* Fix ``setuptools.command.easy_install.extract_wininst_cfg()``
  with Python 2.6 and 2.7.
* Issue #354. Added documentation on building setuptools
  documentation.

12.2
----

* Issue #345: Unload all modules under pkg_resources during
  ``ez_setup.use_setuptools()``.
* Issue #336: Removed deprecation from ``ez_setup.use_setuptools``,
  as it is clearly still used by buildout's bootstrap. ``ez_setup``
  remains deprecated for use by individual packages.
* Simplified implementation of ``ez_setup.use_setuptools``.

12.1
----

* BB Pull Request #118: Soften warning for non-normalized versions in
  Distribution.

12.0.5
------

* Issue #339: Correct Attribute reference in ``cant_write_to_target``.
* Issue #336: Deprecated ``ez_setup.use_setuptools``.

12.0.4
------

* Issue #335: Fix script header generation on Windows.

12.0.3
------

* Fixed incorrect class attribute in ``install_scripts``. Tests would be nice.

12.0.2
------

* Issue #331: Fixed ``install_scripts`` command on Windows systems corrupting
  the header.

12.0.1
------

* Restore ``setuptools.command.easy_install.sys_executable`` for pbr
  compatibility. For the future, tools should construct a CommandSpec
  explicitly.

12.0
----

* Issue #188: Setuptools now support multiple entities in the value for
  ``build.executable``, such that an executable of "/usr/bin/env my-python" may
  be specified. This means that systems with a specified executable whose name
  has spaces in the path must be updated to escape or quote that value.
* Deprecated ``easy_install.ScriptWriter.get_writer``, replaced by ``.best()``
  with slightly different semantics (no force_windows flag).

11.3.1
------

* Issue #327: Formalize and restore support for any printable character in an
  entry point name.

11.3
----

* Expose ``EntryPoint.resolve`` in place of EntryPoint._load, implementing the
  simple, non-requiring load. Deprecated all uses of ``EntryPoint._load``
  except for calling with no parameters, which is just a shortcut for
  ``ep.require(); ep.resolve();``.

  Apps currently invoking ``ep.load(require=False)`` should instead do the
  following if wanting to avoid the deprecating warning::

    getattr(ep, "resolve", lambda: ep.load(require=False))()

11.2
----

* Pip #2326: Report deprecation warning at stacklevel 2 for easier diagnosis.

11.1
----

* Issue #281: Since Setuptools 6.1 (Issue #268), a ValueError would be raised
  in certain cases where VersionConflict was raised with two arguments, which
  occurred in ``pkg_resources.WorkingSet.find``. This release adds support
  for indicating the dependent packages while maintaining support for
  a VersionConflict when no dependent package context is known. New unit tests
  now capture the expected interface.

11.0
----

* Interop #3: Upgrade to Packaging 15.0; updates to PEP 440 so that >1.7 does
  not exclude 1.7.1 but does exclude 1.7.0 and 1.7.0.post1.

10.2.1
------

* Issue #323: Fix regression in entry point name parsing.

10.2
----

* Deprecated use of EntryPoint.load(require=False). Passing a boolean to a
  function to select behavior is an anti-pattern. Instead use
  ``Entrypoint._load()``.
* Substantial refactoring of all unit tests. Tests are now much leaner and
  re-use a lot of fixtures and contexts for better clarity of purpose.

10.1
----

* Issue #320: Added a compatibility implementation of
  ``sdist._default_revctrl``
  so that systems relying on that interface do not fail (namely, Ubuntu 12.04
  and similar Debian releases).

10.0.1
------

* Issue #319: Fixed issue installing pure distutils packages.

10.0
----

* Issue #313: Removed built-in support for subversion. Projects wishing to
  retain support for subversion will need to use a third party library. The
  extant implementation is being ported to `setuptools_svn
  <https://pypi.org/project/setuptools_svn/>`_.
* Issue #315: Updated setuptools to hide its own loaded modules during
  installation of another package. This change will enable setuptools to
  upgrade (or downgrade) itself even when its own metadata and implementation
  change.

9.1
---

* Prefer vendored packaging library `as recommended
  <https://github.com/jaraco/setuptools/commit/170657b68f4b92e7e1bf82f5e19a831f5744af67#commitcomment-9109448>`_.

9.0.1
-----

* Issue #312: Restored presence of pkg_resources API tests (doctest) to sdist.

9.0
---

* Issue #314: Disabled support for ``setup_requires`` metadata to avoid issue
  where Setuptools was unable to upgrade over earlier versions.

8.4
---

* BB Pull Request #106: Now write ``setup_requires`` metadata.

8.3
---

* Issue #311: Decoupled pkg_resources from setuptools once again.
  ``pkg_resources`` is now a package instead of a module.

8.2.1
-----

* Issue #306: Suppress warnings about Version format except in select scenarios
  (such as installation).

8.2
---

* BB Pull Request #85: Search egg-base when adding egg-info to manifest.

8.1
---

* Upgrade ``packaging`` to 14.5, giving preference to "rc" as designator for
  release candidates over "c".
* PEP-440 warnings are now raised as their own class,
  ``pkg_resources.PEP440Warning``, instead of RuntimeWarning.
* Disabled warnings on empty versions.

8.0.4
-----

* Upgrade ``packaging`` to 14.4, fixing an error where there is a
  different result for if 2.0.5 is contained within >2.0dev and >2.0.dev even
  though normalization rules should have made them equal.
* Issue #296: Add warning when a version is parsed as legacy. This warning will
  make it easier for developers to recognize deprecated version numbers.

8.0.3
-----

* Issue #296: Restored support for ``__hash__`` on parse_version results.

8.0.2
-----

* Issue #296: Restored support for ``__getitem__`` and sort operations on
  parse_version result.

8.0.1
-----

* Issue #296: Restore support for iteration over parse_version result, but
  deprecated that usage with a warning. Fixes failure with buildout.

8.0
---

* Implement PEP 440 within
  pkg_resources and setuptools. This change
  deprecates some version numbers such that they will no longer be installable
  without using the ``===`` escape hatch. See `the changes to test_resources
  <https://bitbucket.org/pypa/setuptools/commits/dcd552da643c4448056de84c73d56da6d70769d5#chg-setuptools/tests/test_resources.py>`_
  for specific examples of version numbers and specifiers that are no longer
  supported. Setuptools now "vendors" the `packaging
  <https://github.com/pypa/packaging>`_ library.

7.0
---

* Issue #80, Issue #209: Eggs that are downloaded for ``setup_requires``,
  ``test_requires``, etc. are now placed in a ``./.eggs`` directory instead of
  directly in the current directory. This choice of location means the files
  can be readily managed (removed, ignored). Additionally,
  later phases or invocations of setuptools will not detect the package as
  already installed and ignore it for permanent install (See #209).

  This change is indicated as backward-incompatible as installations that
  depend on the installation in the current directory will need to account for
  the new location. Systems that ignore ``*.egg`` will probably need to be
  adapted to ignore ``.eggs``. The files will need to be manually moved or
  will be retrieved again. Most use cases will require no attention.

6.1
---

* Issue #268: When resolving package versions, a VersionConflict now reports
  which package previously required the conflicting version.

6.0.2
-----

* Issue #262: Fixed regression in pip install due to egg-info directories
  being omitted. Re-opens Issue #118.

6.0.1
-----

* Issue #259: Fixed regression with namespace package handling on ``single
  version, externally managed`` installs.

6.0
---

* Issue #100: When building a distribution, Setuptools will no longer match
  default files using platform-dependent case sensitivity, but rather will
  only match the files if their case matches exactly. As a result, on Windows
  and other case-insensitive file systems, files with names such as
  'readme.txt' or 'README.TXT' will be omitted from the distribution and a
  warning will be issued indicating that 'README.txt' was not found. Other
  filenames affected are:

    - README.rst
    - README
    - setup.cfg
    - setup.py (or the script name)
    - test/test*.py

  Any users producing distributions with filenames that match those above
  case-insensitively, but not case-sensitively, should rename those files in
  their repository for better portability.
* BB Pull Request #72: When using ``single_version_externally_managed``, the
  exclusion list now includes Python 3.2 ``__pycache__`` entries.
* BB Pull Request #76 and BB Pull Request #78: lines in top_level.txt are now
  ordered deterministically.
* Issue #118: The egg-info directory is now no longer included in the list
  of outputs.
* Issue #258: Setuptools now patches distutils msvc9compiler to
  recognize the specially-packaged compiler package for easy extension module
  support on Python 2.6, 2.7, and 3.2.

5.8
---

* Issue #237: ``pkg_resources`` now uses explicit detection of Python 2 vs.
  Python 3, supporting environments where builtins have been patched to make
  Python 3 look more like Python 2.

5.7
---

* Issue #240: Based on real-world performance measures against 5.4, zip
  manifests are now cached in all circumstances. The
  ``PKG_RESOURCES_CACHE_ZIP_MANIFESTS`` environment variable is no longer
  relevant. The observed "memory increase" referenced in the 5.4 release
  notes and detailed in Issue #154 was likely not an increase over the status
  quo, but rather only an increase over not storing the zip info at all.

5.6
---

* Issue #242: Use absolute imports in svn_utils to avoid issues if the
  installing package adds an xml module to the path.

5.5.1
-----

* Issue #239: Fix typo in 5.5 such that fix did not take.

5.5
---

* Issue #239: Setuptools now includes the setup_requires directive on
  Distribution objects and validates the syntax just like install_requires
  and tests_require directives.

5.4.2
-----

* Issue #236: Corrected regression in execfile implementation for Python 2.6.

5.4.1
-----

* Python #7776: (ssl_support) Correct usage of host for validation when
  tunneling for HTTPS.

5.4
---

* Issue #154: ``pkg_resources`` will now cache the zip manifests rather than
  re-processing the same file from disk multiple times, but only if the
  environment variable ``PKG_RESOURCES_CACHE_ZIP_MANIFESTS`` is set. Clients
  that package many modules in the same zip file will see some improvement
  in startup time by enabling this feature. This feature is not enabled by
  default because it causes a substantial increase in memory usage.

5.3
---

* Issue #185: Make svn tagging work on the new style SVN metadata.
  Thanks cazabon!
* Prune revision control directories (e.g .svn) from base path
  as well as sub-directories.

5.2
---

* Added a `Developer Guide
  <https://setuptools.readthedocs.io/en/latest/developer-guide.html>`_ to the official
  documentation.
* Some code refactoring and cleanup was done with no intended behavioral
  changes.
* During install_egg_info, the generated lines for namespace package .pth
  files are now processed even during a dry run.

5.1
---

* Issue #202: Implemented more robust cache invalidation for the ZipImporter,
  building on the work in Issue #168. Special thanks to Jurko Gospodnetic and
  PJE.

5.0.2
-----

* Issue #220: Restored script templates.

5.0.1
-----

* Renamed script templates to end with .tmpl now that they no longer need
  to be processed by 2to3. Fixes spurious syntax errors during build/install.

5.0
---

* Issue #218: Re-release of 3.8.1 to signal that it supersedes 4.x.
* Incidentally, script templates were updated not to include the triple-quote
  escaping.

3.7.1 and 3.8.1 and 4.0.1
-------------------------

* Issue #213: Use legacy StringIO behavior for compatibility under pbr.
* Issue #218: Setuptools 3.8.1 superseded 4.0.1, and 4.x was removed
  from the available versions to install.

4.0
---

* Issue #210: ``setup.py develop`` now copies scripts in binary mode rather
  than text mode, matching the behavior of the ``install`` command.

3.8
---

* Extend Issue #197 workaround to include all Python 3 versions prior to
  3.2.2.

3.7
---

* Issue #193: Improved handling of Unicode filenames when building manifests.

3.6
---

* Issue #203: Honor proxy settings for Powershell downloader in the bootstrap
  routine.

3.5.2
-----

* Issue #168: More robust handling of replaced zip files and stale caches.
  Fixes ZipImportError complaining about a 'bad local header'.

3.5.1
-----

* Issue #199: Restored ``install._install`` for compatibility with earlier
  NumPy versions.

3.5
---

* Issue #195: Follow symbolic links in find_packages (restoring behavior
  broken in 3.4).
* Issue #197: On Python 3.1, PKG-INFO is now saved in a UTF-8 encoding instead
  of ``sys.getpreferredencoding`` to match the behavior on Python 2.6-3.4.
* Issue #192: Preferred bootstrap location is now
  https://bootstrap.pypa.io/ez_setup.py (mirrored from former location).

3.4.4
-----

* Issue #184: Correct failure where find_package over-matched packages
  when directory traversal isn't short-circuited.

3.4.3
-----

* Issue #183: Really fix test command with Python 3.1.

3.4.2
-----

* Issue #183: Fix additional regression in test command on Python 3.1.

3.4.1
-----

* Issue #180: Fix regression in test command not caught by py.test-run tests.

3.4
---

* Issue #176: Add parameter to the test command to support a custom test
  runner: --test-runner or -r.
* Issue #177: Now assume most common invocation to install command on
  platforms/environments without stack support (issuing a warning). Setuptools
  now installs naturally on IronPython. Behavior on CPython should be
  unchanged.

3.3
---

* Add ``include`` parameter to ``setuptools.find_packages()``.

3.2
---

* BB Pull Request #39: Add support for C++ targets from Cython ``.pyx`` files.
* Issue #162: Update dependency on certifi to 1.0.1.
* Issue #164: Update dependency on wincertstore to 0.2.

3.1
---

* Issue #161: Restore Features functionality to allow backward compatibility
  (for Features) until the uses of that functionality is sufficiently removed.

3.0.2
-----

* Correct typo in previous bugfix.

3.0.1
-----

* Issue #157: Restore support for Python 2.6 in bootstrap script where
  ``zipfile.ZipFile`` does not yet have support for context managers.

3.0
---

* Issue #125: Prevent Subversion support from creating a ~/.subversion
  directory just for checking the presence of a Subversion repository.
* Issue #12: Namespace packages are now imported lazily. That is, the mere
  declaration of a namespace package in an egg on ``sys.path`` no longer
  causes it to be imported when ``pkg_resources`` is imported. Note that this
  change means that all of a namespace package's ``__init__.py`` files must
  include a ``declare_namespace()`` call in order to ensure that they will be
  handled properly at runtime. In 2.x it was possible to get away without
  including the declaration, but only at the cost of forcing namespace
  packages to be imported early, which 3.0 no longer does.
* Issue #148: When building (bdist_egg), setuptools no longer adds
  ``__init__.py`` files to namespace packages. Any packages that rely on this
  behavior will need to create ``__init__.py`` files and include the
  ``declare_namespace()``.
* Issue #7: Setuptools itself is now distributed as a zip archive in addition to
  tar archive. ez_setup.py now uses zip archive. This approach avoids the potential
  security vulnerabilities presented by use of tar archives in ez_setup.py.
  It also leverages the security features added to ZipFile.extract in Python 2.7.4.
* Issue #65: Removed deprecated Features functionality.
* BB Pull Request #28: Remove backport of ``_bytecode_filenames`` which is
  available in Python 2.6 and later, but also has better compatibility with
  Python 3 environments.
* Issue #156: Fix spelling of __PYVENV_LAUNCHER__ variable.

2.2
---

* Issue #141: Restored fix for allowing setup_requires dependencies to
  override installed dependencies during setup.
* Issue #128: Fixed issue where only the first dependency link was honored
  in a distribution where multiple dependency links were supplied.

2.1.2
-----

* Issue #144: Read long_description using codecs module to avoid errors
  installing on systems where LANG=C.

2.1.1
-----

* Issue #139: Fix regression in re_finder for CVS repos (and maybe Git repos
  as well).

2.1
---

* Issue #129: Suppress inspection of ``*.whl`` files when searching for files
  in a zip-imported file.
* Issue #131: Fix RuntimeError when constructing an egg fetcher.

2.0.2
-----

* Fix NameError during installation with Python implementations (e.g. Jython)
  not containing parser module.
* Fix NameError in ``sdist:re_finder``.

2.0.1
-----

* Issue #124: Fixed error in list detection in upload_docs.

2.0
---

* Issue #121: Exempt lib2to3 pickled grammars from DirectorySandbox.
* Issue #41: Dropped support for Python 2.4 and Python 2.5. Clients requiring
  setuptools for those versions of Python should use setuptools 1.x.
* Removed ``setuptools.command.easy_install.HAS_USER_SITE``. Clients
  expecting this boolean variable should use ``site.ENABLE_USER_SITE``
  instead.
* Removed ``pkg_resources.ImpWrapper``. Clients that expected this class
  should use ``pkgutil.ImpImporter`` instead.

1.4.2
-----

* Issue #116: Correct TypeError when reading a local package index on Python
  3.

1.4.1
-----

* Issue #114: Use ``sys.getfilesystemencoding`` for decoding config in
  ``bdist_wininst`` distributions.

* Issue #105 and Issue #113: Establish a more robust technique for
  determining the terminal encoding::

    1. Try ``getpreferredencoding``
    2. If that returns US_ASCII or None, try the encoding from
       ``getdefaultlocale``. If that encoding was a "fallback" because Python
       could not figure it out from the environment or OS, encoding remains
       unresolved.
    3. If the encoding is resolved, then make sure Python actually implements
       the encoding.
    4. On the event of an error or unknown codec, revert to fallbacks
       (UTF-8 on Darwin, ASCII on everything else).
    5. On the encoding is 'mac-roman' on Darwin, use UTF-8 as 'mac-roman' was
       a bug on older Python releases.

    On a side note, it would seem that the encoding only matters for when SVN
    does not yet support ``--xml`` and when getting repository and svn version
    numbers. The ``--xml`` technique should yield UTF-8 according to some
    messages on the SVN mailing lists. So if the version numbers are always
    7-bit ASCII clean, it may be best to only support the file parsing methods
    for legacy SVN releases and support for SVN without the subprocess command
    would simple go away as support for the older SVNs does.

1.4
---

* Issue #27: ``easy_install`` will now use credentials from .pypirc if
  present for connecting to the package index.
* BB Pull Request #21: Omit unwanted newlines in ``package_index._encode_auth``
  when the username/password pair length indicates wrapping.

1.3.2
-----

* Issue #99: Fix filename encoding issues in SVN support.

1.3.1
-----

* Remove exuberant warning in SVN support when SVN is not used.

1.3
---

* Address security vulnerability in SSL match_hostname check as reported in
  Python #17997.
* Prefer `backports.ssl_match_hostname
  <https://pypi.org/project/backports.ssl_match_hostname/>`_ for backport
  implementation if present.
* Correct NameError in ``ssl_support`` module (``socket.error``).

1.2
---

* Issue #26: Add support for SVN 1.7. Special thanks to Philip Thiem for the
  contribution.
* Issue #93: Wheels are now distributed with every release. Note that as
  reported in Issue #108, as of Pip 1.4, scripts aren't installed properly
  from wheels. Therefore, if using Pip to install setuptools from a wheel,
  the ``easy_install`` command will not be available.
* Setuptools "natural" launcher support, introduced in 1.0, is now officially
  supported.

1.1.7
-----

* Fixed behavior of NameError handling in 'script template (dev).py' (script
  launcher for 'develop' installs).
* ``ez_setup.py`` now ensures partial downloads are cleaned up following
  a failed download.
* Distribute #363 and Issue #55: Skip an sdist test that fails on locales
  other than UTF-8.

1.1.6
-----

* Distribute #349: ``sandbox.execfile`` now opens the target file in binary
  mode, thus honoring a BOM in the file when compiled.

1.1.5
-----

* Issue #69: Second attempt at fix (logic was reversed).

1.1.4
-----

* Issue #77: Fix error in upload command (Python 2.4).

1.1.3
-----

* Fix NameError in previous patch.

1.1.2
-----

* Issue #69: Correct issue where 404 errors are returned for URLs with
  fragments in them (such as #egg=).

1.1.1
-----

* Issue #75: Add ``--insecure`` option to ez_setup.py to accommodate
  environments where a trusted SSL connection cannot be validated.
* Issue #76: Fix AttributeError in upload command with Python 2.4.

1.1
---

* Issue #71 (Distribute #333): EasyInstall now puts less emphasis on the
  condition when a host is blocked via ``--allow-hosts``.
* Issue #72: Restored Python 2.4 compatibility in ``ez_setup.py``.

1.0
---

* Issue #60: On Windows, Setuptools supports deferring to another launcher,
  such as Vinay Sajip's `pylauncher <https://bitbucket.org/pypa/pylauncher>`_
  (included with Python 3.3) to launch console and GUI scripts and not install
  its own launcher executables. This experimental functionality is currently
  only enabled if  the ``SETUPTOOLS_LAUNCHER`` environment variable is set to
  "natural". In the future, this behavior may become default, but only after
  it has matured and seen substantial adoption. The ``SETUPTOOLS_LAUNCHER``
  also accepts "executable" to force the default behavior of creating launcher
  executables.
* Issue #63: Bootstrap script (ez_setup.py) now prefers Powershell, curl, or
  wget for retrieving the Setuptools tarball for improved security of the
  install. The script will still fall back to a simple ``urlopen`` on
  platforms that do not have these tools.
* Issue #65: Deprecated the ``Features`` functionality.
* Issue #52: In ``VerifyingHTTPSConn``, handle a tunnelled (proxied)
  connection.

Backward-Incompatible Changes
=============================

This release includes a couple of backward-incompatible changes, but most if
not all users will find 1.0 a drop-in replacement for 0.9.

* Issue #50: Normalized API of environment marker support. Specifically,
  removed line number and filename from SyntaxErrors when returned from
  `pkg_resources.invalid_marker`. Any clients depending on the specific
  string representation of exceptions returned by that function may need to
  be updated to account for this change.
* Issue #50: SyntaxErrors generated by `pkg_resources.invalid_marker` are
  normalized for cross-implementation consistency.
* Removed ``--ignore-conflicts-at-my-risk`` and ``--delete-conflicting``
  options to easy_install. These options have been deprecated since 0.6a11.

0.9.8
-----

* Issue #53: Fix NameErrors in `_vcs_split_rev_from_url`.

0.9.7
-----

* Issue #49: Correct AttributeError on PyPy where a hashlib.HASH object does
  not have a `.name` attribute.
* Issue #34: Documentation now refers to bootstrap script in code repository
  referenced by bookmark.
* Add underscore-separated keys to environment markers (markerlib).

0.9.6
-----

* Issue #44: Test failure on Python 2.4 when MD5 hash doesn't have a `.name`
  attribute.

0.9.5
-----

* Python #17980: Fix security vulnerability in SSL certificate validation.

0.9.4
-----

* Issue #43: Fix issue (introduced in 0.9.1) with version resolution when
  upgrading over other releases of Setuptools.

0.9.3
-----

* Issue #42: Fix new ``AttributeError`` introduced in last fix.

0.9.2
-----

* Issue #42: Fix regression where blank checksums would trigger an
  ``AttributeError``.

0.9.1
-----

* Distribute #386: Allow other positional and keyword arguments to os.open.
* Corrected dependency on certifi mis-referenced in 0.9.

0.9
---

* `package_index` now validates hashes other than MD5 in download links.

0.8
---

* Code base now runs on Python 2.4 - Python 3.3 without Python 2to3
  conversion.

0.7.8
-----

* Distribute #375: Yet another fix for yet another regression.

0.7.7
-----

* Distribute #375: Repair AttributeError created in last release (redo).
* Issue #30: Added test for get_cache_path.

0.7.6
-----

* Distribute #375: Repair AttributeError created in last release.

0.7.5
-----

* Issue #21: Restore Python 2.4 compatibility in ``test_easy_install``.
* Distribute #375: Merged additional warning from Distribute 0.6.46.
* Now honor the environment variable
  ``SETUPTOOLS_DISABLE_VERSIONED_EASY_INSTALL_SCRIPT`` in addition to the now
  deprecated ``DISTRIBUTE_DISABLE_VERSIONED_EASY_INSTALL_SCRIPT``.

0.7.4
-----

* Issue #20: Fix comparison of parsed SVN version on Python 3.

0.7.3
-----

* Issue #1: Disable installation of Windows-specific files on non-Windows systems.
* Use new sysconfig module with Python 2.7 or >=3.2.

0.7.2
-----

* Issue #14: Use markerlib when the `parser` module is not available.
* Issue #10: ``ez_setup.py`` now uses HTTPS to download setuptools from PyPI.

0.7.1
-----

* Fix NameError (Issue #3) again - broken in bad merge.

0.7
---

* Merged Setuptools and Distribute. See docs/merge.txt for details.

Added several features that were slated for setuptools 0.6c12:

* Index URL now defaults to HTTPS.
* Added experimental environment marker support. Now clients may designate a
  PEP-426 environment marker for "extra" dependencies. Setuptools uses this
  feature in ``setup.py`` for optional SSL and certificate validation support
  on older platforms. Based on Distutils-SIG discussions, the syntax is
  somewhat tentative. There should probably be a PEP with a firmer spec before
  the feature should be considered suitable for use.
* Added support for SSL certificate validation when installing packages from
  an HTTPS service.

0.7b4
-----

* Issue #3: Fixed NameError in SSL support.

0.6.49
------

* Move warning check in ``get_cache_path`` to follow the directory creation
  to avoid errors when the cache path does not yet exist. Fixes the error
  reported in Distribute #375.

0.6.48
------

* Correct AttributeError in ``ResourceManager.get_cache_path`` introduced in
  0.6.46 (redo).

0.6.47
------

* Correct AttributeError in ``ResourceManager.get_cache_path`` introduced in
  0.6.46.

0.6.46
------

* Distribute #375: Issue a warning if the PYTHON_EGG_CACHE or otherwise
  customized egg cache location specifies a directory that's group- or
  world-writable.

0.6.45
------

* Distribute #379: ``distribute_setup.py`` now traps VersionConflict as well,
  restoring ability to upgrade from an older setuptools version.

0.6.44
------

* ``distribute_setup.py`` has been updated to allow Setuptools 0.7 to
  satisfy use_setuptools.

0.6.43
------

* Distribute #378: Restore support for Python 2.4 Syntax (regression in 0.6.42).

0.6.42
------

* External links finder no longer yields duplicate links.
* Distribute #337: Moved site.py to setuptools/site-patch.py (graft of very old
  patch from setuptools trunk which inspired PR #31).

0.6.41
------

* Distribute #27: Use public api for loading resources from zip files rather than
  the private method `_zip_directory_cache`.
* Added a new function ``easy_install.get_win_launcher`` which may be used by
  third-party libraries such as buildout to get a suitable script launcher.

0.6.40
------

* Distribute #376: brought back cli.exe and gui.exe that were deleted in the
  previous release.

0.6.39
------

* Add support for console launchers on ARM platforms.
* Fix possible issue in GUI launchers where the subsystem was not supplied to
  the linker.
* Launcher build script now refactored for robustness.
* Distribute #375: Resources extracted from a zip egg to the file system now also
  check the contents of the file against the zip contents during each
  invocation of get_resource_filename.

0.6.38
------

* Distribute #371: The launcher manifest file is now installed properly.

0.6.37
------

* Distribute #143: Launcher scripts, including easy_install itself, are now
  accompanied by a manifest on 32-bit Windows environments to avoid the
  Installer Detection Technology and thus undesirable UAC elevation described
  in `this Microsoft article
  <http://technet.microsoft.com/en-us/library/cc709628%28WS.10%29.aspx>`_.

0.6.36
------

* BB Pull Request #35: In Buildout #64, it was reported that
  under Python 3, installation of distutils scripts could attempt to copy
  the ``__pycache__`` directory as a file, causing an error, apparently only
  under Windows. Easy_install now skips all directories when processing
  metadata scripts.

0.6.35
------


Note this release is backward-incompatible with distribute 0.6.23-0.6.34 in
how it parses version numbers.

* Distribute #278: Restored compatibility with distribute 0.6.22 and setuptools
  0.6. Updated the documentation to match more closely with the version
  parsing as intended in setuptools 0.6.

0.6.34
------

* Distribute #341: 0.6.33 fails to build under Python 2.4.

0.6.33
------

* Fix 2 errors with Jython 2.5.
* Fix 1 failure with Jython 2.5 and 2.7.
* Disable workaround for Jython scripts on Linux systems.
* Distribute #336: `setup.py` no longer masks failure exit code when tests fail.
* Fix issue in pkg_resources where try/except around a platform-dependent
  import would trigger hook load failures on Mercurial. See pull request 32
  for details.
* Distribute #341: Fix a ResourceWarning.

0.6.32
------

* Fix test suite with Python 2.6.
* Fix some DeprecationWarnings and ResourceWarnings.
* Distribute #335: Backed out `setup_requires` superceding installed requirements
  until regression can be addressed.

0.6.31
------

* Distribute #303: Make sure the manifest only ever contains UTF-8 in Python 3.
* Distribute #329: Properly close files created by tests for compatibility with
  Jython.
* Work around Jython #1980 and Jython #1981.
* Distribute #334: Provide workaround for packages that reference `sys.__stdout__`
  such as numpy does. This change should address
  `virtualenv #359 <https://github.com/pypa/virtualenv/issues/359>`_ as long
  as the system encoding is UTF-8 or the IO encoding is specified in the
  environment, i.e.::

     PYTHONIOENCODING=utf8 pip install numpy

* Fix for encoding issue when installing from Windows executable on Python 3.
* Distribute #323: Allow `setup_requires` requirements to supercede installed
  requirements. Added some new keyword arguments to existing pkg_resources
  methods. Also had to updated how __path__ is handled for namespace packages
  to ensure that when a new egg distribution containing a namespace package is
  placed on sys.path, the entries in __path__ are found in the same order they
  would have been in had that egg been on the path when pkg_resources was
  first imported.

0.6.30
------

* Distribute #328: Clean up temporary directories in distribute_setup.py.
* Fix fatal bug in distribute_setup.py.

0.6.29
------

* BB Pull Request #14: Honor file permissions in zip files.
* Distribute #327: Merged pull request #24 to fix a dependency problem with pip.
* Merged pull request #23 to fix https://github.com/pypa/virtualenv/issues/301.
* If Sphinx is installed, the `upload_docs` command now runs `build_sphinx`
  to produce uploadable documentation.
* Distribute #326: `upload_docs` provided mangled auth credentials under Python 3.
* Distribute #320: Fix check for "createable" in distribute_setup.py.
* Distribute #305: Remove a warning that was triggered during normal operations.
* Distribute #311: Print metadata in UTF-8 independent of platform.
* Distribute #303: Read manifest file with UTF-8 encoding under Python 3.
* Distribute #301: Allow to run tests of namespace packages when using 2to3.
* Distribute #304: Prevent import loop in site.py under Python 3.3.
* Distribute #283: Reenable scanning of `*.pyc` / `*.pyo` files on Python 3.3.
* Distribute #299: The develop command didn't work on Python 3, when using 2to3,
  as the egg link would go to the Python 2 source. Linking to the 2to3'd code
  in build/lib makes it work, although you will have to rebuild the module
  before testing it.
* Distribute #306: Even if 2to3 is used, we build in-place under Python 2.
* Distribute #307: Prints the full path when .svn/entries is broken.
* Distribute #313: Support for sdist subcommands (Python 2.7)
* Distribute #314: test_local_index() would fail an OS X.
* Distribute #310: Non-ascii characters in a namespace __init__.py causes errors.
* Distribute #218: Improved documentation on behavior of `package_data` and
  `include_package_data`. Files indicated by `package_data` are now included
  in the manifest.
* `distribute_setup.py` now allows a `--download-base` argument for retrieving
  distribute from a specified location.

0.6.28
------

* Distribute #294: setup.py can now be invoked from any directory.
* Scripts are now installed honoring the umask.
* Added support for .dist-info directories.
* Distribute #283: Fix and disable scanning of `*.pyc` / `*.pyo` files on
  Python 3.3.

0.6.27
------

* Support current snapshots of CPython 3.3.
* Distribute now recognizes README.rst as a standard, default readme file.
* Exclude 'encodings' modules when removing modules from sys.modules.
  Workaround for #285.
* Distribute #231: Don't fiddle with system python when used with buildout
  (bootstrap.py)

0.6.26
------

* Distribute #183: Symlinked files are now extracted from source distributions.
* Distribute #227: Easy_install fetch parameters are now passed during the
  installation of a source distribution; now fulfillment of setup_requires
  dependencies will honor the parameters passed to easy_install.

0.6.25
------

* Distribute #258: Workaround a cache issue
* Distribute #260: distribute_setup.py now accepts the --user parameter for
  Python 2.6 and later.
* Distribute #262: package_index.open_with_auth no longer throws LookupError
  on Python 3.
* Distribute #269: AttributeError when an exception occurs reading Manifest.in
  on late releases of Python.
* Distribute #272: Prevent TypeError when namespace package names are unicode
  and single-install-externally-managed is used. Also fixes PIP issue
  449.
* Distribute #273: Legacy script launchers now install with Python2/3 support.

0.6.24
------

* Distribute #249: Added options to exclude 2to3 fixers

0.6.23
------

* Distribute #244: Fixed a test
* Distribute #243: Fixed a test
* Distribute #239: Fixed a test
* Distribute #240: Fixed a test
* Distribute #241: Fixed a test
* Distribute #237: Fixed a test
* Distribute #238: easy_install now uses 64bit executable wrappers on 64bit Python
* Distribute #208: Fixed parsed_versions, it now honors post-releases as noted in the documentation
* Distribute #207: Windows cli and gui wrappers pass CTRL-C to child python process
* Distribute #227: easy_install now passes its arguments to setup.py bdist_egg
* Distribute #225: Fixed a NameError on Python 2.5, 2.4

0.6.21
------

* Distribute #225: FIxed a regression on py2.4

0.6.20
------

* Distribute #135: Include url in warning when processing URLs in package_index.
* Distribute #212: Fix issue where easy_instal fails on Python 3 on windows installer.
* Distribute #213: Fix typo in documentation.

0.6.19
------

* Distribute #206: AttributeError: 'HTTPMessage' object has no attribute 'getheaders'

0.6.18
------

* Distribute #210: Fixed a regression introduced by Distribute #204 fix.

0.6.17
------

* Support 'DISTRIBUTE_DISABLE_VERSIONED_EASY_INSTALL_SCRIPT' environment
  variable to allow to disable installation of easy_install-${version} script.
* Support Python >=3.1.4 and >=3.2.1.
* Distribute #204: Don't try to import the parent of a namespace package in
  declare_namespace
* Distribute #196: Tolerate responses with multiple Content-Length headers
* Distribute #205: Sandboxing doesn't preserve working_set. Leads to setup_requires
  problems.

0.6.16
------

* Builds sdist gztar even on Windows (avoiding Distribute #193).
* Distribute #192: Fixed metadata omitted on Windows when package_dir
  specified with forward-slash.
* Distribute #195: Cython build support.
* Distribute #200: Issues with recognizing 64-bit packages on Windows.

0.6.15
------

* Fixed typo in bdist_egg
* Several issues under Python 3 has been solved.
* Distribute #146: Fixed missing DLL files after easy_install of windows exe package.

0.6.14
------

* Distribute #170: Fixed unittest failure. Thanks to Toshio.
* Distribute #171: Fixed race condition in unittests cause deadlocks in test suite.
* Distribute #143: Fixed a lookup issue with easy_install.
  Thanks to David and Zooko.
* Distribute #174: Fixed the edit mode when its used with setuptools itself

0.6.13
------

* Distribute #160: 2.7 gives ValueError("Invalid IPv6 URL")
* Distribute #150: Fixed using ~/.local even in a --no-site-packages virtualenv
* Distribute #163: scan index links before external links, and don't use the md5 when
  comparing two distributions

0.6.12
------

* Distribute #149: Fixed various failures on 2.3/2.4

0.6.11
------

* Found another case of SandboxViolation - fixed
* Distribute #15 and Distribute #48: Introduced a socket timeout of 15 seconds on url openings
* Added indexsidebar.html into MANIFEST.in
* Distribute #108: Fixed TypeError with Python3.1
* Distribute #121: Fixed --help install command trying to actually install.
* Distribute #112: Added an os.makedirs so that Tarek's solution will work.
* Distribute #133: Added --no-find-links to easy_install
* Added easy_install --user
* Distribute #100: Fixed develop --user not taking '.' in PYTHONPATH into account
* Distribute #134: removed spurious UserWarnings. Patch by VanLindberg
* Distribute #138: cant_write_to_target error when setup_requires is used.
* Distribute #147: respect the sys.dont_write_bytecode flag

0.6.10
------

* Reverted change made for the DistributionNotFound exception because
  zc.buildout uses the exception message to get the name of the
  distribution.

0.6.9
-----

* Distribute #90: unknown setuptools version can be added in the working set
* Distribute #87: setupt.py doesn't try to convert distribute_setup.py anymore
  Initial Patch by arfrever.
* Distribute #89: added a side bar with a download link to the doc.
* Distribute #86: fixed missing sentence in pkg_resources doc.
* Added a nicer error message when a DistributionNotFound is raised.
* Distribute #80: test_develop now works with Python 3.1
* Distribute #93: upload_docs now works if there is an empty sub-directory.
* Distribute #70: exec bit on non-exec files
* Distribute #99: now the standalone easy_install command doesn't uses a
  "setup.cfg" if any exists in the working directory. It will use it
  only if triggered by ``install_requires`` from a setup.py call
  (install, develop, etc).
* Distribute #101: Allowing ``os.devnull`` in Sandbox
* Distribute #92: Fixed the "no eggs" found error with MacPort
  (platform.mac_ver() fails)
* Distribute #103: test_get_script_header_jython_workaround not run
  anymore under py3 with C or POSIX local. Contributed by Arfrever.
* Distribute #104: remvoved the assertion when the installation fails,
  with a nicer message for the end user.
* Distribute #100: making sure there's no SandboxViolation when
  the setup script patches setuptools.

0.6.8
-----

* Added "check_packages" in dist. (added in Setuptools 0.6c11)
* Fixed the DONT_PATCH_SETUPTOOLS state.

0.6.7
-----

* Distribute #58: Added --user support to the develop command
* Distribute #11: Generated scripts now wrap their call to the script entry point
  in the standard "if name == 'main'"
* Added the 'DONT_PATCH_SETUPTOOLS' environment variable, so virtualenv
  can drive an installation that doesn't patch a global setuptools.
* Reviewed unladen-swallow specific change from
  http://code.google.com/p/unladen-swallow/source/detail?spec=svn875&r=719
  and determined that it no longer applies. Distribute should work fine with
  Unladen Swallow 2009Q3.
* Distribute #21: Allow PackageIndex.open_url to gracefully handle all cases of a
  httplib.HTTPException instead of just InvalidURL and BadStatusLine.
* Removed virtual-python.py from this distribution and updated documentation
  to point to the actively maintained virtualenv instead.
* Distribute #64: use_setuptools no longer rebuilds the distribute egg every
  time it is run
* use_setuptools now properly respects the requested version
* use_setuptools will no longer try to import a distribute egg for the
  wrong Python version
* Distribute #74: no_fake should be True by default.
* Distribute #72: avoid a bootstrapping issue with easy_install -U

0.6.6
-----

* Unified the bootstrap file so it works on both py2.x and py3k without 2to3
  (patch by Holger Krekel)

0.6.5
-----

* Distribute #65: cli.exe and gui.exe are now generated at build time,
  depending on the platform in use.

* Distribute #67: Fixed doc typo (PEP 381/PEP 382).

* Distribute no longer shadows setuptools if we require a 0.7-series
  setuptools. And an error is raised when installing a 0.7 setuptools with
  distribute.

* When run from within buildout, no attempt is made to modify an existing
  setuptools egg, whether in a shared egg directory or a system setuptools.

* Fixed a hole in sandboxing allowing builtin file to write outside of
  the sandbox.

0.6.4
-----

* Added the generation of `distribute_setup_3k.py` during the release.
  This closes Distribute #52.

* Added an upload_docs command to easily upload project documentation to
  PyPI's https://pythonhosted.org. This close issue Distribute #56.

* Fixed a bootstrap bug on the use_setuptools() API.

0.6.3
-----

setuptools
==========

* Fixed a bunch of calls to file() that caused crashes on Python 3.

bootstrapping
=============

* Fixed a bug in sorting that caused bootstrap to fail on Python 3.

0.6.2
-----

setuptools
==========

* Added Python 3 support; see docs/python3.txt.
  This closes Old Setuptools #39.

* Added option to run 2to3 automatically when installing on Python 3.
  This closes issue Distribute #31.

* Fixed invalid usage of requirement.parse, that broke develop -d.
  This closes Old Setuptools #44.

* Fixed script launcher for 64-bit Windows.
  This closes Old Setuptools #2.

* KeyError when compiling extensions.
  This closes Old Setuptools #41.

bootstrapping
=============

* Fixed bootstrap not working on Windows. This closes issue Distribute #49.

* Fixed 2.6 dependencies. This closes issue Distribute #50.

* Make sure setuptools is patched when running through easy_install
  This closes Old Setuptools #40.

0.6.1
-----

setuptools
==========

* package_index.urlopen now catches BadStatusLine and malformed url errors.
  This closes Distribute #16 and Distribute #18.

* zip_ok is now False by default. This closes Old Setuptools #33.

* Fixed invalid URL error catching. Old Setuptools #20.

* Fixed invalid bootstraping with easy_install installation (Distribute #40).
  Thanks to Florian Schulze for the help.

* Removed buildout/bootstrap.py. A new repository will create a specific
  bootstrap.py script.


bootstrapping
=============

* The boostrap process leave setuptools alone if detected in the system
  and --root or --prefix is provided, but is not in the same location.
  This closes Distribute #10.

0.6
---

setuptools
==========

* Packages required at build time where not fully present at install time.
  This closes Distribute #12.

* Protected against failures in tarfile extraction. This closes Distribute #10.

* Made Jython api_tests.txt doctest compatible. This closes Distribute #7.

* sandbox.py replaced builtin type file with builtin function open. This
  closes Distribute #6.

* Immediately close all file handles. This closes Distribute #3.

* Added compatibility with Subversion 1.6. This references Distribute #1.

pkg_resources
=============

* Avoid a call to /usr/bin/sw_vers on OSX and use the official platform API
  instead. Based on a patch from ronaldoussoren. This closes issue #5.

* Fixed a SandboxViolation for mkdir that could occur in certain cases.
  This closes Distribute #13.

* Allow to find_on_path on systems with tight permissions to fail gracefully.
  This closes Distribute #9.

* Corrected inconsistency between documentation and code of add_entry.
  This closes Distribute #8.

* Immediately close all file handles. This closes Distribute #3.

easy_install
============

* Immediately close all file handles. This closes Distribute #3.

0.6c9
-----

 * Fixed a missing files problem when using Windows source distributions on
   non-Windows platforms, due to distutils not handling manifest file line
   endings correctly.

 * Updated Pyrex support to work with Pyrex 0.9.6 and higher.

 * Minor changes for Jython compatibility, including skipping tests that can't
   work on Jython.

 * Fixed not installing eggs in ``install_requires`` if they were also used for
   ``setup_requires`` or ``tests_require``.

 * Fixed not fetching eggs in ``install_requires`` when running tests.

 * Allow ``ez_setup.use_setuptools()`` to upgrade existing setuptools
   installations when called from a standalone ``setup.py``.

 * Added a warning if a namespace package is declared, but its parent package
   is not also declared as a namespace.

 * Support Subversion 1.5

 * Removed use of deprecated ``md5`` module if ``hashlib`` is available

 * Fixed ``bdist_wininst upload`` trying to upload the ``.exe`` twice

 * Fixed ``bdist_egg`` putting a ``native_libs.txt`` in the source package's
   ``.egg-info``, when it should only be in the built egg's ``EGG-INFO``.

 * Ensure that _full_name is set on all shared libs before extensions are
   checked for shared lib usage.  (Fixes a bug in the experimental shared
   library build support.)

 * Fix to allow unpacked eggs containing native libraries to fail more
   gracefully under Google App Engine (with an ``ImportError`` loading the
   C-based module, instead of getting a ``NameError``).

0.6c7
-----

 * Fixed ``distutils.filelist.findall()`` crashing on broken symlinks, and
   ``egg_info`` command failing on new, uncommitted SVN directories.

 * Fix import problems with nested namespace packages installed via
   ``--root`` or ``--single-version-externally-managed``, due to the
   parent package not having the child package as an attribute.

0.6c6
-----

 * Added ``--egg-path`` option to ``develop`` command, allowing you to force
   ``.egg-link`` files to use relative paths (allowing them to be shared across
   platforms on a networked drive).

 * Fix not building binary RPMs correctly.

 * Fix "eggsecutables" (such as setuptools' own egg) only being runnable with
   bash-compatible shells.

 * Fix ``#!`` parsing problems in Windows ``.exe`` script wrappers, when there
   was whitespace inside a quoted argument or at the end of the ``#!`` line
   (a regression introduced in 0.6c4).

 * Fix ``test`` command possibly failing if an older version of the project
   being tested was installed on ``sys.path`` ahead of the test source
   directory.

 * Fix ``find_packages()`` treating ``ez_setup`` and directories with ``.`` in
   their names as packages.

0.6c5
-----

 * Fix uploaded ``bdist_rpm`` packages being described as ``bdist_egg``
   packages under Python versions less than 2.5.

 * Fix uploaded ``bdist_wininst`` packages being described as suitable for
   "any" version by Python 2.5, even if a ``--target-version`` was specified.

0.6c4
-----

 * Overhauled Windows script wrapping to support ``bdist_wininst`` better.
   Scripts installed with ``bdist_wininst`` will always use ``#!python.exe`` or
   ``#!pythonw.exe`` as the executable name (even when built on non-Windows
   platforms!), and the wrappers will look for the executable in the script's
   parent directory (which should find the right version of Python).

 * Fix ``upload`` command not uploading files built by ``bdist_rpm`` or
   ``bdist_wininst`` under Python 2.3 and 2.4.

 * Add support for "eggsecutable" headers: a ``#!/bin/sh`` script that is
   prepended to an ``.egg`` file to allow it to be run as a script on Unix-ish
   platforms.  (This is mainly so that setuptools itself can have a single-file
   installer on Unix, without doing multiple downloads, dealing with firewalls,
   etc.)

 * Fix problem with empty revision numbers in Subversion 1.4 ``entries`` files

 * Use cross-platform relative paths in ``easy-install.pth`` when doing
   ``develop`` and the source directory is a subdirectory of the installation
   target directory.

 * Fix a problem installing eggs with a system packaging tool if the project
   contained an implicit namespace package; for example if the ``setup()``
   listed a namespace package ``foo.bar`` without explicitly listing ``foo``
   as a namespace package.

0.6c3
-----

 * Fixed breakages caused by Subversion 1.4's new "working copy" format

0.6c2
-----

 * The ``ez_setup`` module displays the conflicting version of setuptools (and
   its installation location) when a script requests a version that's not
   available.

 * Running ``setup.py develop`` on a setuptools-using project will now install
   setuptools if needed, instead of only downloading the egg.

0.6c1
-----

 * Fixed ``AttributeError`` when trying to download a ``setup_requires``
   dependency when a distribution lacks a ``dependency_links`` setting.

 * Made ``zip-safe`` and ``not-zip-safe`` flag files contain a single byte, so
   as to play better with packaging tools that complain about zero-length
   files.

 * Made ``setup.py develop`` respect the ``--no-deps`` option, which it
   previously was ignoring.

 * Support ``extra_path`` option to ``setup()`` when ``install`` is run in
   backward-compatibility mode.

 * Source distributions now always include a ``setup.cfg`` file that explicitly
   sets ``egg_info`` options such that they produce an identical version number
   to the source distribution's version number.  (Previously, the default
   version number could be different due to the use of ``--tag-date``, or if
   the version was overridden on the command line that built the source
   distribution.)

0.6b4
-----

 * Fix ``register`` not obeying name/version set by ``egg_info`` command, if
   ``egg_info`` wasn't explicitly run first on the same command line.

 * Added ``--no-date`` and ``--no-svn-revision`` options to ``egg_info``
   command, to allow suppressing tags configured in ``setup.cfg``.

 * Fixed redundant warnings about missing ``README`` file(s); it should now
   appear only if you are actually a source distribution.

0.6b3
-----

 * Fix ``bdist_egg`` not including files in subdirectories of ``.egg-info``.

 * Allow ``.py`` files found by the ``include_package_data`` option to be
   automatically included. Remove duplicate data file matches if both
   ``include_package_data`` and ``package_data`` are used to refer to the same
   files.

0.6b1
-----

 * Strip ``module`` from the end of compiled extension modules when computing
   the name of a ``.py`` loader/wrapper.  (Python's import machinery ignores
   this suffix when searching for an extension module.)

0.6a11
------

 * Added ``test_loader`` keyword to support custom test loaders

 * Added ``setuptools.file_finders`` entry point group to allow implementing
   revision control plugins.

 * Added ``--identity`` option to ``upload`` command.

 * Added ``dependency_links`` to allow specifying URLs for ``--find-links``.

 * Enhanced test loader to scan packages as well as modules, and call
   ``additional_tests()`` if present to get non-unittest tests.

 * Support namespace packages in conjunction with system packagers, by omitting
   the installation of any ``__init__.py`` files for namespace packages, and
   adding a special ``.pth`` file to create a working package in
   ``sys.modules``.

 * Made ``--single-version-externally-managed`` automatic when ``--root`` is
   used, so that most system packagers won't require special support for
   setuptools.

 * Fixed ``setup_requires``, ``tests_require``, etc. not using ``setup.cfg`` or
   other configuration files for their option defaults when installing, and
   also made the install use ``--multi-version`` mode so that the project
   directory doesn't need to support .pth files.

 * ``MANIFEST.in`` is now forcibly closed when any errors occur while reading
   it. Previously, the file could be left open and the actual error would be
   masked by problems trying to remove the open file on Windows systems.

0.6a10
------

 * Fixed the ``develop`` command ignoring ``--find-links``.

0.6a9
-----

 * The ``sdist`` command no longer uses the traditional ``MANIFEST`` file to
   create source distributions.  ``MANIFEST.in`` is still read and processed,
   as are the standard defaults and pruning. But the manifest is built inside
   the project's ``.egg-info`` directory as ``SOURCES.txt``, and it is rebuilt
   every time the ``egg_info`` command is run.

 * Added the ``include_package_data`` keyword to ``setup()``, allowing you to
   automatically include any package data listed in revision control or
   ``MANIFEST.in``

 * Added the ``exclude_package_data`` keyword to ``setup()``, allowing you to
   trim back files included via the ``package_data`` and
   ``include_package_data`` options.

 * Fixed ``--tag-svn-revision`` not working when run from a source
   distribution.

 * Added warning for namespace packages with missing ``declare_namespace()``

 * Added ``tests_require`` keyword to ``setup()``, so that e.g. packages
   requiring ``nose`` to run unit tests can make this dependency optional
   unless the ``test`` command is run.

 * Made all commands that use ``easy_install`` respect its configuration
   options, as this was causing some problems with ``setup.py install``.

 * Added an ``unpack_directory()`` driver to ``setuptools.archive_util``, so
   that you can process a directory tree through a processing filter as if it
   were a zipfile or tarfile.

 * Added an internal ``install_egg_info`` command to use as part of old-style
   ``install`` operations, that installs an ``.egg-info`` directory with the
   package.

 * Added a ``--single-version-externally-managed`` option to the ``install``
   command so that you can more easily wrap a "flat" egg in a system package.

 * Enhanced ``bdist_rpm`` so that it installs single-version eggs that
   don't rely on a ``.pth`` file. The ``--no-egg`` option has been removed,
   since all RPMs are now built in a more backwards-compatible format.

 * Support full roundtrip translation of eggs to and from ``bdist_wininst``
   format. Running ``bdist_wininst`` on a setuptools-based package wraps the
   egg in an .exe that will safely install it as an egg (i.e., with metadata
   and entry-point wrapper scripts), and ``easy_install`` can turn the .exe
   back into an ``.egg`` file or directory and install it as such.


0.6a8
-----

 * Fixed some problems building extensions when Pyrex was installed, especially
   with Python 2.4 and/or packages using SWIG.

 * Made ``develop`` command accept all the same options as ``easy_install``,
   and use the ``easy_install`` command's configuration settings as defaults.

 * Made ``egg_info --tag-svn-revision`` fall back to extracting the revision
   number from ``PKG-INFO`` in case it is being run on a source distribution of
   a snapshot taken from a Subversion-based project.

 * Automatically detect ``.dll``, ``.so`` and ``.dylib`` files that are being
   installed as data, adding them to ``native_libs.txt`` automatically.

 * Fixed some problems with fresh checkouts of projects that don't include
   ``.egg-info/PKG-INFO`` under revision control and put the project's source
   code directly in the project directory. If such a package had any
   requirements that get processed before the ``egg_info`` command can be run,
   the setup scripts would fail with a "Missing 'Version:' header and/or
   PKG-INFO file" error, because the egg runtime interpreted the unbuilt
   metadata in a directory on ``sys.path`` (i.e. the current directory) as
   being a corrupted egg. Setuptools now monkeypatches the distribution
   metadata cache to pretend that the egg has valid version information, until
   it has a chance to make it actually be so (via the ``egg_info`` command).

0.6a5
-----

 * Fixed missing gui/cli .exe files in distribution. Fixed bugs in tests.

0.6a3
-----

 * Added ``gui_scripts`` entry point group to allow installing GUI scripts
   on Windows and other platforms.  (The special handling is only for Windows;
   other platforms are treated the same as for ``console_scripts``.)

0.6a2
-----

 * Added ``console_scripts`` entry point group to allow installing scripts
   without the need to create separate script files. On Windows, console
   scripts get an ``.exe`` wrapper so you can just type their name. On other
   platforms, the scripts are written without a file extension.

0.6a1
-----

 * Added support for building "old-style" RPMs that don't install an egg for
   the target package, using a ``--no-egg`` option.

 * The ``build_ext`` command now works better when using the ``--inplace``
   option and multiple Python versions. It now makes sure that all extensions
   match the current Python version, even if newer copies were built for a
   different Python version.

 * The ``upload`` command no longer attaches an extra ``.zip`` when uploading
   eggs, as PyPI now supports egg uploads without trickery.

 * The ``ez_setup`` script/module now displays a warning before downloading
   the setuptools egg, and attempts to check the downloaded egg against an
   internal MD5 checksum table.

 * Fixed the ``--tag-svn-revision`` option of ``egg_info`` not finding the
   latest revision number; it was using the revision number of the directory
   containing ``setup.py``, not the highest revision number in the project.

 * Added ``eager_resources`` setup argument

 * The ``sdist`` command now recognizes Subversion "deleted file" entries and
   does not include them in source distributions.

 * ``setuptools`` now embeds itself more thoroughly into the distutils, so that
   other distutils extensions (e.g. py2exe, py2app) will subclass setuptools'
   versions of things, rather than the native distutils ones.

 * Added ``entry_points`` and ``setup_requires`` arguments to ``setup()``;
   ``setup_requires`` allows you to automatically find and download packages
   that are needed in order to *build* your project (as opposed to running it).

 * ``setuptools`` now finds its commands, ``setup()`` argument validators, and
   metadata writers using entry points, so that they can be extended by
   third-party packages. See `Creating distutils Extensions
   <https://setuptools.readthedocs.io/en/latest/setuptools.html#creating-distutils-extensions>`_
   for more details.

 * The vestigial ``depends`` command has been removed. It was never finished
   or documented, and never would have worked without EasyInstall - which it
   pre-dated and was never compatible with.

0.5a12
------

 * The zip-safety scanner now checks for modules that might be used with
   ``python -m``, and marks them as unsafe for zipping, since Python 2.4 can't
   handle ``-m`` on zipped modules.

0.5a11
------

 * Fix breakage of the "develop" command that was caused by the addition of
   ``--always-unzip`` to the ``easy_install`` command.

0.5a9
-----

 * Include ``svn:externals`` directories in source distributions as well as
   normal subversion-controlled files and directories.

 * Added ``exclude=patternlist`` option to ``setuptools.find_packages()``

 * Changed --tag-svn-revision to include an "r" in front of the revision number
   for better readability.

 * Added ability to build eggs without including source files (except for any
   scripts, of course), using the ``--exclude-source-files`` option to
   ``bdist_egg``.

 * ``setup.py install`` now automatically detects when an "unmanaged" package
   or module is going to be on ``sys.path`` ahead of a package being installed,
   thereby preventing the newer version from being imported. If this occurs,
   a warning message is output to ``sys.stderr``, but installation proceeds
   anyway. The warning message informs the user what files or directories
   need deleting, and advises them they can also use EasyInstall (with the
   ``--delete-conflicting`` option) to do it automatically.

 * The ``egg_info`` command now adds a ``top_level.txt`` file to the metadata
   directory that lists all top-level modules and packages in the distribution.
   This is used by the ``easy_install`` command to find possibly-conflicting
   "unmanaged" packages when installing the distribution.

 * Added ``zip_safe`` and ``namespace_packages`` arguments to ``setup()``.
   Added package analysis to determine zip-safety if the ``zip_safe`` flag
   is not given, and advise the author regarding what code might need changing.

 * Fixed the swapped ``-d`` and ``-b`` options of ``bdist_egg``.

0.5a8
-----

 * The "egg_info" command now always sets the distribution metadata to "safe"
   forms of the distribution name and version, so that distribution files will
   be generated with parseable names (i.e., ones that don't include '-' in the
   name or version). Also, this means that if you use the various ``--tag``
   options of "egg_info", any distributions generated will use the tags in the
   version, not just egg distributions.

 * Added support for defining command aliases in distutils configuration files,
   under the "[aliases]" section. To prevent recursion and to allow aliases to
   call the command of the same name, a given alias can be expanded only once
   per command-line invocation. You can define new aliases with the "alias"
   command, either for the local, global, or per-user configuration.

 * Added "rotate" command to delete old distribution files, given a set of
   patterns to match and the number of files to keep.  (Keeps the most
   recently-modified distribution files matching each pattern.)

 * Added "saveopts" command that saves all command-line options for the current
   invocation to the local, global, or per-user configuration file. Useful for
   setting defaults without having to hand-edit a configuration file.

 * Added a "setopt" command that sets a single option in a specified distutils
   configuration file.

0.5a7
-----

 * Added "upload" support for egg and source distributions, including a bug
   fix for "upload" and a temporary workaround for lack of .egg support in
   PyPI.

0.5a6
-----

 * Beefed up the "sdist" command so that if you don't have a MANIFEST.in, it
   will include all files under revision control (CVS or Subversion) in the
   current directory, and it will regenerate the list every time you create a
   source distribution, not just when you tell it to. This should make the
   default "do what you mean" more often than the distutils' default behavior
   did, while still retaining the old behavior in the presence of MANIFEST.in.

 * Fixed the "develop" command always updating .pth files, even if you
   specified ``-n`` or ``--dry-run``.

 * Slightly changed the format of the generated version when you use
   ``--tag-build`` on the "egg_info" command, so that you can make tagged
   revisions compare *lower* than the version specified in setup.py (e.g. by
   using ``--tag-build=dev``).

0.5a5
-----

 * Added ``develop`` command to ``setuptools``-based packages. This command
   installs an ``.egg-link`` pointing to the package's source directory, and
   script wrappers that ``execfile()`` the source versions of the package's
   scripts. This lets you put your development checkout(s) on sys.path without
   having to actually install them.  (To uninstall the link, use
   use ``setup.py develop --uninstall``.)

 * Added ``egg_info`` command to ``setuptools``-based packages. This command
   just creates or updates the "projectname.egg-info" directory, without
   building an egg.  (It's used by the ``bdist_egg``, ``test``, and ``develop``
   commands.)

 * Enhanced the ``test`` command so that it doesn't install the package, but
   instead builds any C extensions in-place, updates the ``.egg-info``
   metadata, adds the source directory to ``sys.path``, and runs the tests
   directly on the source. This avoids an "unmanaged" installation of the
   package to ``site-packages`` or elsewhere.

 * Made ``easy_install`` a standard ``setuptools`` command, moving it from
   the ``easy_install`` module to ``setuptools.command.easy_install``. Note
   that if you were importing or extending it, you must now change your imports
   accordingly.  ``easy_install.py`` is still installed as a script, but not as
   a module.

0.5a4
-----

 * Setup scripts using setuptools can now list their dependencies directly in
   the setup.py file, without having to manually create a ``depends.txt`` file.
   The ``install_requires`` and ``extras_require`` arguments to ``setup()``
   are used to create a dependencies file automatically. If you are manually
   creating ``depends.txt`` right now, please switch to using these setup
   arguments as soon as practical, because ``depends.txt`` support will be
   removed in the 0.6 release cycle. For documentation on the new arguments,
   see the ``setuptools.dist.Distribution`` class.

 * Setup scripts using setuptools now always install using ``easy_install``
   internally, for ease of uninstallation and upgrading.

0.5a1
-----

 * Added support for "self-installation" bootstrapping. Packages can now
   include ``ez_setup.py`` in their source distribution, and add the following
   to their ``setup.py``, in order to automatically bootstrap installation of
   setuptools as part of their setup process::

    from ez_setup import use_setuptools
    use_setuptools()

    from setuptools import setup
    # etc...

0.4a2
-----

 * Added ``ez_setup.py`` installer/bootstrap script to make initial setuptools
   installation easier, and to allow distributions using setuptools to avoid
   having to include setuptools in their source distribution.

 * All downloads are now managed by the ``PackageIndex`` class (which is now
   subclassable and replaceable), so that embedders can more easily override
   download logic, give download progress reports, etc. The class has also
   been moved to the new ``setuptools.package_index`` module.

 * The ``Installer`` class no longer handles downloading, manages a temporary
   directory, or tracks the ``zip_ok`` option. Downloading is now handled
   by ``PackageIndex``, and ``Installer`` has become an ``easy_install``
   command class based on ``setuptools.Command``.

 * There is a new ``setuptools.sandbox.run_setup()`` API to invoke a setup
   script in a directory sandbox, and a new ``setuptools.archive_util`` module
   with an ``unpack_archive()`` API. These were split out of EasyInstall to
   allow reuse by other tools and applications.

 * ``setuptools.Command`` now supports reinitializing commands using keyword
   arguments to set/reset options. Also, ``Command`` subclasses can now set
   their ``command_consumes_arguments`` attribute to ``True`` in order to
   receive an ``args`` option containing the rest of the command line.

0.3a2
-----

 * Added new options to ``bdist_egg`` to allow tagging the egg's version number
   with a subversion revision number, the current date, or an explicit tag
   value. Run ``setup.py bdist_egg --help`` to get more information.

 * Misc. bug fixes

0.3a1
-----

 * Initial release.

.. image:: https://img.shields.io/pypi/v/setuptools.svg
   :target: https://pypi.org/project/setuptools

.. image:: https://readthedocs.org/projects/setuptools/badge/?version=latest
    :target: https://setuptools.readthedocs.io

.. image:: https://img.shields.io/travis/pypa/setuptools/master.svg?label=Linux%20build%20%40%20Travis%20CI
   :target: https://travis-ci.org/pypa/setuptools

.. image:: https://img.shields.io/appveyor/ci/pypa/setuptools/master.svg?label=Windows%20build%20%40%20Appveyor
   :target: https://ci.appveyor.com/project/pypa/setuptools/branch/master

.. image:: https://img.shields.io/codecov/c/github/pypa/setuptools/master.svg
   :target: https://codecov.io/gh/pypa/setuptools

.. image:: https://img.shields.io/pypi/pyversions/setuptools.svg

See the `Installation Instructions
<https://packaging.python.org/installing/>`_ in the Python Packaging
User's Guide for instructions on installing, upgrading, and uninstalling
Setuptools.

The project is `maintained at GitHub <https://github.com/pypa/setuptools>`_.

Questions and comments should be directed to the `distutils-sig
mailing list <http://mail.python.org/pipermail/distutils-sig/>`_.
Bug reports and especially tested patches may be
submitted directly to the `bug tracker
<https://github.com/pypa/setuptools/issues>`_.


Code of Conduct
---------------

Everyone interacting in the setuptools project's codebases, issue trackers,
chat rooms, and mailing lists is expected to follow the
`PyPA Code of Conduct <https://www.pypa.io/en/latest/code-of-conduct/>`_.
pip
===

The `PyPA recommended
<https://packaging.python.org/en/latest/current/>`_
tool for installing Python packages.

* `Installation <https://pip.pypa.io/en/stable/installing.html>`_
* `Documentation <https://pip.pypa.io/>`_
* `Changelog <https://pip.pypa.io/en/stable/news.html>`_
* `Github Page <https://github.com/pypa/pip>`_
* `Issue Tracking <https://github.com/pypa/pip/issues>`_
* `User mailing list <http://groups.google.com/group/python-virtualenv>`_
* `Dev mailing list <http://groups.google.com/group/pypa-dev>`_
* User IRC: #pypa on Freenode.
* Dev IRC: #pypa-dev on Freenode.


.. image:: https://img.shields.io/pypi/v/pip.svg
   :target: https://pypi.python.org/pypi/pip

.. image:: https://img.shields.io/travis/pypa/pip/master.svg
   :target: http://travis-ci.org/pypa/pip

.. image:: https://img.shields.io/appveyor/ci/pypa/pip.svg
   :target: https://ci.appveyor.com/project/pypa/pip/history

.. image:: https://readthedocs.org/projects/pip/badge/?version=stable
   :target: https://pip.pypa.io/en/stable

Code of Conduct
---------------

Everyone interacting in the pip project's codebases, issue trackers, chat
rooms, and mailing lists is expected to follow the `PyPA Code of Conduct`_.

.. _PyPA Code of Conduct: https://www.pypa.io/en/latest/code-of-conduct/
.. :changelog:

History
-------

2.5 (2017-03-07)
++++++++++++++++

- Fix bug with Katakana middle dot context-rule (Thanks, Greg
  Shikhman.)

2.4 (2017-03-01)
++++++++++++++++

- Restore IDNAError to be a subclass of UnicodeError, as some users of
  this library are only looking for the latter to catch invalid strings.

2.3 (2017-02-28)
++++++++++++++++

- Fix bugs relating to deriving IDNAError from UnicodeError.
- More memory footprint improvements (Thanks, Alex Gaynor)

2.2 (2016-12-21)
++++++++++++++++

- Made some changes to the UTS 46 data that should allow Jython to get around
  64kb Java class limits. (Thanks, John A. Booth and Marcin Płonka.)
- In Python 2.6, skip two tests that rely on data not present in that
  Python version's unicodedata module.
- Use relative imports to help downstream users.

2.1 (2016-03-20)
++++++++++++++++

- Memory consumption optimizations. The library should consume significantly
  less memory through smarter data structures being used to represent
  relevant Unicode properties. Many thanks to Shivaram Lingamneni for this
  patch.
- Patches to make library work better with Python 2.6. The core library
  currently works however the unit testing does not. (Thanks, Robert
  Buchholz)
- Better affix all Unicode codepoint properties to a specific version.

2.0 (2015-05-18)
++++++++++++++++

- Added support for Unicode IDNA Compatibility Processing (aka Unicode
  Technical Standard #46). Big thanks to Jon Ribbens who contributed this
  functionality.

1.1 (2015-01-27)
++++++++++++++++

- Use IDNA properties from Unicode 6.3.0. Internet Architecture Board (IAB)
  issued statement recommending against the use of Unicode 7.0.0 until
  issues relating to U+08A1 codepoint are resolved. See http://goo.gl/Ed1n0K
- Identify some cases when label would be too longer to be a legal DNS name
  and raise an exception. (Thanks, Ed Lewis)

1.0 (2014-10-12)
++++++++++++++++

- Update IDNA properties for Unicode 7.0.0.

0.9 (2014-07-18)
++++++++++++++++

- Fix issue with non-UTF-8 environments reading the README file
  now that it contains non-ASCII. (Thanks, Tom Prince)
- Codec functions are useful, so they are separated into their own
  module, rather than just existing for compatibility reasons.
- Add LICENSE file.

0.8 (2014-07-09)
++++++++++++++++

- Added MANIFEST.in for correct source distribution compilation.

0.7 (2014-07-09)
++++++++++++++++

- Filled out missing tests for various functions.
- Fix bug in CONTEXTO validation for Greek lower numeral sign (U+0375)
- Fix bug in CONTEXTO validation for Japanese middle dot (U+30FB)
- Improved documentation
- Move designation to Stable

0.6 (2014-04-29)
++++++++++++++++

- Minor improvements to Python 3 support, tests (Thanks, Derek Wilson)

0.5 (2014-02-05)
++++++++++++++++

- Update IDNA properties for Unicode 6.3.0.

0.4 (2014-01-07)
++++++++++++++++

- Fix trove classifier for Python 3. (Thanks, Hynek Schlawack)

0.3 (2013-07-18)
++++++++++++++++

- Ported to Python 3.

0.2 (2013-07-16)
++++++++++++++++

- Improve packaging.
- More conformant, passes all relevant tests in the Unicode TR46 test suite.

0.1 (2013-05-27)
++++++++++++++++

- First proof-of-concept version.
Internationalized Domain Names in Applications (IDNA)
=====================================================

Support for the Internationalised Domain Names in Applications
(IDNA) protocol as specified in `RFC 5891 <http://tools.ietf.org/html/rfc5891>`_.
This is the latest version of the protocol and is sometimes referred to as
“IDNA 2008”.

This library also provides support for Unicode Technical Standard 46,
`Unicode IDNA Compatibility Processing <http://unicode.org/reports/tr46/>`_.

This acts as a suitable replacement for the “encodings.idna” module that
comes with the Python standard library, but only supports the
old, deprecated IDNA specification (`RFC 3490 <http://tools.ietf.org/html/rfc3490>`_).

Basic functions are simply executed:

.. code-block:: pycon

    # Python 3
    >>> import idna
    >>> idna.encode('ドメイン.テスト')
    b'xn--eckwd4c7c.xn--zckzah'
    >>> print(idna.decode('xn--eckwd4c7c.xn--zckzah'))
    ドメイン.テスト

    # Python 2
    >>> import idna
    >>> idna.encode(u'ドメイン.テスト')
    'xn--eckwd4c7c.xn--zckzah'
    >>> print idna.decode('xn--eckwd4c7c.xn--zckzah')
    ドメイン.テスト

Packages
--------

The latest tagged release version is published in the PyPI repository:

.. image:: https://badge.fury.io/py/idna.svg
   :target: http://badge.fury.io/py/idna


Installation
------------

To install this library, you can use pip:

.. code-block:: bash

    $ pip install idna

Alternatively, you can install the package using the bundled setup script:

.. code-block:: bash

    $ python setup.py install

This library works with Python 2.6 or later, and Python 3.3 or later.


Usage
-----

For typical usage, the ``encode`` and ``decode`` functions will take a domain
name argument and perform a conversion to A-labels or U-labels respectively.

.. code-block:: pycon

    # Python 3
    >>> import idna
    >>> idna.encode('ドメイン.テスト')
    b'xn--eckwd4c7c.xn--zckzah'
    >>> print(idna.decode('xn--eckwd4c7c.xn--zckzah'))
    ドメイン.テスト

You may use the codec encoding and decoding methods using the
``idna.codec`` module:

.. code-block:: pycon

    # Python 2
    >>> import idna.codec
    >>> print u'домена.испытание'.encode('idna')
    xn--80ahd1agd.xn--80akhbyknj4f
    >>> print 'xn--80ahd1agd.xn--80akhbyknj4f'.decode('idna')
    домена.испытание

Conversions can be applied at a per-label basis using the ``ulabel`` or ``alabel``
functions if necessary:

.. code-block:: pycon

    # Python 2
    >>> idna.alabel(u'测试')
    'xn--0zwm56d'

Compatibility Mapping (UTS #46)
+++++++++++++++++++++++++++++++

As described in `RFC 5895 <http://tools.ietf.org/html/rfc5895>`_, the IDNA
specification no longer normalizes input from different potential ways a user
may input a domain name. This functionality, known as a “mapping”, is now
considered by the specification to be a local user-interface issue distinct
from IDNA conversion functionality.

This library provides one such mapping, that was developed by the Unicode
Consortium. Known as `Unicode IDNA Compatibility Processing <http://unicode.org/reports/tr46/>`_,
it provides for both a regular mapping for typical applications, as well as
a transitional mapping to help migrate from older IDNA 2003 applications.

For example, “Königsgäßchen” is not a permissible label as *LATIN CAPITAL
LETTER K* is not allowed (nor are capital letters in general). UTS 46 will
convert this into lower case prior to applying the IDNA conversion.

.. code-block:: pycon

    # Python 3
    >>> import idna
    >>> idna.encode(u'Königsgäßchen')
    ...
    idna.core.InvalidCodepoint: Codepoint U+004B at position 1 of 'Königsgäßchen' not allowed
    >>> idna.encode('Königsgäßchen', uts46=True)
    b'xn--knigsgchen-b4a3dun'
    >>> print(idna.decode('xn--knigsgchen-b4a3dun'))
    königsgäßchen

Transitional processing provides conversions to help transition from the older
2003 standard to the current standard. For example, in the original IDNA
specification, the *LATIN SMALL LETTER SHARP S* (ß) was converted into two
*LATIN SMALL LETTER S* (ss), whereas in the current IDNA specification this
conversion is not performed.

.. code-block:: pycon

    # Python 2
    >>> idna.encode(u'Königsgäßchen', uts46=True, transitional=True)
    'xn--knigsgsschen-lcb0w'

Implementors should use transitional processing with caution, only in rare
cases where conversion from legacy labels to current labels must be performed
(i.e. IDNA implementations that pre-date 2008). For typical applications
that just need to convert labels, transitional processing is unlikely to be
beneficial and could produce unexpected incompatible results.

``encodings.idna`` Compatibility
++++++++++++++++++++++++++++++++

Function calls from the Python built-in ``encodings.idna`` module are
mapped to their IDNA 2008 equivalents using the ``idna.compat`` module.
Simply substitute the ``import`` clause in your code to refer to the
new module name.

Exceptions
----------

All errors raised during the conversion following the specification should
raise an exception derived from the ``idna.IDNAError`` base class.

More specific exceptions that may be generated as ``idna.IDNABidiError``
when the error reflects an illegal combination of left-to-right and right-to-left
characters in a label; ``idna.InvalidCodepoint`` when a specific codepoint is
an illegal character in an IDN label (i.e. INVALID); and ``idna.InvalidCodepointContext``
when the codepoint is illegal based on its positional context (i.e. it is CONTEXTO
or CONTEXTJ but the contextual requirements are not satisfied.)

Testing
-------

The library has a test suite based on each rule of the IDNA specification, as
well as tests that are provided as part of the Unicode Technical Standard 46,
`Unicode IDNA Compatibility Processing <http://unicode.org/reports/tr46/>`_.

The tests are run automatically on each commit at Travis CI:

.. image:: https://travis-ci.org/kjd/idna.svg?branch=master
   :target: https://travis-ci.org/kjd/idna
===========
QEMU README
===========

QEMU is a generic and open source machine & userspace emulator and
virtualizer.

QEMU is capable of emulating a complete machine in software without any
need for hardware virtualization support. By using dynamic translation,
it achieves very good performance. QEMU can also integrate with the Xen
and KVM hypervisors to provide emulated hardware while allowing the
hypervisor to manage the CPU. With hypervisor support, QEMU can achieve
near native performance for CPUs. When QEMU emulates CPUs directly it is
capable of running operating systems made for one machine (e.g. an ARMv7
board) on a different machine (e.g. an x86_64 PC board).

QEMU is also capable of providing userspace API virtualization for Linux
and BSD kernel interfaces. This allows binaries compiled against one
architecture ABI (e.g. the Linux PPC64 ABI) to be run on a host using a
different architecture ABI (e.g. the Linux x86_64 ABI). This does not
involve any hardware emulation, simply CPU and syscall emulation.

QEMU aims to fit into a variety of use cases. It can be invoked directly
by users wishing to have full control over its behaviour and settings.
It also aims to facilitate integration into higher level management
layers, by providing a stable command line interface and monitor API.
It is commonly invoked indirectly via the libvirt library when using
open source applications such as oVirt, OpenStack and virt-manager.

QEMU as a whole is released under the GNU General Public License,
version 2. For full licensing details, consult the LICENSE file.


Documentation
=============

Documentation can be found hosted online at
`<https://www.qemu.org/documentation/>`_. The documentation for the
current development version that is available at
`<https://www.qemu.org/docs/master/>`_ is generated from the ``docs/``
folder in the source tree, and is built by `Sphinx
<https://www.sphinx-doc.org/en/master/>_`.


Building
========

QEMU is multi-platform software intended to be buildable on all modern
Linux platforms, OS-X, Win32 (via the Mingw64 toolchain) and a variety
of other UNIX targets. The simple steps to build QEMU are:


.. code-block:: shell

  mkdir build
  cd build
  ../configure
  make

Additional information can also be found online via the QEMU website:

* `<https://wiki.qemu.org/Hosts/Linux>`_
* `<https://wiki.qemu.org/Hosts/Mac>`_
* `<https://wiki.qemu.org/Hosts/W32>`_


Submitting patches
==================

The QEMU source code is maintained under the GIT version control system.

.. code-block:: shell

   git clone https://gitlab.com/qemu-project/qemu.git

When submitting patches, one common approach is to use 'git
format-patch' and/or 'git send-email' to format & send the mail to the
qemu-devel@nongnu.org mailing list. All patches submitted must contain
a 'Signed-off-by' line from the author. Patches should follow the
guidelines set out in the `style section
<https://www.qemu.org/docs/master/devel/style.html>` of
the Developers Guide.

Additional information on submitting patches can be found online via
the QEMU website

* `<https://wiki.qemu.org/Contribute/SubmitAPatch>`_
* `<https://wiki.qemu.org/Contribute/TrivialPatches>`_

The QEMU website is also maintained under source control.

.. code-block:: shell

  git clone https://gitlab.com/qemu-project/qemu-web.git

* `<https://www.qemu.org/2017/02/04/the-new-qemu-website-is-up/>`_

A 'git-publish' utility was created to make above process less
cumbersome, and is highly recommended for making regular contributions,
or even just for sending consecutive patch series revisions. It also
requires a working 'git send-email' setup, and by default doesn't
automate everything, so you may want to go through the above steps
manually for once.

For installation instructions, please go to

*  `<https://github.com/stefanha/git-publish>`_

The workflow with 'git-publish' is:

.. code-block:: shell

  $ git checkout master -b my-feature
  $ # work on new commits, add your 'Signed-off-by' lines to each
  $ git publish

Your patch series will be sent and tagged as my-feature-v1 if you need to refer
back to it in the future.

Sending v2:

.. code-block:: shell

  $ git checkout my-feature # same topic branch
  $ # making changes to the commits (using 'git rebase', for example)
  $ git publish

Your patch series will be sent with 'v2' tag in the subject and the git tip
will be tagged as my-feature-v2.

Bug reporting
=============

The QEMU project uses GitLab issues to track bugs. Bugs
found when running code built from QEMU git or upstream released sources
should be reported via:

* `<https://gitlab.com/qemu-project/qemu/-/issues>`_

If using QEMU via an operating system vendor pre-built binary package, it
is preferable to report bugs to the vendor's own bug tracker first. If
the bug is also known to affect latest upstream code, it can also be
reported via GitLab.

For additional information on bug reporting consult:

* `<https://wiki.qemu.org/Contribute/ReportABug>`_


ChangeLog
=========

For version history and release notes, please visit
`<https://wiki.qemu.org/ChangeLog/>`_ or look at the git history for
more detailed information.


Contact
=======

The QEMU community can be contacted in a number of ways, with the two
main methods being email and IRC

* `<mailto:qemu-devel@nongnu.org>`_
* `<https://lists.nongnu.org/mailman/listinfo/qemu-devel>`_
* #qemu on irc.oftc.net

Information on additional methods of contacting the community can be
found online via the QEMU website:

* `<https://wiki.qemu.org/Contribute/StartHere>`_
Changes
=======

1.24.2 (2019-04-17)
-------------------

* Don't load system certificates by default when any other ``ca_certs``, ``ca_certs_dir`` or
  ``ssl_context`` parameters are specified.

* Remove Authorization header regardless of case when redirecting to cross-site. (Issue #1510)
 
* Add support for IPv6 addresses in subjectAltName section of certificates. (Issue #1269)


1.24.1 (2018-11-02)
-------------------

* Remove quadratic behavior within ``GzipDecoder.decompress()`` (Issue #1467)

* Restored functionality of ``ciphers`` parameter for ``create_urllib3_context()``. (Issue #1462)


1.24 (2018-10-16)
-----------------

* Allow key_server_hostname to be specified when initializing a PoolManager to allow custom SNI to be overridden. (Pull #1449)

* Test against Python 3.7 on AppVeyor. (Pull #1453)

* Early-out ipv6 checks when running on App Engine. (Pull #1450)

* Change ambiguous description of backoff_factor (Pull #1436)

* Add ability to handle multiple Content-Encodings (Issue #1441 and Pull #1442)

* Skip DNS names that can't be idna-decoded when using pyOpenSSL (Issue #1405).

* Add a server_hostname parameter to HTTPSConnection which allows for
  overriding the SNI hostname sent in the handshake. (Pull #1397)

* Drop support for EOL Python 2.6 (Pull #1429 and Pull #1430)

* Fixed bug where responses with header Content-Type: message/* erroneously
  raised HeaderParsingError, resulting in a warning being logged. (Pull #1439)

* Move urllib3 to src/urllib3 (Pull #1409)


1.23 (2018-06-04)
-----------------

* Allow providing a list of headers to strip from requests when redirecting
  to a different host. Defaults to the ``Authorization`` header. Different
  headers can be set via ``Retry.remove_headers_on_redirect``. (Issue #1316)

* Fix ``util.selectors._fileobj_to_fd`` to accept ``long`` (Issue #1247).

* Dropped Python 3.3 support. (Pull #1242)

* Put the connection back in the pool when calling stream() or read_chunked() on
  a chunked HEAD response. (Issue #1234)

* Fixed pyOpenSSL-specific ssl client authentication issue when clients
  attempted to auth via certificate + chain (Issue #1060)

* Add the port to the connectionpool connect print (Pull #1251)

* Don't use the ``uuid`` module to create multipart data boundaries. (Pull #1380)

* ``read_chunked()`` on a closed response returns no chunks. (Issue #1088)

* Add Python 2.6 support to ``contrib.securetransport`` (Pull #1359)

* Added support for auth info in url for SOCKS proxy (Pull #1363)


1.22 (2017-07-20)
-----------------

* Fixed missing brackets in ``HTTP CONNECT`` when connecting to IPv6 address via
  IPv6 proxy. (Issue #1222)

* Made the connection pool retry on ``SSLError``.  The original ``SSLError``
  is available on ``MaxRetryError.reason``. (Issue #1112)

* Drain and release connection before recursing on retry/redirect.  Fixes
  deadlocks with a blocking connectionpool. (Issue #1167)

* Fixed compatibility for cookiejar. (Issue #1229)

* pyopenssl: Use vendored version of ``six``. (Issue #1231)


1.21.1 (2017-05-02)
-------------------

* Fixed SecureTransport issue that would cause long delays in response body
  delivery. (Pull #1154)

* Fixed regression in 1.21 that threw exceptions when users passed the
  ``socket_options`` flag to the ``PoolManager``.  (Issue #1165)

* Fixed regression in 1.21 that threw exceptions when users passed the
  ``assert_hostname`` or ``assert_fingerprint`` flag to the ``PoolManager``.
  (Pull #1157)


1.21 (2017-04-25)
-----------------

* Improved performance of certain selector system calls on Python 3.5 and
  later. (Pull #1095)

* Resolved issue where the PyOpenSSL backend would not wrap SysCallError
  exceptions appropriately when sending data. (Pull #1125)

* Selectors now detects a monkey-patched select module after import for modules
  that patch the select module like eventlet, greenlet. (Pull #1128)

* Reduced memory consumption when streaming zlib-compressed responses
  (as opposed to raw deflate streams). (Pull #1129)

* Connection pools now use the entire request context when constructing the
  pool key. (Pull #1016)

* ``PoolManager.connection_from_*`` methods now accept a new keyword argument,
  ``pool_kwargs``, which are merged with the existing ``connection_pool_kw``.
  (Pull #1016)

* Add retry counter for ``status_forcelist``. (Issue #1147)

* Added ``contrib`` module for using SecureTransport on macOS:
  ``urllib3.contrib.securetransport``.  (Pull #1122)

* urllib3 now only normalizes the case of ``http://`` and ``https://`` schemes:
  for schemes it does not recognise, it assumes they are case-sensitive and
  leaves them unchanged.
  (Issue #1080)


1.20 (2017-01-19)
-----------------

* Added support for waiting for I/O using selectors other than select,
  improving urllib3's behaviour with large numbers of concurrent connections.
  (Pull #1001)

* Updated the date for the system clock check. (Issue #1005)

* ConnectionPools now correctly consider hostnames to be case-insensitive.
  (Issue #1032)

* Outdated versions of PyOpenSSL now cause the PyOpenSSL contrib module
  to fail when it is injected, rather than at first use. (Pull #1063)

* Outdated versions of cryptography now cause the PyOpenSSL contrib module
  to fail when it is injected, rather than at first use. (Issue #1044)

* Automatically attempt to rewind a file-like body object when a request is
  retried or redirected. (Pull #1039)

* Fix some bugs that occur when modules incautiously patch the queue module.
  (Pull #1061)

* Prevent retries from occurring on read timeouts for which the request method
  was not in the method whitelist. (Issue #1059)

* Changed the PyOpenSSL contrib module to lazily load idna to avoid
  unnecessarily bloating the memory of programs that don't need it. (Pull
  #1076)

* Add support for IPv6 literals with zone identifiers. (Pull #1013)

* Added support for socks5h:// and socks4a:// schemes when working with SOCKS
  proxies, and controlled remote DNS appropriately. (Issue #1035)


1.19.1 (2016-11-16)
-------------------

* Fixed AppEngine import that didn't function on Python 3.5. (Pull #1025)


1.19 (2016-11-03)
-----------------

* urllib3 now respects Retry-After headers on 413, 429, and 503 responses when
  using the default retry logic. (Pull #955)

* Remove markers from setup.py to assist ancient setuptools versions. (Issue
  #986)

* Disallow superscripts and other integerish things in URL ports. (Issue #989)

* Allow urllib3's HTTPResponse.stream() method to continue to work with
  non-httplib underlying FPs. (Pull #990)

* Empty filenames in multipart headers are now emitted as such, rather than
  being suppressed. (Issue #1015)

* Prefer user-supplied Host headers on chunked uploads. (Issue #1009)


1.18.1 (2016-10-27)
-------------------

* CVE-2016-9015. Users who are using urllib3 version 1.17 or 1.18 along with
  PyOpenSSL injection and OpenSSL 1.1.0 *must* upgrade to this version. This
  release fixes a vulnerability whereby urllib3 in the above configuration
  would silently fail to validate TLS certificates due to erroneously setting
  invalid flags in OpenSSL's ``SSL_CTX_set_verify`` function. These erroneous
  flags do not cause a problem in OpenSSL versions before 1.1.0, which
  interprets the presence of any flag as requesting certificate validation.

  There is no PR for this patch, as it was prepared for simultaneous disclosure
  and release. The master branch received the same fix in PR #1010.


1.18 (2016-09-26)
-----------------

* Fixed incorrect message for IncompleteRead exception. (PR #973)

* Accept ``iPAddress`` subject alternative name fields in TLS certificates.
  (Issue #258)

* Fixed consistency of ``HTTPResponse.closed`` between Python 2 and 3.
  (Issue #977)

* Fixed handling of wildcard certificates when using PyOpenSSL. (Issue #979)


1.17 (2016-09-06)
-----------------

* Accept ``SSLContext`` objects for use in SSL/TLS negotiation. (Issue #835)

* ConnectionPool debug log now includes scheme, host, and port. (Issue #897)

* Substantially refactored documentation. (Issue #887)

* Used URLFetch default timeout on AppEngine, rather than hardcoding our own.
  (Issue #858)

* Normalize the scheme and host in the URL parser (Issue #833)

* ``HTTPResponse`` contains the last ``Retry`` object, which now also
  contains retries history. (Issue #848)

* Timeout can no longer be set as boolean, and must be greater than zero.
  (PR #924)

* Removed pyasn1 and ndg-httpsclient from dependencies used for PyOpenSSL. We
  now use cryptography and idna, both of which are already dependencies of
  PyOpenSSL. (PR #930)

* Fixed infinite loop in ``stream`` when amt=None. (Issue #928)

* Try to use the operating system's certificates when we are using an
  ``SSLContext``. (PR #941)

* Updated cipher suite list to allow ChaCha20+Poly1305. AES-GCM is preferred to
  ChaCha20, but ChaCha20 is then preferred to everything else. (PR #947)

* Updated cipher suite list to remove 3DES-based cipher suites. (PR #958)

* Removed the cipher suite fallback to allow HIGH ciphers. (PR #958)

* Implemented ``length_remaining`` to determine remaining content
  to be read. (PR #949)

* Implemented ``enforce_content_length`` to enable exceptions when
  incomplete data chunks are received. (PR #949)

* Dropped connection start, dropped connection reset, redirect, forced retry,
  and new HTTPS connection log levels to DEBUG, from INFO. (PR #967)


1.16 (2016-06-11)
-----------------

* Disable IPv6 DNS when IPv6 connections are not possible. (Issue #840)

* Provide ``key_fn_by_scheme`` pool keying mechanism that can be
  overridden. (Issue #830)

* Normalize scheme and host to lowercase for pool keys, and include
  ``source_address``. (Issue #830)

* Cleaner exception chain in Python 3 for ``_make_request``.
  (Issue #861)

* Fixed installing ``urllib3[socks]`` extra. (Issue #864)

* Fixed signature of ``ConnectionPool.close`` so it can actually safely be
  called by subclasses. (Issue #873)

* Retain ``release_conn`` state across retries. (Issues #651, #866)

* Add customizable ``HTTPConnectionPool.ResponseCls``, which defaults to
  ``HTTPResponse`` but can be replaced with a subclass. (Issue #879)


1.15.1 (2016-04-11)
-------------------

* Fix packaging to include backports module. (Issue #841)


1.15 (2016-04-06)
-----------------

* Added Retry(raise_on_status=False). (Issue #720)

* Always use setuptools, no more distutils fallback. (Issue #785)

* Dropped support for Python 3.2. (Issue #786)

* Chunked transfer encoding when requesting with ``chunked=True``.
  (Issue #790)

* Fixed regression with IPv6 port parsing. (Issue #801)

* Append SNIMissingWarning messages to allow users to specify it in
  the PYTHONWARNINGS environment variable. (Issue #816)

* Handle unicode headers in Py2. (Issue #818)

* Log certificate when there is a hostname mismatch. (Issue #820)

* Preserve order of request/response headers. (Issue #821)


1.14 (2015-12-29)
-----------------

* contrib: SOCKS proxy support! (Issue #762)

* Fixed AppEngine handling of transfer-encoding header and bug
  in Timeout defaults checking. (Issue #763)


1.13.1 (2015-12-18)
-------------------

* Fixed regression in IPv6 + SSL for match_hostname. (Issue #761)


1.13 (2015-12-14)
-----------------

* Fixed ``pip install urllib3[secure]`` on modern pip. (Issue #706)

* pyopenssl: Fixed SSL3_WRITE_PENDING error. (Issue #717)

* pyopenssl: Support for TLSv1.1 and TLSv1.2. (Issue #696)

* Close connections more defensively on exception. (Issue #734)

* Adjusted ``read_chunked`` to handle gzipped, chunk-encoded bodies without
  repeatedly flushing the decoder, to function better on Jython. (Issue #743)

* Accept ``ca_cert_dir`` for SSL-related PoolManager configuration. (Issue #758)


1.12 (2015-09-03)
-----------------

* Rely on ``six`` for importing ``httplib`` to work around
  conflicts with other Python 3 shims. (Issue #688)

* Add support for directories of certificate authorities, as supported by
  OpenSSL. (Issue #701)

* New exception: ``NewConnectionError``, raised when we fail to establish
  a new connection, usually ``ECONNREFUSED`` socket error.


1.11 (2015-07-21)
-----------------

* When ``ca_certs`` is given, ``cert_reqs`` defaults to
  ``'CERT_REQUIRED'``. (Issue #650)

* ``pip install urllib3[secure]`` will install Certifi and
  PyOpenSSL as dependencies. (Issue #678)

* Made ``HTTPHeaderDict`` usable as a ``headers`` input value
  (Issues #632, #679)

* Added `urllib3.contrib.appengine <https://urllib3.readthedocs.io/en/latest/contrib.html#google-app-engine>`_
  which has an ``AppEngineManager`` for using ``URLFetch`` in a
  Google AppEngine environment. (Issue #664)

* Dev: Added test suite for AppEngine. (Issue #631)

* Fix performance regression when using PyOpenSSL. (Issue #626)

* Passing incorrect scheme (e.g. ``foo://``) will raise
  ``ValueError`` instead of ``AssertionError`` (backwards
  compatible for now, but please migrate). (Issue #640)

* Fix pools not getting replenished when an error occurs during a
  request using ``release_conn=False``. (Issue #644)

* Fix pool-default headers not applying for url-encoded requests
  like GET. (Issue #657)

* log.warning in Python 3 when headers are skipped due to parsing
  errors. (Issue #642)

* Close and discard connections if an error occurs during read.
  (Issue #660)

* Fix host parsing for IPv6 proxies. (Issue #668)

* Separate warning type SubjectAltNameWarning, now issued once
  per host. (Issue #671)

* Fix ``httplib.IncompleteRead`` not getting converted to
  ``ProtocolError`` when using ``HTTPResponse.stream()``
  (Issue #674)

1.10.4 (2015-05-03)
-------------------

* Migrate tests to Tornado 4. (Issue #594)

* Append default warning configuration rather than overwrite.
  (Issue #603)

* Fix streaming decoding regression. (Issue #595)

* Fix chunked requests losing state across keep-alive connections.
  (Issue #599)

* Fix hanging when chunked HEAD response has no body. (Issue #605)


1.10.3 (2015-04-21)
-------------------

* Emit ``InsecurePlatformWarning`` when SSLContext object is missing.
  (Issue #558)

* Fix regression of duplicate header keys being discarded.
  (Issue #563)

* ``Response.stream()`` returns a generator for chunked responses.
  (Issue #560)

* Set upper-bound timeout when waiting for a socket in PyOpenSSL.
  (Issue #585)

* Work on platforms without `ssl` module for plain HTTP requests.
  (Issue #587)

* Stop relying on the stdlib's default cipher list. (Issue #588)


1.10.2 (2015-02-25)
-------------------

* Fix file descriptor leakage on retries. (Issue #548)

* Removed RC4 from default cipher list. (Issue #551)

* Header performance improvements. (Issue #544)

* Fix PoolManager not obeying redirect retry settings. (Issue #553)


1.10.1 (2015-02-10)
-------------------

* Pools can be used as context managers. (Issue #545)

* Don't re-use connections which experienced an SSLError. (Issue #529)

* Don't fail when gzip decoding an empty stream. (Issue #535)

* Add sha256 support for fingerprint verification. (Issue #540)

* Fixed handling of header values containing commas. (Issue #533)


1.10 (2014-12-14)
-----------------

* Disabled SSLv3. (Issue #473)

* Add ``Url.url`` property to return the composed url string. (Issue #394)

* Fixed PyOpenSSL + gevent ``WantWriteError``. (Issue #412)

* ``MaxRetryError.reason`` will always be an exception, not string.
  (Issue #481)

* Fixed SSL-related timeouts not being detected as timeouts. (Issue #492)

* Py3: Use ``ssl.create_default_context()`` when available. (Issue #473)

* Emit ``InsecureRequestWarning`` for *every* insecure HTTPS request.
  (Issue #496)

* Emit ``SecurityWarning`` when certificate has no ``subjectAltName``.
  (Issue #499)

* Close and discard sockets which experienced SSL-related errors.
  (Issue #501)

* Handle ``body`` param in ``.request(...)``. (Issue #513)

* Respect timeout with HTTPS proxy. (Issue #505)

* PyOpenSSL: Handle ZeroReturnError exception. (Issue #520)


1.9.1 (2014-09-13)
------------------

* Apply socket arguments before binding. (Issue #427)

* More careful checks if fp-like object is closed. (Issue #435)

* Fixed packaging issues of some development-related files not
  getting included. (Issue #440)

* Allow performing *only* fingerprint verification. (Issue #444)

* Emit ``SecurityWarning`` if system clock is waaay off. (Issue #445)

* Fixed PyOpenSSL compatibility with PyPy. (Issue #450)

* Fixed ``BrokenPipeError`` and ``ConnectionError`` handling in Py3.
  (Issue #443)



1.9 (2014-07-04)
----------------

* Shuffled around development-related files. If you're maintaining a distro
  package of urllib3, you may need to tweak things. (Issue #415)

* Unverified HTTPS requests will trigger a warning on the first request. See
  our new `security documentation
  <https://urllib3.readthedocs.io/en/latest/security.html>`_ for details.
  (Issue #426)

* New retry logic and ``urllib3.util.retry.Retry`` configuration object.
  (Issue #326)

* All raised exceptions should now wrapped in a
  ``urllib3.exceptions.HTTPException``-extending exception. (Issue #326)

* All errors during a retry-enabled request should be wrapped in
  ``urllib3.exceptions.MaxRetryError``, including timeout-related exceptions
  which were previously exempt. Underlying error is accessible from the
  ``.reason`` property. (Issue #326)

* ``urllib3.exceptions.ConnectionError`` renamed to
  ``urllib3.exceptions.ProtocolError``. (Issue #326)

* Errors during response read (such as IncompleteRead) are now wrapped in
  ``urllib3.exceptions.ProtocolError``. (Issue #418)

* Requesting an empty host will raise ``urllib3.exceptions.LocationValueError``.
  (Issue #417)

* Catch read timeouts over SSL connections as
  ``urllib3.exceptions.ReadTimeoutError``. (Issue #419)

* Apply socket arguments before connecting. (Issue #427)


1.8.3 (2014-06-23)
------------------

* Fix TLS verification when using a proxy in Python 3.4.1. (Issue #385)

* Add ``disable_cache`` option to ``urllib3.util.make_headers``. (Issue #393)

* Wrap ``socket.timeout`` exception with
  ``urllib3.exceptions.ReadTimeoutError``. (Issue #399)

* Fixed proxy-related bug where connections were being reused incorrectly.
  (Issues #366, #369)

* Added ``socket_options`` keyword parameter which allows to define
  ``setsockopt`` configuration of new sockets. (Issue #397)

* Removed ``HTTPConnection.tcp_nodelay`` in favor of
  ``HTTPConnection.default_socket_options``. (Issue #397)

* Fixed ``TypeError`` bug in Python 2.6.4. (Issue #411)


1.8.2 (2014-04-17)
------------------

* Fix ``urllib3.util`` not being included in the package.


1.8.1 (2014-04-17)
------------------

* Fix AppEngine bug of HTTPS requests going out as HTTP. (Issue #356)

* Don't install ``dummyserver`` into ``site-packages`` as it's only needed
  for the test suite. (Issue #362)

* Added support for specifying ``source_address``. (Issue #352)


1.8 (2014-03-04)
----------------

* Improved url parsing in ``urllib3.util.parse_url`` (properly parse '@' in
  username, and blank ports like 'hostname:').

* New ``urllib3.connection`` module which contains all the HTTPConnection
  objects.

* Several ``urllib3.util.Timeout``-related fixes. Also changed constructor
  signature to a more sensible order. [Backwards incompatible]
  (Issues #252, #262, #263)

* Use ``backports.ssl_match_hostname`` if it's installed. (Issue #274)

* Added ``.tell()`` method to ``urllib3.response.HTTPResponse`` which
  returns the number of bytes read so far. (Issue #277)

* Support for platforms without threading. (Issue #289)

* Expand default-port comparison in ``HTTPConnectionPool.is_same_host``
  to allow a pool with no specified port to be considered equal to to an
  HTTP/HTTPS url with port 80/443 explicitly provided. (Issue #305)

* Improved default SSL/TLS settings to avoid vulnerabilities.
  (Issue #309)

* Fixed ``urllib3.poolmanager.ProxyManager`` not retrying on connect errors.
  (Issue #310)

* Disable Nagle's Algorithm on the socket for non-proxies. A subset of requests
  will send the entire HTTP request ~200 milliseconds faster; however, some of
  the resulting TCP packets will be smaller. (Issue #254)

* Increased maximum number of SubjectAltNames in ``urllib3.contrib.pyopenssl``
  from the default 64 to 1024 in a single certificate. (Issue #318)

* Headers are now passed and stored as a custom
  ``urllib3.collections_.HTTPHeaderDict`` object rather than a plain ``dict``.
  (Issue #329, #333)

* Headers no longer lose their case on Python 3. (Issue #236)

* ``urllib3.contrib.pyopenssl`` now uses the operating system's default CA
  certificates on inject. (Issue #332)

* Requests with ``retries=False`` will immediately raise any exceptions without
  wrapping them in ``MaxRetryError``. (Issue #348)

* Fixed open socket leak with SSL-related failures. (Issue #344, #348)


1.7.1 (2013-09-25)
------------------

* Added granular timeout support with new ``urllib3.util.Timeout`` class.
  (Issue #231)

* Fixed Python 3.4 support. (Issue #238)


1.7 (2013-08-14)
----------------

* More exceptions are now pickle-able, with tests. (Issue #174)

* Fixed redirecting with relative URLs in Location header. (Issue #178)

* Support for relative urls in ``Location: ...`` header. (Issue #179)

* ``urllib3.response.HTTPResponse`` now inherits from ``io.IOBase`` for bonus
  file-like functionality. (Issue #187)

* Passing ``assert_hostname=False`` when creating a HTTPSConnectionPool will
  skip hostname verification for SSL connections. (Issue #194)

* New method ``urllib3.response.HTTPResponse.stream(...)`` which acts as a
  generator wrapped around ``.read(...)``. (Issue #198)

* IPv6 url parsing enforces brackets around the hostname. (Issue #199)

* Fixed thread race condition in
  ``urllib3.poolmanager.PoolManager.connection_from_host(...)`` (Issue #204)

* ``ProxyManager`` requests now include non-default port in ``Host: ...``
  header. (Issue #217)

* Added HTTPS proxy support in ``ProxyManager``. (Issue #170 #139)

* New ``RequestField`` object can be passed to the ``fields=...`` param which
  can specify headers. (Issue #220)

* Raise ``urllib3.exceptions.ProxyError`` when connecting to proxy fails.
  (Issue #221)

* Use international headers when posting file names. (Issue #119)

* Improved IPv6 support. (Issue #203)


1.6 (2013-04-25)
----------------

* Contrib: Optional SNI support for Py2 using PyOpenSSL. (Issue #156)

* ``ProxyManager`` automatically adds ``Host: ...`` header if not given.

* Improved SSL-related code. ``cert_req`` now optionally takes a string like
  "REQUIRED" or "NONE". Same with ``ssl_version`` takes strings like "SSLv23"
  The string values reflect the suffix of the respective constant variable.
  (Issue #130)

* Vendored ``socksipy`` now based on Anorov's fork which handles unexpectedly
  closed proxy connections and larger read buffers. (Issue #135)

* Ensure the connection is closed if no data is received, fixes connection leak
  on some platforms. (Issue #133)

* Added SNI support for SSL/TLS connections on Py32+. (Issue #89)

* Tests fixed to be compatible with Py26 again. (Issue #125)

* Added ability to choose SSL version by passing an ``ssl.PROTOCOL_*`` constant
  to the ``ssl_version`` parameter of ``HTTPSConnectionPool``. (Issue #109)

* Allow an explicit content type to be specified when encoding file fields.
  (Issue #126)

* Exceptions are now pickleable, with tests. (Issue #101)

* Fixed default headers not getting passed in some cases. (Issue #99)

* Treat "content-encoding" header value as case-insensitive, per RFC 2616
  Section 3.5. (Issue #110)

* "Connection Refused" SocketErrors will get retried rather than raised.
  (Issue #92)

* Updated vendored ``six``, no longer overrides the global ``six`` module
  namespace. (Issue #113)

* ``urllib3.exceptions.MaxRetryError`` contains a ``reason`` property holding
  the exception that prompted the final retry. If ``reason is None`` then it
  was due to a redirect. (Issue #92, #114)

* Fixed ``PoolManager.urlopen()`` from not redirecting more than once.
  (Issue #149)

* Don't assume ``Content-Type: text/plain`` for multi-part encoding parameters
  that are not files. (Issue #111)

* Pass `strict` param down to ``httplib.HTTPConnection``. (Issue #122)

* Added mechanism to verify SSL certificates by fingerprint (md5, sha1) or
  against an arbitrary hostname (when connecting by IP or for misconfigured
  servers). (Issue #140)

* Streaming decompression support. (Issue #159)


1.5 (2012-08-02)
----------------

* Added ``urllib3.add_stderr_logger()`` for quickly enabling STDERR debug
  logging in urllib3.

* Native full URL parsing (including auth, path, query, fragment) available in
  ``urllib3.util.parse_url(url)``.

* Built-in redirect will switch method to 'GET' if status code is 303.
  (Issue #11)

* ``urllib3.PoolManager`` strips the scheme and host before sending the request
  uri. (Issue #8)

* New ``urllib3.exceptions.DecodeError`` exception for when automatic decoding,
  based on the Content-Type header, fails.

* Fixed bug with pool depletion and leaking connections (Issue #76). Added
  explicit connection closing on pool eviction. Added
  ``urllib3.PoolManager.clear()``.

* 99% -> 100% unit test coverage.


1.4 (2012-06-16)
----------------

* Minor AppEngine-related fixes.

* Switched from ``mimetools.choose_boundary`` to ``uuid.uuid4()``.

* Improved url parsing. (Issue #73)

* IPv6 url support. (Issue #72)


1.3 (2012-03-25)
----------------

* Removed pre-1.0 deprecated API.

* Refactored helpers into a ``urllib3.util`` submodule.

* Fixed multipart encoding to support list-of-tuples for keys with multiple
  values. (Issue #48)

* Fixed multiple Set-Cookie headers in response not getting merged properly in
  Python 3. (Issue #53)

* AppEngine support with Py27. (Issue #61)

* Minor ``encode_multipart_formdata`` fixes related to Python 3 strings vs
  bytes.


1.2.2 (2012-02-06)
------------------

* Fixed packaging bug of not shipping ``test-requirements.txt``. (Issue #47)


1.2.1 (2012-02-05)
------------------

* Fixed another bug related to when ``ssl`` module is not available. (Issue #41)

* Location parsing errors now raise ``urllib3.exceptions.LocationParseError``
  which inherits from ``ValueError``.


1.2 (2012-01-29)
----------------

* Added Python 3 support (tested on 3.2.2)

* Dropped Python 2.5 support (tested on 2.6.7, 2.7.2)

* Use ``select.poll`` instead of ``select.select`` for platforms that support
  it.

* Use ``Queue.LifoQueue`` instead of ``Queue.Queue`` for more aggressive
  connection reusing. Configurable by overriding ``ConnectionPool.QueueCls``.

* Fixed ``ImportError`` during install when ``ssl`` module is not available.
  (Issue #41)

* Fixed ``PoolManager`` redirects between schemes (such as HTTP -> HTTPS) not
  completing properly. (Issue #28, uncovered by Issue #10 in v1.1)

* Ported ``dummyserver`` to use ``tornado`` instead of ``webob`` +
  ``eventlet``. Removed extraneous unsupported dummyserver testing backends.
  Added socket-level tests.

* More tests. Achievement Unlocked: 99% Coverage.


1.1 (2012-01-07)
----------------

* Refactored ``dummyserver`` to its own root namespace module (used for
  testing).

* Added hostname verification for ``VerifiedHTTPSConnection`` by vendoring in
  Py32's ``ssl_match_hostname``. (Issue #25)

* Fixed cross-host HTTP redirects when using ``PoolManager``. (Issue #10)

* Fixed ``decode_content`` being ignored when set through ``urlopen``. (Issue
  #27)

* Fixed timeout-related bugs. (Issues #17, #23)


1.0.2 (2011-11-04)
------------------

* Fixed typo in ``VerifiedHTTPSConnection`` which would only present as a bug if
  you're using the object manually. (Thanks pyos)

* Made RecentlyUsedContainer (and consequently PoolManager) more thread-safe by
  wrapping the access log in a mutex. (Thanks @christer)

* Made RecentlyUsedContainer more dict-like (corrected ``__delitem__`` and
  ``__getitem__`` behaviour), with tests. Shouldn't affect core urllib3 code.


1.0.1 (2011-10-10)
------------------

* Fixed a bug where the same connection would get returned into the pool twice,
  causing extraneous "HttpConnectionPool is full" log warnings.


1.0 (2011-10-08)
----------------

* Added ``PoolManager`` with LRU expiration of connections (tested and
  documented).
* Added ``ProxyManager`` (needs tests, docs, and confirmation that it works
  with HTTPS proxies).
* Added optional partial-read support for responses when
  ``preload_content=False``. You can now make requests and just read the headers
  without loading the content.
* Made response decoding optional (default on, same as before).
* Added optional explicit boundary string for ``encode_multipart_formdata``.
* Convenience request methods are now inherited from ``RequestMethods``. Old
  helpers like ``get_url`` and ``post_url`` should be abandoned in favour of
  the new ``request(method, url, ...)``.
* Refactored code to be even more decoupled, reusable, and extendable.
* License header added to ``.py`` files.
* Embiggened the documentation: Lots of Sphinx-friendly docstrings in the code
  and docs in ``docs/`` and on https://urllib3.readthedocs.io/.
* Embettered all the things!
* Started writing this file.


0.4.1 (2011-07-17)
------------------

* Minor bug fixes, code cleanup.


0.4 (2011-03-01)
----------------

* Better unicode support.
* Added ``VerifiedHTTPSConnection``.
* Added ``NTLMConnectionPool`` in contrib.
* Minor improvements.


0.3.1 (2010-07-13)
------------------

* Added ``assert_host_name`` optional parameter. Now compatible with proxies.


0.3 (2009-12-10)
----------------

* Added HTTPS support.
* Minor bug fixes.
* Refactored, broken backwards compatibility with 0.2.
* API to be treated as stable from this version forward.


0.2 (2008-11-17)
----------------

* Added unit tests.
* Bug fixes.


0.1 (2008-11-16)
----------------

* First release.
urllib3
=======

.. image:: https://travis-ci.org/urllib3/urllib3.svg?branch=master
        :alt: Build status on Travis
        :target: https://travis-ci.org/urllib3/urllib3

.. image:: https://img.shields.io/appveyor/ci/urllib3/urllib3/master.svg
        :alt: Build status on AppVeyor
        :target: https://ci.appveyor.com/project/urllib3/urllib3

.. image:: https://readthedocs.org/projects/urllib3/badge/?version=latest
        :alt: Documentation Status
        :target: https://urllib3.readthedocs.io/en/latest/
        
.. image:: https://img.shields.io/codecov/c/github/urllib3/urllib3.svg
        :alt: Coverage Status
        :target: https://codecov.io/gh/urllib3/urllib3

.. image:: https://img.shields.io/pypi/v/urllib3.svg?maxAge=86400
        :alt: PyPI version
        :target: https://pypi.org/project/urllib3/

.. image:: https://www.bountysource.com/badge/tracker?tracker_id=192525
        :alt: Bountysource
        :target: https://www.bountysource.com/trackers/192525-urllib3?utm_source=192525&utm_medium=shield&utm_campaign=TRACKER_BADGE

.. image:: https://badges.gitter.im/python-urllib3/Lobby.svg
        :alt: Gitter
        :target: https://gitter.im/python-urllib3/Lobby?utm_source=badge&utm_medium=badge&utm_campaign=pr-badge&utm_content=badge

urllib3 is a powerful, *sanity-friendly* HTTP client for Python. Much of the
Python ecosystem already uses urllib3 and you should too.
urllib3 brings many critical features that are missing from the Python
standard libraries:

- Thread safety.
- Connection pooling.
- Client-side SSL/TLS verification.
- File uploads with multipart encoding.
- Helpers for retrying requests and dealing with HTTP redirects.
- Support for gzip and deflate encoding.
- Proxy support for HTTP and SOCKS.
- 100% test coverage.

urllib3 is powerful and easy to use::

    >>> import urllib3
    >>> http = urllib3.PoolManager()
    >>> r = http.request('GET', 'http://httpbin.org/robots.txt')
    >>> r.status
    200
    >>> r.data
    'User-agent: *\nDisallow: /deny\n'


Installing
----------

urllib3 can be installed with `pip <https://pip.pypa.io>`_::

    $ pip install urllib3

Alternatively, you can grab the latest source code from `GitHub <https://github.com/urllib3/urllib3>`_::

    $ git clone git://github.com/urllib3/urllib3.git
    $ python setup.py install


Documentation
-------------

urllib3 has usage and reference documentation at `urllib3.readthedocs.io <https://urllib3.readthedocs.io>`_.


Contributing
------------

urllib3 happily accepts contributions. Please see our
`contributing documentation <https://urllib3.readthedocs.io/en/latest/contributing.html>`_
for some tips on getting started.


Maintainers
-----------

- `@theacodes <https://github.com/theacodes>`_ (Thea Flowers)
- `@SethMichaelLarson <https://github.com/SethMichaelLarson>`_ (Seth M. Larson)
- `@haikuginger <https://github.com/haikuginger>`_ (Jesse Shapiro)
- `@lukasa <https://github.com/lukasa>`_ (Cory Benfield)
- `@sigmavirus24 <https://github.com/sigmavirus24>`_ (Ian Cordasco)
- `@shazow <https://github.com/shazow>`_ (Andrey Petrov)

👋


Sponsorship
-----------

If your company benefits from this library, please consider `sponsoring its
development <https://urllib3.readthedocs.io/en/latest/contributing.html#sponsorship>`_.

Sponsors include:

- Google Cloud Platform (2018-present), sponsors `@theacodes <https://github.com/theacodes>`_'s work on an ongoing basis
- Abbott (2018-present), sponsors `@SethMichaelLarson <https://github.com/SethMichaelLarson>`_'s work on an ongoing basis
- Akamai (2017-present), sponsors `@haikuginger <https://github.com/haikuginger>`_'s work on an ongoing basis
- Hewlett Packard Enterprise (2016-2017), sponsored `@Lukasa’s <https://github.com/Lukasa>`_ work on urllib3
Version 1.1.1
-------------

Released 2019-02-23

-   Fix segfault when ``__html__`` method raises an exception when using
    the C speedups. The exception is now propagated correctly. (`#109`_)

.. _#109: https://github.com/pallets/markupsafe/pull/109


Version 1.1.0
-------------

Released 2018-11-05

-   Drop support for Python 2.6 and 3.3.
-   Build wheels for Linux, Mac, and Windows, allowing systems without
    a compiler to take advantage of the C extension speedups. (`#104`_)
-   Use newer CPython API on Python 3, resulting in a 1.5x speedup.
    (`#64`_)
-   ``escape`` wraps ``__html__`` result in ``Markup``, consistent with
    documented behavior. (`#69`_)

.. _#64: https://github.com/pallets/markupsafe/pull/64
.. _#69: https://github.com/pallets/markupsafe/pull/69
.. _#104: https://github.com/pallets/markupsafe/pull/104


Version 1.0
-----------

Released 2017-03-07

-   Fixed custom types not invoking ``__unicode__`` when used with
    ``format()``.
-   Added ``__version__`` module attribute.
-   Improve unescape code to leave lone ampersands alone.


Version 0.18
------------

Released 2013-05-22

-   Fixed ``__mul__`` and string splitting on Python 3.


Version 0.17
------------

Released 2013-05-21

-   Fixed a bug with broken interpolation on tuples.


Version 0.16
------------

Released 2013-05-20

-   Improved Python 3 Support and removed 2to3.
-   Removed support for Python 3.2 and 2.5.


Version 0.15
------------

Released 2011-07-20

-   Fixed a typo that caused the library to fail to install on pypy and
    jython.


Version 0.14
------------

Released 2011-07-20

-   Release fix for 0.13.


Version 0.13
------------

Released 2011-07-20

-   Do not attempt to compile extension for PyPy or Jython.
-   Work around some 64bit Windows issues.


Version 0.12
------------

Released 2011-02-17

-   Improved PyPy compatibility.
MarkupSafe
==========

Implements a unicode subclass that supports HTML strings:

>>> from markupsafe import Markup, escape
>>> escape("<script>alert(document.cookie);</script>")
Markup(u'&lt;script&gt;alert(document.cookie);&lt;/script&gt;')
>>> tmpl = Markup("<em>%s</em>")
>>> tmpl % "Peter > Lustig"
Markup(u'<em>Peter &gt; Lustig</em>')

If you want to make an object unicode that is not yet unicode
but don't want to lose the taint information, you can use the
`soft_unicode` function.  (On Python 3 you can also use `soft_str` which
is a different name for the same function).

>>> from markupsafe import soft_unicode
>>> soft_unicode(42)
u'42'
>>> soft_unicode(Markup('foo'))
Markup(u'foo')

HTML Representations
--------------------

Objects can customize their HTML markup equivalent by overriding
the `__html__` function:

>>> class Foo(object):
...  def __html__(self):
...   return '<strong>Nice</strong>'
...
>>> escape(Foo())
Markup(u'<strong>Nice</strong>')
>>> Markup(Foo())
Markup(u'<strong>Nice</strong>')

Silent Escapes
--------------

Since MarkupSafe 0.10 there is now also a separate escape function
called `escape_silent` that returns an empty string for `None` for
consistency with other systems that return empty strings for `None`
when escaping (for instance Pylons' webhelpers).

If you also want to use this for the escape method of the Markup
object, you can create your own subclass that does that::

    from markupsafe import Markup, escape_silent as escape

    class SilentMarkup(Markup):
        __slots__ = ()

        @classmethod
        def escape(cls, s):
            return cls(escape(s))

New-Style String Formatting
---------------------------

Starting with MarkupSafe 0.21 new style string formats from Python 2.6 and
3.x are now fully supported.  Previously the escape behavior of those
functions was spotty at best.  The new implementations operates under the
following algorithm:

1.  if an object has an ``__html_format__`` method it is called as
    replacement for ``__format__`` with the format specifier.  It either
    has to return a string or markup object.
2.  if an object has an ``__html__`` method it is called.
3.  otherwise the default format system of Python kicks in and the result
    is HTML escaped.

Here is how you can implement your own formatting::

    class User(object):

        def __init__(self, id, username):
            self.id = id
            self.username = username

        def __html_format__(self, format_spec):
            if format_spec == 'link':
                return Markup('<a href="/user/{0}">{1}</a>').format(
                    self.id,
                    self.__html__(),
                )
            elif format_spec:
                raise ValueError('Invalid format spec')
            return self.__html__()

        def __html__(self):
            return Markup('<span class=user>{0}</span>').format(self.username)

And to format that user:

>>> user = User(1, 'foo')
>>> Markup('<p>User: {0:link}').format(user)
Markup(u'<p>User: <a href="/user/1"><span class=user>foo</span></a>')
=========================
Mako Templates for Python
=========================

Mako is a template library written in Python. It provides a familiar, non-XML 
syntax which compiles into Python modules for maximum performance. Mako's 
syntax and API borrows from the best ideas of many others, including Django
templates, Cheetah, Myghty, and Genshi. Conceptually, Mako is an embedded 
Python (i.e. Python Server Page) language, which refines the familiar ideas
of componentized layout and inheritance to produce one of the most 
straightforward and flexible models available, while also maintaining close 
ties to Python calling and scoping semantics.

Nutshell
========

::

    <%inherit file="base.html"/>
    <%
        rows = [[v for v in range(0,10)] for row in range(0,10)]
    %>
    <table>
        % for row in rows:
            ${makerow(row)}
        % endfor
    </table>

    <%def name="makerow(row)">
        <tr>
        % for name in row:
            <td>${name}</td>\
        % endfor
        </tr>
    </%def>

Philosophy
===========

Python is a great scripting language. Don't reinvent the wheel...your templates can handle it !

Documentation
==============

See documentation for Mako at http://www.makotemplates.org/docs/

License
========

Mako is licensed under an MIT-style license (see LICENSE).
Other incorporated projects may be licensed under different licenses.
All licenses allow for non-commercial and commercial use.
libfuse 3.3.0 (2018-11-06)
==========================

* Added support for fuse kernel feature `max_pages` which allows to increase
  the maximum number of pages that can be used per request. This feature was
  introduced in kernel 4.20. `max_pages` is set based on the value in
  `max_write`. By default `max_write` will be 1MiB now for kernels that support
  `max_pages`. If you want smaller buffers or writes you have to set
  `max_write` manually.

* The `auto_unmount` mode now works correctly in combination with
  autofs.

* The FUSE_CAP_READDIRPLUS_AUTO capability is no longer enabled by
  default unless the file system defines both a readdir() and a
  readdirplus() handler.

* The description of the FUSE_CAP_READDIRPLUS_AUTO flag has been
  improved.

* Allow open `/dev/fuse` file descriptors to be passed via mountpoints of the
  special format `/dev/fd/%u`. This allows mounting to be handled by the parent
  so the FUSE filesystem process can run fully unprivileged.

* Add a `drop_privileges` option to mount.fuse3 which causes it to open
  `/dev/fuse` and mount the file system itself, then run the FUSE file
  filesystem fully unprivileged and unable to re-acquire privilege via setuid,
  fscaps, etc.

* Documented under which conditions the `fuse_lowlevel_notify_*`
  functions may block.

libfuse 3.2.6 (2018-08-31)
==========================

* The fuse_main() function now returns more fine-grained error codes.
* FUSE filesystems may now be mounted on mountpoint within
  bcachefs, aufs and FAT filesystems.
* libfuse may now be used as a Meson subproject.
* Fix a few low-impact memory leaks.
* The `fuse.conf` file is no longer looked for in `/etc`, but in the
  *sysconfdir* directory (which can be set with `meson configure`). By
  default, the location is thus `/usr/local/etc/fuse.conf`.

libfuse 3.2.5 (2018-07-24)
==========================

* SECURITY UPDATE: In previous versions of libfuse it was possible to
  for unprivileged users to specify the `allow_other` option even when
  this was forbidden in `/etc/fuse.conf`.  The vulnerability is
  present only on systems where SELinux is active (including in
  permissive mode).
* The fusermount binary has been hardened in several ways to reduce
  potential attack surface. Most importantly, mountpoints and mount
  options must now match a hard-coded whitelist. It is expected that
  this whitelist covers all regular use-cases.
* Added a test of `seekdir` to test_syscalls.
* Fixed `readdir` bug when non-zero offsets are given to filler and the
  filesystem client, after reading a whole directory, re-reads it from a
  non-zero offset e. g. by calling `seekdir` followed by `readdir`.

libfuse 3.2.4 (2018-07-11)
==========================

* Fixed `rename` deadlock on FreeBSD.

libfuse 3.2.3 (2018-05-11)
==========================

* Fixed a number of compiler warnings.  

libfuse 3.2.2 (2018-03-31)
==========================

* Added example fuse.conf file.
* Added "support" for -o nofail mount option (the option is accepted
  and ignored).
* Various small bugfixes.  

libfuse 3.2.1 (2017-11-14)
==========================

* Various small bugfixes.

libfuse 3.2.0 (2017-09-12)
==========================

* Support for building with autotools has been dropped.

* Added new `fuse_invalidate_path()` routine for cache invalidation
  from the high-level FUSE API, along with an example and tests.

* There's a new `printcap` example that can be used to determine the
  capabilities of the running kernel.

* `fuse_loop_mt()` now returns the minus the actual errno if there was
  an error (instead of just -1).

* `fuse_loop()` no longer returns a positive value if the filesystem
  loop was terminated without errors or signals.

* Improved documentation of `fuse_lowlevel_notify_*` functions.

* `fuse_lowlevel_notify_inval_inode()` and
  `fuse_lowlevel_notify_inval_entry()` now return -ENOSYS instead of
  an undefined error if the function is not supported by the kernel.

* Documented the special meaning of the *zero* offset for the
  fuse_fill_dir_t function.

* The `passthrough_fh` example now works under FreeBSD.

* libfuse can now be build without libiconv.

* Fixed support for `FUSE_CAP_POSIX_ACL`: setting this capability
  flag had no effect in the previous versions of libfuse 3.x;
  now ACLs should actually work.

* Fixed a number of compilation problems under FreeBSD.

* Fixed installation directory for udev rules.

* Fixed compilation with LTO.

libfuse 3.1.1 (2017-08-06)
==========================

* Documentation: clarified how filesystems are supposed to process
  open() and create() flags (see include/fuse_lowlevel.h).

* Fixed a compilation problem of the passthrough_ll example on
  32 bit systems (wrong check and wrong error message).

* pkg-config is now used to determine the proper directory for
  udev rules.

* Fixed a symbol versioning problem that resulted in very strange
  failures (segfaults, unexpected behavior) in different situations.

* Fixed a test failure when /tmp is on btrfs.

* The maximum number of idle worker threads used by `fuse_loop_mt()`
  is now configurable.

* `fuse_loop_mt()` and `fuse_session_loop_mt()` now take a
  `struct fuse_loop_config` parameter that supersedes the *clone_fd*
  parameter.

* Incorporated several patches from the FreeBSD port. libfuse should
  now compile under FreeBSD without the need for patches.

* The passthrough_ll example now supports writeback caching.

libfuse 3.1.0 (2017-07-08)
==========================

* Added new `fuse_lib_help()` function. File-systems that previously
  passed a ``--help`` option to `fuse_new()` must now process the
  ``--help`` option internally and call `fuse_lib_help()` to print the
  help for generic FUSE options.
* Fixed description of the `fuse_conn_info->time_gran`. The default
  value of zero actually corresponds to full nanosecond resolution,
  not one second resolution.
* The init script is now installed into the right location
  (``$DESTDIR/etc/init.d`` rather than ``$prefix/$sysconfdir/init.d``)
* The `example/passthrough_ll` filesystem now supports creating
  and writing to files.
* `fuse_main()` / `fuse_remove_signal_handlers()`: do not reset
  `SIGPIPE` handler to `SIG_DFL` if it was not set by us.
* Documented the `RENAME_EXCHANGE` and `RENAME_NOREPLACE` flags that
  may be passed to the `rename` handler of both the high- and
  low-level API. Filesystem authors are strongly encouraged to check
  that these flags are handled correctly.

libfuse 3.0.2 (2017-05-24)
==========================

* Option parsing for the high-level API now works correctly
  (previously, default values would override specified values).
* Tests should now build (and run) under FreeBSD.
* Improved documentation of `struct fuse_context`
* Internal: calculate request buffer size from page size and kernel
  page limit instead of using hardcoded 128 kB limit.


libfuse 3.0.1 (2017-04-10)
==========================

* Re-introduced *examples/null.c*.
* Added experimental support for building with Meson.
* Document that `-o auto_unmount` implies `-o nodev,nosuid`.
* Document that the *use_ino* option of the high-level interface does
  not affect the inode that libfuse and the kernel use internally.
* Fixed test cases for passthrough* examples (they weren't actually
  testing the examples).
* Fixed several bugs in the passthrough* examples.

libfuse 3.0.0 (2016-12-08)
==========================

* NOTE TO PACKAGERS:

  libfuse 3 is designed to be co-installable with libfuse 2. However,
  some files will be installed by both libfuse 2 and libfuse 3
  (e.g. /etc/fuse.conf, the udev and init scripts, and the
  mount.fuse(8) manpage). These files should be taken from
  libfuse 3. The format/content is guaranteed to remain backwards
  compatible with libfuse 2.

  We recommend to ship libfuse2 and libfuse3 in three separate
  packages: a libfuse-common package that contains files shared by
  libfuse 2+3 (taken from the libfuse3 tarball), and libfuse2 and
  libfuse3 packages that contain the shared library and helper
  programs for the respective version.

* Fixed test errors when running tests as root.

* Made check for util-linux version more robust.

* Added documentation for all fuse capability flags (`FUSE_CAP_*`) and
  `struct fuse_conn_info` fields.

* fuse_loop(), fuse_loop_mt(), fuse_session_loop() and
  fuse_session_loop_mt() now return more detailed error codes instead
  of just -1. See the documentation of fuse_session_loop() for details.

* The FUSE main loop is now aborted if the file-system requests
  capabilities that are not supported by the kernel. In this case, the
  session loop is exited with a return code of ``-EPROTO``.

* Most file-system capabilities that were opt-in in libfuse2 are now
  enabled by default. Filesystem developers are encouraged to review
  the documentation of the FUSE_CAP_* features to ensure that their
  filesystem is compatible with the new semantics. As before, a
  particular capability can still be disabled by unsetting the
  corresponding bit of `fuse_conn_info.wants` in the init() handler.

* Added FUSE_CAP_PARALLEL_DIROPS and FUSE_CAP_POSIX_ACL,
  FUSE_HANDLE_KILLPRIV feature flags.

* FUSE filesystems are now responsible for unsetting the setuid/setgid
  flags when a file is written, truncated, or its owner
  changed. Previously, this was handled by the kernel but subject to
  race conditions.

* The fusermount and mount.fuse binaries have been renamed to
  fusermount3 and mount.fuse3 to allow co-installation of libfuse 2.x
  and 3.x

* Added a `max_read` field to `struct fuse_conn_info`. For the time
  being, the maximum size of read requests has to be specified both
  there *and* passed to fuse_session_new() using the ``-o
  max_read=<n>`` mount option. At some point in the future, specifying
  the mount option will no longer be necessary.

* Documentation: clarified that the fuse_argv structure that is passed
  to `fuse_new()` and `fuse_lowlevel_new()` must always contain at
  least one element.

* The high-level init() handler now receives an additional struct
  fuse_config pointer that can be used to adjust high-level API
  specific configuration options.

* The `nopath_flag` field of struct fuse_operations has been
  removed. Instead, a new `nullpath_ok` flag can now be set
  in struct fuse_config.

* File systems that use the low-level API and support lookup requests
  for '.' and '..' should continue make sure to set the
  FUSE_CAP_EXPORT_SUPPORT bit in fuse_conn_info->want.

  (This has actually always been the case, but was not very obvious
  from the documentation).

* The help text generated by fuse_lowlevel_help(), fuse_new() (and
  indirectly fuse_main()) no longer includes options that are unlikely
  to be of interest to end-users. The full list of accepted options is
  now included in the respective function's documentation (located in
  the fuse.h/fuse_lowlevel.h and doc/html).

* The ``-o nopath`` option has been dropped - it never actually did
  anything (since it is unconditionally overwritten with the value of
  the `nopath` flag in `struct fuse_operations).

* The ``-o large_read`` mount option has been dropped. Hopefully no
  one uses a Linux 2.4 kernel anymore.

* The `-o nonempty` mount point has been removed, mounting over
  non-empty directories is now always allowed. This brings the
  behavior of FUSE file systems in-line with the behavior of the
  regular `mount` command.

  File systems that do not want to allow mounting to non-empty
  directories should perform this check themselves before handing
  control to libfuse.

* The chmod, chown, truncate, utimens and getattr handlers of the
  high-level API now all receive an additional struct fuse_file_info
  pointer (which, however, may be NULL even if the file is currently
  open).

  The fgetattr and ftruncate handlers have become obsolete and have
  been removed.

* The `fuse_session_new` function no longer accepts the ``-o
  clone_fd`` option. Instead, this has become a parameter of the
  `fuse_session_loop_mt` and ``fuse_loop_mt` functions.

* For low-level file systems that implement the `write_buf` handler,
  the `splice_read` option is now enabled by default. As usual, this
  can be changed in the file system's `init` handler.

* The treatment of low-level options has been made more consistent:

  Options that can be set in the init() handler (via the
  fuse_conn_info parameter) can now be set only here,
  i.e. fuse_session_new() no longer accepts arguments that change the
  fuse_conn_info object before or after the call do init(). As a side
  effect, this removes the ambiguity where some options can be
  overwritten by init(), while others overwrite the choices made by
  init().

  For file systems that wish to offer command line options for these
  settings, the new fuse_parse_conn_info_opts() and
  fuse_apply_conn_info_opts() functions are available.

  Consequently, the fuse_lowlevel_help() method has been dropped.

* The `async_read` field in `struct fuse_conn_info` has been
  removed. To determine if the kernel supports asynchronous reads,
  file systems should check the `FUSE_CAP_ASYNC_READ` bit of the
  `capable` field. To enable/disable asynchronous reads, file systems
  should set the flag in the `wanted` field.

* The `fuse_parse_cmdline` function no longer prints out help when the
  ``--verbose`` or ``--help`` flags are given. This needs to be done
  by the file system (e.g. using the `fuse_cmdline_help()` and
  `fuse_lowlevel_help()` functions).

* Added ``example/cuse_client.c`` to test ``example/cuse.c``.

* Removed ``example/null.c``. This has not been working for a while
  for unknown reasons -- maybe because it tries to treat the
  mountpoint as a file rather than a directory?

* There are several new examples that demonstrate the use of
  the ``fuse_lowlevel_notify_*`` functions:

  - ``example/notify_store_retrieve.c``
  - ``example/notify_inval_inode.c``
  - ``example/notify_inval_entry.c``

* The ``-o big_writes`` mount option has been removed. It is now
  always active. File systems that want to limit the size of write
  requests should use the ``-o max_write=<N>`` option instead.

* The `fuse_lowlevel_new` function has been renamed to
  `fuse_session_new` and no longer interprets the --version or --help
  options. To print help or version information, use the new
  `fuse_lowlevel_help` and `fuse_lowlevel_version` functions.

* The ``allow_other`` and ``allow_root`` mount options (accepted by
  `fuse_session_new()`) may now be specified together. In this case,
  ``allow_root`` takes precedence.

* There are new `fuse_session_unmount` and `fuse_session_mount`
  functions that should be used in the low-level API. The `fuse_mount`
  and `fuse_unmount` functions should be used with the high-level API
  only.

* Neither `fuse_mount` nor `fuse_session_mount` take struct fuse_opts
  parameters anymore. Mount options are parsed by `fuse_new` (for the
  high-level API) and `fuse_session_new` (for the low-level API)
  instead. To print help or version information, use the new
  `fuse_mount_help` and `fuse_mount_version` functions.

* The ``fuse_lowlevel_notify_*`` functions now all take a `struct
  fuse_session` parameter instead of a `struct fuse_chan`.

* The channel interface (``fuse_chan_*`` functions) has been made
  private. As a result, the typical initialization sequence of a
  low-level file system has changed from ::

        ch = fuse_mount(mountpoint, &args);
        se = fuse_lowlevel_new(&args, &lo_oper, sizeof(lo_oper), &lo);
        fuse_set_signal_handlers(se);
        fuse_session_add_chan(se, ch);
        fuse_daemonize(fg);
        if (mt)
            fuse_session_loop_mt(se);
        else
            fuse_session_loop(se);
        fuse_remove_signal_handlers(se);
        fuse_session_remove_chan(ch);
        fuse_session_destroy(se);
        fuse_unmount(mountpoint, ch);

  to ::

        se = fuse_session_new(&args, &ll_ops, sizeof(ll_ops), NULL);
        fuse_set_signal_handlers(se);
        fuse_session_mount(se, mountpoint);
        fuse_daemonize(fg);
        if (mt)
            fuse_session_loop_mt(se);
        else
            fuse_session_loop(se);
        fuse_remove_signal_handlers(se);
        fuse_session_unmount(se);
        fuse_lowlevel_destroy(se);

  The typical high-level setup has changed from ::

        ch = fuse_mount(*mountpoint, &args);
        fuse = fuse_new(ch, &args, op, op_size, user_data);
        se = fuse_get_session(fuse);
        fuse_set_signal_handlers(se);
        fuse_daemonize(fg);
        if (mt)
            fuse_loop_mt(fuse);
        else
            fuse_loop(fuse);
        fuse_remove_signal_handlers(se);
        fuse_unmount(mountpoint, ch);
        fuse_destroy(fuse);

  to ::

        fuse = fuse_new(&args, op, op_size, user_data);
        se = fuse_get_session(fuse);
        fuse_set_signal_handlers(se);
        fuse_mount(fuse, mountpoint);
        fuse_daemonize(fg);
         if (mt)
            fuse_loop_mt(fuse);
        else
            fuse_loop(fuse);
        fuse_remove_signal_handlers(se);
        fuse_unmount(fuse);
        fuse_destroy(fuse);

  File systems that use `fuse_main` are not affected by this change.

  For integration with custom event loops, the new `fuse_session_fd`
  function provides the file descriptor that's used for communication
  with the kernel.

* Added *clone_fd* option.  This creates a separate device file
  descriptor for each processing thread, which might improve
  performance.

* Added *writeback_cache* option. With kernel 3.14 and newer this
  enables write-back caching which can significantly improve
  performance.

* Added *async_dio* option. With kernel 3.13 and newer, this allows
  direct I/O to be done asynchronously.

* The (high- and low-level) `rename` handlers now takes a *flags*
  parameter (with values corresponding to the *renameat2* system call
  introduced in Linux 3.15).

* The "ulockmgr_server" has been dropped.

* There is a new (low-level) `readdirplus` handler, with a
  corresponding example in ``examples/fuse_lo-plus.c`` and a new
  `fuse_add_direntry_plus` API function.

* The (high-level) `readdir` handler now takes a *flags* argument.

* The (high-level) `filler` function passed to `readdir` now takes an
  additional *flags* argument.

* The (high-level) `getdir` handler has been dropped.

* The *flag_nullpath_ok* and *flag_utime_omit_ok* flags have been
  dropped.

* The (high-level) *utime* handler has been dropped.

* The `fuse_invalidate` function has been removed.

* The `fuse_is_lib_option` function has been removed.

* The *fh_old* member of `struct fuse_file_info` has been dropped.

* The type of the *writepage* member of `struct fuse_file_info` was
  changed from *int* to *unsigned int*.

* The `struct fuse_file_info` gained a new *poll_events* member.

* There is a new `fuse_pkgversion` function.

* The *fuse_off_t* and *fuse_ino_t* changed from *unsigned long* to
  *uint64_t*, i.e. they are now 64 bits also on 32-bit systems.

* The type of the *generation* member of `struct fuse_entry_param*
  changed from *unsigned* to *uint64_t*.

* The (low-level) `setattr` handler gained a *FUSE_SET_ATTR_CTIME* bit
  *for its *to_set* parameter.

* The `struct fuse_session_ops` data structure has been dropped.

* The documentation has been clarified and improved in many places.


FUSE 2.9.7 (2016-06-20)
=======================

* Added SELinux support.
* Fixed race-condition when session is terminated right after starting
  a FUSE file system.

FUSE 2.9.6 (2016-04-23)
=======================

* Tarball now includes documentation.
* Shared-object version has now been bumped correctly.

FUSE 2.9.5 (2016-01-14)
=======================

* New maintainer: Nikolaus Rath <Nikolaus@rath.org>. Many thanks to
  Miklos Szeredi <miklos@szeredi.hu> for bringing FUSE to where it is
  now!

* fix warning in mount.c:receive_fd().  Reported by Albert Berger

* fix possible memory leak.  Reported by Jose R. Guzman

FUSE 2.9.4 (2015-05-22)
=======================

* fix exec environment for mount and umount.  Found by Tavis Ormandy
  (CVE-2015-3202).

* fix fuse_remove_signal_handlers() to properly restore the default
  signal handler.  Reported by: Chris Johnson

* highlevel API: fix directory file handle passed to ioctl() method.
  Reported by Eric Biggers

* libfuse: document deadlock avoidance for fuse_notify_inval_entry()
  and fuse_notify_delete()

* fusermount, libfuse: send value as unsigned in "user_id=" and
  "group_id=" options.  Uids/gids larger than 2147483647 would result
  in EINVAL when mounting the filesystem.  This also needs a fix in
  the kernel.

* Initialize stat buffer passed to ->getattr() and ->fgetattr() to
  zero in all cases.  Reported by Daniel Iwan

* libfuse: Add missing includes.  This allows compiling fuse with
  musl.  Patch by Daniel Thau


Older Versions (before 2013-01-01)
==================================

Please see Git history, e.g. at
https://github.com/libfuse/libfuse/blob/fuse_2_9_3/ChangeLog.
argcomplete - Bash tab completion for argparse
==============================================
*Tab complete all the things!*

Argcomplete provides easy, extensible command line tab completion of arguments for your Python script.

It makes two assumptions:

* You're using bash as your shell (limited support for zsh and tcsh is available)
* You're using `argparse <http://docs.python.org/2.7/library/argparse.html>`_ to manage your command line arguments/options

Argcomplete is particularly useful if your program has lots of options or subparsers, and if your program can
dynamically suggest completions for your argument/option values (for example, if the user is browsing resources over
the network).

Installation
------------
::

    pip install argcomplete
    activate-global-python-argcomplete

See `Activating global completion`_ below for details about the second step (or if it reports an error).

Refresh your bash environment (start a new shell or ``source /etc/profile``).

Synopsis
--------
Python code (e.g. ``my-awesome-script``):

.. code-block:: python

    #!/usr/bin/env python
    # PYTHON_ARGCOMPLETE_OK
    import argcomplete, argparse
    parser = argparse.ArgumentParser()
    ...
    argcomplete.autocomplete(parser)
    args = parser.parse_args()
    ...

Shellcode (only necessary if global completion is not activated - see `Global completion`_ below), to be put in e.g. ``.bashrc``::

    eval "$(register-python-argcomplete my-awesome-script)"

Note that the script name is passed directly to ``complete``, meaning it is only tab completed when invoked exactly
as it was registered. In the above example, ``my-awesome-script`` must be on the path, and the user must be
attempting to complete it by that name. The above line alone would **not** allow you to complete ``./my-awesome-script``,
or ``/path/to/my-awesome-script``. If you need this, you must register them separately, or use global completion.

argcomplete.autocomplete(*parser*)
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
This method is the entry point to the module. It must be called **after** ArgumentParser construction is complete, but
**before** the ``ArgumentParser.parse_args()`` method is called. The method looks for an environment variable that the
completion hook shellcode sets, and if it's there, collects completions, prints them to the output stream (fd 8 by
default), and exits. Otherwise, it returns to the caller immediately.

.. admonition:: Side effects

 Argcomplete gets completions by running your program. It intercepts the execution flow at the moment
 ``argcomplete.autocomplete()`` is called. After sending completions, it exits using ``exit_method`` (``os._exit``
 by default). This means if your program has any side effects that happen before ``argcomplete`` is called, those
 side effects will happen every time the user presses ``<TAB>`` (although anything your program prints to stdout or
 stderr will be suppressed). For this reason it's best to construct the argument parser and call
 ``argcomplete.autocomplete()`` as early as possible in your execution flow.

.. admonition:: Performance

 If the program takes a long time to get to the point where ``argcomplete.autocomplete()`` is called, the tab completion
 process will feel sluggish, and the user may lose confidence in it. So it's also important to minimize the startup time
 of the program up to that point (for example, by deferring initialization or importing of large modules until after
 parsing options).

Specifying completers
---------------------
You can specify custom completion functions for your options and arguments. Two styles are supported: callable and
readline-style. Callable completers are simpler. They are called with the following keyword arguments:

* ``prefix``: The prefix text of the last word before the cursor on the command line.
  For dynamic completers, this can be used to reduce the work required to generate possible completions.
* ``action``: The ``argparse.Action`` instance that this completer was called for.
* ``parser``: The ``argparse.ArgumentParser`` instance that the action was taken by.
* ``parsed_args``: The result of argument parsing so far (the ``argparse.Namespace`` args object normally returned by
  ``ArgumentParser.parse_args()``).

Completers should return their completions as a list of strings. An example completer for names of environment
variables might look like this:

.. code-block:: python

    def EnvironCompleter(**kwargs):
        return os.environ

To specify a completer for an argument or option, set the ``completer`` attribute of its associated action. An easy
way to do this at definition time is:

.. code-block:: python

    from argcomplete.completers import EnvironCompleter

    parser = argparse.ArgumentParser()
    parser.add_argument("--env-var1").completer = EnvironCompleter
    parser.add_argument("--env-var2").completer = EnvironCompleter
    argcomplete.autocomplete(parser)

If you specify the ``choices`` keyword for an argparse option or argument (and don't specify a completer), it will be
used for completions.

A completer that is initialized with a set of all possible choices of values for its action might look like this:

.. code-block:: python

    class ChoicesCompleter(object):
        def __init__(self, choices):
            self.choices = choices

        def __call__(self, **kwargs):
            return self.choices

The following two ways to specify a static set of choices are equivalent for completion purposes:

.. code-block:: python

    from argcomplete.completers import ChoicesCompleter

    parser.add_argument("--protocol", choices=('http', 'https', 'ssh', 'rsync', 'wss'))
    parser.add_argument("--proto").completer=ChoicesCompleter(('http', 'https', 'ssh', 'rsync', 'wss'))

Note that if you use the ``choices=<completions>`` option, argparse will show
all these choices in the ``--help`` output by default. To prevent this, set
``metavar`` (like ``parser.add_argument("--protocol", metavar="PROTOCOL",
choices=('http', 'https', 'ssh', 'rsync', 'wss'))``).

The following `script <https://raw.github.com/kislyuk/argcomplete/master/docs/examples/describe_github_user.py>`_ uses
``parsed_args`` and `Requests <http://python-requests.org/>`_ to query GitHub for publicly known members of an
organization and complete their names, then prints the member description:

.. code-block:: python

    #!/usr/bin/env python
    # PYTHON_ARGCOMPLETE_OK
    import argcomplete, argparse, requests, pprint

    def github_org_members(prefix, parsed_args, **kwargs):
        resource = "https://api.github.com/orgs/{org}/members".format(org=parsed_args.organization)
        return (member['login'] for member in requests.get(resource).json() if member['login'].startswith(prefix))

    parser = argparse.ArgumentParser()
    parser.add_argument("--organization", help="GitHub organization")
    parser.add_argument("--member", help="GitHub member").completer = github_org_members

    argcomplete.autocomplete(parser)
    args = parser.parse_args()

    pprint.pprint(requests.get("https://api.github.com/users/{m}".format(m=args.member)).json())

`Try it <https://raw.github.com/kislyuk/argcomplete/master/docs/examples/describe_github_user.py>`_ like this::

    ./describe_github_user.py --organization heroku --member <TAB>

If you have a useful completer to add to the `completer library
<https://github.com/kislyuk/argcomplete/blob/master/argcomplete/completers.py>`_, send a pull request!

Readline-style completers
~~~~~~~~~~~~~~~~~~~~~~~~~
The readline_ module defines a completer protocol in rlcompleter_. Readline-style completers are also supported by
argcomplete, so you can use the same completer object both in an interactive readline-powered shell and on the bash
command line. For example, you can use the readline-style completer provided by IPython_ to get introspective
completions like you would get in the IPython shell:

.. _readline: http://docs.python.org/2/library/readline.html
.. _rlcompleter: http://docs.python.org/2/library/rlcompleter.html#completer-objects
.. _IPython: http://ipython.org/

.. code-block:: python

    import IPython
    parser.add_argument("--python-name").completer = IPython.core.completer.Completer()

You can also use `argcomplete.CompletionFinder.rl_complete <https://argcomplete.readthedocs.org/en/latest/#argcomplete.CompletionFinder.rl_complete>`_
to plug your entire argparse parser as a readline completer.

Printing warnings in completers
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
Normal stdout/stderr output is suspended when argcomplete runs. Sometimes, though, when the user presses ``<TAB>``, it's
appropriate to print information about why completions generation failed. To do this, use ``warn``:

.. code-block:: python

    from argcomplete import warn

    def AwesomeWebServiceCompleter(prefix, **kwargs):
        if login_failed:
            warn("Please log in to Awesome Web Service to use autocompletion")
        return completions

Using a custom completion validator
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
By default, argcomplete validates your completions by checking if they start with the prefix given to the completer. You
can override this validation check by supplying the ``validator`` keyword to ``argcomplete.autocomplete()``:

.. code-block:: python

    def my_validator(current_input, keyword_to_check_against):
        # Pass through ALL options even if they don't all start with 'current_input'
        return True

    argcomplete.autocomplete(parser, validator=my_validator)

Global completion
-----------------
In global completion mode, you don't have to register each argcomplete-capable executable separately. Instead, bash
will look for the string **PYTHON_ARGCOMPLETE_OK** in the first 1024 bytes of any executable that it's running
completion for, and if it's found, follow the rest of the argcomplete protocol as described above.

Additionally, completion is activated for scripts run as ``python <script>`` and ``python -m <module>``.
This also works for alternate Python versions (e.g. ``python3`` and ``pypy``), as long as that version of Python has
argcomplete installed.

.. admonition:: Bash version compatibility

 Global completion requires bash support for ``complete -D``, which was introduced in bash 4.2. On OS X or older Linux
 systems, you will need to update bash to use this feature. Check the version of the running copy of bash with
 ``echo $BASH_VERSION``. On OS X, install bash via `Homebrew <http://brew.sh/>`_ (``brew install bash``), add
 ``/usr/local/bin/bash`` to ``/etc/shells``, and run ``chsh`` to change your shell.

 Global completion is not currently compatible with zsh.

.. note:: If you use setuptools/distribute ``scripts`` or ``entry_points`` directives to package your module,
 argcomplete will follow the wrapper scripts to their destination and look for ``PYTHON_ARGCOMPLETE_OK`` in the
 destination code.

Activating global completion
~~~~~~~~~~~~~~~~~~~~~~~~~~~~
The script ``activate-global-python-argcomplete`` will try to install the file
``bash_completion.d/python-argcomplete.sh`` (`see on GitHub`_) into an appropriate location on your system
(``/etc/bash_completion.d/`` or ``~/.bash_completion.d/``). If it
fails, but you know the correct location of your bash completion scripts directory, you can specify it with ``--dest``::

    activate-global-python-argcomplete --dest=/path/to/bash_completion.d

Otherwise, you can redirect its shellcode output into a file::

    activate-global-python-argcomplete --dest=- > file

The file's contents should then be sourced in e.g. ``~/.bashrc``.

.. _`see on GitHub`: https://github.com/kislyuk/argcomplete/blob/master/argcomplete/bash_completion.d/python-argcomplete.sh

Tcsh Support
------------
To activate completions for tcsh use::

    eval `register-python-argcomplete --shell tcsh my-awesome-script`

The ``python-argcomplete-tcsh`` script provides completions for tcsh.
The following is an example of the tcsh completion syntax for
``my-awesome-script`` emitted by ``register-python-argcomplete``::

    complete my-awesome-script 'p@*@`python-argcomplete-tcsh my-awesome-script`@'

Python Support
--------------
Argcomplete requires Python 2.7 or 3.3+.

Common Problems
---------------
If global completion is not completing your script, bash may have registered a
default completion function::

    $ complete | grep my-awesome-script
    complete -F _minimal my-awesome-script

You can fix this by restarting your shell, or by running
``complete -r my-awesome-script``.

Debugging
---------
Set the ``_ARC_DEBUG`` variable in your shell to enable verbose debug output every time argcomplete runs. Alternatively,
you can bypass the bash completion shellcode altogether, and interact with the Python code directly with something like
this::

    PROGNAME=./{YOUR_PY_SCRIPT} TEST_ARGS='some_arguments with autocompl' _ARC_DEBUG=1 COMP_LINE="$PROGNAME $TEST_ARGS" COMP_POINT=31 _ARGCOMPLETE=1 $PROGNAME 8>&1 9>>~/autocomplete_debug.log

Then tail::

    tail -f ~/autocomplete_debug.log

Acknowledgments
---------------
Inspired and informed by the optcomplete_ module by Martin Blais.

.. _optcomplete: http://pypi.python.org/pypi/optcomplete

Links
-----
* `Project home page (GitHub) <https://github.com/kislyuk/argcomplete>`_
* `Documentation (Read the Docs) <https://argcomplete.readthedocs.io/en/latest/>`_
* `Package distribution (PyPI) <https://pypi.python.org/pypi/argcomplete>`_
* `Change log <https://github.com/kislyuk/argcomplete/blob/master/Changes.rst>`_

Bugs
~~~~
Please report bugs, issues, feature requests, etc. on `GitHub <https://github.com/kislyuk/argcomplete/issues>`_.

License
-------
Licensed under the terms of the `Apache License, Version 2.0 <http://www.apache.org/licenses/LICENSE-2.0>`_.

.. image:: https://travis-ci.org/kislyuk/argcomplete.png
        :target: https://travis-ci.org/kislyuk/argcomplete
.. image:: https://codecov.io/github/kislyuk/argcomplete/coverage.svg?branch=master
        :target: https://codecov.io/github/kislyuk/argcomplete?branch=master
.. image:: https://img.shields.io/pypi/v/argcomplete.svg
        :target: https://pypi.python.org/pypi/argcomplete
.. image:: https://img.shields.io/pypi/l/argcomplete.svg
        :target: https://pypi.python.org/pypi/argcomplete
.. image:: https://readthedocs.org/projects/argcomplete/badge/?version=latest
        :target: https://argcomplete.readthedocs.org/
This is Python version 3.8.13
=============================

.. image:: https://travis-ci.org/python/cpython.svg?branch=3.8
   :alt: CPython build status on Travis CI
   :target: https://travis-ci.org/python/cpython/branches

.. image:: https://dev.azure.com/python/cpython/_apis/build/status/Azure%20Pipelines%20CI?branchName=3.8
   :alt: CPython build status on Azure DevOps
   :target: https://dev.azure.com/python/cpython/_build/latest?definitionId=4&branchName=3.8

.. image:: https://codecov.io/gh/python/cpython/branch/3.8/graph/badge.svg
   :alt: CPython code coverage on Codecov
   :target: https://codecov.io/gh/python/cpython/branch/3.8

.. image:: https://img.shields.io/badge/discourse-join_chat-brightgreen.svg
   :alt: Python Discourse chat
   :target: https://discuss.python.org/


Copyright (c) 2001-2022 Python Software Foundation.  All rights reserved.

See the end of this file for further copyright and license information.

.. contents::

General Information
-------------------

- Website: https://www.python.org
- Source code: https://github.com/python/cpython
- Issue tracker: https://bugs.python.org
- Documentation: https://docs.python.org
- Developer's Guide: https://devguide.python.org/

Contributing to CPython
-----------------------

For more complete instructions on contributing to CPython development,
see the `Developer Guide`_.

.. _Developer Guide: https://devguide.python.org/

Using Python
------------

Installable Python kits, and information about using Python, are available at
`python.org`_.

.. _python.org: https://www.python.org/

Build Instructions
------------------

On Unix, Linux, BSD, macOS, and Cygwin::

    ./configure
    make
    make test
    sudo make install

This will install Python as ``python3``.

You can pass many options to the configure script; run ``./configure --help``
to find out more.  On macOS case-insensitive file systems and on Cygwin,
the executable is called ``python.exe``; elsewhere it's just ``python``.

Building a complete Python installation requires the use of various
additional third-party libraries, depending on your build platform and
configure options.  Not all standard library modules are buildable or
useable on all platforms.  Refer to the
`Install dependencies <https://devguide.python.org/setup/#install-dependencies>`_
section of the `Developer Guide`_ for current detailed information on
dependencies for various Linux distributions and macOS.

On macOS, there are additional configure and build options related
to macOS framework and universal builds.  Refer to `Mac/README.rst
<https://github.com/python/cpython/blob/3.8/Mac/README.rst>`_.

On Windows, see `PCbuild/readme.txt
<https://github.com/python/cpython/blob/3.8/PCbuild/readme.txt>`_.

If you wish, you can create a subdirectory and invoke configure from there.
For example::

    mkdir debug
    cd debug
    ../configure --with-pydebug
    make
    make test

(This will fail if you *also* built at the top-level directory.  You should do
a ``make clean`` at the top-level first.)

To get an optimized build of Python, ``configure --enable-optimizations``
before you run ``make``.  This sets the default make targets up to enable
Profile Guided Optimization (PGO) and may be used to auto-enable Link Time
Optimization (LTO) on some platforms.  For more details, see the sections
below.

Profile Guided Optimization
^^^^^^^^^^^^^^^^^^^^^^^^^^^

PGO takes advantage of recent versions of the GCC or Clang compilers.  If used,
either via ``configure --enable-optimizations`` or by manually running
``make profile-opt`` regardless of configure flags, the optimized build
process will perform the following steps:

The entire Python directory is cleaned of temporary files that may have
resulted from a previous compilation.

An instrumented version of the interpreter is built, using suitable compiler
flags for each flavour. Note that this is just an intermediary step.  The
binary resulting from this step is not good for real life workloads as it has
profiling instructions embedded inside.

After the instrumented interpreter is built, the Makefile will run a training
workload.  This is necessary in order to profile the interpreter execution.
Note also that any output, both stdout and stderr, that may appear at this step
is suppressed.

The final step is to build the actual interpreter, using the information
collected from the instrumented one.  The end result will be a Python binary
that is optimized; suitable for distribution or production installation.


Link Time Optimization
^^^^^^^^^^^^^^^^^^^^^^

Enabled via configure's ``--with-lto`` flag.  LTO takes advantage of the
ability of recent compiler toolchains to optimize across the otherwise
arbitrary ``.o`` file boundary when building final executables or shared
libraries for additional performance gains.


What's New
----------

We have a comprehensive overview of the changes in the `What's New in Python
3.8 <https://docs.python.org/3.8/whatsnew/3.8.html>`_ document.  For a more
detailed change log, read `Misc/NEWS
<https://github.com/python/cpython/blob/3.8/Misc/NEWS.d>`_, but a full
accounting of changes can only be gleaned from the `commit history
<https://github.com/python/cpython/commits/3.8>`_.

If you want to install multiple versions of Python, see the section below
entitled "Installing multiple versions".


Documentation
-------------

`Documentation for Python 3.8 <https://docs.python.org/3.8/>`_ is online,
updated daily.

It can also be downloaded in many formats for faster access.  The documentation
is downloadable in HTML, PDF, and reStructuredText formats; the latter version
is primarily for documentation authors, translators, and people with special
formatting requirements.

For information about building Python's documentation, refer to `Doc/README.rst
<https://github.com/python/cpython/blob/3.8/Doc/README.rst>`_.


Converting From Python 2.x to 3.x
---------------------------------

Significant backward incompatible changes were made for the release of Python
3.0, which may cause programs written for Python 2 to fail when run with Python
3.  For more information about porting your code from Python 2 to Python 3, see
the `Porting HOWTO <https://docs.python.org/3/howto/pyporting.html>`_.


Testing
-------

To test the interpreter, type ``make test`` in the top-level directory.  The
test set produces some output.  You can generally ignore the messages about
skipped tests due to optional features which can't be imported.  If a message
is printed about a failed test or a traceback or core dump is produced,
something is wrong.

By default, tests are prevented from overusing resources like disk space and
memory.  To enable these tests, run ``make testall``.

If any tests fail, you can re-run the failing test(s) in verbose mode.  For
example, if ``test_os`` and ``test_gdb`` failed, you can run::

    make test TESTOPTS="-v test_os test_gdb"

If the failure persists and appears to be a problem with Python rather than
your environment, you can `file a bug report <https://bugs.python.org>`_ and
include relevant output from that command to show the issue.

See `Running & Writing Tests <https://devguide.python.org/runtests/>`_
for more on running tests.

Installing multiple versions
----------------------------

On Unix and Mac systems if you intend to install multiple versions of Python
using the same installation prefix (``--prefix`` argument to the configure
script) you must take care that your primary python executable is not
overwritten by the installation of a different version.  All files and
directories installed using ``make altinstall`` contain the major and minor
version and can thus live side-by-side.  ``make install`` also creates
``${prefix}/bin/python3`` which refers to ``${prefix}/bin/pythonX.Y``.  If you
intend to install multiple versions using the same prefix you must decide which
version (if any) is your "primary" version.  Install that version using ``make
install``.  Install all other versions using ``make altinstall``.

For example, if you want to install Python 2.7, 3.6, and 3.8 with 3.8 being the
primary version, you would execute ``make install`` in your 3.8 build directory
and ``make altinstall`` in the others.


Issue Tracker and Mailing List
------------------------------

Bug reports are welcome!  You can use the `issue tracker
<https://bugs.python.org>`_ to report bugs, and/or submit pull requests `on
GitHub <https://github.com/python/cpython>`_.

You can also follow development discussion on the `python-dev mailing list
<https://mail.python.org/mailman/listinfo/python-dev/>`_.


Proposals for enhancement
-------------------------

If you have a proposal to change Python, you may want to send an email to the
comp.lang.python or `python-ideas`_ mailing lists for initial feedback.  A
Python Enhancement Proposal (PEP) may be submitted if your idea gains ground.
All current PEPs, as well as guidelines for submitting a new PEP, are listed at
`python.org/dev/peps/ <https://www.python.org/dev/peps/>`_.

.. _python-ideas: https://mail.python.org/mailman/listinfo/python-ideas/


Release Schedule
----------------

See :pep:`569` for Python 3.8 release details.


Copyright and License Information
---------------------------------

Copyright (c) 2001-2022 Python Software Foundation.  All rights reserved.

Copyright (c) 2000 BeOpen.com.  All rights reserved.

Copyright (c) 1995-2001 Corporation for National Research Initiatives.  All
rights reserved.

Copyright (c) 1991-1995 Stichting Mathematisch Centrum.  All rights reserved.

See the file "LICENSE" for information on the history of this software, terms &
conditions for usage, and a DISCLAIMER OF ALL WARRANTIES.

This Python distribution contains *no* GNU General Public License (GPL) code,
so it may be used in proprietary projects.  There are interfaces to some GNU
code but these are entirely optional.

All trademarks referenced herein are property of their respective holders.
This is Python version 3.8.13
=============================

.. image:: https://travis-ci.org/python/cpython.svg?branch=3.8
   :alt: CPython build status on Travis CI
   :target: https://travis-ci.org/python/cpython/branches

.. image:: https://dev.azure.com/python/cpython/_apis/build/status/Azure%20Pipelines%20CI?branchName=3.8
   :alt: CPython build status on Azure DevOps
   :target: https://dev.azure.com/python/cpython/_build/latest?definitionId=4&branchName=3.8

.. image:: https://codecov.io/gh/python/cpython/branch/3.8/graph/badge.svg
   :alt: CPython code coverage on Codecov
   :target: https://codecov.io/gh/python/cpython/branch/3.8

.. image:: https://img.shields.io/badge/discourse-join_chat-brightgreen.svg
   :alt: Python Discourse chat
   :target: https://discuss.python.org/


Copyright (c) 2001-2022 Python Software Foundation.  All rights reserved.

See the end of this file for further copyright and license information.

.. contents::

General Information
-------------------

- Website: https://www.python.org
- Source code: https://github.com/python/cpython
- Issue tracker: https://bugs.python.org
- Documentation: https://docs.python.org
- Developer's Guide: https://devguide.python.org/

Contributing to CPython
-----------------------

For more complete instructions on contributing to CPython development,
see the `Developer Guide`_.

.. _Developer Guide: https://devguide.python.org/

Using Python
------------

Installable Python kits, and information about using Python, are available at
`python.org`_.

.. _python.org: https://www.python.org/

Build Instructions
------------------

On Unix, Linux, BSD, macOS, and Cygwin::

    ./configure
    make
    make test
    sudo make install

This will install Python as ``python3``.

You can pass many options to the configure script; run ``./configure --help``
to find out more.  On macOS case-insensitive file systems and on Cygwin,
the executable is called ``python.exe``; elsewhere it's just ``python``.

Building a complete Python installation requires the use of various
additional third-party libraries, depending on your build platform and
configure options.  Not all standard library modules are buildable or
useable on all platforms.  Refer to the
`Install dependencies <https://devguide.python.org/setup/#install-dependencies>`_
section of the `Developer Guide`_ for current detailed information on
dependencies for various Linux distributions and macOS.

On macOS, there are additional configure and build options related
to macOS framework and universal builds.  Refer to `Mac/README.rst
<https://github.com/python/cpython/blob/3.8/Mac/README.rst>`_.

On Windows, see `PCbuild/readme.txt
<https://github.com/python/cpython/blob/3.8/PCbuild/readme.txt>`_.

If you wish, you can create a subdirectory and invoke configure from there.
For example::

    mkdir debug
    cd debug
    ../configure --with-pydebug
    make
    make test

(This will fail if you *also* built at the top-level directory.  You should do
a ``make clean`` at the top-level first.)

To get an optimized build of Python, ``configure --enable-optimizations``
before you run ``make``.  This sets the default make targets up to enable
Profile Guided Optimization (PGO) and may be used to auto-enable Link Time
Optimization (LTO) on some platforms.  For more details, see the sections
below.

Profile Guided Optimization
^^^^^^^^^^^^^^^^^^^^^^^^^^^

PGO takes advantage of recent versions of the GCC or Clang compilers.  If used,
either via ``configure --enable-optimizations`` or by manually running
``make profile-opt`` regardless of configure flags, the optimized build
process will perform the following steps:

The entire Python directory is cleaned of temporary files that may have
resulted from a previous compilation.

An instrumented version of the interpreter is built, using suitable compiler
flags for each flavour. Note that this is just an intermediary step.  The
binary resulting from this step is not good for real life workloads as it has
profiling instructions embedded inside.

After the instrumented interpreter is built, the Makefile will run a training
workload.  This is necessary in order to profile the interpreter execution.
Note also that any output, both stdout and stderr, that may appear at this step
is suppressed.

The final step is to build the actual interpreter, using the information
collected from the instrumented one.  The end result will be a Python binary
that is optimized; suitable for distribution or production installation.


Link Time Optimization
^^^^^^^^^^^^^^^^^^^^^^

Enabled via configure's ``--with-lto`` flag.  LTO takes advantage of the
ability of recent compiler toolchains to optimize across the otherwise
arbitrary ``.o`` file boundary when building final executables or shared
libraries for additional performance gains.


What's New
----------

We have a comprehensive overview of the changes in the `What's New in Python
3.8 <https://docs.python.org/3.8/whatsnew/3.8.html>`_ document.  For a more
detailed change log, read `Misc/NEWS
<https://github.com/python/cpython/blob/3.8/Misc/NEWS.d>`_, but a full
accounting of changes can only be gleaned from the `commit history
<https://github.com/python/cpython/commits/3.8>`_.

If you want to install multiple versions of Python, see the section below
entitled "Installing multiple versions".


Documentation
-------------

`Documentation for Python 3.8 <https://docs.python.org/3.8/>`_ is online,
updated daily.

It can also be downloaded in many formats for faster access.  The documentation
is downloadable in HTML, PDF, and reStructuredText formats; the latter version
is primarily for documentation authors, translators, and people with special
formatting requirements.

For information about building Python's documentation, refer to `Doc/README.rst
<https://github.com/python/cpython/blob/3.8/Doc/README.rst>`_.


Converting From Python 2.x to 3.x
---------------------------------

Significant backward incompatible changes were made for the release of Python
3.0, which may cause programs written for Python 2 to fail when run with Python
3.  For more information about porting your code from Python 2 to Python 3, see
the `Porting HOWTO <https://docs.python.org/3/howto/pyporting.html>`_.


Testing
-------

To test the interpreter, type ``make test`` in the top-level directory.  The
test set produces some output.  You can generally ignore the messages about
skipped tests due to optional features which can't be imported.  If a message
is printed about a failed test or a traceback or core dump is produced,
something is wrong.

By default, tests are prevented from overusing resources like disk space and
memory.  To enable these tests, run ``make testall``.

If any tests fail, you can re-run the failing test(s) in verbose mode.  For
example, if ``test_os`` and ``test_gdb`` failed, you can run::

    make test TESTOPTS="-v test_os test_gdb"

If the failure persists and appears to be a problem with Python rather than
your environment, you can `file a bug report <https://bugs.python.org>`_ and
include relevant output from that command to show the issue.

See `Running & Writing Tests <https://devguide.python.org/runtests/>`_
for more on running tests.

Installing multiple versions
----------------------------

On Unix and Mac systems if you intend to install multiple versions of Python
using the same installation prefix (``--prefix`` argument to the configure
script) you must take care that your primary python executable is not
overwritten by the installation of a different version.  All files and
directories installed using ``make altinstall`` contain the major and minor
version and can thus live side-by-side.  ``make install`` also creates
``${prefix}/bin/python3`` which refers to ``${prefix}/bin/pythonX.Y``.  If you
intend to install multiple versions using the same prefix you must decide which
version (if any) is your "primary" version.  Install that version using ``make
install``.  Install all other versions using ``make altinstall``.

For example, if you want to install Python 2.7, 3.6, and 3.8 with 3.8 being the
primary version, you would execute ``make install`` in your 3.8 build directory
and ``make altinstall`` in the others.


Issue Tracker and Mailing List
------------------------------

Bug reports are welcome!  You can use the `issue tracker
<https://bugs.python.org>`_ to report bugs, and/or submit pull requests `on
GitHub <https://github.com/python/cpython>`_.

You can also follow development discussion on the `python-dev mailing list
<https://mail.python.org/mailman/listinfo/python-dev/>`_.


Proposals for enhancement
-------------------------

If you have a proposal to change Python, you may want to send an email to the
comp.lang.python or `python-ideas`_ mailing lists for initial feedback.  A
Python Enhancement Proposal (PEP) may be submitted if your idea gains ground.
All current PEPs, as well as guidelines for submitting a new PEP, are listed at
`python.org/dev/peps/ <https://www.python.org/dev/peps/>`_.

.. _python-ideas: https://mail.python.org/mailman/listinfo/python-ideas/


Release Schedule
----------------

See :pep:`569` for Python 3.8 release details.


Copyright and License Information
---------------------------------

Copyright (c) 2001-2022 Python Software Foundation.  All rights reserved.

Copyright (c) 2000 BeOpen.com.  All rights reserved.

Copyright (c) 1995-2001 Corporation for National Research Initiatives.  All
rights reserved.

Copyright (c) 1991-1995 Stichting Mathematisch Centrum.  All rights reserved.

See the file "LICENSE" for information on the history of this software, terms &
conditions for usage, and a DISCLAIMER OF ALL WARRANTIES.

This Python distribution contains *no* GNU General Public License (GPL) code,
so it may be used in proprietary projects.  There are interfaces to some GNU
code but these are entirely optional.

All trademarks referenced herein are property of their respective holders.
v50.3.2
-------


Documentation changes
^^^^^^^^^^^^^^^^^^^^^
* #2394: Extended towncrier news template to include change note categories.
  This allows to see what types of changes a given version introduces
  -- by :user:`webknjaz`
* #2427: Started enforcing strict syntax and reference validation
  in the Sphinx docs -- by :user:`webknjaz`
* #2428: Removed redundant Sphinx ``Makefile`` support -- by :user:`webknjaz`

Misc
^^^^
* #2401: Enabled test results reporting in AppVeyor CI
  -- by :user:`webknjaz`
* #2420: Replace Python 3.9.0 beta with 3.9.0 final on GitHub Actions.
* #2421: Python 3.9 Trove classifier got added to the dist metadata
  -- by :user:`webknjaz`


v50.3.1
-------

* #2093: Finalized doc revamp.
* #2097: doc: simplify index and group deprecated files
* #2102: doc overhaul step 2: break main doc into multiple sections
* #2111: doc overhaul step 3: update userguide
* #2395: Added a ``:user:`` role to Sphinx config -- by :user:`webknjaz`
* #2395: Added an illustrative explanation about the change notes to fragments dir -- by :user:`webknjaz`
* #2379: Travis CI test suite now tests against PPC64.
* #2413: Suppress EOF errors (and other exceptions) when importing lib2to3.


v50.3.0
-------

* #2368: In distutils, restore support for monkeypatched CCompiler.spawn per pypa/distutils#15.


v50.2.0
-------

* #2355: When pip is imported as part of a build, leave distutils patched.
* #2380: There are some setuptools specific changes in the
  ``setuptools.command.bdist_rpm`` module that are no longer needed, because
  they are part of the ``bdist_rpm`` module in distutils in Python
  3.5.0. Therefore, code was removed from ``setuptools.command.bdist_rpm``.


v50.1.0
-------

* #2350: Setuptools reverts using the included distutils by default. Platform maintainers and system integrators and others are *strongly* encouraged to set ``SETUPTOOLS_USE_DISTUTILS=local`` to help identify and work through the reported issues with distutils adoption, mainly to file issues and pull requests with pypa/distutils such that distutils performs as needed across every supported environment.


v50.0.3
-------

* #2363: Restore link_libpython support on Python 3.7 and earlier (see pypa/distutils#9).


v50.0.2
-------

* #2352: In distutils hack, use absolute import rather than relative to avoid bpo-30876.


v50.0.1
-------

* #2357: Restored Python 3.5 support in distutils.util for missing ``subprocess._optim_args_from_interpreter_flags``.
* #2358: Restored AIX support on Python 3.8 and earlier.
* #2361: Add Python 3.10 support to _distutils_hack. Get the 'Loader' abstract class
  from importlib.abc rather than importlib.util.abc (alias removed in Python
  3.10).


v50.0.0
-------

* #2232: Once again, Setuptools overrides the stdlib distutils on import. For environments or invocations where this behavior is undesirable, users are provided with a temporary escape hatch. If the environment variable ``SETUPTOOLS_USE_DISTUTILS`` is set to ``stdlib``, Setuptools will fall back to the legacy behavior. Use of this escape hatch is discouraged, but it is provided to ease the transition while proper fixes for edge cases can be addressed.
* #2334: In MSVC module, refine text in error message.


v49.6.0
-------

* #2129: In pkg_resources, no longer detect any pathname ending in .egg as a Python egg. Now the path must be an unpacked egg or a zip file.


v49.5.0
-------

* #2306: When running as a PEP 517 backend, setuptools does not try to install
  ``setup_requires`` itself. They are reported as build requirements for the
  frontend to install.


v49.4.0
-------

* #2310: Updated vendored packaging version to 20.4.


v49.3.2
-------

* #2300: Improve the ``safe_version`` function documentation
* #2297: Once again, in stubs prefer exec_module to the deprecated load_module.


v49.3.1
-------

* #2316: Removed warning when ``distutils`` is imported before ``setuptools`` when ``distutils`` replacement is not enabled.


v49.3.0
-------

* #2259: Setuptools now provides a .pth file (except for editable installs of setuptools) to the target environment to ensure that when enabled, the setuptools-provided distutils is preferred before setuptools has been imported (and even if setuptools is never imported). Honors the SETUPTOOLS_USE_DISTUTILS environment variable.


v49.2.1
-------

* #2257: Fixed two flaws in distutils._msvccompiler.MSVCCompiler.spawn.


v49.2.0
-------

* #2230: Now warn the user when setuptools is imported after distutils modules have been loaded (exempting PyPy for 3.6), directing the users of packages to import setuptools first.


v49.1.3
-------

* #2212: (Distutils) Allow spawn to accept environment. Avoid monkey-patching global state.
* #2249: Fix extension loading technique in stubs.


v49.1.2
-------

* #2232: In preparation for re-enabling a local copy of distutils, Setuptools now honors an environment variable, SETUPTOOLS_USE_DISTUTILS. If set to 'stdlib' (current default), distutils will be used from the standard library. If set to 'local' (default in a imminent backward-incompatible release), the local copy of distutils will be used.


v49.1.1
-------

* #2094: Removed pkg_resources.py2_warn module, which is no longer reachable.


v49.0.1
-------

* #2228: Applied fix for pypa/distutils#3, restoring expectation that spawn will raise a DistutilsExecError when attempting to execute a missing file.


v49.1.0
-------

* #2228: Disabled distutils adoption for now while emergent issues are addressed.


v49.0.0
-------

* #2165: Setuptools no longer installs a site.py file during easy_install or develop installs. As a result, .eggs on PYTHONPATH will no longer take precedence over other packages on sys.path. If this issue affects your production environment, please reach out to the maintainers at #2165.
* #2137: Removed (private) pkg_resources.RequirementParseError, now replaced by packaging.requirements.InvalidRequirement. Kept the name for compatibility, but users should catch InvalidRequirement instead.
* #2180: Update vendored packaging in pkg_resources to 19.2.
* #2199: Fix exception causes all over the codebase by using ``raise new_exception from old_exception``


v48.0.0
-------

* #2143: Setuptools adopts distutils from the Python 3.9 standard library and no longer depends on distutils in the standard library. When importing ``setuptools`` or ``setuptools.distutils_patch``, Setuptools will expose its bundled version as a top-level ``distutils`` package (and unload any previously-imported top-level distutils package), retaining the expectation that ``distutils``' objects are actually Setuptools objects.
  To avoid getting any legacy behavior from the standard library, projects are advised to always "import setuptools" prior to importing anything from distutils. This behavior happens by default when using ``pip install`` or ``pep517.build``. Workflows that rely on ``setup.py (anything)`` will need to first ensure setuptools is imported. One way to achieve this behavior without modifying code is to invoke Python thus: ``python -c "import setuptools; exec(open('setup.py').read())" (anything)``.


v47.3.2
-------

* #2071: Replaced references to the deprecated imp package with references to importlib


v47.3.1
-------

* #1973: Removed ``pkg_resources.py31compat.makedirs`` in favor of the stdlib. Use ``os.makedirs()`` instead.
* #2198: Restore ``__requires__`` directive in easy-install wrapper scripts.


v47.3.0
-------

* #2197: Console script wrapper for editable installs now has a unified template and honors importlib_metadata if present for faster script execution on older Pythons.
* #2195: Fix broken entry points generated by easy-install (pip editable installs).


v47.2.0
-------

* #2194: Editable-installed entry points now load significantly faster on Python versions 3.8+.


v47.1.1
-------

* #2156: Update mailing list pointer in developer docs

Incorporate changes from v44.1.1:

* #2158: Avoid loading working set during ``Distribution.finalize_options`` prior to invoking ``_install_setup_requires``, broken since v42.0.0.


v44.1.1
-------

* #2158: Avoid loading working set during ``Distribution.finalize_options`` prior to invoking ``_install_setup_requires``, broken since v42.0.0.


v47.1.0
-------

* #2070: In wheel-to-egg conversion, use simple pkg_resources-style namespace declaration for packages that declare namespace_packages.


v47.0.0
-------

* #2094: Setuptools now actively crashes under Python 2. Python 3.5 or later is required. Users of Python 2 should use ``setuptools<45``.
* #1700: Document all supported keywords by migrating the ones from distutils.


v46.4.0
-------

* #1753: ``attr:`` now extracts variables through rudimentary examination of the AST,
  thereby supporting modules with third-party imports. If examining the AST
  fails to find the variable, ``attr:`` falls back to the old behavior of
  importing the module. Works on Python 3 only.


v46.3.1
-------

No significant changes.


v46.3.0
-------

* #2089: Package index functionality no longer attempts to remove an md5 fragment from the index URL. This functionality, added for distribute #163 is no longer relevant.
* #2041: Preserve file modes during pkg files copying, but clear read only flag for target afterwards.
* #2105: Filter ``2to3`` deprecation warnings from ``TestDevelop.test_2to3_user_mode``.


v46.2.0
-------

* #2040: Deprecated the ``bdist_wininst`` command. Binary packages should be built as wheels instead.
* #2062: Change 'Mac OS X' to 'macOS' in code.
* #2075: Stop recognizing files ending with ``.dist-info`` as distribution metadata.
* #2086: Deprecate 'use_2to3' functionality. Packagers are encouraged to use single-source solutions or build tool chains to manage conversions outside of setuptools.
* #1698: Added documentation for ``build_meta`` (a bare minimum, not completed).
* #2082: Filter ``lib2to3`` ``PendingDeprecationWarning`` and ``DeprecationWarning`` in tests,
  because ``lib2to3`` is `deprecated in Python 3.9 <https://bugs.python.org/issue40360>`_.


v46.1.3
-------

No significant changes.


v46.1.2
-------

* #1458: Added template for reporting Python 2 incompatibilities.


v46.1.1
-------

No significant changes.


v46.1.0
-------

* #308: Allow version number normalization to be bypassed by wrapping in a 'setuptools.sic()' call.
* #1424: Prevent keeping files mode for package_data build. It may break a build if user's package data has read only flag.
* #1431: In ``easy_install.check_site_dir``, ensure the installation directory exists.
* #1563: In ``pkg_resources`` prefer ``find_spec`` (PEP 451) to ``find_module``.

Incorporate changes from v44.1.0:

* #1704: Set sys.argv[0] in setup script run by build_meta.__legacy__
* #1959: Fix for Python 4: replace unsafe six.PY3 with six.PY2
* #1994: Fixed a bug in the "setuptools.finalize_distribution_options" hook that lead to ignoring the order attribute of entry points managed by this hook.


v44.1.0
-------

* #1704: Set sys.argv[0] in setup script run by build_meta.__legacy__
* #1959: Fix for Python 4: replace unsafe six.PY3 with six.PY2
* #1994: Fixed a bug in the "setuptools.finalize_distribution_options" hook that lead to ignoring the order attribute of entry points managed by this hook.


v46.0.0
-------

* #65: Once again as in 3.0, removed the Features feature.
* #1890: Fix vendored dependencies so importing ``setuptools.extern.some_module`` gives the same object as ``setuptools._vendor.some_module``. This makes Metadata picklable again.
* #1899: Test suite now fails on warnings.
* #2011: Fix broken link to distutils docs on package_data
* #1991: Include pkg_resources test data in sdist, so tests can be executed from it.


v45.3.0
-------

* #1557: Deprecated eggsecutable scripts and updated docs.
* #1904: Update msvc.py to use CPython 3.8.0 mechanism to find msvc 14+


v45.2.0
-------

* #1905: Fixed defect in _imp, introduced in 41.6.0 when the 'tests' directory is not present.
* #1941: Improve editable installs with PEP 518 build isolation:

  * The ``--user`` option is now always available. A warning is issued if the user site directory is not available.
  * The error shown when the install directory is not in ``PYTHONPATH`` has been turned into a warning.
* #1981: Setuptools now declares its ``tests`` and ``docs`` dependencies in metadata (extras).
* #1985: Add support for installing scripts in environments where bdist_wininst is missing (i.e. Python 3.9).
* #1968: Add flake8-2020 to check for misuse of sys.version or sys.version_info.


v45.1.0
-------

* #1458: Add minimum sunset date and preamble to Python 2 warning.
* #1704: Set sys.argv[0] in setup script run by build_meta.__legacy__
* #1974: Add Python 3 Only Trove Classifier and remove universal wheel declaration for more complete transition from Python 2.


v45.0.0
-------

* #1458: Drop support for Python 2. Setuptools now requires Python 3.5 or later. Install setuptools using pip >=9 or pin to Setuptools <45 to maintain 2.7 support.
* #1959: Fix for Python 4: replace unsafe six.PY3 with six.PY2


v44.0.0
-------

* #1908: Drop support for Python 3.4.


v43.0.0
-------

* #1634: Include ``pyproject.toml`` in source distribution by default. Projects relying on the previous behavior where ``pyproject.toml`` was excluded by default should stop relying on that behavior or add ``exclude pyproject.toml`` to their MANIFEST.in file.
* #1927: Setuptools once again declares 'setuptools' in the ``build-system.requires`` and adds PEP 517 build support by declaring itself as the ``build-backend``. It additionally specifies ``build-system.backend-path`` to rely on itself for those builders that support it.


v42.0.2
-------

* #1921: Fix support for easy_install's ``find-links`` option in ``setup.cfg``.
* #1922: Build dependencies (setup_requires and tests_require) now install transitive dependencies indicated by extras.


v42.0.1
-------

* #1918: Fix regression in handling wheels compatibility tags.


v42.0.0
-------

* #1830, #1909: Mark the easy_install script and setuptools command as deprecated, and use `pip <https://pip.pypa.io/en/stable/>`_ when available to fetch/build wheels for missing ``setup_requires``/``tests_require`` requirements, with the following differences in behavior:
   * support for ``python_requires``
   * better support for wheels (proper handling of priority with respect to PEP 425 tags)
   * PEP 517/518 support
   * eggs are not supported
   * no support for the ``allow_hosts`` easy_install option (``index_url``/``find_links`` are still honored)
   * pip environment variables are honored (and take precedence over easy_install options)
* #1898: Removed the "upload" and "register" commands in favor of `twine <https://pypi.org/p/twine>`_.
* #1767: Add support for the ``license_files`` option in ``setup.cfg`` to automatically
  include multiple license files in a source distribution.
* #1829: Update handling of wheels compatibility tags:
  * add support for manylinux2010
  * fix use of removed 'm' ABI flag in Python 3.8 on Windows
* #1861: Fix empty namespace package installation from wheel.
* #1877: Setuptools now exposes a new entry point hook "setuptools.finalize_distribution_options", enabling plugins like `setuptools_scm <https://pypi.org/project/setuptools_scm>`_ to configure options on the distribution at finalization time.


v41.6.0
-------

* #479: Replace usage of deprecated ``imp`` module with local re-implementation in ``setuptools._imp``.


v41.5.1
-------

* #1891: Fix code for detecting Visual Studio's version on Windows under Python 2.


v41.5.0
-------

* #1811: Improve Visual C++ 14.X support, mainly for Visual Studio 2017 and 2019.
* #1814: Fix ``pkg_resources.Requirement`` hash/equality implementation: take PEP 508 direct URL into account.
* #1824: Fix tests when running under ``python3.10``.
* #1878: Formally deprecated the ``test`` command, with the recommendation that users migrate to ``tox``.
* #1860: Update documentation to mention the egg format is not supported by pip and dependency links support was dropped starting with pip 19.0.
* #1862: Drop ez_setup documentation: deprecated for some time (last updated in 2016), and still relying on easy_install (deprecated too).
* #1868: Drop most documentation references to (deprecated) EasyInstall.
* #1884: Added a trove classifier to document support for Python 3.8.
* #1886: Added Python 3.8 release to the Travis test matrix.


v41.4.0
-------

* #1847: In declarative config, now traps errors when invalid ``python_requires`` values are supplied.


v41.3.0
-------

* #1690: When storing extras, rely on OrderedSet to retain order of extras as indicated by the packager, which will also be deterministic on Python 2.7 (with PYTHONHASHSEED unset) and Python 3.6+.
* #1858: Fixed failing integration test triggered by 'long_description_content_type' in packaging.


v41.2.0
-------

* #479: Remove some usage of the deprecated ``imp`` module.
* #1565: Changed html_sidebars from string to list of string as per
  https://www.sphinx-doc.org/en/master/changes.html#id58


v41.1.0
-------

* #1697: Moved most of the constants from setup.py to setup.cfg
* #1749: Fixed issue with the PEP 517 backend where building a source distribution would fail if any tarball existed in the destination directory.
* #1750: Fixed an issue with PEP 517 backend where wheel builds would fail if the destination directory did not already exist.
* #1756: Force metadata-version >= 1.2. when project urls are present.
* #1769: Improve ``package_data`` check: ensure the dictionary values are lists/tuples of strings.
* #1788: Changed compatibility fallback logic for ``html.unescape`` to avoid accessing ``HTMLParser.unescape`` when not necessary. ``HTMLParser.unescape`` is deprecated and will be removed in Python 3.9.
* #1790: Added the file path to the error message when a ``UnicodeDecodeError`` occurs while reading a metadata file.
* #1776: Use license classifiers rather than the license field.


v41.0.1
-------

* #1671: Fixed issue with the PEP 517 backend that prevented building a wheel when the ``dist/`` directory contained existing ``.whl`` files.
* #1709: In test.paths_on_python_path, avoid adding unnecessary duplicates to the PYTHONPATH.
* #1741: In package_index, now honor "current directory" during a checkout of git and hg repositories under Windows


v41.0.0
-------

* #1735: When parsing setup.cfg files, setuptools now requires the files to be encoded as UTF-8. Any other encoding will lead to a UnicodeDecodeError. This change removes support for specifying an encoding using a 'coding: ' directive in the header of the file, a feature that was introduces in 40.7. Given the recent release of the aforementioned feature, it is assumed that few if any projects are utilizing the feature to specify an encoding other than UTF-8.


v40.9.0
-------

* #1675: Added support for ``setup.cfg``-only projects when using the ``setuptools.build_meta`` backend. Projects that have enabled PEP 517 no longer need to have a ``setup.py`` and can use the purely declarative ``setup.cfg`` configuration file instead.
* #1720: Added support for ``pkg_resources.parse_requirements``-style requirements in ``setup_requires`` when ``setup.py`` is invoked from the ``setuptools.build_meta`` build backend.
* #1664: Added the path to the ``PKG-INFO`` or ``METADATA`` file in the exception
  text when the ``Version:`` header can't be found.
* #1705: Removed some placeholder documentation sections referring to deprecated features.


v40.8.0
-------

* #1652: Added the ``build_meta:__legacy__`` backend, a "compatibility mode" PEP 517 backend that can be used as the default when ``build-backend`` is left unspecified in ``pyproject.toml``.
* #1635: Resource paths are passed to ``pkg_resources.resource_string`` and similar no longer accept paths that traverse parents, that begin with a leading ``/``. Violations of this expectation raise DeprecationWarnings and will become errors. Additionally, any paths that are absolute on Windows are strictly disallowed and will raise ValueErrors.
* #1536: ``setuptools`` will now automatically include licenses if ``setup.cfg`` contains a ``license_file`` attribute, unless this file is manually excluded inside ``MANIFEST.in``.


v40.7.3
-------

* #1670: In package_index, revert to using a copy of splituser from Python 3.8. Attempts to use ``urllib.parse.urlparse`` led to problems as reported in #1663 and #1668. This change serves as an alternative to #1499 and fixes #1668.


v40.7.2
-------

* #1666: Restore port in URL handling in package_index.


v40.7.1
-------

* #1660: On Python 2, when reading config files, downcast options from text to bytes to satisfy distutils expectations.


v40.7.0
-------

* #1551: File inputs for the ``license`` field in ``setup.cfg`` files now explicitly raise an error.
* #1180: Add support for non-ASCII in setup.cfg (#1062). Add support for native strings on some parameters (#1136).
* #1499: ``setuptools.package_index`` no longer relies on the deprecated ``urllib.parse.splituser`` per Python #27485.
* #1544: Added tests for PackageIndex.download (for git URLs).
* #1625: In PEP 517 build_meta builder, ensure that sdists are built as gztar per the spec.


v40.6.3
-------

* #1594: PEP 517 backend no longer declares setuptools as a dependency as it can be assumed.


v40.6.2
-------

* #1592: Fix invalid dependency on external six module (instead of vendored version).


v40.6.1
-------

* #1590: Fixed regression where packages without ``author`` or ``author_email`` fields generated malformed package metadata.


v40.6.0
-------

* #1541: Officially deprecated the ``requires`` parameter in ``setup()``.
* #1519: In ``pkg_resources.normalize_path``, additional path normalization is now performed to ensure path values to a directory is always the same, preventing false positives when checking scripts have a consistent prefix to set up on Windows.
* #1545: Changed the warning class of all deprecation warnings; deprecation warning classes are no longer derived from ``DeprecationWarning`` and are thus visible by default.
* #1554: ``build_meta.build_sdist`` now includes ``setup.py`` in source distributions by default.
* #1576: Started monkey-patching ``get_metadata_version`` and ``read_pkg_file`` onto ``distutils.DistributionMetadata`` to retain the correct version on the ``PKG-INFO`` file in the (deprecated) ``upload`` command.
* #1533: Restricted the ``recursive-include setuptools/_vendor`` to contain only .py and .txt files.
* #1395: Changed Pyrex references to Cython in the documentation.
* #1456: Documented that the ``rpmbuild`` packages is required for the ``bdist_rpm`` command.
* #1537: Documented how to use ``setup.cfg`` for ``src/ layouts``
* #1539: Added minimum version column in ``setup.cfg`` metadata table.
* #1552: Fixed a minor typo in the python 2/3 compatibility documentation.
* #1553: Updated installation instructions to point to ``pip install`` instead of ``ez_setup.py``.
* #1560: Updated ``setuptools`` distribution documentation to remove some outdated information.
* #1564: Documented ``setup.cfg`` minimum version for version and project_urls.
* #1572: Added the ``concurrent.futures`` backport ``futures`` to the Python 2.7 test suite requirements.


v40.5.0
-------

* #1335: In ``pkg_resources.normalize_path``, fix issue on Cygwin when cwd contains symlinks.
* #1502: Deprecated support for downloads from Subversion in package_index/easy_install.
* #1517: Dropped use of six.u in favor of ``u""`` literals.
* #1520: Added support for ``data_files`` in ``setup.cfg``.
* #1525: Fixed rendering of the deprecation warning in easy_install doc.


v40.4.3
-------

* #1480: Bump vendored pyparsing in pkg_resources to 2.2.1.


v40.4.2
-------

* #1497: Updated gitignore in repo.


v40.4.1
-------

* #1480: Bump vendored pyparsing to 2.2.1.


v40.4.0
-------

* #1481: Join the sdist ``--dist-dir`` and the ``build_meta`` sdist directory argument to point to the same target (meaning the build frontend no longer needs to clean manually the dist dir to avoid multiple sdist presence, and setuptools no longer needs to handle conflicts between the two).


v40.3.0
-------

* #1402: Fixed a bug with namespace packages under Python 3.6 when one package in
  current directory hides another which is installed.
* #1427: Set timestamp of ``.egg-info`` directory whenever ``egg_info`` command is run.
* #1474: ``build_meta.get_requires_for_build_sdist`` now does not include the ``wheel`` package anymore.
* #1486: Suppress warnings in pkg_resources.handle_ns.
* #1479: Remove internal use of six.binary_type.


v40.2.0
-------

* #1466: Fix handling of Unicode arguments in PEP 517 backend


v40.1.1
--------

* #1465: Fix regression with ``egg_info`` command when tagging is used.


v40.1.0
-------

* #1410: Deprecated ``upload`` and ``register`` commands.
* #1312: Introduced find_namespace_packages() to find PEP 420 namespace packages.
* #1420: Added find_namespace: directive to config parser.
* #1418: Solved race in when creating egg cache directories.
* #1450: Upgraded vendored PyParsing from 2.1.10 to 2.2.0.
* #1451: Upgraded vendored appdirs from 1.4.0 to 1.4.3.
* #1388: Fixed "Microsoft Visual C++ Build Tools" link in exception when Visual C++ not found.
* #1389: Added support for scripts which have unicode content.
* #1416: Moved several Python version checks over to using ``six.PY2`` and ``six.PY3``.
* #1441: Removed spurious executable permissions from files that don't need them.


v40.0.0
-------

* #1342: Drop support for Python 3.3.
* #1366: In package_index, fixed handling of encoded entities in URLs.
* #1383: In pkg_resources VendorImporter, avoid removing packages imported from the root.
* #1379: Minor doc fixes after actually using the new release process.
* #1385: Removed section on non-package data files.
* #1403: Fix developer's guide.
* #1404: Fix PEP 518 configuration: set build requirements in ``pyproject.toml`` to ``["wheel"]``.


v39.2.0
-------

* #1359: Support using "file:" to load a PEP 440-compliant package version from
  a text file.
* #1360: Fixed issue with a mismatch between the name of the package and the
  name of the .dist-info file in wheel files
* #1364: Add ``__dir__()`` implementation to ``pkg_resources.Distribution()`` that
  includes the attributes in the ``_provider`` instance variable.
* #1365: Take the package_dir option into account when loading the version from
  a module attribute.
* #1353: Added coverage badge to README.
* #1356: Made small fixes to the developer guide documentation.
* #1357: Fixed warnings in documentation builds and started enforcing that the
  docs build without warnings in tox.
* #1376: Updated release process docs.
* #1343: The ``setuptools`` specific ``long_description_content_type``,
  ``project_urls`` and ``provides_extras`` fields are now set consistently
  after any ``distutils`` ``setup_keywords`` calls, allowing them to override
  values.
* #1352: Added ``tox`` environment for documentation builds.
* #1354: Added ``towncrier`` for changelog management.
* #1355: Add PR template.
* #1368: Fixed tests which failed without network connectivity.
* #1369: Added unit tests for PEP 425 compatibility tags support.
* #1372: Stop testing Python 3.3 in Travis CI, now that the latest version of
  ``wheel`` no longer installs on it.

v39.1.0
-------

* #1340: Update all PyPI URLs to reflect the switch to the
  new Warehouse codebase.
* #1337: In ``pkg_resources``, now support loading resources
  for modules loaded by the ``SourcelessFileLoader``.
* #1332: Silence spurious wheel related warnings on Windows.

v39.0.1
-------

* #1297: Restore Unicode handling for Maintainer fields in
  metadata.

v39.0.0
-------

* #1296: Setuptools now vendors its own direct dependencies, no
  longer relying on the dependencies as vendored by pkg_resources.

* #296: Removed long-deprecated support for iteration on
  Version objects as returned by ``pkg_resources.parse_version``.
  Removed the ``SetuptoolsVersion`` and
  ``SetuptoolsLegacyVersion`` names as well. They should not
  have been used, but if they were, replace with
  ``Version`` and ``LegacyVersion`` from ``packaging.version``.

v38.7.0
-------

* #1288: Add support for maintainer in PKG-INFO.

v38.6.1
-------

* #1292: Avoid generating ``Provides-Extra`` in metadata when
  no extra is present (but environment markers are).

v38.6.0
-------

* #1286: Add support for Metadata 2.1 (PEP 566).

v38.5.2
-------

* #1285: Fixed RuntimeError in pkg_resources.parse_requirements
  on Python 3.7 (stemming from PEP 479).

v38.5.1
-------

* #1271: Revert to Cython legacy ``build_ext`` behavior for
  compatibility.

v38.5.0
-------

* #1229: Expand imports in ``build_ext`` to refine detection of
  Cython availability.

* #1270: When Cython is available, ``build_ext`` now uses the
  new_build_ext.

v38.4.1
-------

* #1257: In bdist_egg.scan_module, fix ValueError on Python 3.7.

v38.4.0
-------

* #1231: Removed warning when PYTHONDONTWRITEBYTECODE is enabled.

v38.3.0
-------

* #1210: Add support for PEP 345 Project-URL metadata.
* #1207: Add support for ``long_description_type`` to setup.cfg
  declarative config as intended and documented.

v38.2.5
-------

* #1232: Fix trailing slash handling in ``pkg_resources.ZipProvider``.

v38.2.4
-------

* #1220: Fix ``data_files`` handling when installing from wheel.

v38.2.3
-------

* fix Travis' Python 3.3 job.

v38.2.2
-------

* #1214: fix handling of namespace packages when installing
  from a wheel.

v38.2.1
-------

* #1212: fix encoding handling of metadata when installing
  from a wheel.

v38.2.0
-------

* #1200: easy_install now support installing from wheels:
  they will be installed as standalone unzipped eggs.

v38.1.0
-------

* #1208: Improve error message when failing to locate scripts
  in egg-info metadata.

v38.0.0
-------

* #458: In order to support deterministic builds, Setuptools no
  longer allows packages to declare ``install_requires`` as
  unordered sequences (sets or dicts).

v37.0.0
-------

* #878: Drop support for Python 2.6. Python 2.6 users should
  rely on 'setuptools < 37dev'.

v36.8.0
-------

* #1190: In SSL support for package index operations, use SNI
  where available.

v36.7.3
-------

* #1175: Bug fixes to ``build_meta`` module.

v36.7.2
-------

* #701: Fixed duplicate test discovery on Python 3.

v36.7.1
-------

* #1193: Avoid test failures in bdist_egg when
  PYTHONDONTWRITEBYTECODE is set.

v36.7.0
-------

* #1054: Support ``setup_requires`` in ``setup.cfg`` files.

v36.6.1
-------

* #1132: Removed redundant and costly serialization/parsing step
  in ``EntryPoint.__init__``.

* #844: ``bdist_egg --exclude-source-files`` now tested and works
  on Python 3.

v36.6.0
-------

* #1143: Added ``setuptools.build_meta`` module, an implementation
  of PEP-517 for Setuptools-defined packages.

* #1143: Added ``dist_info`` command for producing dist_info
  metadata.

v36.5.0
-------

* #170: When working with Mercurial checkouts, use Windows-friendly
  syntax for suppressing output.

* Inspired by #1134, performed substantial refactoring of
  ``pkg_resources.find_on_path`` to facilitate an optimization
  for paths with many non-version entries.

v36.4.0
-------

* #1075: Add new ``Description-Content-Type`` metadata field. `See here for
  documentation on how to use this field.
  <https://packaging.python.org/specifications/#description-content-type>`_

* #1068: Sort files and directories when building eggs for
  deterministic order.

* #196: Remove caching of easy_install command in fetch_build_egg.
  Fixes issue where ``pytest-runner-N.N`` would satisfy the installation
  of ``pytest``.

* #1129: Fix working set dependencies handling when replacing conflicting
  distributions (e.g. when using ``setup_requires`` with a conflicting
  transitive dependency, fix #1124).

* #1133: Improved handling of README files extensions and added
  Markdown to the list of searched READMES.

* #1135: Improve performance of pkg_resources import by not invoking
  ``access`` or ``stat`` and using ``os.listdir`` instead.

v36.3.0
-------

* #1131: Make possible using several files within ``file:`` directive
  in metadata.long_description in ``setup.cfg``.

v36.2.7
-------

* fix #1105: Fix handling of requirements with environment
  markers when declared in ``setup.cfg`` (same treatment as
  for #1081).

v36.2.6
-------

* #462: Don't assume a directory is an egg by the ``.egg``
  extension alone.

v36.2.5
-------

* #1093: Fix test command handler with extras_require.
* #1112, #1091, #1115: Now using Trusty containers in
  Travis for CI and CD.

v36.2.4
-------

* #1092: ``pkg_resources`` now uses ``inspect.getmro`` to
  resolve classes in method resolution order.

v36.2.3
-------

* #1102: Restore behavior for empty extras.

v36.2.2
-------

* #1099: Revert commit a3ec721, restoring intended purpose of
  extras as part of a requirement declaration.

v36.2.1
-------

* fix #1086
* fix #1087
* support extras specifiers in install_requires requirements

v36.2.0
-------

* #1081: Environment markers indicated in ``install_requires``
  are now processed and treated as nameless ``extras_require``
  with markers, allowing their metadata in requires.txt to be
  correctly generated.

* #1053: Tagged commits are now released using Travis-CI
  build stages, meaning releases depend on passing tests on
  all supported Python versions (Linux) and not just the latest
  Python version.

v36.1.1
-------

* #1083: Correct ``py31compat.makedirs`` to correctly honor
  ``exist_ok`` parameter.
* #1083: Also use makedirs compatibility throughout setuptools.

v36.1.0
-------

* #1083: Avoid race condition on directory creation in
  ``pkg_resources.ensure_directory``.

* Removed deprecation of and restored support for
  ``upload_docs`` command for sites other than PyPI.
  Only warehouse is dropping support, but services like
  `devpi <http://doc.devpi.net/latest/>`_ continue to
  support docs built by setuptools' plugins. See
  `this comment <https://bitbucket.org/hpk42/devpi/issues/388/support-rtd-model-for-building-uploading#comment-34292423>`_
  for more context on the motivation for this change.

v36.0.1
-------

* #1042: Fix import in py27compat module that still
  referenced six directly, rather than through the externs
  module (vendored packages hook).

v36.0.0
-------

* #980 and others: Once again, Setuptools vendors all
  of its dependencies. It seems to be the case that in
  the Python ecosystem, all build tools must run without
  any dependencies (build, runtime, or otherwise). At
  such a point that a mechanism exists that allows
  build tools to have dependencies, Setuptools will adopt
  it.

v35.0.2
-------

* #1015: Fix test failures on Python 3.7.

* #1024: Add workaround for Jython #2581 in monkey module.

v35.0.1
-------

* #992: Revert change introduced in v34.4.1, now
  considered invalid.

* #1016: Revert change introduced in v35.0.0 per #1014,
  referencing #436. The approach had unintended
  consequences, causing sdist installs to be missing
  files.

v35.0.0
-------

* #436: In egg_info.manifest_maker, no longer read
  the file list from the manifest file, and instead
  re-build it on each build. In this way, files removed
  from the specification will not linger in the manifest.
  As a result, any files manually added to the manifest
  will be removed on subsequent egg_info invocations.
  No projects should be manually adding files to the
  manifest and should instead use MANIFEST.in or SCM
  file finders to force inclusion of files in the manifest.

v34.4.1
-------

* #1008: In MSVC support, use always the last version available for Windows SDK and UCRT SDK.

* #1008: In MSVC support, fix "vcruntime140.dll" returned path with Visual Studio 2017.

* #992: In msvc.msvc9_query_vcvarsall, ensure the
  returned dicts have str values and not Unicode for
  compatibility with os.environ.

v34.4.0
-------

* #995: In MSVC support, add support for "Microsoft Visual Studio 2017" and "Microsoft Visual Studio Build Tools 2017".

* #999 via #1007: Extend support for declarative package
  config in a setup.cfg file to include the options
  ``python_requires`` and ``py_modules``.

v34.3.3
-------

* #967 (and #997): Explicitly import submodules of
  packaging to account for environments where the imports
  of those submodules is not implied by other behavior.

v34.3.2
-------

* #993: Fix documentation upload by correcting
  rendering of content-type in _build_multipart
  on Python 3.

v34.3.1
-------

* #988: Trap ``os.unlink`` same as ``os.remove`` in
  ``auto_chmod`` error handler.

* #983: Fixes to invalid escape sequence deprecations on
  Python 3.6.

v34.3.0
-------

* #941: In the upload command, if the username is blank,
  default to ``getpass.getuser()``.

* #971: Correct distutils findall monkeypatch to match
  appropriate versions (namely Python 3.4.6).

v34.2.0
-------

* #966: Add support for reading dist-info metadata and
  thus locating Distributions from zip files.

* #968: Allow '+' and '!' in egg fragments
  so that it can take package names that contain
  PEP 440 conforming version specifiers.

v34.1.1
-------

* #953: More aggressively employ the compatibility issue
  originally added in #706.

v34.1.0
-------

* #930: ``build_info`` now accepts two new parameters
  to optimize and customize the building of C libraries.

v34.0.3
-------

* #947: Loosen restriction on the version of six required,
  restoring compatibility with environments relying on
  six 1.6.0 and later.

v34.0.2
-------

* #882: Ensure extras are honored when building the
  working set.
* #913: Fix issue in develop if package directory has
  a trailing slash.

v34.0.1
-------

* #935: Fix glob syntax in graft.

v34.0.0
-------

* #581: Instead of vendoring the growing list of
  dependencies that Setuptools requires to function,
  Setuptools now requires these dependencies just like
  any other project. Unlike other projects, however,
  Setuptools cannot rely on ``setup_requires`` to
  demand the dependencies it needs to install because
  its own machinery would be necessary to pull those
  dependencies if not present (a bootstrapping problem).
  As a result, Setuptools no longer supports self upgrade or
  installation in the general case. Instead, users are
  directed to use pip to install and upgrade using the
  ``wheel`` distributions of setuptools.

  Users are welcome to contrive other means to install
  or upgrade Setuptools using other means, such as
  pre-installing the Setuptools dependencies with pip
  or a bespoke bootstrap tool, but such usage is not
  recommended and is not supported.

  As discovered in #940, not all versions of pip will
  successfully install Setuptools from its pre-built
  wheel. If you encounter issues with "No module named
  six" or "No module named packaging", especially
  following a line "Running setup.py egg_info for package
  setuptools", then your pip is not new enough.

  There's an additional issue in pip where setuptools
  is upgraded concurrently with other source packages,
  described in pip #4253. The proposed workaround is to
  always upgrade Setuptools first prior to upgrading
  other packages that would upgrade Setuptools.

v33.1.1
-------

* #921: Correct issue where certifi fallback not being
  reached on Windows.

v33.1.0
-------

Installation via pip, as indicated in the `Python Packaging
User's Guide <https://packaging.python.org/installing/>`_,
is the officially-supported mechanism for installing
Setuptools, and this recommendation is now explicit in the
much more concise README.

Other edits and tweaks were made to the documentation. The
codebase is unchanged.

v33.0.0
-------

* #619: Removed support for the ``tag_svn_revision``
  distribution option. If Subversion tagging support is
  still desired, consider adding the functionality to
  setuptools_svn in setuptools_svn #2.

v32.3.1
-------

* #866: Use ``dis.Bytecode`` on Python 3.4 and later in
  ``setuptools.depends``.

v32.3.0
-------

* #889: Backport proposed fix for disabling interpolation in
  distutils.Distribution.parse_config_files.

v32.2.0
-------

* #884: Restore support for running the tests under
  `pytest-runner <https://github.com/pytest-dev/pytest-runner>`_
  by ensuring that PYTHONPATH is honored in tests invoking
  a subprocess.

v32.1.3
-------

* #706: Add rmtree compatibility shim for environments where
  rmtree fails when passed a unicode string.

v32.1.2
-------

* #893: Only release sdist in zip format as warehouse now
  disallows releasing two different formats.

v32.1.1
-------

* #704: More selectively ensure that 'rmtree' is not invoked with
  a byte string, enabling it to remove files that are non-ascii,
  even on Python 2.

* #712: In 'sandbox.run_setup', ensure that ``__file__`` is
  always a ``str``, modeling the behavior observed by the
  interpreter when invoking scripts and modules.

v32.1.0
-------

* #891: In 'test' command on test failure, raise DistutilsError,
  suppression invocation of subsequent commands.

v32.0.0
-------

* #890: Revert #849. ``global-exclude .foo`` will not match all
  ``*.foo`` files any more. Package authors must add an explicit
  wildcard, such as ``global-exclude *.foo``, to match all
  ``.foo`` files. See #886, #849.

v31.0.1
-------

* #885: Fix regression where 'pkg_resources._rebuild_mod_path'
  would fail when a namespace package's '__path__' was not
  a list with a sort attribute.

v31.0.0
-------

* #250: Install '-nspkg.pth' files for packages installed
  with 'setup.py develop'. These .pth files allow
  namespace packages installed by pip or develop to
  co-mingle. This change required the removal of the
  change for #805 and pip #1924, introduced in 28.3.0 and implicated
  in #870, but means that namespace packages not in a
  site packages directory will no longer work on Python
  earlier than 3.5, whereas before they would work on
  Python not earlier than 3.3.

v30.4.0
-------

* #879: For declarative config:

  - read_configuration() now accepts ignore_option_errors argument. This allows scraping tools to read metadata without a need to download entire packages. E.g. we can gather some stats right from GitHub repos just by downloading setup.cfg.

  - packages find: directive now supports fine tuning from a subsection. The same arguments as for find() are accepted.

v30.3.0
-------

* #394 via #862: Added support for `declarative package
  config in a setup.cfg file
  <https://setuptools.readthedocs.io/en/latest/setuptools.html#configuring-setup-using-setup-cfg-files>`_.

v30.2.1
-------

* #850: In test command, invoke unittest.main with
  indication not to exit the process.

v30.2.0
-------

* #854: Bump to vendored Packaging 16.8.

v30.1.0
-------

* #846: Also trap 'socket.error' when opening URLs in
  package_index.

* #849: Manifest processing now matches the filename
  pattern anywhere in the filename and not just at the
  start. Restores behavior found prior to 28.5.0.

v30.0.0
-------

* #864: Drop support for Python 3.2. Systems requiring
  Python 3.2 support must use 'setuptools < 30'.

* #825: Suppress warnings for single files.

* #830 via #843: Once again restored inclusion of data
  files to sdists, but now trap TypeError caused by
  techniques employed rjsmin and similar.

v29.0.1
-------

* #861: Re-release of v29.0.1 with the executable script
  launchers bundled. Now, launchers are included by default
  and users that want to disable this behavior must set the
  environment variable
  'SETUPTOOLS_INSTALL_WINDOWS_SPECIFIC_FILES' to
  a false value like "false" or "0".

v29.0.0
-------

* #841: Drop special exception for packages invoking
  win32com during the build/install process. See
  Distribute #118 for history.

v28.8.0
-------

* #629: Per the discussion, refine the sorting to use version
  value order for more accurate detection of the latest
  available version when scanning for packages. See also
  #829.

* #837: Rely on the config var "SO" for Python 3.3.0 only
  when determining the ext filename.

v28.7.1
-------

* #827: Update PyPI root for dependency links.

* #833: Backed out changes from #830 as the implementation
  seems to have problems in some cases.

v28.7.0
-------

* #832: Moved much of the namespace package handling
  functionality into a separate module for re-use in something
  like #789.
* #830: ``sdist`` command no longer suppresses the inclusion
  of data files, re-aligning with the expectation of distutils
  and addressing #274 and #521.

v28.6.1
-------

* #816: Fix manifest file list order in tests.

v28.6.0
-------

* #629: When scanning for packages, ``pkg_resources`` now
  ignores empty egg-info directories and gives precedence to
  packages whose versions are lexicographically greatest,
  a rough approximation for preferring the latest available
  version.

v28.5.0
-------

* #810: Tests are now invoked with tox and not setup.py test.
* #249 and #450 via #764: Avoid scanning the whole tree
  when building the manifest. Also fixes a long-standing bug
  where patterns in ``MANIFEST.in`` had implicit wildcard
  matching. This caused ``global-exclude .foo`` to exclude
  all ``*.foo`` files, but also ``global-exclude bar.py`` to
  exclude ``foo_bar.py``.

v28.4.0
-------

* #732: Now extras with a hyphen are honored per PEP 426.
* #811: Update to pyparsing 2.1.10.
* Updated ``setuptools.command.sdist`` to re-use most of
  the functionality directly from ``distutils.command.sdist``
  for the ``add_defaults`` method with strategic overrides.
  See #750 for rationale.
* #760 via #762: Look for certificate bundle where SUSE
  Linux typically presents it. Use ``certifi.where()`` to locate
  the bundle.

v28.3.0
-------

* #809: In ``find_packages()``, restore support for excluding
  a parent package without excluding a child package.

* #805: Disable ``-nspkg.pth`` behavior on Python 3.3+ where
  PEP-420 functionality is adequate. Fixes pip #1924.

v28.1.0
-------

* #803: Bump certifi to 2016.9.26.

v28.0.0
-------

* #733: Do not search excluded directories for packages.
  This introduced a backwards incompatible change in ``find_packages()``
  so that ``find_packages(exclude=['foo']) == []``, excluding subpackages of ``foo``.
  Previously, ``find_packages(exclude=['foo']) == ['foo.bar']``,
  even though the parent ``foo`` package was excluded.

* #795: Bump certifi.

* #719: Suppress decoding errors and instead log a warning
  when metadata cannot be decoded.

v27.3.1
-------

* #790: In MSVC monkeypatching, explicitly patch each
  function by name in the target module instead of inferring
  the module from the function's ``__module__``. Improves
  compatibility with other packages that might have previously
  patched distutils functions (i.e. NumPy).

v27.3.0
-------

* #794: In test command, add installed eggs to PYTHONPATH
  when invoking tests so that subprocesses will also have the
  dependencies available. Fixes `tox 330
  <https://github.com/tox-dev/tox/issues/330>`_.

* #795: Update vendored pyparsing 2.1.9.

v27.2.0
-------

* #520 and #513: Suppress ValueErrors in fixup_namespace_packages
  when lookup fails.

* Nicer, more consistent interfaces for msvc monkeypatching.

v27.1.2
-------

* #779 via #781: Fix circular import.

v27.1.1
-------

* #778: Fix MSVC monkeypatching.

v27.1.0
-------

* Introduce the (private) ``monkey`` module to encapsulate
  the distutils monkeypatching behavior.

v27.0.0
-------

* Now use Warehouse by default for
  ``upload``, patching ``distutils.config.PyPIRCCommand`` to
  affect default behavior.

  Any config in .pypirc should be updated to replace

    https://pypi.python.org/pypi/

  with

    https://upload.pypi.org/legacy/

  Similarly, any passwords stored in the keyring should be
  updated to use this new value for "system".

  The ``upload_docs`` command will continue to use the python.org
  site, but the command is now deprecated. Users are urged to use
  Read The Docs instead.

* #776: Use EXT_SUFFIX for py_limited_api renaming.

* #774 and #775: Use LegacyVersion from packaging when
  detecting numpy versions.

v26.1.1
-------

* Re-release of 26.1.0 with pytest pinned to allow for automated
  deployment and thus proper packaging environment variables,
  fixing issues with missing executable launchers.

v26.1.0
-------

* #763: ``pkg_resources.get_default_cache`` now defers to the
  `appdirs project <https://pypi.org/project/appdirs>`_ to
  resolve the cache directory. Adds a vendored dependency on
  appdirs to pkg_resources.

v26.0.0
-------

* #748: By default, sdists are now produced in gzipped tarfile
  format by default on all platforms, adding forward compatibility
  for the same behavior in Python 3.6 (See Python #27819).

* #459 via #736: On Windows with script launchers,
  sys.argv[0] now reflects
  the name of the entry point, consistent with the behavior in
  distlib and pip wrappers.

* #752 via #753: When indicating ``py_limited_api`` to Extension,
  it must be passed as a keyword argument.

v25.4.0
-------

* Add Extension(py_limited_api=True). When set to a truthy value,
  that extension gets a filename appropriate for code using Py_LIMITED_API.
  When used correctly this allows a single compiled extension to work on
  all future versions of CPython 3.
  The py_limited_api argument only controls the filename. To be
  compatible with multiple versions of Python 3, the C extension
  will also need to set -DPy_LIMITED_API=... and be modified to use
  only the functions in the limited API.

v25.3.0
-------

* #739 Fix unquoted libpaths by fixing compatibility between ``numpy.distutils`` and ``distutils._msvccompiler`` for numpy < 1.11.2 (Fix issue #728, error also fixed in Numpy).

* #731: Bump certifi.

* Style updates. See #740, #741, #743, #744, #742, #747.

* #735: include license file.

v25.2.0
-------

* #612 via #730: Add a LICENSE file which needs to be provided by the terms of
  the MIT license.

v25.1.6
-------

* #725: revert ``library_dir_option`` patch (Error is related to ``numpy.distutils`` and make errors on non Numpy users).

v25.1.5
-------

* #720
* #723: Improve patch for ``library_dir_option``.

v25.1.4
-------

* #717
* #713
* #707: Fix Python 2 compatibility for MSVC by catching errors properly.
* #715: Fix unquoted libpaths by patching ``library_dir_option``.

v25.1.3
-------

* #714 and #704: Revert fix as it breaks other components
  downstream that can't handle unicode. See #709, #710,
  and #712.

v25.1.2
-------

* #704: Fix errors when installing a zip sdist that contained
  files named with non-ascii characters on Windows would
  crash the install when it attempted to clean up the build.
* #646: MSVC compatibility - catch errors properly in
  RegistryInfo.lookup.
* #702: Prevent UnboundLocalError when initial working_set
  is empty.

v25.1.1
-------

* #686: Fix issue in sys.path ordering by pkg_resources when
  rewrite technique is "raw".
* #699: Fix typo in msvc support.

v25.1.0
-------

* #609: Setuptools will now try to download a distribution from
  the next possible download location if the first download fails.
  This means you can now specify multiple links as ``dependency_links``
  and all links will be tried until a working download link is encountered.

v25.0.2
-------

* #688: Fix AttributeError in setup.py when invoked not from
  the current directory.

v25.0.1
-------

* Cleanup of setup.py script.

* Fixed documentation builders by allowing setup.py
  to be imported without having bootstrapped the
  metadata.

* More style cleanup. See #677, #678, #679, #681, #685.

v25.0.0
-------

* #674: Default ``sys.path`` manipulation by easy-install.pth
  is now "raw", meaning that when writing easy-install.pth
  during any install operation, the ``sys.path`` will not be
  rewritten and will no longer give preference to easy_installed
  packages.

  To retain the old behavior when using any easy_install
  operation (including ``setup.py install`` when setuptools is
  present), set the environment variable:

    SETUPTOOLS_SYS_PATH_TECHNIQUE=rewrite

  This project hopes that that few if any environments find it
  necessary to retain the old behavior, and intends to drop
  support for it altogether in a future release. Please report
  any relevant concerns in the ticket for this change.

v24.3.1
-------

* #398: Fix shebang handling on Windows in script
  headers where spaces in ``sys.executable`` would
  produce an improperly-formatted shebang header,
  introduced in 12.0 with the fix for #188.

* #663, #670: More style updates.

v24.3.0
-------

* #516: Disable ``os.link`` to avoid hard linking
  in ``sdist.make_distribution``, avoiding errors on
  systems that support hard links but not on the
  file system in which the build is occurring.

v24.2.1
-------

* #667: Update Metadata-Version to 1.2 when
  ``python_requires`` is supplied.

v24.2.0
-------

* #631: Add support for ``python_requires`` keyword.

v24.1.1
-------

* More style updates. See #660, #661, #641.

v24.1.0
-------

* #659: ``setup.py`` now will fail fast and with a helpful
  error message when the necessary metadata is missing.
* More style updates. See #656, #635, #640,
  #644, #650, #652, and #655.

v24.0.3
-------

* Updated style in much of the codebase to match
  community expectations. See #632, #633, #634,
  #637, #639, #638, #642, #648.

v24.0.2
-------

* If MSVC++14 is needed ``setuptools.msvc`` now redirect
  user to Visual C++ Build Tools web page.

v24.0.1
-------

* #625 and #626: Fixes on ``setuptools.msvc`` mainly
  for Python 2 and Linux.

v24.0.0
-------

* Pull Request #174: Add more aggressive support for
  standalone Microsoft Visual C++ compilers in
  msvc9compiler patch.
  Particularly : Windows SDK 6.1 and 7.0
  (MSVC++ 9.0), Windows SDK 7.1 (MSVC++ 10.0),
  Visual C++ Build Tools 2015 (MSVC++14)
* Renamed ``setuptools.msvc9_support`` to
  ``setuptools.msvc``.

v23.2.1
-------

Re-release of v23.2.0, which was missing the intended
commits.

* #623: Remove used of deprecated 'U' flag when reading
  manifests.

v23.1.0
-------

* #619: Deprecated ``tag_svn_revision`` distribution
  option.

v23.0.0
-------

* #611: Removed ARM executables for CLI and GUI script
  launchers on Windows. If this was a feature you cared
  about, please comment in the ticket.
* #604: Removed docs building support. The project
  now relies on documentation hosted at
  https://setuptools.readthedocs.io/.

v22.0.5
-------

* #604: Restore repository for upload_docs command
  to restore publishing of docs during release.

v22.0.4
-------

* #589: Upload releases to pypi.io using the upload
  hostname and legacy path.

v22.0.3
-------

* #589: Releases are now uploaded to pypi.io (Warehouse)
  even when releases are made on Twine via Travis.

v22.0.2
-------

* #589: Releases are now uploaded to pypi.io (Warehouse).

v22.0.1
-------

* #190: On Python 2, if unicode is passed for packages to
  ``build_py`` command, it will be handled just as with
  text on Python 3.

v22.0.0
-------

Intended to be v21.3.0, but jaraco accidentally released as
a major bump.

* #598: Setuptools now lists itself first in the User-Agent
  for web requests, better following the guidelines in
  `RFC 7231
  <https://tools.ietf.org/html/rfc7231#section-5.5.3>`_.

v21.2.2
-------

* Minor fixes to changelog and docs.

v21.2.1
-------

* #261: Exclude directories when resolving globs in
  package_data.

v21.2.0
-------

* #539: In the easy_install get_site_dirs, honor all
  paths found in ``site.getsitepackages``.

v21.1.0
-------

* #572: In build_ext, now always import ``_CONFIG_VARS``
  from ``distutils`` rather than from ``sysconfig``
  to allow ``distutils.sysconfig.customize_compiler``
  configure the OS X compiler for ``-dynamiclib``.

v21.0.0
-------

* Removed ez_setup.py from Setuptools sdist. The
  bootstrap script will be maintained in its own
  branch and should be generally be retrieved from
  its canonical location at
  https://bootstrap.pypa.io/ez_setup.py.

v20.10.0
--------

* #553: egg_info section is now generated in a
  deterministic order, matching the order generated
  by earlier versions of Python. Except on Python 2.6,
  order is preserved when existing settings are present.
* #556: Update to Packaging 16.7, restoring support
  for deprecated ``python_implmentation`` marker.
* #555: Upload command now prompts for a password
  when uploading to PyPI (or other repository) if no
  password is present in .pypirc or in the keyring.

v20.9.0
-------

* #548: Update certify version to 2016.2.28
* #545: Safely handle deletion of non-zip eggs in rotate
  command.

v20.8.1
-------

* Issue #544: Fix issue with extra environment marker
  processing in WorkingSet due to refactor in v20.7.0.

v20.8.0
-------

* Issue #543: Re-release so that latest release doesn't
  cause déjà vu with distribute and setuptools 0.7 in
  older environments.

v20.7.0
-------

* Refactored extra environment marker processing
  in WorkingSet.
* Issue #533: Fixed intermittent test failures.
* Issue #536: In msvc9_support, trap additional exceptions
  that might occur when importing
  ``distutils.msvc9compiler`` in mingw environments.
* Issue #537: Provide better context when package
  metadata fails to decode in UTF-8.

v20.6.8
-------

* Issue #523: Restored support for environment markers,
  now honoring 'extra' environment markers.

v20.6.7
-------

* Issue #523: Disabled support for environment markers
  introduced in v20.5.

v20.6.6
-------

* Issue #503: Restore support for PEP 345 environment
  markers by updating to Packaging 16.6.

v20.6.0
-------

* New release process that relies on
  `bumpversion <https://github.com/peritus/bumpversion>`_
  and Travis CI for continuous deployment.
* Project versioning semantics now follow
  `semver <https://semver.org>`_ precisely.
  The 'v' prefix on version numbers now also allows
  version numbers to be referenced in the changelog,
  e.g. http://setuptools.readthedocs.io/en/latest/history.html#v20-6-0.

20.5
----

* BB Pull Request #185, #470: Add support for environment markers
  in requirements in install_requires, setup_requires,
  tests_require as well as adding a test for the existing
  extra_requires machinery.

20.4
----

* Issue #422: Moved hosting to
  `Github <https://github.com/pypa/setuptools>`_
  from `Bitbucket <https://bitbucket.org/pypa/setuptools>`_.
  Issues have been migrated, though all issues and comments
  are attributed to bb-migration. So if you have a particular
  issue or issues to which you've been subscribed, you will
  want to "watch" the equivalent issue in Github.
  The Bitbucket project will be retained for the indefinite
  future, but Github now hosts the canonical project repository.

20.3.1
------

* Issue #519: Remove import hook when reloading the
  ``pkg_resources`` module.
* BB Pull Request #184: Update documentation in ``pkg_resources``
  around new ``Requirement`` implementation.

20.3
----

* BB Pull Request #179: ``pkg_resources.Requirement`` objects are
  now a subclass of ``packaging.requirements.Requirement``,
  allowing any environment markers and url (if any) to be
  affiliated with the requirement
* BB Pull Request #179: Restore use of RequirementParseError
  exception unintentionally dropped in 20.2.

20.2.2
------

* Issue #502: Correct regression in parsing of multiple
  version specifiers separated by commas and spaces.

20.2.1
------

* Issue #499: Restore compatibility for legacy versions
  by bumping to packaging 16.4.

20.2
----

* Changelog now includes release dates and links to PEPs.
* BB Pull Request #173: Replace dual PEP 345 _markerlib implementation
  and PEP 426 implementation of environment marker support from
  packaging 16.1 and PEP 508. Fixes Issue #122.
  See also BB Pull Request #175, BB Pull Request #168, and
  BB Pull Request #164. Additionally:

   - ``Requirement.parse`` no longer retains the order of extras.
   - ``parse_requirements`` now requires that all versions be
     PEP-440 compliant, as revealed in #499. Packages released
     with invalid local versions should be re-released using
     the proper local version syntax, e.g. ``mypkg-1.0+myorg.1``.

20.1.1
------

* Update ``upload_docs`` command to also honor keyring
  for password resolution.

20.1
----

* Added support for using passwords from keyring in the upload
  command. See `the upload docs
  <https://setuptools.readthedocs.io/en/latest/setuptools.html#upload-upload-source-and-or-egg-distributions-to-pypi>`_
  for details.

20.0
----

* Issue #118: Once again omit the package metadata (egg-info)
  from the list of outputs in ``--record``. This version of setuptools
  can no longer be used to upgrade pip earlier than 6.0.

19.7
----

* Off-project PR: `0dcee79 <https://github.com/pypa/setuptools/commit/0dcee791dfdcfacddaaec79b29f30a347a147413>`_ and `f9bd9b9 <https://github.com/pypa/setuptools/commit/f9bd9b9f5df54ef5a0bf8d16c3a889ab8c640580>`_
  For FreeBSD, also `honor root certificates from ca_root_nss <https://github.com/pypa/setuptools/commit/3ae46c30225eb46e1f5aada1a19e88b79f04dc72>`_.

19.6.2
------

* Issue #491: Correct regression incurred in 19.4 where
  a double-namespace package installed using pip would
  cause a TypeError.

19.6.1
------

* Restore compatibility for PyPy 3 compatibility lost in
  19.4.1 addressing Issue #487.
* ``setuptools.launch`` shim now loads scripts in a new
  namespace, avoiding getting relative imports from
  the setuptools package on Python 2.

19.6
----

* Added a new entry script ``setuptools.launch``,
  implementing the shim found in
  ``pip.util.setuptools_build``. Use this command to launch
  distutils-only packages under setuptools in the same way that
  pip does, causing the setuptools monkeypatching of distutils
  to be invoked prior to invoking a script. Useful for debugging
  or otherwise installing a distutils-only package under
  setuptools when pip isn't available or otherwise does not
  expose the desired functionality. For example::

    $ python -m setuptools.launch setup.py develop

* Issue #488: Fix dual manifestation of Extension class in
  extension packages installed as dependencies when Cython
  is present.

19.5
----

* Issue #486: Correct TypeError when getfilesystemencoding
  returns None.
* Issue #139: Clarified the license as MIT.
* BB Pull Request #169: Removed special handling of command
  spec in scripts for Jython.

19.4.1
------

* Issue #487: Use direct invocation of ``importlib.machinery``
  in ``pkg_resources`` to avoid missing detection on relevant
  platforms.

19.4
----

* Issue #341: Correct error in path handling of package data
  files in ``build_py`` command when package is empty.
* Distribute #323, Issue #141, Issue #207, and
  BB Pull Request #167: Another implementation of
  ``pkg_resources.WorkingSet`` and ``pkg_resources.Distribution``
  that supports replacing an extant package with a new one,
  allowing for setup_requires dependencies to supersede installed
  packages for the session.

19.3
----

* Issue #229: Implement new technique for readily incorporating
  dependencies conditionally from vendored copies or primary
  locations. Adds a new dependency on six.

19.2
----

* BB Pull Request #163: Add get_command_list method to Distribution.
* BB Pull Request #162: Add missing whitespace to multiline string
  literals.

19.1.1
------

* Issue #476: Cast version to string (using default encoding)
  to avoid creating Unicode types on Python 2 clients.
* Issue #477: In Powershell downloader, use explicit rendering
  of strings, rather than rely on ``repr``, which can be
  incorrect (especially on Python 2).

19.1
----

* Issue #215: The bootstrap script ``ez_setup.py`` now
  automatically detects
  the latest version of setuptools (using PyPI JSON API) rather
  than hard-coding a particular value.
* Issue #475: Fix incorrect usage in _translate_metadata2.

19.0
----

* Issue #442: Use RawConfigParser for parsing .pypirc file.
  Interpolated values are no longer honored in .pypirc files.

18.8.1
------

* Issue #440: Prevent infinite recursion when a SandboxViolation
  or other UnpickleableException occurs in a sandbox context
  with setuptools hidden. Fixes regression introduced in Setuptools
  12.0.

18.8
----

* Deprecated ``egg_info.get_pkg_info_revision``.
* Issue #471: Don't rely on repr for an HTML attribute value in
  package_index.
* Issue #419: Avoid errors in FileMetadata when the metadata directory
  is broken.
* Issue #472: Remove deprecated use of 'U' in mode parameter
  when opening files.

18.7.1
------

* Issue #469: Refactored logic for Issue #419 fix to re-use metadata
  loading from Provider.

18.7
----

* Update dependency on certify.
* BB Pull Request #160: Improve detection of gui script in
  ``easy_install._adjust_header``.
* Made ``test.test_args`` a non-data property; alternate fix
  for the issue reported in BB Pull Request #155.
* Issue #453: In ``ez_setup`` bootstrap module, unload all
  ``pkg_resources`` modules following download.
* BB Pull Request #158: Honor PEP-488 when excluding
  files for namespace packages.
* Issue #419 and BB Pull Request #144: Add experimental support for
  reading the version info from distutils-installed metadata rather
  than using the version in the filename.

18.6.1
------

* Issue #464: Correct regression in invocation of superclass on old-style
  class on Python 2.

18.6
----

* Issue #439: When installing entry_point scripts under development,
  omit the version number of the package, allowing any version of the
  package to be used.

18.5
----

* In preparation for dropping support for Python 3.2, a warning is
  now logged when pkg_resources is imported on Python 3.2 or earlier
  Python 3 versions.
* `Add support for python_platform_implementation environment marker
  <https://github.com/pypa/setuptools/commit/94416707fd59a65f4a8f7f70541d6b3fc018b626>`_.
* `Fix dictionary mutation during iteration
  <https://github.com/pypa/setuptools/commit/57ebfa41e0f96b97e599ecd931b7ae8a143e096e>`_.

18.4
----

* Issue #446: Test command now always invokes unittest, even
  if no test suite is supplied.

18.3.2
------

* Correct another regression in setuptools.findall
  where the fix for Python #12885 was lost.

18.3.1
------

* Issue #425: Correct regression in setuptools.findall.

18.3
----

* BB Pull Request #135: Setuptools now allows disabling of
  the manipulation of the sys.path
  during the processing of the easy-install.pth file. To do so, set
  the environment variable ``SETUPTOOLS_SYS_PATH_TECHNIQUE`` to
  anything but "rewrite" (consider "raw"). During any install operation
  with manipulation disabled, setuptools packages will be appended to
  sys.path naturally.

  Future versions may change the default behavior to disable
  manipulation. If so, the default behavior can be retained by setting
  the variable to "rewrite".

* Issue #257: ``easy_install --version`` now shows more detail
  about the installation location and Python version.

* Refactor setuptools.findall in preparation for re-submission
  back to distutils.

18.2
----

* Issue #412: More efficient directory search in ``find_packages``.

18.1
----

* Upgrade to vendored packaging 15.3.

18.0.1
------

* Issue #401: Fix failure in test suite.

18.0
----

* Dropped support for builds with Pyrex. Only Cython is supported.
* Issue #288: Detect Cython later in the build process, after
  ``setup_requires`` dependencies are resolved.
  Projects backed by Cython can now be readily built
  with a ``setup_requires`` dependency. For example::

    ext = setuptools.Extension('mylib', ['src/CythonStuff.pyx', 'src/CStuff.c'])
    setuptools.setup(
        ...
        ext_modules=[ext],
        setup_requires=['cython'],
    )

  For compatibility with older versions of setuptools, packagers should
  still include ``src/CythonMod.c`` in the source distributions or
  require that Cython be present before building source distributions.
  However, for systems with this build of setuptools, Cython will be
  downloaded on demand.
* Issue #396: Fixed test failure on OS X.
* BB Pull Request #136: Remove excessive quoting from shebang headers
  for Jython.

17.1.1
------

* Backed out unintended changes to pkg_resources, restoring removal of
  deprecated imp module (`ref
  <https://bitbucket.org/pypa/setuptools/commits/f572ec9563d647fa8d4ffc534f2af8070ea07a8b#comment-1881283>`_).

17.1
----

* Issue #380: Add support for range operators on environment
  marker evaluation.

17.0
----

* Issue #378: Do not use internal importlib._bootstrap module.
* Issue #390: Disallow console scripts with path separators in
  the name. Removes unintended functionality and brings behavior
  into parity with pip.

16.0
----

* BB Pull Request #130: Better error messages for errors in
  parsed requirements.
* BB Pull Request #133: Removed ``setuptools.tests`` from the
  installed packages.
* BB Pull Request #129: Address deprecation warning due to usage
  of imp module.

15.2
----

* Issue #373: Provisionally expose
  ``pkg_resources._initialize_master_working_set``, allowing for
  imperative re-initialization of the master working set.

15.1
----

* Updated to Packaging 15.1 to address Packaging #28.
* Fix ``setuptools.sandbox._execfile()`` with Python 3.1.

15.0
----

* BB Pull Request #126: DistributionNotFound message now lists the package or
  packages that required it. E.g.::

      pkg_resources.DistributionNotFound: The 'colorama>=0.3.1' distribution was not found and is required by smlib.log.

  Note that zc.buildout once dependended on the string rendering of this
  message to determine the package that was not found. This expectation
  has since been changed, but older versions of buildout may experience
  problems. See Buildout #242 for details.

14.3.1
------

* Issue #307: Removed PEP-440 warning during parsing of versions
  in ``pkg_resources.Distribution``.
* Issue #364: Replace deprecated usage with recommended usage of
  ``EntryPoint.load``.

14.3
----

* Issue #254: When creating temporary egg cache on Unix, use mode 755
  for creating the directory to avoid the subsequent warning if
  the directory is group writable.

14.2
----

* Issue #137: Update ``Distribution.hashcmp`` so that Distributions with
  None for pyversion or platform can be compared against Distributions
  defining those attributes.

14.1.1
------

* Issue #360: Removed undesirable behavior from test runs, preventing
  write tests and installation to system site packages.

14.1
----

* BB Pull Request #125: Add ``__ne__`` to Requirement class.
* Various refactoring of easy_install.

14.0
----

* Bootstrap script now accepts ``--to-dir`` to customize save directory or
  allow for re-use of existing repository of setuptools versions. See
  BB Pull Request #112 for background.
* Issue #285: ``easy_install`` no longer will default to installing
  packages to the "user site packages" directory if it is itself installed
  there. Instead, the user must pass ``--user`` in all cases to install
  packages to the user site packages.
  This behavior now matches that of "pip install". To configure
  an environment to always install to the user site packages, consider
  using the "install-dir" and "scripts-dir" parameters to easy_install
  through an appropriate distutils config file.

13.0.2
------

* Issue #359: Include pytest.ini in the sdist so invocation of py.test on the
  sdist honors the pytest configuration.

13.0.1
------

Re-release of 13.0. Intermittent connectivity issues caused the release
process to fail and PyPI uploads no longer accept files for 13.0.

13.0
----

* Issue #356: Back out BB Pull Request #119 as it requires Setuptools 10 or later
  as the source during an upgrade.
* Removed build_py class from setup.py. According to 892f439d216e, this
  functionality was added to support upgrades from old Distribute versions,
  0.6.5 and 0.6.6.

12.4
----

* BB Pull Request #119: Restore writing of ``setup_requires`` to metadata
  (previously added in 8.4 and removed in 9.0).

12.3
----

* Documentation is now linked using the rst.linker package.
* Fix ``setuptools.command.easy_install.extract_wininst_cfg()``
  with Python 2.6 and 2.7.
* Issue #354. Added documentation on building setuptools
  documentation.

12.2
----

* Issue #345: Unload all modules under pkg_resources during
  ``ez_setup.use_setuptools()``.
* Issue #336: Removed deprecation from ``ez_setup.use_setuptools``,
  as it is clearly still used by buildout's bootstrap. ``ez_setup``
  remains deprecated for use by individual packages.
* Simplified implementation of ``ez_setup.use_setuptools``.

12.1
----

* BB Pull Request #118: Soften warning for non-normalized versions in
  Distribution.

12.0.5
------

* Issue #339: Correct Attribute reference in ``cant_write_to_target``.
* Issue #336: Deprecated ``ez_setup.use_setuptools``.

12.0.4
------

* Issue #335: Fix script header generation on Windows.

12.0.3
------

* Fixed incorrect class attribute in ``install_scripts``. Tests would be nice.

12.0.2
------

* Issue #331: Fixed ``install_scripts`` command on Windows systems corrupting
  the header.

12.0.1
------

* Restore ``setuptools.command.easy_install.sys_executable`` for pbr
  compatibility. For the future, tools should construct a CommandSpec
  explicitly.

12.0
----

* Issue #188: Setuptools now support multiple entities in the value for
  ``build.executable``, such that an executable of "/usr/bin/env my-python" may
  be specified. This means that systems with a specified executable whose name
  has spaces in the path must be updated to escape or quote that value.
* Deprecated ``easy_install.ScriptWriter.get_writer``, replaced by ``.best()``
  with slightly different semantics (no force_windows flag).

11.3.1
------

* Issue #327: Formalize and restore support for any printable character in an
  entry point name.

11.3
----

* Expose ``EntryPoint.resolve`` in place of EntryPoint._load, implementing the
  simple, non-requiring load. Deprecated all uses of ``EntryPoint._load``
  except for calling with no parameters, which is just a shortcut for
  ``ep.require(); ep.resolve();``.

  Apps currently invoking ``ep.load(require=False)`` should instead do the
  following if wanting to avoid the deprecating warning::

    getattr(ep, "resolve", lambda: ep.load(require=False))()

11.2
----

* Pip #2326: Report deprecation warning at stacklevel 2 for easier diagnosis.

11.1
----

* Issue #281: Since Setuptools 6.1 (Issue #268), a ValueError would be raised
  in certain cases where VersionConflict was raised with two arguments, which
  occurred in ``pkg_resources.WorkingSet.find``. This release adds support
  for indicating the dependent packages while maintaining support for
  a VersionConflict when no dependent package context is known. New unit tests
  now capture the expected interface.

11.0
----

* Interop #3: Upgrade to Packaging 15.0; updates to PEP 440 so that >1.7 does
  not exclude 1.7.1 but does exclude 1.7.0 and 1.7.0.post1.

10.2.1
------

* Issue #323: Fix regression in entry point name parsing.

10.2
----

* Deprecated use of EntryPoint.load(require=False). Passing a boolean to a
  function to select behavior is an anti-pattern. Instead use
  ``Entrypoint._load()``.
* Substantial refactoring of all unit tests. Tests are now much leaner and
  re-use a lot of fixtures and contexts for better clarity of purpose.

10.1
----

* Issue #320: Added a compatibility implementation of
  ``sdist._default_revctrl``
  so that systems relying on that interface do not fail (namely, Ubuntu 12.04
  and similar Debian releases).

10.0.1
------

* Issue #319: Fixed issue installing pure distutils packages.

10.0
----

* Issue #313: Removed built-in support for subversion. Projects wishing to
  retain support for subversion will need to use a third party library. The
  extant implementation is being ported to `setuptools_svn
  <https://pypi.org/project/setuptools_svn/>`_.
* Issue #315: Updated setuptools to hide its own loaded modules during
  installation of another package. This change will enable setuptools to
  upgrade (or downgrade) itself even when its own metadata and implementation
  change.

9.1
---

* Prefer vendored packaging library `as recommended
  <https://github.com/pypa/setuptools/commit/170657b68f4b92e7e1bf82f5e19a831f5744af67>`_.

9.0.1
-----

* Issue #312: Restored presence of pkg_resources API tests (doctest) to sdist.

9.0
---

* Issue #314: Disabled support for ``setup_requires`` metadata to avoid issue
  where Setuptools was unable to upgrade over earlier versions.

8.4
---

* BB Pull Request #106: Now write ``setup_requires`` metadata.

8.3
---

* Issue #311: Decoupled pkg_resources from setuptools once again.
  ``pkg_resources`` is now a package instead of a module.

8.2.1
-----

* Issue #306: Suppress warnings about Version format except in select scenarios
  (such as installation).

8.2
---

* BB Pull Request #85: Search egg-base when adding egg-info to manifest.

8.1
---

* Upgrade ``packaging`` to 14.5, giving preference to "rc" as designator for
  release candidates over "c".
* PEP-440 warnings are now raised as their own class,
  ``pkg_resources.PEP440Warning``, instead of RuntimeWarning.
* Disabled warnings on empty versions.

8.0.4
-----

* Upgrade ``packaging`` to 14.4, fixing an error where there is a
  different result for if 2.0.5 is contained within >2.0dev and >2.0.dev even
  though normalization rules should have made them equal.
* Issue #296: Add warning when a version is parsed as legacy. This warning will
  make it easier for developers to recognize deprecated version numbers.

8.0.3
-----

* Issue #296: Restored support for ``__hash__`` on parse_version results.

8.0.2
-----

* Issue #296: Restored support for ``__getitem__`` and sort operations on
  parse_version result.

8.0.1
-----

* Issue #296: Restore support for iteration over parse_version result, but
  deprecated that usage with a warning. Fixes failure with buildout.

8.0
---

* Implement PEP 440 within
  pkg_resources and setuptools. This change
  deprecates some version numbers such that they will no longer be installable
  without using the ``===`` escape hatch. See `the changes to test_resources
  <https://bitbucket.org/pypa/setuptools/commits/dcd552da643c4448056de84c73d56da6d70769d5#chg-setuptools/tests/test_resources.py>`_
  for specific examples of version numbers and specifiers that are no longer
  supported. Setuptools now "vendors" the `packaging
  <https://github.com/pypa/packaging>`_ library.

7.0
---

* Issue #80, Issue #209: Eggs that are downloaded for ``setup_requires``,
  ``test_requires``, etc. are now placed in a ``./.eggs`` directory instead of
  directly in the current directory. This choice of location means the files
  can be readily managed (removed, ignored). Additionally,
  later phases or invocations of setuptools will not detect the package as
  already installed and ignore it for permanent install (See #209).

  This change is indicated as backward-incompatible as installations that
  depend on the installation in the current directory will need to account for
  the new location. Systems that ignore ``*.egg`` will probably need to be
  adapted to ignore ``.eggs``. The files will need to be manually moved or
  will be retrieved again. Most use cases will require no attention.

6.1
---

* Issue #268: When resolving package versions, a VersionConflict now reports
  which package previously required the conflicting version.

6.0.2
-----

* Issue #262: Fixed regression in pip install due to egg-info directories
  being omitted. Re-opens Issue #118.

6.0.1
-----

* Issue #259: Fixed regression with namespace package handling on ``single
  version, externally managed`` installs.

6.0
---

* Issue #100: When building a distribution, Setuptools will no longer match
  default files using platform-dependent case sensitivity, but rather will
  only match the files if their case matches exactly. As a result, on Windows
  and other case-insensitive file systems, files with names such as
  'readme.txt' or 'README.TXT' will be omitted from the distribution and a
  warning will be issued indicating that 'README.txt' was not found. Other
  filenames affected are:

    - README.rst
    - README
    - setup.cfg
    - setup.py (or the script name)
    - test/test*.py

  Any users producing distributions with filenames that match those above
  case-insensitively, but not case-sensitively, should rename those files in
  their repository for better portability.
* BB Pull Request #72: When using ``single_version_externally_managed``, the
  exclusion list now includes Python 3.2 ``__pycache__`` entries.
* BB Pull Request #76 and BB Pull Request #78: lines in top_level.txt are now
  ordered deterministically.
* Issue #118: The egg-info directory is now no longer included in the list
  of outputs.
* Issue #258: Setuptools now patches distutils msvc9compiler to
  recognize the specially-packaged compiler package for easy extension module
  support on Python 2.6, 2.7, and 3.2.

5.8
---

* Issue #237: ``pkg_resources`` now uses explicit detection of Python 2 vs.
  Python 3, supporting environments where builtins have been patched to make
  Python 3 look more like Python 2.

5.7
---

* Issue #240: Based on real-world performance measures against 5.4, zip
  manifests are now cached in all circumstances. The
  ``PKG_RESOURCES_CACHE_ZIP_MANIFESTS`` environment variable is no longer
  relevant. The observed "memory increase" referenced in the 5.4 release
  notes and detailed in Issue #154 was likely not an increase over the status
  quo, but rather only an increase over not storing the zip info at all.

5.6
---

* Issue #242: Use absolute imports in svn_utils to avoid issues if the
  installing package adds an xml module to the path.

5.5.1
-----

* Issue #239: Fix typo in 5.5 such that fix did not take.

5.5
---

* Issue #239: Setuptools now includes the setup_requires directive on
  Distribution objects and validates the syntax just like install_requires
  and tests_require directives.

5.4.2
-----

* Issue #236: Corrected regression in execfile implementation for Python 2.6.

5.4.1
-----

* Python #7776: (ssl_support) Correct usage of host for validation when
  tunneling for HTTPS.

5.4
---

* Issue #154: ``pkg_resources`` will now cache the zip manifests rather than
  re-processing the same file from disk multiple times, but only if the
  environment variable ``PKG_RESOURCES_CACHE_ZIP_MANIFESTS`` is set. Clients
  that package many modules in the same zip file will see some improvement
  in startup time by enabling this feature. This feature is not enabled by
  default because it causes a substantial increase in memory usage.

5.3
---

* Issue #185: Make svn tagging work on the new style SVN metadata.
  Thanks cazabon!
* Prune revision control directories (e.g .svn) from base path
  as well as sub-directories.

5.2
---

* Added a `Developer Guide
  <https://setuptools.readthedocs.io/en/latest/developer-guide.html>`_ to the official
  documentation.
* Some code refactoring and cleanup was done with no intended behavioral
  changes.
* During install_egg_info, the generated lines for namespace package .pth
  files are now processed even during a dry run.

5.1
---

* Issue #202: Implemented more robust cache invalidation for the ZipImporter,
  building on the work in Issue #168. Special thanks to Jurko Gospodnetic and
  PJE.

5.0.2
-----

* Issue #220: Restored script templates.

5.0.1
-----

* Renamed script templates to end with .tmpl now that they no longer need
  to be processed by 2to3. Fixes spurious syntax errors during build/install.

5.0
---

* Issue #218: Re-release of 3.8.1 to signal that it supersedes 4.x.
* Incidentally, script templates were updated not to include the triple-quote
  escaping.

3.7.1 and 3.8.1 and 4.0.1
-------------------------

* Issue #213: Use legacy StringIO behavior for compatibility under pbr.
* Issue #218: Setuptools 3.8.1 superseded 4.0.1, and 4.x was removed
  from the available versions to install.

4.0
---

* Issue #210: ``setup.py develop`` now copies scripts in binary mode rather
  than text mode, matching the behavior of the ``install`` command.

3.8
---

* Extend Issue #197 workaround to include all Python 3 versions prior to
  3.2.2.

3.7
---

* Issue #193: Improved handling of Unicode filenames when building manifests.

3.6
---

* Issue #203: Honor proxy settings for Powershell downloader in the bootstrap
  routine.

3.5.2
-----

* Issue #168: More robust handling of replaced zip files and stale caches.
  Fixes ZipImportError complaining about a 'bad local header'.

3.5.1
-----

* Issue #199: Restored ``install._install`` for compatibility with earlier
  NumPy versions.

3.5
---

* Issue #195: Follow symbolic links in find_packages (restoring behavior
  broken in 3.4).
* Issue #197: On Python 3.1, PKG-INFO is now saved in a UTF-8 encoding instead
  of ``sys.getpreferredencoding`` to match the behavior on Python 2.6-3.4.
* Issue #192: Preferred bootstrap location is now
  https://bootstrap.pypa.io/ez_setup.py (mirrored from former location).

3.4.4
-----

* Issue #184: Correct failure where find_package over-matched packages
  when directory traversal isn't short-circuited.

3.4.3
-----

* Issue #183: Really fix test command with Python 3.1.

3.4.2
-----

* Issue #183: Fix additional regression in test command on Python 3.1.

3.4.1
-----

* Issue #180: Fix regression in test command not caught by py.test-run tests.

3.4
---

* Issue #176: Add parameter to the test command to support a custom test
  runner: --test-runner or -r.
* Issue #177: Now assume most common invocation to install command on
  platforms/environments without stack support (issuing a warning). Setuptools
  now installs naturally on IronPython. Behavior on CPython should be
  unchanged.

3.3
---

* Add ``include`` parameter to ``setuptools.find_packages()``.

3.2
---

* BB Pull Request #39: Add support for C++ targets from Cython ``.pyx`` files.
* Issue #162: Update dependency on certifi to 1.0.1.
* Issue #164: Update dependency on wincertstore to 0.2.

3.1
---

* Issue #161: Restore Features functionality to allow backward compatibility
  (for Features) until the uses of that functionality is sufficiently removed.

3.0.2
-----

* Correct typo in previous bugfix.

3.0.1
-----

* Issue #157: Restore support for Python 2.6 in bootstrap script where
  ``zipfile.ZipFile`` does not yet have support for context managers.

3.0
---

* Issue #125: Prevent Subversion support from creating a ~/.subversion
  directory just for checking the presence of a Subversion repository.
* Issue #12: Namespace packages are now imported lazily. That is, the mere
  declaration of a namespace package in an egg on ``sys.path`` no longer
  causes it to be imported when ``pkg_resources`` is imported. Note that this
  change means that all of a namespace package's ``__init__.py`` files must
  include a ``declare_namespace()`` call in order to ensure that they will be
  handled properly at runtime. In 2.x it was possible to get away without
  including the declaration, but only at the cost of forcing namespace
  packages to be imported early, which 3.0 no longer does.
* Issue #148: When building (bdist_egg), setuptools no longer adds
  ``__init__.py`` files to namespace packages. Any packages that rely on this
  behavior will need to create ``__init__.py`` files and include the
  ``declare_namespace()``.
* Issue #7: Setuptools itself is now distributed as a zip archive in addition to
  tar archive. ez_setup.py now uses zip archive. This approach avoids the potential
  security vulnerabilities presented by use of tar archives in ez_setup.py.
  It also leverages the security features added to ZipFile.extract in Python 2.7.4.
* Issue #65: Removed deprecated Features functionality.
* BB Pull Request #28: Remove backport of ``_bytecode_filenames`` which is
  available in Python 2.6 and later, but also has better compatibility with
  Python 3 environments.
* Issue #156: Fix spelling of __PYVENV_LAUNCHER__ variable.

2.2
---

* Issue #141: Restored fix for allowing setup_requires dependencies to
  override installed dependencies during setup.
* Issue #128: Fixed issue where only the first dependency link was honored
  in a distribution where multiple dependency links were supplied.

2.1.2
-----

* Issue #144: Read long_description using codecs module to avoid errors
  installing on systems where LANG=C.

2.1.1
-----

* Issue #139: Fix regression in re_finder for CVS repos (and maybe Git repos
  as well).

2.1
---

* Issue #129: Suppress inspection of ``*.whl`` files when searching for files
  in a zip-imported file.
* Issue #131: Fix RuntimeError when constructing an egg fetcher.

2.0.2
-----

* Fix NameError during installation with Python implementations (e.g. Jython)
  not containing parser module.
* Fix NameError in ``sdist:re_finder``.

2.0.1
-----

* Issue #124: Fixed error in list detection in upload_docs.

2.0
---

* Issue #121: Exempt lib2to3 pickled grammars from DirectorySandbox.
* Issue #41: Dropped support for Python 2.4 and Python 2.5. Clients requiring
  setuptools for those versions of Python should use setuptools 1.x.
* Removed ``setuptools.command.easy_install.HAS_USER_SITE``. Clients
  expecting this boolean variable should use ``site.ENABLE_USER_SITE``
  instead.
* Removed ``pkg_resources.ImpWrapper``. Clients that expected this class
  should use ``pkgutil.ImpImporter`` instead.

1.4.2
-----

* Issue #116: Correct TypeError when reading a local package index on Python
  3.

1.4.1
-----

* Issue #114: Use ``sys.getfilesystemencoding`` for decoding config in
  ``bdist_wininst`` distributions.

* Issue #105 and Issue #113: Establish a more robust technique for
  determining the terminal encoding::

    1. Try ``getpreferredencoding``
    2. If that returns US_ASCII or None, try the encoding from
       ``getdefaultlocale``. If that encoding was a "fallback" because Python
       could not figure it out from the environment or OS, encoding remains
       unresolved.
    3. If the encoding is resolved, then make sure Python actually implements
       the encoding.
    4. On the event of an error or unknown codec, revert to fallbacks
       (UTF-8 on Darwin, ASCII on everything else).
    5. On the encoding is 'mac-roman' on Darwin, use UTF-8 as 'mac-roman' was
       a bug on older Python releases.

    On a side note, it would seem that the encoding only matters for when SVN
    does not yet support ``--xml`` and when getting repository and svn version
    numbers. The ``--xml`` technique should yield UTF-8 according to some
    messages on the SVN mailing lists. So if the version numbers are always
    7-bit ASCII clean, it may be best to only support the file parsing methods
    for legacy SVN releases and support for SVN without the subprocess command
    would simple go away as support for the older SVNs does.

1.4
---

* Issue #27: ``easy_install`` will now use credentials from .pypirc if
  present for connecting to the package index.
* BB Pull Request #21: Omit unwanted newlines in ``package_index._encode_auth``
  when the username/password pair length indicates wrapping.

1.3.2
-----

* Issue #99: Fix filename encoding issues in SVN support.

1.3.1
-----

* Remove exuberant warning in SVN support when SVN is not used.

1.3
---

* Address security vulnerability in SSL match_hostname check as reported in
  Python #17997.
* Prefer `backports.ssl_match_hostname
  <https://pypi.org/project/backports.ssl_match_hostname/>`_ for backport
  implementation if present.
* Correct NameError in ``ssl_support`` module (``socket.error``).

1.2
---

* Issue #26: Add support for SVN 1.7. Special thanks to Philip Thiem for the
  contribution.
* Issue #93: Wheels are now distributed with every release. Note that as
  reported in Issue #108, as of Pip 1.4, scripts aren't installed properly
  from wheels. Therefore, if using Pip to install setuptools from a wheel,
  the ``easy_install`` command will not be available.
* Setuptools "natural" launcher support, introduced in 1.0, is now officially
  supported.

1.1.7
-----

* Fixed behavior of NameError handling in 'script template (dev).py' (script
  launcher for 'develop' installs).
* ``ez_setup.py`` now ensures partial downloads are cleaned up following
  a failed download.
* Distribute #363 and Issue #55: Skip an sdist test that fails on locales
  other than UTF-8.

1.1.6
-----

* Distribute #349: ``sandbox.execfile`` now opens the target file in binary
  mode, thus honoring a BOM in the file when compiled.

1.1.5
-----

* Issue #69: Second attempt at fix (logic was reversed).

1.1.4
-----

* Issue #77: Fix error in upload command (Python 2.4).

1.1.3
-----

* Fix NameError in previous patch.

1.1.2
-----

* Issue #69: Correct issue where 404 errors are returned for URLs with
  fragments in them (such as #egg=).

1.1.1
-----

* Issue #75: Add ``--insecure`` option to ez_setup.py to accommodate
  environments where a trusted SSL connection cannot be validated.
* Issue #76: Fix AttributeError in upload command with Python 2.4.

1.1
---

* Issue #71 (Distribute #333): EasyInstall now puts less emphasis on the
  condition when a host is blocked via ``--allow-hosts``.
* Issue #72: Restored Python 2.4 compatibility in ``ez_setup.py``.

1.0
---

* Issue #60: On Windows, Setuptools supports deferring to another launcher,
  such as Vinay Sajip's `pylauncher <https://bitbucket.org/pypa/pylauncher>`_
  (included with Python 3.3) to launch console and GUI scripts and not install
  its own launcher executables. This experimental functionality is currently
  only enabled if  the ``SETUPTOOLS_LAUNCHER`` environment variable is set to
  "natural". In the future, this behavior may become default, but only after
  it has matured and seen substantial adoption. The ``SETUPTOOLS_LAUNCHER``
  also accepts "executable" to force the default behavior of creating launcher
  executables.
* Issue #63: Bootstrap script (ez_setup.py) now prefers Powershell, curl, or
  wget for retrieving the Setuptools tarball for improved security of the
  install. The script will still fall back to a simple ``urlopen`` on
  platforms that do not have these tools.
* Issue #65: Deprecated the ``Features`` functionality.
* Issue #52: In ``VerifyingHTTPSConn``, handle a tunnelled (proxied)
  connection.

Backward-Incompatible Changes
=============================

This release includes a couple of backward-incompatible changes, but most if
not all users will find 1.0 a drop-in replacement for 0.9.

* Issue #50: Normalized API of environment marker support. Specifically,
  removed line number and filename from SyntaxErrors when returned from
  ``pkg_resources.invalid_marker``. Any clients depending on the specific
  string representation of exceptions returned by that function may need to
  be updated to account for this change.
* Issue #50: SyntaxErrors generated by ``pkg_resources.invalid_marker`` are
  normalized for cross-implementation consistency.
* Removed ``--ignore-conflicts-at-my-risk`` and ``--delete-conflicting``
  options to easy_install. These options have been deprecated since 0.6a11.

0.9.8
-----

* Issue #53: Fix NameErrors in ``_vcs_split_rev_from_url``.

0.9.7
-----

* Issue #49: Correct AttributeError on PyPy where a hashlib.HASH object does
  not have a ``.name`` attribute.
* Issue #34: Documentation now refers to bootstrap script in code repository
  referenced by bookmark.
* Add underscore-separated keys to environment markers (markerlib).

0.9.6
-----

* Issue #44: Test failure on Python 2.4 when MD5 hash doesn't have a ``.name``
  attribute.

0.9.5
-----

* Python #17980: Fix security vulnerability in SSL certificate validation.

0.9.4
-----

* Issue #43: Fix issue (introduced in 0.9.1) with version resolution when
  upgrading over other releases of Setuptools.

0.9.3
-----

* Issue #42: Fix new ``AttributeError`` introduced in last fix.

0.9.2
-----

* Issue #42: Fix regression where blank checksums would trigger an
  ``AttributeError``.

0.9.1
-----

* Distribute #386: Allow other positional and keyword arguments to os.open.
* Corrected dependency on certifi mis-referenced in 0.9.

0.9
---

* ``package_index`` now validates hashes other than MD5 in download links.

0.8
---

* Code base now runs on Python 2.4 - Python 3.3 without Python 2to3
  conversion.

0.7.8
-----

* Distribute #375: Yet another fix for yet another regression.

0.7.7
-----

* Distribute #375: Repair AttributeError created in last release (redo).
* Issue #30: Added test for get_cache_path.

0.7.6
-----

* Distribute #375: Repair AttributeError created in last release.

0.7.5
-----

* Issue #21: Restore Python 2.4 compatibility in ``test_easy_install``.
* Distribute #375: Merged additional warning from Distribute 0.6.46.
* Now honor the environment variable
  ``SETUPTOOLS_DISABLE_VERSIONED_EASY_INSTALL_SCRIPT`` in addition to the now
  deprecated ``DISTRIBUTE_DISABLE_VERSIONED_EASY_INSTALL_SCRIPT``.

0.7.4
-----

* Issue #20: Fix comparison of parsed SVN version on Python 3.

0.7.3
-----

* Issue #1: Disable installation of Windows-specific files on non-Windows systems.
* Use new sysconfig module with Python 2.7 or >=3.2.

0.7.2
-----

* Issue #14: Use markerlib when the ``parser`` module is not available.
* Issue #10: ``ez_setup.py`` now uses HTTPS to download setuptools from PyPI.

0.7.1
-----

* Fix NameError (Issue #3) again - broken in bad merge.

0.7
---

* Merged Setuptools and Distribute. See docs/merge.txt for details.

Added several features that were slated for setuptools 0.6c12:

* Index URL now defaults to HTTPS.
* Added experimental environment marker support. Now clients may designate a
  PEP-426 environment marker for "extra" dependencies. Setuptools uses this
  feature in ``setup.py`` for optional SSL and certificate validation support
  on older platforms. Based on Distutils-SIG discussions, the syntax is
  somewhat tentative. There should probably be a PEP with a firmer spec before
  the feature should be considered suitable for use.
* Added support for SSL certificate validation when installing packages from
  an HTTPS service.

0.7b4
-----

* Issue #3: Fixed NameError in SSL support.

0.6.49
------

* Move warning check in ``get_cache_path`` to follow the directory creation
  to avoid errors when the cache path does not yet exist. Fixes the error
  reported in Distribute #375.

0.6.48
------

* Correct AttributeError in ``ResourceManager.get_cache_path`` introduced in
  0.6.46 (redo).

0.6.47
------

* Correct AttributeError in ``ResourceManager.get_cache_path`` introduced in
  0.6.46.

0.6.46
------

* Distribute #375: Issue a warning if the PYTHON_EGG_CACHE or otherwise
  customized egg cache location specifies a directory that's group- or
  world-writable.

0.6.45
------

* Distribute #379: ``distribute_setup.py`` now traps VersionConflict as well,
  restoring ability to upgrade from an older setuptools version.

0.6.44
------

* ``distribute_setup.py`` has been updated to allow Setuptools 0.7 to
  satisfy use_setuptools.

0.6.43
------

* Distribute #378: Restore support for Python 2.4 Syntax (regression in 0.6.42).

0.6.42
------

* External links finder no longer yields duplicate links.
* Distribute #337: Moved site.py to setuptools/site-patch.py (graft of very old
  patch from setuptools trunk which inspired PR #31).

0.6.41
------

* Distribute #27: Use public api for loading resources from zip files rather than
  the private method ``_zip_directory_cache``.
* Added a new function ``easy_install.get_win_launcher`` which may be used by
  third-party libraries such as buildout to get a suitable script launcher.

0.6.40
------

* Distribute #376: brought back cli.exe and gui.exe that were deleted in the
  previous release.

0.6.39
------

* Add support for console launchers on ARM platforms.
* Fix possible issue in GUI launchers where the subsystem was not supplied to
  the linker.
* Launcher build script now refactored for robustness.
* Distribute #375: Resources extracted from a zip egg to the file system now also
  check the contents of the file against the zip contents during each
  invocation of get_resource_filename.

0.6.38
------

* Distribute #371: The launcher manifest file is now installed properly.

0.6.37
------

* Distribute #143: Launcher scripts, including easy_install itself, are now
  accompanied by a manifest on 32-bit Windows environments to avoid the
  Installer Detection Technology and thus undesirable UAC elevation described
  in `this Microsoft article
  <http://technet.microsoft.com/en-us/library/cc709628%28WS.10%29.aspx>`_.

0.6.36
------

* BB Pull Request #35: In Buildout #64, it was reported that
  under Python 3, installation of distutils scripts could attempt to copy
  the ``__pycache__`` directory as a file, causing an error, apparently only
  under Windows. Easy_install now skips all directories when processing
  metadata scripts.

0.6.35
------


Note this release is backward-incompatible with distribute 0.6.23-0.6.34 in
how it parses version numbers.

* Distribute #278: Restored compatibility with distribute 0.6.22 and setuptools
  0.6. Updated the documentation to match more closely with the version
  parsing as intended in setuptools 0.6.

0.6.34
------

* Distribute #341: 0.6.33 fails to build under Python 2.4.

0.6.33
------

* Fix 2 errors with Jython 2.5.
* Fix 1 failure with Jython 2.5 and 2.7.
* Disable workaround for Jython scripts on Linux systems.
* Distribute #336: ``setup.py`` no longer masks failure exit code when tests fail.
* Fix issue in pkg_resources where try/except around a platform-dependent
  import would trigger hook load failures on Mercurial. See pull request 32
  for details.
* Distribute #341: Fix a ResourceWarning.

0.6.32
------

* Fix test suite with Python 2.6.
* Fix some DeprecationWarnings and ResourceWarnings.
* Distribute #335: Backed out ``setup_requires`` superceding installed requirements
  until regression can be addressed.

0.6.31
------

* Distribute #303: Make sure the manifest only ever contains UTF-8 in Python 3.
* Distribute #329: Properly close files created by tests for compatibility with
  Jython.
* Work around Jython #1980 and Jython #1981.
* Distribute #334: Provide workaround for packages that reference ``sys.__stdout__``
  such as numpy does. This change should address
  `virtualenv #359 <https://github.com/pypa/virtualenv/issues/359>`_ as long
  as the system encoding is UTF-8 or the IO encoding is specified in the
  environment, i.e.::

     PYTHONIOENCODING=utf8 pip install numpy

* Fix for encoding issue when installing from Windows executable on Python 3.
* Distribute #323: Allow ``setup_requires`` requirements to supercede installed
  requirements. Added some new keyword arguments to existing pkg_resources
  methods. Also had to updated how __path__ is handled for namespace packages
  to ensure that when a new egg distribution containing a namespace package is
  placed on sys.path, the entries in __path__ are found in the same order they
  would have been in had that egg been on the path when pkg_resources was
  first imported.

0.6.30
------

* Distribute #328: Clean up temporary directories in distribute_setup.py.
* Fix fatal bug in distribute_setup.py.

0.6.29
------

* BB Pull Request #14: Honor file permissions in zip files.
* Distribute #327: Merged pull request #24 to fix a dependency problem with pip.
* Merged pull request #23 to fix https://github.com/pypa/virtualenv/issues/301.
* If Sphinx is installed, the ``upload_docs`` command now runs ``build_sphinx``
  to produce uploadable documentation.
* Distribute #326: ``upload_docs`` provided mangled auth credentials under Python 3.
* Distribute #320: Fix check for "createable" in distribute_setup.py.
* Distribute #305: Remove a warning that was triggered during normal operations.
* Distribute #311: Print metadata in UTF-8 independent of platform.
* Distribute #303: Read manifest file with UTF-8 encoding under Python 3.
* Distribute #301: Allow to run tests of namespace packages when using 2to3.
* Distribute #304: Prevent import loop in site.py under Python 3.3.
* Distribute #283: Reenable scanning of ``*.pyc`` / ``*.pyo`` files on Python 3.3.
* Distribute #299: The develop command didn't work on Python 3, when using 2to3,
  as the egg link would go to the Python 2 source. Linking to the 2to3'd code
  in build/lib makes it work, although you will have to rebuild the module
  before testing it.
* Distribute #306: Even if 2to3 is used, we build in-place under Python 2.
* Distribute #307: Prints the full path when .svn/entries is broken.
* Distribute #313: Support for sdist subcommands (Python 2.7)
* Distribute #314: test_local_index() would fail an OS X.
* Distribute #310: Non-ascii characters in a namespace __init__.py causes errors.
* Distribute #218: Improved documentation on behavior of ``package_data`` and
  ``include_package_data``. Files indicated by ``package_data`` are now included
  in the manifest.
* ``distribute_setup.py`` now allows a ``--download-base`` argument for retrieving
  distribute from a specified location.

0.6.28
------

* Distribute #294: setup.py can now be invoked from any directory.
* Scripts are now installed honoring the umask.
* Added support for .dist-info directories.
* Distribute #283: Fix and disable scanning of ``*.pyc`` / ``*.pyo`` files on
  Python 3.3.

0.6.27
------

* Support current snapshots of CPython 3.3.
* Distribute now recognizes README.rst as a standard, default readme file.
* Exclude 'encodings' modules when removing modules from sys.modules.
  Workaround for #285.
* Distribute #231: Don't fiddle with system python when used with buildout
  (bootstrap.py)

0.6.26
------

* Distribute #183: Symlinked files are now extracted from source distributions.
* Distribute #227: Easy_install fetch parameters are now passed during the
  installation of a source distribution; now fulfillment of setup_requires
  dependencies will honor the parameters passed to easy_install.

0.6.25
------

* Distribute #258: Workaround a cache issue
* Distribute #260: distribute_setup.py now accepts the --user parameter for
  Python 2.6 and later.
* Distribute #262: package_index.open_with_auth no longer throws LookupError
  on Python 3.
* Distribute #269: AttributeError when an exception occurs reading Manifest.in
  on late releases of Python.
* Distribute #272: Prevent TypeError when namespace package names are unicode
  and single-install-externally-managed is used. Also fixes PIP issue
  449.
* Distribute #273: Legacy script launchers now install with Python2/3 support.

0.6.24
------

* Distribute #249: Added options to exclude 2to3 fixers

0.6.23
------

* Distribute #244: Fixed a test
* Distribute #243: Fixed a test
* Distribute #239: Fixed a test
* Distribute #240: Fixed a test
* Distribute #241: Fixed a test
* Distribute #237: Fixed a test
* Distribute #238: easy_install now uses 64bit executable wrappers on 64bit Python
* Distribute #208: Fixed parsed_versions, it now honors post-releases as noted in the documentation
* Distribute #207: Windows cli and gui wrappers pass CTRL-C to child python process
* Distribute #227: easy_install now passes its arguments to setup.py bdist_egg
* Distribute #225: Fixed a NameError on Python 2.5, 2.4

0.6.21
------

* Distribute #225: FIxed a regression on py2.4

0.6.20
------

* Distribute #135: Include url in warning when processing URLs in package_index.
* Distribute #212: Fix issue where easy_instal fails on Python 3 on windows installer.
* Distribute #213: Fix typo in documentation.

0.6.19
------

* Distribute #206: AttributeError: 'HTTPMessage' object has no attribute 'getheaders'

0.6.18
------

* Distribute #210: Fixed a regression introduced by Distribute #204 fix.

0.6.17
------

* Support 'DISTRIBUTE_DISABLE_VERSIONED_EASY_INSTALL_SCRIPT' environment
  variable to allow to disable installation of easy_install-${version} script.
* Support Python >=3.1.4 and >=3.2.1.
* Distribute #204: Don't try to import the parent of a namespace package in
  declare_namespace
* Distribute #196: Tolerate responses with multiple Content-Length headers
* Distribute #205: Sandboxing doesn't preserve working_set. Leads to setup_requires
  problems.

0.6.16
------

* Builds sdist gztar even on Windows (avoiding Distribute #193).
* Distribute #192: Fixed metadata omitted on Windows when package_dir
  specified with forward-slash.
* Distribute #195: Cython build support.
* Distribute #200: Issues with recognizing 64-bit packages on Windows.

0.6.15
------

* Fixed typo in bdist_egg
* Several issues under Python 3 has been solved.
* Distribute #146: Fixed missing DLL files after easy_install of windows exe package.

0.6.14
------

* Distribute #170: Fixed unittest failure. Thanks to Toshio.
* Distribute #171: Fixed race condition in unittests cause deadlocks in test suite.
* Distribute #143: Fixed a lookup issue with easy_install.
  Thanks to David and Zooko.
* Distribute #174: Fixed the edit mode when its used with setuptools itself

0.6.13
------

* Distribute #160: 2.7 gives ValueError("Invalid IPv6 URL")
* Distribute #150: Fixed using ~/.local even in a --no-site-packages virtualenv
* Distribute #163: scan index links before external links, and don't use the md5 when
  comparing two distributions

0.6.12
------

* Distribute #149: Fixed various failures on 2.3/2.4

0.6.11
------

* Found another case of SandboxViolation - fixed
* Distribute #15 and Distribute #48: Introduced a socket timeout of 15 seconds on url openings
* Added indexsidebar.html into MANIFEST.in
* Distribute #108: Fixed TypeError with Python3.1
* Distribute #121: Fixed --help install command trying to actually install.
* Distribute #112: Added an os.makedirs so that Tarek's solution will work.
* Distribute #133: Added --no-find-links to easy_install
* Added easy_install --user
* Distribute #100: Fixed develop --user not taking '.' in PYTHONPATH into account
* Distribute #134: removed spurious UserWarnings. Patch by VanLindberg
* Distribute #138: cant_write_to_target error when setup_requires is used.
* Distribute #147: respect the sys.dont_write_bytecode flag

0.6.10
------

* Reverted change made for the DistributionNotFound exception because
  zc.buildout uses the exception message to get the name of the
  distribution.

0.6.9
-----

* Distribute #90: unknown setuptools version can be added in the working set
* Distribute #87: setupt.py doesn't try to convert distribute_setup.py anymore
  Initial Patch by arfrever.
* Distribute #89: added a side bar with a download link to the doc.
* Distribute #86: fixed missing sentence in pkg_resources doc.
* Added a nicer error message when a DistributionNotFound is raised.
* Distribute #80: test_develop now works with Python 3.1
* Distribute #93: upload_docs now works if there is an empty sub-directory.
* Distribute #70: exec bit on non-exec files
* Distribute #99: now the standalone easy_install command doesn't uses a
  "setup.cfg" if any exists in the working directory. It will use it
  only if triggered by ``install_requires`` from a setup.py call
  (install, develop, etc).
* Distribute #101: Allowing ``os.devnull`` in Sandbox
* Distribute #92: Fixed the "no eggs" found error with MacPort
  (platform.mac_ver() fails)
* Distribute #103: test_get_script_header_jython_workaround not run
  anymore under py3 with C or POSIX local. Contributed by Arfrever.
* Distribute #104: remvoved the assertion when the installation fails,
  with a nicer message for the end user.
* Distribute #100: making sure there's no SandboxViolation when
  the setup script patches setuptools.

0.6.8
-----

* Added "check_packages" in dist. (added in Setuptools 0.6c11)
* Fixed the DONT_PATCH_SETUPTOOLS state.

0.6.7
-----

* Distribute #58: Added --user support to the develop command
* Distribute #11: Generated scripts now wrap their call to the script entry point
  in the standard "if name == 'main'"
* Added the 'DONT_PATCH_SETUPTOOLS' environment variable, so virtualenv
  can drive an installation that doesn't patch a global setuptools.
* Reviewed unladen-swallow specific change from
  http://code.google.com/p/unladen-swallow/source/detail?spec=svn875&r=719
  and determined that it no longer applies. Distribute should work fine with
  Unladen Swallow 2009Q3.
* Distribute #21: Allow PackageIndex.open_url to gracefully handle all cases of a
  httplib.HTTPException instead of just InvalidURL and BadStatusLine.
* Removed virtual-python.py from this distribution and updated documentation
  to point to the actively maintained virtualenv instead.
* Distribute #64: use_setuptools no longer rebuilds the distribute egg every
  time it is run
* use_setuptools now properly respects the requested version
* use_setuptools will no longer try to import a distribute egg for the
  wrong Python version
* Distribute #74: no_fake should be True by default.
* Distribute #72: avoid a bootstrapping issue with easy_install -U

0.6.6
-----

* Unified the bootstrap file so it works on both py2.x and py3k without 2to3
  (patch by Holger Krekel)

0.6.5
-----

* Distribute #65: cli.exe and gui.exe are now generated at build time,
  depending on the platform in use.

* Distribute #67: Fixed doc typo (PEP 381/PEP 382).

* Distribute no longer shadows setuptools if we require a 0.7-series
  setuptools. And an error is raised when installing a 0.7 setuptools with
  distribute.

* When run from within buildout, no attempt is made to modify an existing
  setuptools egg, whether in a shared egg directory or a system setuptools.

* Fixed a hole in sandboxing allowing builtin file to write outside of
  the sandbox.

0.6.4
-----

* Added the generation of ``distribute_setup_3k.py`` during the release.
  This closes Distribute #52.

* Added an upload_docs command to easily upload project documentation to
  PyPI's https://pythonhosted.org. This close issue Distribute #56.

* Fixed a bootstrap bug on the use_setuptools() API.

0.6.3
-----

setuptools
==========

* Fixed a bunch of calls to file() that caused crashes on Python 3.

bootstrapping
=============

* Fixed a bug in sorting that caused bootstrap to fail on Python 3.

0.6.2
-----

setuptools
==========

* Added Python 3 support; see docs/python3.txt.
  This closes Old Setuptools #39.

* Added option to run 2to3 automatically when installing on Python 3.
  This closes issue Distribute #31.

* Fixed invalid usage of requirement.parse, that broke develop -d.
  This closes Old Setuptools #44.

* Fixed script launcher for 64-bit Windows.
  This closes Old Setuptools #2.

* KeyError when compiling extensions.
  This closes Old Setuptools #41.

bootstrapping
=============

* Fixed bootstrap not working on Windows. This closes issue Distribute #49.

* Fixed 2.6 dependencies. This closes issue Distribute #50.

* Make sure setuptools is patched when running through easy_install
  This closes Old Setuptools #40.

0.6.1
-----

setuptools
==========

* package_index.urlopen now catches BadStatusLine and malformed url errors.
  This closes Distribute #16 and Distribute #18.

* zip_ok is now False by default. This closes Old Setuptools #33.

* Fixed invalid URL error catching. Old Setuptools #20.

* Fixed invalid bootstraping with easy_install installation (Distribute #40).
  Thanks to Florian Schulze for the help.

* Removed buildout/bootstrap.py. A new repository will create a specific
  bootstrap.py script.


bootstrapping
=============

* The boostrap process leave setuptools alone if detected in the system
  and --root or --prefix is provided, but is not in the same location.
  This closes Distribute #10.

0.6
---

setuptools
==========

* Packages required at build time where not fully present at install time.
  This closes Distribute #12.

* Protected against failures in tarfile extraction. This closes Distribute #10.

* Made Jython api_tests.txt doctest compatible. This closes Distribute #7.

* sandbox.py replaced builtin type file with builtin function open. This
  closes Distribute #6.

* Immediately close all file handles. This closes Distribute #3.

* Added compatibility with Subversion 1.6. This references Distribute #1.

pkg_resources
=============

* Avoid a call to /usr/bin/sw_vers on OSX and use the official platform API
  instead. Based on a patch from ronaldoussoren. This closes issue #5.

* Fixed a SandboxViolation for mkdir that could occur in certain cases.
  This closes Distribute #13.

* Allow to find_on_path on systems with tight permissions to fail gracefully.
  This closes Distribute #9.

* Corrected inconsistency between documentation and code of add_entry.
  This closes Distribute #8.

* Immediately close all file handles. This closes Distribute #3.

easy_install
============

* Immediately close all file handles. This closes Distribute #3.

0.6c9
-----

 * Fixed a missing files problem when using Windows source distributions on
   non-Windows platforms, due to distutils not handling manifest file line
   endings correctly.

 * Updated Pyrex support to work with Pyrex 0.9.6 and higher.

 * Minor changes for Jython compatibility, including skipping tests that can't
   work on Jython.

 * Fixed not installing eggs in ``install_requires`` if they were also used for
   ``setup_requires`` or ``tests_require``.

 * Fixed not fetching eggs in ``install_requires`` when running tests.

 * Allow ``ez_setup.use_setuptools()`` to upgrade existing setuptools
   installations when called from a standalone ``setup.py``.

 * Added a warning if a namespace package is declared, but its parent package
   is not also declared as a namespace.

 * Support Subversion 1.5

 * Removed use of deprecated ``md5`` module if ``hashlib`` is available

 * Fixed ``bdist_wininst upload`` trying to upload the ``.exe`` twice

 * Fixed ``bdist_egg`` putting a ``native_libs.txt`` in the source package's
   ``.egg-info``, when it should only be in the built egg's ``EGG-INFO``.

 * Ensure that _full_name is set on all shared libs before extensions are
   checked for shared lib usage.  (Fixes a bug in the experimental shared
   library build support.)

 * Fix to allow unpacked eggs containing native libraries to fail more
   gracefully under Google App Engine (with an ``ImportError`` loading the
   C-based module, instead of getting a ``NameError``).

 * Fixed ``win32.exe`` support for .pth files, so unnecessary directory nesting
   is flattened out in the resulting egg.  (There was a case-sensitivity
   problem that affected some distributions, notably ``pywin32``.)

 * Prevent ``--help-commands`` and other junk from showing under Python 2.5
   when running ``easy_install --help``.

 * Fixed GUI scripts sometimes not executing on Windows

 * Fixed not picking up dependency links from recursive dependencies.

 * Only make ``.py``, ``.dll`` and ``.so`` files executable when unpacking eggs

 * Changes for Jython compatibility

 * Improved error message when a requirement is also a directory name, but the
   specified directory is not a source package.

 * Fixed ``--allow-hosts`` option blocking ``file:`` URLs

 * Fixed HTTP SVN detection failing when the page title included a project
   name (e.g. on SourceForge-hosted SVN)

 * Fix Jython script installation to handle ``#!`` lines better when
   ``sys.executable`` is a script.

 * Removed use of deprecated ``md5`` module if ``hashlib`` is available

 * Keep site directories (e.g. ``site-packages``) from being included in
   ``.pth`` files.

0.6c7
-----

 * Fixed ``distutils.filelist.findall()`` crashing on broken symlinks, and
   ``egg_info`` command failing on new, uncommitted SVN directories.

 * Fix import problems with nested namespace packages installed via
   ``--root`` or ``--single-version-externally-managed``, due to the
   parent package not having the child package as an attribute.

 * ``ftp:`` download URLs now work correctly.

 * The default ``--index-url`` is now ``https://pypi.python.org/simple``, to use
   the Python Package Index's new simpler (and faster!) REST API.

0.6c6
-----

 * Added ``--egg-path`` option to ``develop`` command, allowing you to force
   ``.egg-link`` files to use relative paths (allowing them to be shared across
   platforms on a networked drive).

 * Fix not building binary RPMs correctly.

 * Fix "eggsecutables" (such as setuptools' own egg) only being runnable with
   bash-compatible shells.

 * Fix ``#!`` parsing problems in Windows ``.exe`` script wrappers, when there
   was whitespace inside a quoted argument or at the end of the ``#!`` line
   (a regression introduced in 0.6c4).

 * Fix ``test`` command possibly failing if an older version of the project
   being tested was installed on ``sys.path`` ahead of the test source
   directory.

 * Fix ``find_packages()`` treating ``ez_setup`` and directories with ``.`` in
   their names as packages.

 * EasyInstall no longer aborts the installation process if a URL it wants to
   retrieve can't be downloaded, unless the URL is an actual package download.
   Instead, it issues a warning and tries to keep going.

 * Fixed distutils-style scripts originally built on Windows having their line
   endings doubled when installed on any platform.

 * Added ``--local-snapshots-ok`` flag, to allow building eggs from projects
   installed using ``setup.py develop``.

 * Fixed not HTML-decoding URLs scraped from web pages

0.6c5
-----

 * Fix uploaded ``bdist_rpm`` packages being described as ``bdist_egg``
   packages under Python versions less than 2.5.

 * Fix uploaded ``bdist_wininst`` packages being described as suitable for
   "any" version by Python 2.5, even if a ``--target-version`` was specified.

 * Fixed ``.dll`` files on Cygwin not having executable permissions when an egg
   is installed unzipped.

0.6c4
-----

 * Overhauled Windows script wrapping to support ``bdist_wininst`` better.
   Scripts installed with ``bdist_wininst`` will always use ``#!python.exe`` or
   ``#!pythonw.exe`` as the executable name (even when built on non-Windows
   platforms!), and the wrappers will look for the executable in the script's
   parent directory (which should find the right version of Python).

 * Fix ``upload`` command not uploading files built by ``bdist_rpm`` or
   ``bdist_wininst`` under Python 2.3 and 2.4.

 * Add support for "eggsecutable" headers: a ``#!/bin/sh`` script that is
   prepended to an ``.egg`` file to allow it to be run as a script on Unix-ish
   platforms.  (This is mainly so that setuptools itself can have a single-file
   installer on Unix, without doing multiple downloads, dealing with firewalls,
   etc.)

 * Fix problem with empty revision numbers in Subversion 1.4 ``entries`` files

 * Use cross-platform relative paths in ``easy-install.pth`` when doing
   ``develop`` and the source directory is a subdirectory of the installation
   target directory.

 * Fix a problem installing eggs with a system packaging tool if the project
   contained an implicit namespace package; for example if the ``setup()``
   listed a namespace package ``foo.bar`` without explicitly listing ``foo``
   as a namespace package.

 * Added support for HTTP "Basic" authentication using ``http://user:pass@host``
   URLs.  If a password-protected page contains links to the same host (and
   protocol), those links will inherit the credentials used to access the
   original page.

 * Removed all special support for Sourceforge mirrors, as Sourceforge's
   mirror system now works well for non-browser downloads.

 * Fixed not recognizing ``win32.exe`` installers that included a custom
   bitmap.

 * Fixed not allowing ``os.open()`` of paths outside the sandbox, even if they
   are opened read-only (e.g. reading ``/dev/urandom`` for random numbers, as
   is done by ``os.urandom()`` on some platforms).

 * Fixed a problem with ``.pth`` testing on Windows when ``sys.executable``
   has a space in it (e.g., the user installed Python to a ``Program Files``
   directory).

0.6c3
-----

 * Fixed breakages caused by Subversion 1.4's new "working copy" format

 * You can once again use "python -m easy_install" with Python 2.4 and above.

 * Python 2.5 compatibility fixes added.

0.6c2
-----

 * The ``ez_setup`` module displays the conflicting version of setuptools (and
   its installation location) when a script requests a version that's not
   available.

 * Running ``setup.py develop`` on a setuptools-using project will now install
   setuptools if needed, instead of only downloading the egg.

 * Windows script wrappers now support quoted arguments and arguments
   containing spaces.  (Patch contributed by Jim Fulton.)

 * The ``ez_setup.py`` script now actually works when you put a setuptools
   ``.egg`` alongside it for bootstrapping an offline machine.

 * A writable installation directory on ``sys.path`` is no longer required to
   download and extract a source distribution using ``--editable``.

 * Generated scripts now use ``-x`` on the ``#!`` line when ``sys.executable``
   contains non-ASCII characters, to prevent deprecation warnings about an
   unspecified encoding when the script is run.

0.6c1
-----

 * Fixed ``AttributeError`` when trying to download a ``setup_requires``
   dependency when a distribution lacks a ``dependency_links`` setting.

 * Made ``zip-safe`` and ``not-zip-safe`` flag files contain a single byte, so
   as to play better with packaging tools that complain about zero-length
   files.

 * Made ``setup.py develop`` respect the ``--no-deps`` option, which it
   previously was ignoring.

 * Support ``extra_path`` option to ``setup()`` when ``install`` is run in
   backward-compatibility mode.

 * Source distributions now always include a ``setup.cfg`` file that explicitly
   sets ``egg_info`` options such that they produce an identical version number
   to the source distribution's version number.  (Previously, the default
   version number could be different due to the use of ``--tag-date``, or if
   the version was overridden on the command line that built the source
   distribution.)

 * EasyInstall now includes setuptools version information in the
   ``User-Agent`` string sent to websites it visits.

0.6b4
-----

 * Fix ``register`` not obeying name/version set by ``egg_info`` command, if
   ``egg_info`` wasn't explicitly run first on the same command line.

 * Added ``--no-date`` and ``--no-svn-revision`` options to ``egg_info``
   command, to allow suppressing tags configured in ``setup.cfg``.

 * Fixed redundant warnings about missing ``README`` file(s); it should now
   appear only if you are actually a source distribution.

 * Fix creating Python wrappers for non-Python scripts

 * Fix ``ftp://`` directory listing URLs from causing a crash when used in the
   "Home page" or "Download URL" slots on PyPI.

 * Fix ``sys.path_importer_cache`` not being updated when an existing zipfile
   or directory is deleted/overwritten.

 * Fix not recognizing HTML 404 pages from package indexes.

 * Allow ``file://`` URLs to be used as a package index.  URLs that refer to
   directories will use an internally-generated directory listing if there is
   no ``index.html`` file in the directory.

 * Allow external links in a package index to be specified using
   ``rel="homepage"`` or ``rel="download"``, without needing the old
   PyPI-specific visible markup.

 * Suppressed warning message about possibly-misspelled project name, if an egg
   or link for that project name has already been seen.

0.6b3
-----

 * Fix ``bdist_egg`` not including files in subdirectories of ``.egg-info``.

 * Allow ``.py`` files found by the ``include_package_data`` option to be
   automatically included. Remove duplicate data file matches if both
   ``include_package_data`` and ``package_data`` are used to refer to the same
   files.

 * Fix local ``--find-links`` eggs not being copied except with
   ``--always-copy``.

 * Fix sometimes not detecting local packages installed outside of "site"
   directories.

 * Fix mysterious errors during initial ``setuptools`` install, caused by
   ``ez_setup`` trying to run ``easy_install`` twice, due to a code fallthru
   after deleting the egg from which it's running.

0.6b2
-----

 * Don't install or update a ``site.py`` patch when installing to a
   ``PYTHONPATH`` directory with ``--multi-version``, unless an
   ``easy-install.pth`` file is already in use there.

 * Construct ``.pth`` file paths in such a way that installing an egg whose
   name begins with ``import`` doesn't cause a syntax error.

 * Fixed a bogus warning message that wasn't updated since the 0.5 versions.

0.6b1
-----

 * Strip ``module`` from the end of compiled extension modules when computing
   the name of a ``.py`` loader/wrapper.  (Python's import machinery ignores
   this suffix when searching for an extension module.)

 * Better ambiguity management: accept ``#egg`` name/version even if processing
   what appears to be a correctly-named distutils file, and ignore ``.egg``
   files with no ``-``, since valid Python ``.egg`` files always have a version
   number (but Scheme eggs often don't).

 * Support ``file://`` links to directories in ``--find-links``, so that
   easy_install can build packages from local source checkouts.

 * Added automatic retry for Sourceforge mirrors.  The new download process is
   to first just try dl.sourceforge.net, then randomly select mirror IPs and
   remove ones that fail, until something works.  The removed IPs stay removed
   for the remainder of the run.

 * Ignore bdist_dumb distributions when looking at download URLs.

0.6a11
------

 * Added ``test_loader`` keyword to support custom test loaders

 * Added ``setuptools.file_finders`` entry point group to allow implementing
   revision control plugins.

 * Added ``--identity`` option to ``upload`` command.

 * Added ``dependency_links`` to allow specifying URLs for ``--find-links``.

 * Enhanced test loader to scan packages as well as modules, and call
   ``additional_tests()`` if present to get non-unittest tests.

 * Support namespace packages in conjunction with system packagers, by omitting
   the installation of any ``__init__.py`` files for namespace packages, and
   adding a special ``.pth`` file to create a working package in
   ``sys.modules``.

 * Made ``--single-version-externally-managed`` automatic when ``--root`` is
   used, so that most system packagers won't require special support for
   setuptools.

 * Fixed ``setup_requires``, ``tests_require``, etc. not using ``setup.cfg`` or
   other configuration files for their option defaults when installing, and
   also made the install use ``--multi-version`` mode so that the project
   directory doesn't need to support .pth files.

 * ``MANIFEST.in`` is now forcibly closed when any errors occur while reading
   it. Previously, the file could be left open and the actual error would be
   masked by problems trying to remove the open file on Windows systems.

 * Process ``dependency_links.txt`` if found in a distribution, by adding the
   URLs to the list for scanning.

 * Use relative paths in ``.pth`` files when eggs are being installed to the
   same directory as the ``.pth`` file.  This maximizes portability of the
   target directory when building applications that contain eggs.

 * Added ``easy_install-N.N`` script(s) for convenience when using multiple
   Python versions.

 * Added automatic handling of installation conflicts.  Eggs are now shifted to
   the front of sys.path, in an order consistent with where they came from,
   making EasyInstall seamlessly co-operate with system package managers.

   The ``--delete-conflicting`` and ``--ignore-conflicts-at-my-risk`` options
   are now no longer necessary, and will generate warnings at the end of a
   run if you use them.

 * Don't recursively traverse subdirectories given to ``--find-links``.

0.6a10
------

 * Fixed the ``develop`` command ignoring ``--find-links``.

 * Added exhaustive testing of the install directory, including a spawn test
   for ``.pth`` file support, and directory writability/existence checks.  This
   should virtually eliminate the need to set or configure ``--site-dirs``.

 * Added ``--prefix`` option for more do-what-I-mean-ishness in the absence of
   RTFM-ing.  :)

 * Enhanced ``PYTHONPATH`` support so that you don't have to put any eggs on it
   manually to make it work.  ``--multi-version`` is no longer a silent
   default; you must explicitly use it if installing to a non-PYTHONPATH,
   non-"site" directory.

 * Expand ``$variables`` used in the ``--site-dirs``, ``--build-directory``,
   ``--install-dir``, and ``--script-dir`` options, whether on the command line
   or in configuration files.

 * Improved SourceForge mirror processing to work faster and be less affected
   by transient HTML changes made by SourceForge.

 * PyPI searches now use the exact spelling of requirements specified on the
   command line or in a project's ``install_requires``.  Previously, a
   normalized form of the name was used, which could lead to unnecessary
   full-index searches when a project's name had an underscore (``_``) in it.

 * EasyInstall can now download bare ``.py`` files and wrap them in an egg,
   as long as you include an ``#egg=name-version`` suffix on the URL, or if
   the ``.py`` file is listed as the "Download URL" on the project's PyPI page.
   This allows third parties to "package" trivial Python modules just by
   linking to them (e.g. from within their own PyPI page or download links
   page).

 * The ``--always-copy`` option now skips "system" and "development" eggs since
   they can't be reliably copied.  Note that this may cause EasyInstall to
   choose an older version of a package than what you expected, or it may cause
   downloading and installation of a fresh version of what's already installed.

 * The ``--find-links`` option previously scanned all supplied URLs and
   directories as early as possible, but now only directories and direct
   archive links are scanned immediately.  URLs are not retrieved unless a
   package search was already going to go online due to a package not being
   available locally, or due to the use of the ``--update`` or ``-U`` option.

 * Fixed the annoying ``--help-commands`` wart.

0.6a9
-----

 * The ``sdist`` command no longer uses the traditional ``MANIFEST`` file to
   create source distributions.  ``MANIFEST.in`` is still read and processed,
   as are the standard defaults and pruning. But the manifest is built inside
   the project's ``.egg-info`` directory as ``SOURCES.txt``, and it is rebuilt
   every time the ``egg_info`` command is run.

 * Added the ``include_package_data`` keyword to ``setup()``, allowing you to
   automatically include any package data listed in revision control or
   ``MANIFEST.in``

 * Added the ``exclude_package_data`` keyword to ``setup()``, allowing you to
   trim back files included via the ``package_data`` and
   ``include_package_data`` options.

 * Fixed ``--tag-svn-revision`` not working when run from a source
   distribution.

 * Added warning for namespace packages with missing ``declare_namespace()``

 * Added ``tests_require`` keyword to ``setup()``, so that e.g. packages
   requiring ``nose`` to run unit tests can make this dependency optional
   unless the ``test`` command is run.

 * Made all commands that use ``easy_install`` respect its configuration
   options, as this was causing some problems with ``setup.py install``.

 * Added an ``unpack_directory()`` driver to ``setuptools.archive_util``, so
   that you can process a directory tree through a processing filter as if it
   were a zipfile or tarfile.

 * Added an internal ``install_egg_info`` command to use as part of old-style
   ``install`` operations, that installs an ``.egg-info`` directory with the
   package.

 * Added a ``--single-version-externally-managed`` option to the ``install``
   command so that you can more easily wrap a "flat" egg in a system package.

 * Enhanced ``bdist_rpm`` so that it installs single-version eggs that
   don't rely on a ``.pth`` file. The ``--no-egg`` option has been removed,
   since all RPMs are now built in a more backwards-compatible format.

 * Support full roundtrip translation of eggs to and from ``bdist_wininst``
   format. Running ``bdist_wininst`` on a setuptools-based package wraps the
   egg in an .exe that will safely install it as an egg (i.e., with metadata
   and entry-point wrapper scripts), and ``easy_install`` can turn the .exe
   back into an ``.egg`` file or directory and install it as such.

 * Fixed ``.pth`` file processing picking up nested eggs (i.e. ones inside
   "baskets") when they weren't explicitly listed in the ``.pth`` file.

 * If more than one URL appears to describe the exact same distribution, prefer
   the shortest one.  This helps to avoid "table of contents" CGI URLs like the
   ones on effbot.org.

 * Quote arguments to python.exe (including python's path) to avoid problems
   when Python (or a script) is installed in a directory whose name contains
   spaces on Windows.

 * Support full roundtrip translation of eggs to and from ``bdist_wininst``
   format.  Running ``bdist_wininst`` on a setuptools-based package wraps the
   egg in an .exe that will safely install it as an egg (i.e., with metadata
   and entry-point wrapper scripts), and ``easy_install`` can turn the .exe
   back into an ``.egg`` file or directory and install it as such.

0.6a8
-----

 * Fixed some problems building extensions when Pyrex was installed, especially
   with Python 2.4 and/or packages using SWIG.

 * Made ``develop`` command accept all the same options as ``easy_install``,
   and use the ``easy_install`` command's configuration settings as defaults.

 * Made ``egg_info --tag-svn-revision`` fall back to extracting the revision
   number from ``PKG-INFO`` in case it is being run on a source distribution of
   a snapshot taken from a Subversion-based project.

 * Automatically detect ``.dll``, ``.so`` and ``.dylib`` files that are being
   installed as data, adding them to ``native_libs.txt`` automatically.

 * Fixed some problems with fresh checkouts of projects that don't include
   ``.egg-info/PKG-INFO`` under revision control and put the project's source
   code directly in the project directory. If such a package had any
   requirements that get processed before the ``egg_info`` command can be run,
   the setup scripts would fail with a "Missing 'Version:' header and/or
   PKG-INFO file" error, because the egg runtime interpreted the unbuilt
   metadata in a directory on ``sys.path`` (i.e. the current directory) as
   being a corrupted egg. Setuptools now monkeypatches the distribution
   metadata cache to pretend that the egg has valid version information, until
   it has a chance to make it actually be so (via the ``egg_info`` command).

 * Update for changed SourceForge mirror format

 * Fixed not installing dependencies for some packages fetched via Subversion

 * Fixed dependency installation with ``--always-copy`` not using the same
   dependency resolution procedure as other operations.

 * Fixed not fully removing temporary directories on Windows, if a Subversion
   checkout left read-only files behind

 * Fixed some problems building extensions when Pyrex was installed, especially
   with Python 2.4 and/or packages using SWIG.

0.6a7
-----

 * Fixed not being able to install Windows script wrappers using Python 2.3

0.6a6
-----

 * Added support for "traditional" PYTHONPATH-based non-root installation, and
   also the convenient ``virtual-python.py`` script, based on a contribution
   by Ian Bicking.  The setuptools egg now contains a hacked ``site`` module
   that makes the PYTHONPATH-based approach work with .pth files, so that you
   can get the full EasyInstall feature set on such installations.

 * Added ``--no-deps`` and ``--allow-hosts`` options.

 * Improved Windows ``.exe`` script wrappers so that the script can have the
   same name as a module without confusing Python.

 * Changed dependency processing so that it's breadth-first, allowing a
   depender's preferences to override those of a dependee, to prevent conflicts
   when a lower version is acceptable to the dependee, but not the depender.
   Also, ensure that currently installed/selected packages aren't given
   precedence over ones desired by a package being installed, which could
   cause conflict errors.

0.6a5
-----

 * Fixed missing gui/cli .exe files in distribution. Fixed bugs in tests.

0.6a3
-----

 * Added ``gui_scripts`` entry point group to allow installing GUI scripts
   on Windows and other platforms.  (The special handling is only for Windows;
   other platforms are treated the same as for ``console_scripts``.)

 * Improved error message when trying to use old ways of running
   ``easy_install``.  Removed the ability to run via ``python -m`` or by
   running ``easy_install.py``; ``easy_install`` is the command to run on all
   supported platforms.

 * Improved wrapper script generation and runtime initialization so that a
   VersionConflict doesn't occur if you later install a competing version of a
   needed package as the default version of that package.

 * Fixed a problem parsing version numbers in ``#egg=`` links.

0.6a2
-----

 * Added ``console_scripts`` entry point group to allow installing scripts
   without the need to create separate script files. On Windows, console
   scripts get an ``.exe`` wrapper so you can just type their name. On other
   platforms, the scripts are written without a file extension.

 * EasyInstall can now install "console_scripts" defined by packages that use
   ``setuptools`` and define appropriate entry points.  On Windows, console
   scripts get an ``.exe`` wrapper so you can just type their name.  On other
   platforms, the scripts are installed without a file extension.

 * Using ``python -m easy_install`` or running ``easy_install.py`` is now
   DEPRECATED, since an ``easy_install`` wrapper is now available on all
   platforms.

0.6a1
-----

 * Added support for building "old-style" RPMs that don't install an egg for
   the target package, using a ``--no-egg`` option.

 * The ``build_ext`` command now works better when using the ``--inplace``
   option and multiple Python versions. It now makes sure that all extensions
   match the current Python version, even if newer copies were built for a
   different Python version.

 * The ``upload`` command no longer attaches an extra ``.zip`` when uploading
   eggs, as PyPI now supports egg uploads without trickery.

 * The ``ez_setup`` script/module now displays a warning before downloading
   the setuptools egg, and attempts to check the downloaded egg against an
   internal MD5 checksum table.

 * Fixed the ``--tag-svn-revision`` option of ``egg_info`` not finding the
   latest revision number; it was using the revision number of the directory
   containing ``setup.py``, not the highest revision number in the project.

 * Added ``eager_resources`` setup argument

 * The ``sdist`` command now recognizes Subversion "deleted file" entries and
   does not include them in source distributions.

 * ``setuptools`` now embeds itself more thoroughly into the distutils, so that
   other distutils extensions (e.g. py2exe, py2app) will subclass setuptools'
   versions of things, rather than the native distutils ones.

 * Added ``entry_points`` and ``setup_requires`` arguments to ``setup()``;
   ``setup_requires`` allows you to automatically find and download packages
   that are needed in order to *build* your project (as opposed to running it).

 * ``setuptools`` now finds its commands, ``setup()`` argument validators, and
   metadata writers using entry points, so that they can be extended by
   third-party packages. See `Creating distutils Extensions
   <https://setuptools.readthedocs.io/en/latest/setuptools.html#creating-distutils-extensions>`_
   for more details.

 * The vestigial ``depends`` command has been removed. It was never finished
   or documented, and never would have worked without EasyInstall - which it
   pre-dated and was never compatible with.

 * EasyInstall now does MD5 validation of downloads from PyPI, or from any link
   that has an "#md5=..." trailer with a 32-digit lowercase hex md5 digest.

 * EasyInstall now handles symlinks in target directories by removing the link,
   rather than attempting to overwrite the link's destination.  This makes it
   easier to set up an alternate Python "home" directory (as described in
   the Non-Root Installation section of the docs).

 * Added support for handling MacOS platform information in ``.egg`` filenames,
   based on a contribution by Kevin Dangoor.  You may wish to delete and
   reinstall any eggs whose filename includes "darwin" and "Power_Macintosh",
   because the format for this platform information has changed so that minor
   OS X upgrades (such as 10.4.1 to 10.4.2) do not cause eggs built with a
   previous OS version to become obsolete.

 * easy_install's dependency processing algorithms have changed.  When using
   ``--always-copy``, it now ensures that dependencies are copied too.  When
   not using ``--always-copy``, it tries to use a single resolution loop,
   rather than recursing.

 * Fixed installing extra ``.pyc`` or ``.pyo`` files for scripts with ``.py``
   extensions.

 * Added ``--site-dirs`` option to allow adding custom "site" directories.
   Made ``easy-install.pth`` work in platform-specific alternate site
   directories (e.g. ``~/Library/Python/2.x/site-packages`` on Mac OS X).

 * If you manually delete the current version of a package, the next run of
   EasyInstall against the target directory will now remove the stray entry
   from the ``easy-install.pth`` file.

 * EasyInstall now recognizes URLs with a ``#egg=project_name`` fragment ID
   as pointing to the named project's source checkout.  Such URLs have a lower
   match precedence than any other kind of distribution, so they'll only be
   used if they have a higher version number than any other available
   distribution, or if you use the ``--editable`` option.  The ``#egg``
   fragment can contain a version if it's formatted as ``#egg=proj-ver``,
   where ``proj`` is the project name, and ``ver`` is the version number.  You
   *must* use the format for these values that the ``bdist_egg`` command uses;
   i.e., all non-alphanumeric runs must be condensed to single underscore
   characters.

 * Added the ``--editable`` option; see Editing and Viewing Source Packages
   in the docs.  Also, slightly changed the behavior of the
   ``--build-directory`` option.

 * Fixed the setup script sandbox facility not recognizing certain paths as
   valid on case-insensitive platforms.

0.5a12
------

 * The zip-safety scanner now checks for modules that might be used with
   ``python -m``, and marks them as unsafe for zipping, since Python 2.4 can't
   handle ``-m`` on zipped modules.

 * Fix ``python -m easy_install`` not working due to setuptools being installed
   as a zipfile.  Update safety scanner to check for modules that might be used
   as ``python -m`` scripts.

 * Misc. fixes for win32.exe support, including changes to support Python 2.4's
   changed ``bdist_wininst`` format.

0.5a11
------

 * Fix breakage of the "develop" command that was caused by the addition of
   ``--always-unzip`` to the ``easy_install`` command.

0.5a10
------

 * Put the ``easy_install`` module back in as a module, as it's needed for
   ``python -m`` to run it!

 * Allow ``--find-links/-f`` to accept local directories or filenames as well
   as URLs.

0.5a9
-----

 * Include ``svn:externals`` directories in source distributions as well as
   normal subversion-controlled files and directories.

 * Added ``exclude=patternlist`` option to ``setuptools.find_packages()``

 * Changed --tag-svn-revision to include an "r" in front of the revision number
   for better readability.

 * Added ability to build eggs without including source files (except for any
   scripts, of course), using the ``--exclude-source-files`` option to
   ``bdist_egg``.

 * ``setup.py install`` now automatically detects when an "unmanaged" package
   or module is going to be on ``sys.path`` ahead of a package being installed,
   thereby preventing the newer version from being imported. If this occurs,
   a warning message is output to ``sys.stderr``, but installation proceeds
   anyway. The warning message informs the user what files or directories
   need deleting, and advises them they can also use EasyInstall (with the
   ``--delete-conflicting`` option) to do it automatically.

 * The ``egg_info`` command now adds a ``top_level.txt`` file to the metadata
   directory that lists all top-level modules and packages in the distribution.
   This is used by the ``easy_install`` command to find possibly-conflicting
   "unmanaged" packages when installing the distribution.

 * Added ``zip_safe`` and ``namespace_packages`` arguments to ``setup()``.
   Added package analysis to determine zip-safety if the ``zip_safe`` flag
   is not given, and advise the author regarding what code might need changing.

 * Fixed the swapped ``-d`` and ``-b`` options of ``bdist_egg``.

 * EasyInstall now automatically detects when an "unmanaged" package or
   module is going to be on ``sys.path`` ahead of a package you're installing,
   thereby preventing the newer version from being imported.  By default, it
   will abort installation to alert you of the problem, but there are also
   new options (``--delete-conflicting`` and ``--ignore-conflicts-at-my-risk``)
   available to change the default behavior.  (Note: this new feature doesn't
   take effect for egg files that were built with older ``setuptools``
   versions, because they lack the new metadata file required to implement it.)

 * The ``easy_install`` distutils command now uses ``DistutilsError`` as its
   base error type for errors that should just issue a message to stderr and
   exit the program without a traceback.

 * EasyInstall can now be given a path to a directory containing a setup
   script, and it will attempt to build and install the package there.

 * EasyInstall now performs a safety analysis on module contents to determine
   whether a package is likely to run in zipped form, and displays
   information about what modules may be doing introspection that would break
   when running as a zipfile.

 * Added the ``--always-unzip/-Z`` option, to force unzipping of packages that
   would ordinarily be considered safe to unzip, and changed the meaning of
   ``--zip-ok/-z`` to "always leave everything zipped".

0.5a8
-----

 * The "egg_info" command now always sets the distribution metadata to "safe"
   forms of the distribution name and version, so that distribution files will
   be generated with parseable names (i.e., ones that don't include '-' in the
   name or version). Also, this means that if you use the various ``--tag``
   options of "egg_info", any distributions generated will use the tags in the
   version, not just egg distributions.

 * Added support for defining command aliases in distutils configuration files,
   under the "[aliases]" section. To prevent recursion and to allow aliases to
   call the command of the same name, a given alias can be expanded only once
   per command-line invocation. You can define new aliases with the "alias"
   command, either for the local, global, or per-user configuration.

 * Added "rotate" command to delete old distribution files, given a set of
   patterns to match and the number of files to keep.  (Keeps the most
   recently-modified distribution files matching each pattern.)

 * Added "saveopts" command that saves all command-line options for the current
   invocation to the local, global, or per-user configuration file. Useful for
   setting defaults without having to hand-edit a configuration file.

 * Added a "setopt" command that sets a single option in a specified distutils
   configuration file.

 * There is now a separate documentation page for setuptools; revision
   history that's not specific to EasyInstall has been moved to that page.

0.5a7
-----

 * Added "upload" support for egg and source distributions, including a bug
   fix for "upload" and a temporary workaround for lack of .egg support in
   PyPI.

0.5a6
-----

 * Beefed up the "sdist" command so that if you don't have a MANIFEST.in, it
   will include all files under revision control (CVS or Subversion) in the
   current directory, and it will regenerate the list every time you create a
   source distribution, not just when you tell it to. This should make the
   default "do what you mean" more often than the distutils' default behavior
   did, while still retaining the old behavior in the presence of MANIFEST.in.

 * Fixed the "develop" command always updating .pth files, even if you
   specified ``-n`` or ``--dry-run``.

 * Slightly changed the format of the generated version when you use
   ``--tag-build`` on the "egg_info" command, so that you can make tagged
   revisions compare *lower* than the version specified in setup.py (e.g. by
   using ``--tag-build=dev``).

0.5a5
-----

 * Added ``develop`` command to ``setuptools``-based packages. This command
   installs an ``.egg-link`` pointing to the package's source directory, and
   script wrappers that ``execfile()`` the source versions of the package's
   scripts. This lets you put your development checkout(s) on sys.path without
   having to actually install them.  (To uninstall the link, use
   use ``setup.py develop --uninstall``.)

 * Added ``egg_info`` command to ``setuptools``-based packages. This command
   just creates or updates the "projectname.egg-info" directory, without
   building an egg.  (It's used by the ``bdist_egg``, ``test``, and ``develop``
   commands.)

 * Enhanced the ``test`` command so that it doesn't install the package, but
   instead builds any C extensions in-place, updates the ``.egg-info``
   metadata, adds the source directory to ``sys.path``, and runs the tests
   directly on the source. This avoids an "unmanaged" installation of the
   package to ``site-packages`` or elsewhere.

 * Made ``easy_install`` a standard ``setuptools`` command, moving it from
   the ``easy_install`` module to ``setuptools.command.easy_install``. Note
   that if you were importing or extending it, you must now change your imports
   accordingly.  ``easy_install.py`` is still installed as a script, but not as
   a module.

0.5a4
-----

 * Setup scripts using setuptools can now list their dependencies directly in
   the setup.py file, without having to manually create a ``depends.txt`` file.
   The ``install_requires`` and ``extras_require`` arguments to ``setup()``
   are used to create a dependencies file automatically. If you are manually
   creating ``depends.txt`` right now, please switch to using these setup
   arguments as soon as practical, because ``depends.txt`` support will be
   removed in the 0.6 release cycle. For documentation on the new arguments,
   see the ``setuptools.dist.Distribution`` class.

 * Setup scripts using setuptools now always install using ``easy_install``
   internally, for ease of uninstallation and upgrading.

 * Added ``--always-copy/-a`` option to always copy needed packages to the
   installation directory, even if they're already present elsewhere on
   sys.path. (In previous versions, this was the default behavior, but now
   you must request it.)

 * Added ``--upgrade/-U`` option to force checking PyPI for latest available
   version(s) of all packages requested by name and version, even if a matching
   version is available locally.

 * Added automatic installation of dependencies declared by a distribution
   being installed.  These dependencies must be listed in the distribution's
   ``EGG-INFO`` directory, so the distribution has to have declared its
   dependencies by using setuptools.  If a package has requirements it didn't
   declare, you'll still have to deal with them yourself.  (E.g., by asking
   EasyInstall to find and install them.)

 * Added the ``--record`` option to ``easy_install`` for the benefit of tools
   that run ``setup.py install --record=filename`` on behalf of another
   packaging system.)

0.5a3
-----

 * Fixed not setting script permissions to allow execution.

 * Improved sandboxing so that setup scripts that want a temporary directory
   (e.g. pychecker) can still run in the sandbox.

0.5a2
-----

 * Fix stupid stupid refactoring-at-the-last-minute typos.  :(

0.5a1
-----

 * Added support for "self-installation" bootstrapping. Packages can now
   include ``ez_setup.py`` in their source distribution, and add the following
   to their ``setup.py``, in order to automatically bootstrap installation of
   setuptools as part of their setup process::

    from ez_setup import use_setuptools
    use_setuptools()

    from setuptools import setup
    # etc...

 * Added support for converting ``.win32.exe`` installers to eggs on the fly.
   EasyInstall will now recognize such files by name and install them.

 * Fixed a problem with picking the "best" version to install (versions were
   being sorted as strings, rather than as parsed values)

0.4a4
-----

 * Added support for the distutils "verbose/quiet" and "dry-run" options, as
   well as the "optimize" flag.

 * Support downloading packages that were uploaded to PyPI (by scanning all
   links on package pages, not just the homepage/download links).

0.4a3
-----

 * Add progress messages to the search/download process so that you can tell
   what URLs it's reading to find download links.  (Hopefully, this will help
   people report out-of-date and broken links to package authors, and to tell
   when they've asked for a package that doesn't exist.)

0.4a2
-----

 * Added ``ez_setup.py`` installer/bootstrap script to make initial setuptools
   installation easier, and to allow distributions using setuptools to avoid
   having to include setuptools in their source distribution.

 * All downloads are now managed by the ``PackageIndex`` class (which is now
   subclassable and replaceable), so that embedders can more easily override
   download logic, give download progress reports, etc. The class has also
   been moved to the new ``setuptools.package_index`` module.

 * The ``Installer`` class no longer handles downloading, manages a temporary
   directory, or tracks the ``zip_ok`` option. Downloading is now handled
   by ``PackageIndex``, and ``Installer`` has become an ``easy_install``
   command class based on ``setuptools.Command``.

 * There is a new ``setuptools.sandbox.run_setup()`` API to invoke a setup
   script in a directory sandbox, and a new ``setuptools.archive_util`` module
   with an ``unpack_archive()`` API. These were split out of EasyInstall to
   allow reuse by other tools and applications.

 * ``setuptools.Command`` now supports reinitializing commands using keyword
   arguments to set/reset options. Also, ``Command`` subclasses can now set
   their ``command_consumes_arguments`` attribute to ``True`` in order to
   receive an ``args`` option containing the rest of the command line.

 * Added support for installing scripts

 * Added support for setting options via distutils configuration files, and
   using distutils' default options as a basis for EasyInstall's defaults.

 * Renamed ``--scan-url/-s`` to ``--find-links/-f`` to free up ``-s`` for the
   script installation directory option.

 * Use ``urllib2`` instead of ``urllib``, to allow use of ``https:`` URLs if
   Python includes SSL support.

0.4a1
-----

 * Added ``--scan-url`` and ``--index-url`` options, to scan download pages
   and search PyPI for needed packages.

0.3a4
-----

 * Restrict ``--build-directory=DIR/-b DIR`` option to only be used with single
   URL installs, to avoid running the wrong setup.py.

0.3a3
-----

 * Added ``--build-directory=DIR/-b DIR`` option.

 * Added "installation report" that explains how to use 'require()' when doing
   a multiversion install or alternate installation directory.

 * Added SourceForge mirror auto-select (Contributed by Ian Bicking)

 * Added "sandboxing" that stops a setup script from running if it attempts to
   write to the filesystem outside of the build area

 * Added more workarounds for packages with quirky ``install_data`` hacks

0.3a2
-----

 * Added new options to ``bdist_egg`` to allow tagging the egg's version number
   with a subversion revision number, the current date, or an explicit tag
   value. Run ``setup.py bdist_egg --help`` to get more information.

 * Added subversion download support for ``svn:`` and ``svn+`` URLs, as well as
   automatic recognition of HTTP subversion URLs (Contributed by Ian Bicking)

 * Misc. bug fixes

0.3a1
-----

 * Initial release.

.. image:: https://img.shields.io/pypi/v/setuptools.svg
   :target: `PyPI link`_

.. image:: https://img.shields.io/pypi/pyversions/setuptools.svg
   :target: `PyPI link`_

.. _PyPI link: https://pypi.org/project/setuptools

.. image:: https://dev.azure.com/jaraco/setuptools/_apis/build/status/pypa.setuptools?branchName=master
   :target: https://dev.azure.com/jaraco/setuptools/_build/latest?definitionId=1&branchName=master

.. image:: https://img.shields.io/travis/pypa/setuptools/master.svg?label=Linux%20CI&logo=travis&logoColor=white
   :target: https://travis-ci.org/pypa/setuptools

.. image:: https://img.shields.io/appveyor/ci/pypa/setuptools/master.svg?label=Windows%20CI&logo=appveyor&logoColor=white
   :target: https://ci.appveyor.com/project/pypa/setuptools/branch/master

.. image:: https://img.shields.io/readthedocs/setuptools/latest.svg
    :target: https://setuptools.readthedocs.io

.. image:: https://img.shields.io/codecov/c/github/pypa/setuptools/master.svg?logo=codecov&logoColor=white
   :target: https://codecov.io/gh/pypa/setuptools

.. image:: https://tidelift.com/badges/github/pypa/setuptools?style=flat
   :target: https://tidelift.com/subscription/pkg/pypi-setuptools?utm_source=pypi-setuptools&utm_medium=readme

See the `Installation Instructions
<https://packaging.python.org/installing/>`_ in the Python Packaging
User's Guide for instructions on installing, upgrading, and uninstalling
Setuptools.

Questions and comments should be directed to the `distutils-sig
mailing list <http://mail.python.org/pipermail/distutils-sig/>`_.
Bug reports and especially tested patches may be
submitted directly to the `bug tracker
<https://github.com/pypa/setuptools/issues>`_.

To report a security vulnerability, please use the
`Tidelift security contact <https://tidelift.com/security>`_.
Tidelift will coordinate the fix and disclosure.


For Enterprise
==============

Available as part of the Tidelift Subscription.

Setuptools and the maintainers of thousands of other packages are working with Tidelift to deliver one enterprise subscription that covers all of the open source you use.

`Learn more <https://tidelift.com/subscription/pkg/pypi-setuptools?utm_source=pypi-setuptools&utm_medium=referral&utm_campaign=github>`_.

Code of Conduct
===============

Everyone interacting in the setuptools project's codebases, issue trackers,
chat rooms, and mailing lists is expected to follow the
`PSF Code of Conduct <https://github.com/pypa/.github/blob/main/CODE_OF_CONDUCT.md>`_.
pip - The Python Package Installer
==================================

.. image:: https://img.shields.io/pypi/v/pip.svg
   :target: https://pypi.org/project/pip/

.. image:: https://readthedocs.org/projects/pip/badge/?version=latest
   :target: https://pip.pypa.io/en/latest

pip is the `package installer`_ for Python. You can use pip to install packages from the `Python Package Index`_ and other indexes.

Please take a look at our documentation for how to install and use pip:

* `Installation`_
* `Usage`_

We release updates regularly, with a new version every 3 months. Find more details in our documentation:

* `Release notes`_
* `Release process`_

In 2020, we're working on improvements to the heart of pip. Please `learn more and take our survey`_ to help us do it right.

If you find bugs, need help, or want to talk to the developers, please use our mailing lists or chat rooms:

* `Issue tracking`_
* `Discourse channel`_
* `User IRC`_

If you want to get involved head over to GitHub to get the source code, look at our development documentation and feel free to jump on the developer mailing lists and chat rooms:

* `GitHub page`_
* `Development documentation`_
* `Development mailing list`_
* `Development IRC`_

Code of Conduct
---------------

Everyone interacting in the pip project's codebases, issue trackers, chat
rooms, and mailing lists is expected to follow the `PyPA Code of Conduct`_.

.. _package installer: https://packaging.python.org/guides/tool-recommendations/
.. _Python Package Index: https://pypi.org
.. _Installation: https://pip.pypa.io/en/stable/installing.html
.. _Usage: https://pip.pypa.io/en/stable/
.. _Release notes: https://pip.pypa.io/en/stable/news.html
.. _Release process: https://pip.pypa.io/en/latest/development/release-process/
.. _GitHub page: https://github.com/pypa/pip
.. _Development documentation: https://pip.pypa.io/en/latest/development
.. _learn more and take our survey: https://pyfound.blogspot.com/2020/03/new-pip-resolver-to-roll-out-this-year.html
.. _Issue tracking: https://github.com/pypa/pip/issues
.. _Discourse channel: https://discuss.python.org/c/packaging
.. _Development mailing list: https://mail.python.org/mailman3/lists/distutils-sig.python.org/
.. _User IRC: https://webchat.freenode.net/?channels=%23pypa
.. _Development IRC: https://webchat.freenode.net/?channels=%23pypa-dev
.. _PyPA Code of Conduct: https://www.pypa.io/en/latest/code-of-conduct/
composer-cli
============

:Authors:
    Brian C. Lane <bcl@redhat.com>

``composer-cli`` is an interactive tool for use with a WELDR API server,
managing blueprints, exploring available packages, and building new images.
`lorax-composer <lorax-composer.html>` and `osbuild-composer
<https://osbuild.org>` both implement compatible servers.

It requires server to be installed on the local system, and the user running it
needs to be a member of the ``weldr`` group. They do not need to be root, but
all of the `security precautions <lorax-composer.html#security>`_ apply.

composer-cli cmdline arguments
------------------------------

.. argparse::
   :ref: composer.cli.cmdline.composer_cli_parser
   :prog: composer-cli

Edit a Blueprint
----------------

Start out by listing the available blueprints using ``composer-cli blueprints
list``, pick one and save it to the local directory by running ``composer-cli
blueprints save http-server``. If there are no blueprints available you can
copy one of the examples `from the test suite
<https://github.com/weldr/lorax/tree/rhel8-branch/tests/pylorax/blueprints/>`_.

Edit the file (it will be saved with a .toml extension) and change the
description, add a package or module to it. Send it back to the server by
running ``composer-cli blueprints push http-server.toml``. You can verify that it was
saved by viewing the changelog - ``composer-cli blueprints changes http-server``.

The full blueprint documentation `is here
<https://www.osbuild.org/guides/blueprint-reference/blueprint-reference.html>`_.

Build an image
----------------

Build a ``qcow2`` disk image from this blueprint by running ``composer-cli
compose start http-server qcow2``. It will print a UUID that you can use to
keep track of the build. You can also cancel the build if needed.

The available types of images is displayed by ``composer-cli compose types``.
Currently this consists of: alibaba, ami, ext4-filesystem, google, live-iso,
openstack, partitioned-disk, qcow2, tar, vhd, vmdk

Monitor the build status
------------------------

Monitor it using ``composer-cli compose status``, which will show the status of
all the builds on the system. You can view the end of the anaconda build logs
once it is in the ``RUNNING`` state using ``composer-cli compose log UUID``
where UUID is the UUID returned by the start command.

Once the build is in the ``FINISHED`` state you can download the image.

Download the image
------------------

Downloading the final image is done with ``composer-cli compose image UUID`` and it will
save the qcow2 image as ``UUID-disk.qcow2`` which you can then use to boot a VM like this::

    qemu-kvm --name test-image -m 1024 -hda ./UUID-disk.qcow2


Image Uploads
-------------

``composer-cli`` can upload the images to a number of services, including AWS,
OpenStack, and vSphere. The upload can be started when the build is finished
by using ``composer-cli compose start ...``. In order to access the service you need
to pass authentication details to composer-cli using a TOML file.

.. note::

    This is only supported when running the ``osbuild-composer`` API server.


Providers
---------

Providers are where the images are uploaded to. You
will need to gather some provider
specific information in order to authenticate with it. Please refer to the ``osbuild-composer``
documentation for the provider specific fields. You will then create a TOML file with the
name of the provider and the settings, like this::

    provider = "aws"

    [settings]
    aws_access_key = "AWS Access Key"
    aws_bucket = "AWS Bucket"
    aws_region = "AWS Region"
    aws_secret_key = "AWS Secret Key"

Save this into an ``aws-credentials.toml`` file and use it when running ``start``.

AWS
^^^

The access key and secret key can be created by going to the
``IAM->Users->Security Credentials`` section and creating a new access key. The
secret key will only be shown when it is first created so make sure to record
it in a secure place. The region should be the region that you want to use the
AMI in, and the bucket can be an existing bucket, or a new one, following the
normal AWS bucket naming rules. It will be created if it doesn't already exist.

When uploading the image it is first uploaded to the s3 bucket, and then
converted to an AMI.  If the conversion is successful the s3 object will be
deleted. If it fails, re-trying after correcting the problem will re-use the
object if you have not deleted it in the meantime, speeding up the process.


Build an image and upload results
---------------------------------

With the settings stored in a TOML file::

    composer-cli compose start example-http-server ami "http image" aws-settings.toml

It will return the UUID of the image build. Once
the build has finished successfully it will start the upload process.


Debugging
---------

There are a couple of arguments that can be helpful when debugging problems.
These are only meant for debugging and should not be used to script access to
the API. If you need to do that you can communicate with it directly in the
language of your choice.

``--json`` will return the server's response as a nicely formatted json output
instead of printing what the command would usually print.

``--test=1`` will cause a compose start to start creating an image, and then
end with a failed state.

``--test=2`` will cause a compose to start and then end with a finished state,
without actually composing anything.
livemedia-creator
=================

:Authors:
    Brian C. Lane <bcl@redhat.com>

livemedia-creator uses `Anaconda <https://github.com/rhinstaller/anaconda>`_,
`kickstart <https://github.com/rhinstaller/pykickstart>`_ and `Lorax
<https://github.com/rhinstaller/lorax>`_ to create bootable media that use the
same install path as a normal system installation. It can be used to make live
isos, bootable (partitioned) disk images, tarfiles, and filesystem images for
use with virtualization and container solutions like libvirt, docker, and
OpenStack.

The general idea is to use qemu with kickstart and an Anaconda boot.iso to
install into a disk image and then use the disk image to create the bootable
media.

livemedia-creator --help will describe all of the options available. At the
minimum you need:

``--make-iso`` to create a final bootable .iso or one of the other ``--make-*`` options.

``--iso`` to specify the Anaconda install media to use with qemu.

``--ks`` to select the kickstart file describing what to install.

To use livemedia-creator with virtualization you will need to have qemu-kvm installed.

If you are going to be using Anaconda directly, with ``--no-virt`` mode, make sure
you have the anaconda-tui package installed.

Conventions used in this document:

``lmc`` is an abbreviation for livemedia-creator.

``builder`` is the system where livemedia-creator is being run

``image`` is the disk image being created by running livemedia-creator


livemedia-creator cmdline arguments
-----------------------------------

.. argparse::
    :ref: pylorax.cmdline.lmc_parser
    :prog: livemedia-creator

    --macboot : @replace
        Make the iso bootable on UEFI based Mac systems

        Default: True

    --nomacboot : @replace
        Do not create a Mac bootable iso

        Default: False


Quickstart
----------

Run this to create a bootable live iso::

    sudo livemedia-creator --make-iso \
    --iso=/extra/iso/boot.iso --ks=./docs/rhel-livemedia.ks

You can run it directly from the lorax git repo like this::

    sudo PATH=./src/sbin/:$PATH PYTHONPATH=./src/ ./src/sbin/livemedia-creator \
    --make-iso --iso=/extra/iso/boot.iso \
    --ks=./docs/rhel-livemedia.ks --lorax-templates=./share/

You can observe the installation using vnc. The logs will show what port was
chosen, or you can use a specific port by passing it. eg. ``--vnc vnc:127.0.0.1:5``

This is usually a good idea when testing changes to the kickstart. lmc tries
to monitor the logs for fatal errors, but may not catch everything.


How ISO creation works
----------------------

There are 2 stages, the install stage which produces a disk or filesystem image
as its output, and the boot media creation which uses the image as its input.
Normally you would run both stages, but it is possible to stop after the
install stage, by using ``--image-only``, or to skip the install stage and use
a previously created disk image by passing ``--disk-image`` or ``--fs-image``

When creating an iso qemu boots using the passed Anaconda installer iso
and installs the system based on the kickstart. The ``%post`` section of the
kickstart is used to customize the installed system in the same way that
current spin-kickstarts do.

livemedia-creator monitors the install process for problems by watching the
install logs. They are written to the current directory or to the base
directory specified by the --logfile command. You can also monitor the install
by using a vnc client. This is recommended when first modifying a kickstart,
since there are still places where Anaconda may get stuck without the log
monitor catching it.

The output from this process is a partitioned disk image. kpartx can be used
to mount and examine it when there is a problem with the install. It can also
be booted using kvm.

When creating an iso the disk image's / partition is copied into a formatted
filesystem image which is then used as the input to lorax for creation of the
final media.

The final image is created by lorax, using the templates in /usr/share/lorax/live/
or the live directory below the directory specified by ``--lorax-templates``. The
templates are written using the Mako template system with some extra commands
added by lorax.

.. note::
    The output from --make-iso includes the artifacts used to create the boot.iso;
    the kernel, initrd, the squashfs filesystem, etc. If you only want the
    boot.iso you can pass ``--iso-only`` and the other files will be removed. You
    can also name the iso by using ``--iso-name my-live.iso``.


Kickstarts
----------

The docs/ directory includes several example kickstarts, one to create a live
desktop iso using GNOME, and another to create a minimal disk image. When
creating your own kickstarts you should start with the minimal example, it
includes several needed packages that are not always included by dependencies.

Or you can use existing spin kickstarts to create live media with a few
changes. Here are the steps I used to convert the Fedora XFCE spin.

1. Flatten the xfce kickstart using ksflatten
2. Add zerombr so you don't get the disk init dialog
3. Add clearpart --all
4. Add swap partition
5. bootloader target
6. Add shutdown to the kickstart
7. Add network --bootproto=dhcp --activate to activate the network
   This works for F16 builds but for F15 and before you need to pass
   something on the cmdline that activate the network, like sshd:

    ``livemedia-creator --kernel-args="sshd"``

8. Add a root password::

    rootpw rootme
    network --bootproto=dhcp --activate
    zerombr
    clearpart --all
    bootloader --location=mbr
    part swap --size=512
    shutdown

9. In the livesys script section of the %post remove the root password. This
   really depends on how the spin wants to work. You could add the live user
   that you create to the %wheel group so that sudo works if you wanted to.

    ``passwd -d root > /dev/null``

10. Remove /etc/fstab in %post, dracut handles mounting the rootfs

    ``cat /dev/null > /dev/fstab``

    Do this only for live iso's, the filesystem will be mounted read only if
    there is no /etc/fstab

11. Don't delete initramfs files from /boot in %post
12. When creating live iso's you need to have, at least, these packages in the %package section::
    dracut-config-generic
    dracut-live
    -dracut-config-rescue
    grub-efi
    memtest86+
    syslinux

One drawback to using qemu is that it pulls the packages from the repo each
time you run it. To speed things up you either need a local mirror of the
packages, or you can use a caching proxy. When using a proxy you pass it to
livemedia-creator like this:

    ``--proxy=http://proxy.yourdomain.com:3128``

You also need to use a specific mirror instead of mirrormanager so that the
packages will get cached, so your kickstart url would look like:

    ``url --url="http://dl.fedoraproject.org/pub/fedora/linux/development/rawhide/x86_64/os/"``

You can also add an update repo, but don't name it updates. Add --proxy to it
as well.


Anaconda image install (no-virt)
--------------------------------

You can create images without using qemu by passing ``--no-virt`` on the
cmdline. This will use Anaconda's directory install feature to handle the
install.  There are a couple of things to keep in mind when doing this:

1. It will be most reliable when building images for the same release that the
   host is running. Because Anaconda has expectations about the system it is
   running under you may encounter strange bugs if you try to build newer or
   older releases.

2. It may totally trash your host. So far I haven't had this happen, but the
   possibility exists that a bug in Anaconda could result in it operating on
   real devices. I recommend running it in a virt or on a system that you can
   afford to lose all data from.

The logs from anaconda will be placed in an ./anaconda/ directory in either
the current directory or in the directory used for --logfile

Example cmdline:

``sudo livemedia-creator --make-iso --no-virt --ks=./rhel-livemedia.ks``

.. note::
    Using no-virt to create a partitioned disk image (eg. --make-disk or
    --make-vagrant) will only create disks usable on the host platform (BIOS
    or UEFI). You can create BIOS partitioned disk images on UEFI by using
    virt.

.. note::
    As of version 30.7 SELinux can be set to Enforcing. The current state is
    logged for debugging purposes and if there are SELinux denials they should
    be reported as a bug.

AMI Images
----------

Amazon EC2 images can be created by using the --make-ami switch and an appropriate
kickstart file. All of the work to customize the image is handled by the kickstart.
The example currently included was modified from the cloud-kickstarts version so
that it would work with livemedia-creator.

Example cmdline:

``sudo livemedia-creator --make-ami --iso=/path/to/boot.iso --ks=./docs/rhel-livemedia-ec2.ks``

This will produce an ami-root.img file in the working directory.

At this time I have not tested the image with EC2. Feedback would be welcome.


Appliance Creation
------------------

livemedia-creator can now replace appliance-tools by using the --make-appliance
switch. This will create the partitioned disk image and an XML file that can be
used with virt-image to setup a virtual system.

The XML is generated using the Mako template from
/usr/share/lorax/appliance/libvirt.xml You can use a different template by
passing ``--app-template <template path>``

Documentation on the Mako template system can be found at the `Mako site
<http://docs.makotemplates.org/en/latest/index.html>`_

The name of the final output XML is appliance.xml, this can be changed with
``--app-file <file path>``

The following variables are passed to the template:

    ``disks``
       A list of disk_info about each disk.
       Each entry has the following attributes:

        ``name``
        base name of the disk image file

        ``format``
        "raw"

        ``checksum_type``
        "sha256"

        ``checksum``
        sha256 checksum of the disk image

    ``name``
    Name of appliance, from --app-name argument

    ``arch``
    Architecture

    ``memory``
    Memory in KB (from ``--ram``)

    ``vcpus``
    from ``--vcpus``

    ``networks``
    list of networks from the kickstart or []

    ``title``
    from ``--title``

    ``project``
    from ``--project``

    ``releasever``
    from ``--releasever``

The created image can be imported into libvirt using:

    ``virt-image appliance.xml``

You can also create qcow2 appliance images using ``--image-type=qcow2``, for example::

    sudo livemedia-creator --make-appliance --iso=/path/to/boot.iso --ks=./docs/rhel-minimal.ks \
    --image-type=qcow2 --app-file=minimal-test.xml --image-name=minimal-test.img


Filesystem Image Creation
-------------------------

livemedia-creator can be used to create un-partitined filesystem images using
the ``--make-fsimage`` option. As of version 21.8 this works with both qemu and
no-virt modes of operation. Previously it was only available with no-virt.

Kickstarts should have a single / partition with no extra mountpoints.

    ``livemedia-creator --make-fsimage --iso=/path/to/boot.iso --ks=./docs/rhel-minimal.ks``

You can name the output image with ``--image-name`` and set a label on the filesystem with ``--fs-label``


TAR File Creation
-----------------

The ``--make-tar`` command can be used to create a tar of the root filesystem. By
default it is compressed using xz, but this can be changed using the
``--compression`` and ``--compress-arg`` options. This option works with both virt and
no-virt install methods.

As with ``--make-fsimage`` the kickstart should be limited to a single / partition.

For example::

    livemedia-creator --make-tar --iso=/path/to/boot.iso --ks=./docs/rhel-minimal.ks \
    --image-name=rhel-root.tar.xz


Live Image for PXE Boot
-----------------------

The ``--make-pxe-live`` command will produce squashfs image containing live root
filesystem that can be used for pxe boot. Directory with results will contain
the live image, kernel image, initrd image and template of pxe configuration
for the images.


Atomic Live Image for PXE Boot
------------------------------

The ``--make-ostree-live`` command will produce the same result as ``--make-pxe-live``
for installations of Atomic Host.  Example kickstart for such an installation
using Atomic installer iso with local repo included in the image can be found
in docs/rhel-atomic-pxe-live.ks.

The PXE images can also be created with ``--no-virt`` by using the example
kickstart in docs/rhel-atomic-pxe-live-novirt.ks. This also works inside the
mock environment.


Using Mock and --no-virt to Create Images
-----------------------------------------

As of lorax version 22.2 you can use livemedia-creator and anaconda version
22.15 inside of a mock chroot with --make-iso and --make-fsimage.

.. note::
    As of mock 1.3.4 you need to use ``--old-chroot`` with mock. Mock now defaults to using systemd-nspawn
    which cannot create the needed loop device nodes. Passing ``--old-chroot`` will use the old system
    where ``/dev/loop*`` is setup for you.

On the host system:

1. yum install -y mock

2. Add a user to the mock group to use for running mock. eg. builder

3. Create a new /etc/mock/ config file based on the rawhide one, or modify the
   existing one so that the following options are setup::

       config_opts['chroot_setup_cmd'] = 'install @buildsys-build anaconda-tui lorax'

       # build results go into /home/builder/results/
       config_opts['plugin_conf']['bind_mount_opts']['dirs'].append(('/home/builder/results','/results/'))

   If you are creating images for a branched release of Fedora you should also enable
   the updates-testing repository so that you get the latest builds in your mock chroot.

The following steps are run as the builder user who is a member of the mock
group.

4. Make a directory for results matching the bind mount above
   ``mkdir ~/results/``

5. Copy the example kickstarts
   ``cp /usr/share/docs/lorax/*ks .``

6. Make sure tar and dracut-network are in the %packages section and that the
   ``url points to the correct repo``

7. Init the mock
   ``mock -r rhel-8-x86_64 --old-chroot --init``

8. Copy the kickstart inside the mock
   ``mock -r rhel-8-x86_64 --old-chroot --copyin ./rhel-minimal.ks /root/``

9. Make a minimal iso::

        mock -r rhel-8-x86_64 --old-chroot --chroot -- livemedia-creator --no-virt \
        --resultdir=/results/try-1 --logfile=/results/logs/try-1/try-1.log \
        --make-iso --ks /root/rhel-minimal.ks

Results will be in ./results/try-1 and logs under /results/logs/try-1/
including anaconda logs and livemedia-creator logs. The new iso will be
located at ~/results/try-1/images/boot.iso, and the ~/results/try-1/
directory tree will also contain the vmlinuz, initrd, etc.


Using Mock and qemu to Create Images
------------------------------------

Version 25.0 of livemedia-creator switches to using qemu for virtualization.
This allows creation of all image types, and use of the KVM on the host if
/dev/kvm is present in the mock environment.

On the host system:

1. yum install -y mock

2. Add a user to the mock group to use for running mock. eg. builder

3. Create a new /etc/mock/ config file based on the rawhide one, or modify the
   existing one so that the following options are setup::

       config_opts['chroot_setup_cmd'] = 'install @buildsys-build lorax qemu'

       # build results go into /home/builder/results/
       config_opts['plugin_conf']['bind_mount_opts']['dirs'].append(('/home/builder/results','/results/'))

   If you are creating images for a branched release of Fedora you should also enable
   the updates-testing repository so that you get the latest builds in your mock chroot.

The following steps are run as the builder user who is a member of the mock
group.

4. Make a directory for results matching the bind mount above
   ``mkdir ~/results/``

5. Copy the example kickstarts
   ``cp /usr/share/docs/lorax/*ks .``

6. Make sure tar and dracut-network are in the %packages section and that the
   ``url points to the correct repo``

7. Init the mock
   ``mock -r rhel-8-x86_64 --old-chroot --init``

8. Copy the kickstart inside the mock
   ``mock -r rhel-8-x86_64 --old-chroot --copyin ./rhel-minimal.ks /root/``

9. Copy the Anaconda boot.iso inside the mock
   ``mock -r rhel-8-x86_64 --old-chroot --copyin ./boot.iso /root/``

10. Make a minimal iso::

        mock -r rhel-8-x86_64 --old-chroot --chroot -- livemedia-creator \
        --resultdir=/results/try-1 --logfile=/results/logs/try-1/try-1.log \
        --make-iso --ks /root/rhel-minimal.ks --iso /root/boot.iso

Results will be in ./results/try-1 and logs under /results/logs/try-1/
including anaconda logs and livemedia-creator logs. The new iso will be
located at ~/results/try-1/images/boot.iso, and the ~/results/try-1/
directory tree will also contain the vmlinuz, initrd, etc.

This will run qemu without kvm support, which is going to be very slow. You can
add ``mknod /dev/kvm c 10 232;`` to create the device node before running lmc.


OpenStack Image Creation
------------------------

OpenStack supports partitioned disk images so ``--make-disk`` can be used to
create images for importing into glance, OpenStack's image storage component.
You need to have access to an OpenStack provider that allows image uploads, or
setup your own using the instructions from the `RDO Project
<https://www.rdoproject.org/Quickstart>`_.

The example kickstart, rhel-openstack.ks, is only slightly different than the
rhel-minimal.ks one.  It adds the cloud-init and cloud-utils-growpart
packages. OpenStack supports setting up the image using cloud-init, and
cloud-utils-growpart will grow the image to fit the instance's disk size.

Create a qcow2 image using the kickstart like this:

    ``sudo livemedia-creator --make-disk --iso=/path/to/boot.iso --ks=/path/to/rhel-openstack.ks --image-type=qcow2``

.. note::
    On the RHEL7 version of lmc ``--image-type`` isn't supported. You can only create a bare partitioned disk image.

Import the resulting disk image into the OpenStack system, either via the web UI, or glance on the cmdline::

    glance image-create --name "rhel-openstack" --is-public true --disk-format qcow2 \
    --container-format bare --file ./rhel-openstack.qcow2

If qcow2 wasn't used then ``--disk-format`` should be set to raw.


Container Image Creation
------------------------

Use lmc to create a tarfile as described in the `TAR File Creation`_ section, but substitute the
rhel-container.ks example kickstart which removes the requirement for core files and the kernel.

You can then import the tarfile into podman or docker like this:

    ``podman import /var/tmp/root.tar.xz rhel-root``

And then run bash inside of it:

    ``podman run -i -t rhel-root /bin/bash``


Open Container Initiative Image Creation
----------------------------------------

The OCI is a new specification that is still being worked on. You can read more about it at
`the Open Container Initiative website <https://www.opencontainers.org/>`_. You can create
OCI images using the following command::

    sudo livemedia-creator --make-oci --oci-config /path/to/config.json --oci-runtime /path/to/runtime.json \
    --iso=/path/to/boot.iso --ks=/path/to/rhel-minimal.ks

You must provide the config.json and runtime.json files to be included in the bundle,
their specifications can be found `on the OCI github project <https://github.com/opencontainers/specs>`_
output will be in the results directory with a default name of bundle.tar.xz

This will work with ``--no-virt`` and inside a mock since it doesn't use any
partitioned disk images.


Vagrant Image Creation
----------------------

`Vagrant <https://www.vagrantup.com/>`_ images can be created using the following command::

    sudo livemedia-creator --make-vagrant --vagrant-metadata /path/to/metadata.json \
    --iso=/path/to/boot.iso --ks=/path/to/rhel-vagrant.ks

The image created is a `vagrant-libvirt
<https://github.com/pradels/vagrant-libvirt>`_ provider image and needs to have
vagrant setup with libvirt before you can use it.

The ``--vagrant-metadata`` file is optional, it will create a minimal one by
default, and if one is passed it will make sure the disk size  is setup
correctly. If you pass a ``--vagrant-vagrantfile`` it will be included in the
image verbatim. By default no vagrantfile is created.

There is an example Vagrant kickstart file in the docs directory that sets up
the vagrant user with the default insecure SSH pubkey and a few useful
utilities.

This also works with ``--no-virt``, but will not work inside a mock due to its
use of partitioned disk images and qcow2.


Creating UEFI disk images with virt
-----------------------------------

Partitioned disk images can only be created for the same platform as the host system (BIOS or
UEFI). You can use virt to create BIOS images on UEFI systems, and it is also possible
to create UEFI images on BIOS systems using OVMF firmware and qemu.

Install the lorax-lmc-virt package, this will install qemu and the OVMF
firmware files.

Now you can run livemedia-creator with ``--virt-uefi`` to boot and install using UEFI::

    sudo livemedia-creator --make-disk --virt-uefi --iso=/path/to/boot.iso \
    --ks=/path/to/rhel-minimal.ks

Make sure that the kickstart you are using creates a /boot/efi partition by including this::

    part /boot/efi --fstype="efi" --size=500

Or use ``reqpart`` in the kickstart and Anaconda will create the required partitions.

.. note::
    The --virt-uefi method is currently only supported on the x86_64 architecture.


Debugging problems
------------------

Sometimes an installation will get stuck. When using qemu the logs will
be written to ./virt-install.log and most of the time any problems that happen
will be near the end of the file. lmc tries to detect common errors and will
cancel the installation when they happen. But not everything can be caught.
When creating a new kickstart it is helpful to use vnc so that you can monitor
the installation as it happens, and if it gets stuck without lmc detecting the
problem you can switch to tty1 and examine the system directly.

If it does get stuck the best way to cancel is to use kill -9 on the qemu pid,
lmc will detect that the process died and cleanup.

If lmc didn't handle the cleanup for some reason you can do this:
1. ``sudo umount /tmp/lmc-XXXX`` to unmount the iso from its mountpoint.
2. ``sudo rm -rf /tmp/lmc-XXXX``
3. ``sudo rm /var/tmp/lmc-disk-XXXXX`` to remove the disk image.

Note that lmc uses the lmc- prefix for all of its temporary files and
directories to make it easier to find and clean up leftovers.

The logs from the qemu run are stored in virt-install.log, logs from
livemedia-creator are in livemedia.log and program.log

You can add ``--image-only`` to skip the .iso creation and examine the resulting
disk image. Or you can pass ``--keep-image`` to keep it around after the iso has
been created.

Cleaning up aborted ``--no-virt`` installs can sometimes be accomplished by
running the ``anaconda-cleanup`` script. As of Fedora 18 anaconda is
multi-threaded and it can sometimes become stuck and refuse to exit. When this
happens you can usually clean up by first killing the anaconda process then
running ``anaconda-cleanup``.


Hacking
-------

Development on this will take place as part of the lorax project, and on the
anaconda-devel-list mailing list, and `on github <https://github.com/rhinstaller/lorax>`_

Feedback, enhancements and bugs are welcome.  You can use `bugzilla
<https://bugzilla.redhat.com/enter_bug.cgi?product=Red Hat Enterprise Linux 8&component=lorax>`_ to
report bugs against the lorax component.

lorax-composer
==============

:Authors:
    Brian C. Lane <bcl@redhat.com>

``lorax-composer`` is a WELDR API server that allows you to build disk images using
`Blueprints`_ to describe the package versions to be installed into the image.
It is compatible with the Weldr project's bdcs-api REST protocol. More
information on Weldr can be found `on the Weldr blog <http://www.weldr.io>`_.

Behind the scenes it uses `livemedia-creator <livemedia-creator.html>`_ and
`Anaconda <https://anaconda-installer.readthedocs.io/en/latest/>`_ to handle the
installation and configuration of the images.

.. note::

    ``lorax-composer`` is now deprecated. It is being replaced by the
    ``osbuild-composer`` WELDR API server which implements more features (eg.
    ostree, image uploads, etc.) You can still use ``composer-cli`` and
    ``cockpit-composer`` with ``osbuild-composer``. See the documentation or
    the `osbuild website <https://www.osbuild.org/>`_ for more information.


Important Things To Note
------------------------

* As of version 30.7 SELinux can be set to Enforcing. The current state is
  logged for debugging purposes and if there are SELinux denials they should
  be reported as a bug.

* All image types lock the root account, except for live-iso. You will need to either
  use one of the `Customizations`_ methods for setting a ssh key/password, install a
  package that creates a user, or use something like `cloud-init` to setup access at
  boot time.


Installation
------------

The best way to install ``lorax-composer`` is to use ``sudo dnf install
lorax-composer composer-cli``, this will setup the weldr user and install the
systemd socket activation service. You will then need to enable it with ``sudo
systemctl enable lorax-composer.socket && sudo systemctl start
lorax-composer.socket``. This will leave the server off until the first request
is made. Systemd will then launch the server and it will remain running until
the system is rebooted.

Quickstart
----------

1. Create a ``weldr`` user and group by running ``useradd weldr``
2. Remove any pre-existing socket directory with ``rm -rf /run/weldr/``
   A new directory with correct permissions will be created the first time the server runs.
3. Enable the socket activation with ``systemctl enable lorax-composer.socket
   && sudo systemctl start lorax-composer.socket``.

NOTE: You can also run it directly with ``lorax-composer /path/to/blueprints``.  However,
``lorax-composer`` does not react well to being started both on the command line and via
socket activation at the same time.  It is therefore recommended that you run it directly
on the command line only for testing or development purposes.  For real use or development
of other projects that simply use the API, you should stick to socket activation only.

The ``/path/to/blueprints/`` directory is where the blueprints' git repo will
be created, and all the blueprints created with the ``/api/v0/blueprints/new``
route will be stored.  If there are blueprint ``.toml`` files in the top level
of the directory they will be imported into the blueprint git storage when
``lorax-composer`` starts.

Logs
----

Logs are stored under ``/var/log/lorax-composer/`` and include all console
messages as well as extra debugging info and API requests.

Security
--------

Some security related issues that you should be aware of before running ``lorax-composer``:

* One of the API server threads needs to retain root privileges in order to run Anaconda.
* Only allow authorized users access to the ``weldr`` group and socket.

Since Anaconda kickstarts are used there is the possibility that a user could
inject commands into a blueprint that would result in the kickstart executing
arbitrary code on the host.  Only authorized users should be allowed to build
images using ``lorax-composer``.

lorax-composer cmdline arguments
--------------------------------

.. argparse::
   :ref: pylorax.api.cmdline.lorax_composer_parser
   :prog: lorax-composer


How it Works
------------

The server runs as root, and as ``weldr``. Communication with it is via a unix
domain socket (``/run/weldr/api.socket`` by default). The directory and socket
are owned by ``root:weldr`` so that any user in the ``weldr`` group can use the API
to control ``lorax-composer``.

At startup the server will check for the correct permissions and
ownership of a pre-existing directory, or it will create a new one if it
doesn't exist.  The socket path and group owner's name can be changed from the
cmdline by passing it the ``--socket`` and ``--group`` arguments.

It will then drop root privileges for the API thread and run as the ``weldr``
user. The queue and compose thread still runs as root because it needs to be
able to mount/umount files and run Anaconda.

Composing Images
----------------

The `welder-web <https://github.com/weldr/welder-web/>`_ GUI project can be used to construct
blueprints and create composes using a web browser.

Or use the command line with `composer-cli <composer-cli.html>`_.

Blueprints
----------

Blueprints are simple text files in `TOML <https://github.com/toml-lang/toml>`_ format that describe
which packages, and what versions, to install into the image. They can also define a limited set
of customizations to make to the final image.

Example blueprints can be found in the ``lorax-composer`` `test suite
<https://github.com/weldr/lorax/tree/master/tests/pylorax/blueprints/>`_, with a simple one
looking like this::

    name = "base"
    description = "A base system with bash"
    version = "0.0.1"

    [[packages]]
    name = "bash"
    version = "4.4.*"

The ``name`` field is the name of the blueprint. It can contain spaces, but they will be converted to ``-``
when it is written to disk. It should be short and descriptive.

``description`` can be a longer description of the blueprint, it is only used for display purposes.

``version`` is a `semver compatible <https://semver.org/>`_ version number. If
a new blueprint is uploaded with the same ``version`` the server will
automatically bump the PATCH level of the ``version``. If the ``version``
doesn't match it will be used as is. eg. Uploading a blueprint with ``version``
set to ``0.1.0`` when the existing blueprint ``version`` is ``0.0.1`` will
result in the new blueprint being stored as ``version 0.1.0``.

[[packages]] and [[modules]]
~~~~~~~~~~~~~~~~~~~~~~~~~~~~

These entries describe the package names and matching version glob to be installed into the image.

The names must match the names exactly, and the versions can be an exact match
or a filesystem-like glob of the version using ``*`` wildcards and ``?``
character matching.

NOTE: As of lorax-composer-29.2-1 the versions are not used for depsolving,
that is planned for a future release. And currently there are no differences
between ``packages`` and ``modules`` in ``lorax-composer``.

[[groups]]
~~~~~~~~~~

These entries describe a group of packages to be installed into the image.  Package groups are
defined in the repository metadata.  Each group has a descriptive name used primarily for display
in user interfaces and an ID more commonly used in kickstart files.  Here, the ID is the expected
way of listing a group.

Groups have three different ways of categorizing their packages:  mandatory, default, and optional.
For purposes of blueprints, mandatory and default packages will be installed.  There is no mechanism
for selecting optional packages.

Customizations
~~~~~~~~~~~~~~

The ``[customizations]`` section can be used to configure the hostname of the final image. eg.::

    [customizations]
    hostname = "baseimage"

This is optional and may be left out to use the defaults.


[customizations.kernel]
***********************

This allows you to append arguments to the bootloader's kernel commandline. This will not have any
effect on ``tar`` or ``ext4-filesystem`` images since they do not include a bootloader.

For example::

    [customizations.kernel]
    append = "nosmt=force"


[[customizations.sshkey]]
*************************

Set an existing user's ssh key in the final image::

    [[customizations.sshkey]]
    user = "root"
    key = "PUBLIC SSH KEY"

The key will be added to the user's authorized_keys file.


[[customizations.user]]
***********************

Add a user to the image, and/or set their ssh key.
All fields for this section are optional except for the ``name``, here is a complete example::

    [[customizations.user]]
    name = "admin"
    description = "Administrator account"
    password = "$6$CHO2$3rN8eviE2t50lmVyBYihTgVRHcaecmeCk31L..."
    key = "PUBLIC SSH KEY"
    home = "/srv/widget/"
    shell = "/usr/bin/bash"
    groups = ["widget", "users", "wheel"]
    uid = 1200
    gid = 1200

If the password starts with ``$6$``, ``$5$``, or ``$2b$`` it will be stored as
an encrypted password. Otherwise it will be treated as a plain text password.


[[customizations.group]]
************************

Add a group to the image. ``name`` is required and ``gid`` is optional::

    [[customizations.group]]
    name = "widget"
    gid = 1130


[customizations.timezone]
*************************

Customizing the timezone and the NTP servers to use for the system::

    [customizations.timezone]
    timezone = "US/Eastern"
    ntpservers = ["0.north-america.pool.ntp.org", "1.north-america.pool.ntp.org"]

The values supported by ``timezone`` can be listed by running ``timedatectl list-timezones``.

If no timezone is setup the system will default to using `UTC`. The ntp servers are also
optional and will default to using the distribution defaults which are fine for most uses.

In some image types there are already NTP servers setup, eg. Google cloud image, and they
cannot be overridden because they are required to boot in the selected environment. But the
timezone will be updated to the one selected in the blueprint.


[customizations.locale]
***********************

Customize the locale settings for the system::

    [customizations.locale]
    languages = ["en_US.UTF-8"]
    keyboard = "us"

The values supported by ``languages`` can be listed by running ``localectl list-locales`` from
the command line.

The values supported by ``keyboard`` can be listed by running ``localectl list-keymaps`` from
the command line.

Multiple languages can be added. The first one becomes the
primary, and the others are added as secondary. One or the other of ``languages``
or ``keyboard`` must be included (or both) in the section.


[customizations.firewall]
*************************

By default the firewall blocks all access except for services that enable their ports explicitly,
like ``sshd``. This command can be used to open other ports or services. Ports are configured using
the port:protocol format::

    [customizations.firewall]
    ports = ["22:tcp", "80:tcp", "imap:tcp", "53:tcp", "53:udp"]

Numeric ports, or their names from ``/etc/services`` can be used in the ``ports`` enabled/disabled lists.

The blueprint settings extend any existing settings in the image templates, so if ``sshd`` is
already enabled it will extend the list of ports with the ones listed by the blueprint.

If the distribution uses ``firewalld`` you can specify services listed by ``firewall-cmd --get-services``
in a ``customizations.firewall.services`` section::

    [customizations.firewall.services]
    enabled = ["ftp", "ntp", "dhcp"]
    disabled = ["telnet"]

Remember that the ``firewall.services`` are different from the names in ``/etc/services``.

Both are optional, if they are not used leave them out or set them to an empty list ``[]``. If you
only want the default firewall setup this section can be omitted from the blueprint.

NOTE: The ``Google`` and ``OpenStack`` templates explicitly disable the firewall for their environment.
This cannot be overridden by the blueprint.

[customizations.services]
*************************

This section can be used to control which services are enabled at boot time.
Some image types already have services enabled or disabled in order for the
image to work correctly, and cannot be overridden. eg. ``ami`` requires
``sshd``, ``chronyd``, and ``cloud-init``. Without them the image will not
boot. Blueprint services are added to, not replacing, the list already in the
templates, if any.

The service names are systemd service units. You may specify any systemd unit
file accepted by ``systemctl enable`` eg. ``cockpit.socket``::

    [customizations.services]
    enabled = ["sshd", "cockpit.socket", "httpd"]
    disabled = ["postfix", "telnetd"]


[[repos.git]]
~~~~~~~~~~~~~

The ``[[repos.git]]`` entries are used to add files from a `git repository<https://git-scm.com/>`
repository to the created image. The repository is cloned, the specified ``ref`` is checked out
and an rpm is created to install the files to a ``destination`` path. The rpm includes a summary
with the details of the repository and reference used to create it. The rpm is also included in the
image build metadata.

To create an rpm named ``server-config-1.0-1.noarch.rpm`` you would add this to your blueprint::

    [[repos.git]]
    rpmname="server-config"
    rpmversion="1.0"
    rpmrelease="1"
    summary="Setup files for server deployment"
    repo="PATH OF GIT REPO TO CLONE"
    ref="v1.0"
    destination="/opt/server/"

* rpmname: Name of the rpm to create, also used as the prefix name in the tar archive
* rpmversion: Version of the rpm, eg. "1.0.0"
* rpmrelease: Release of the rpm, eg. "1"
* summary: Summary string for the rpm
* repo: URL of the get repo to clone and create the archive from
* ref: Git reference to check out. eg. origin/branch-name, git tag, or git commit hash
* destination: Path to install the / of the git repo at when installing the rpm

An rpm will be created with the contents of the git repository referenced, with the files
being installed under ``/opt/server/`` in this case.

``ref`` can be any valid git reference for use with ``git archive``. eg. to use the head
of a branch set it to ``origin/branch-name``, a tag name, or a commit hash.

Note that the repository is cloned in full each time a build is started, so pointing to a
repository with a large amount of history may take a while to clone and use a significant
amount of disk space. The clone is temporary and is removed once the rpm is created.


Adding Output Types
-------------------

``livemedia-creator`` supports a large number of output types, and only some of
these are currently available via ``lorax-composer``. To add a new output type to
lorax-composer a kickstart file needs to be added to ``./share/composer/``. The
name of the kickstart is what will be used by the ``/compose/types`` route, and the
``compose_type`` field of the POST to start a compose. It also needs to have
code added to the :py:func:`pylorax.api.compose.compose_args` function. The
``_MAP`` entry in this function defines what lorax-composer will pass to
:py:func:`pylorax.installer.novirt_install` when it runs the compose.  When the
compose is finished the output files need to be copied out of the build
directory (``/var/lib/lorax/composer/results/<UUID>/compose/``),
:py:func:`pylorax.api.compose.move_compose_results` handles this for each type.
You should move them instead of copying to save space.

If the new output type does not have support in livemedia-creator it should be
added there first. This will make the output available to the widest number of
users.

Example: Add partitioned disk support
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

Partitioned disk support is something that livemedia-creator already supports
via the ``--make-disk`` cmdline argument. To add this to lorax-composer it
needs 3 things:

* A ``partitioned-disk.ks`` file in ``./share/composer/``
* A new entry in the _MAP in :py:func:`pylorax.api.compose.compose_args`
* Add a bit of code to :py:func:`pylorax.api.compose.move_compose_results` to move the disk image from
  the compose directory to the results directory.

The ``partitioned-disk.ks`` is pretty similar to the example minimal kickstart
in ``./docs/rhel-minimal.ks``. You should remove the ``url`` and ``repo``
commands, they will be added by the compose process. Make sure the bootloader
packages are included in the ``%packages`` section at the end of the kickstart,
and you will want to leave off the ``%end`` so that the compose can append the
list of packages from the blueprint.

The new ``_MAP`` entry should be a copy of one of the existing entries, but with ``make_disk`` set
to ``True``. Make sure that none of the other ``make_*`` options are ``True``. The ``image_name`` is
what the name of the final image will be.

``move_compose_results()`` can be as simple as moving the output file into
the results directory, or it could do some post-processing on it. The end of
the function should always clean up the ``./compose/`` directory, removing any
unneeded extra files. This is especially true for the ``live-iso`` since it produces
the contents of the iso as well as the boot.iso itself.

Package Sources
---------------

By default lorax-composer uses the host's configured repositories. It copies
the ``*.repo`` files from ``/etc/yum.repos.d/`` into
``/var/lib/lorax/composer/repos.d/`` at startup, these are immutable system
repositories and cannot be deleted or changed. If you want to add additional
repos you can put them into ``/var/lib/lorax/composer/repos.d/`` or use the
``/api/v0/projects/source/*`` API routes to create them.

The new source can be added by doing a POST to the ``/api/v0/projects/source/new``
route using JSON (with `Content-Type` header set to `application/json`) or TOML
(with it set to `text/x-toml`).  The format of the source looks like this (in
TOML)::

    name = "custom-source-1"
    url = "https://url/path/to/repository/"
    type = "yum-baseurl"
    proxy = "https://proxy-url/"
    check_ssl = true
    check_gpg = true
    gpgkey_urls = ["https://url/path/to/gpg-key"]

The ``proxy`` and ``gpgkey_urls`` entries are optional. All of the others are required. The supported
types for the urls are:

* ``yum-baseurl`` is a URL to a yum repository.
* ``yum-mirrorlist`` is a URL for a mirrorlist.
* ``yum-metalink`` is a URL for a metalink.

If ``check_ssl`` is true the https certificates must be valid. If they are self-signed you can either set
this to false, or add your Certificate Authority to the host system.

If ``check_gpg`` is true the GPG key must either be installed on the host system, or ``gpgkey_urls``
should point to it.

You can edit an existing source (other than system sources), by doing a POST to the ``new`` route
with the new version of the source. It will overwrite the previous one.

A list of existing sources is available from ``/api/v0/projects/source/list``, and detailed info
on a source can be retrieved with the ``/api/v0/projects/source/info/<source-name>`` route. By default
it returns JSON but it can also return TOML if ``?format=toml`` is added to the request.

Non-system sources can be deleted by doing a ``DELETE`` request to the
``/api/v0/projects/source/delete/<source-name>`` route.

The documentation for the source API routes can be `found here <pylorax.api.html#api-v0-projects-source-list>`_

The configured sources are used for all blueprint depsolve operations, and for composing images.
When adding additional sources you must make sure that the packages in the source do not
conflict with any other package sources, otherwise depsolving will fail.

DVD ISO Package Source
~~~~~~~~~~~~~~~~~~~~~~

In some situations the system may want to *only* use a DVD iso as the package
source, not the repos from the network. ``lorax-composer`` and ``anaconda``
understand ``file://`` URLs so you can mount an iso on the host, and replace the
system repo files with a configuration file pointing to the DVD.

* Stop the ``lorax-composer.service`` if it is running
* Move the repo files in ``/etc/yum.repos.d/`` someplace safe
* Create a new ``iso.repo`` file in ``/etc/yum.repos.d/``::

     [iso]
     name=iso
     baseurl=file:///mnt/iso/
     enabled=1
     gpgcheck=1
     gpgkey=file:///mnt/iso/RPM-GPG-KEY-redhat-release

* Remove all the cached repo files from ``/var/lib/lorax/composer/repos/``
* Restart the ``lorax-composer.service``
* Check the output of ``composer-cli status show`` for any output specific depsolve errors.
  For example, the DVD usually does not include ``grub2-efi-*-cdboot-*`` so the live-iso image
  type will not be available.

If you want to *add* the DVD source to the existing sources you can do that by
mounting the iso and creating a source file to point to it as described in the
`Package Sources`_ documentation.  In that case there is no need to remove the other
sources from ``/etc/yum.repos.d/`` or clear the cached repos.
Lorax
=====

:Authors:
    Brian C. Lane <bcl@redhat.com>

"I am the Lorax.  I speak for the trees [and images]."

The `lorax <https://github.com/rhinstaller/lorax>`_ tool is used to create the
`Anaconda <https://github.com/rhinstaller/anaconda>`_ installer boot.iso as
well as the basic release tree, and .treeinfo metadata file. Its dependencies
are fairly light-weight because it needs to be able to run in a mock chroot
environment. It is best to run lorax from the same release as is being targeted
because the templates may have release specific logic in them. eg. Use the
rawhide version to build the boot.iso for rawhide, along with the rawhide
repositories.


lorax cmdline arguments
-----------------------

.. argparse::
    :ref: pylorax.cmdline.lorax_parser
    :prog: lorax

    --macboot : @replace
        Make the iso bootable on UEFI based Mac systems

        Default: True

    --nomacboot : @replace
        Do not create a Mac bootable iso

        Default: False


Quickstart
----------

Run this as root to create a boot.iso in ``./results/``::

    dnf install lorax
    setenforce 0
    lorax -p Fedora -v 23 -r 23 \
    -s http://dl.fedoraproject.org/pub/fedora/linux/releases/23/Everything/x86_64/os/ \
    -s http://dl.fedoraproject.org/pub/fedora/linux/updates/23/x86_64/ \
    ./results/
    setenforce 1

You can add your own repos with ``-s`` and packages with higher NVRs will
override the ones in the distribution repositories.

Under ``./results/`` will be the release tree files: .discinfo, .treeinfo, everything that
goes onto the boot.iso, the pxeboot directory, and the boot.iso under ``./images/``.


Branding
--------

By default lorax will search for the first package that provides ``system-release``
that doesn't start with ``generic-`` and will install it. It then selects a
corresponding logo package by using the first part of the system-release package and
appending ``-logos`` to it. eg. fedora-release and fedora-logos.

Custom Branding
~~~~~~~~~~~~~~~

If ``--skip-branding`` is passed to lorax it will skip selecting the
``system-release``, and logos packages and leave it up to the user to pass any
branding related packages to lorax using ``--installpkgs``. When using
``skip-branding`` you must make sure that you provide all of the expected files,
otherwise Anaconda may not work as expected. See the contents of ``fedora-release``
and ``fedora-logos`` for examples of what to include.

Note that this does not prevent something else in the dependency tree from
causing these packages to be included. Using ``--excludepkgs`` may help if they
are unexpectedly included.


Running inside of mock
----------------------

If you are using lorax with mock v1.3.4 or later you will need to pass
``--old-chroot`` to mock. Mock now defaults to using systemd-nspawn which cannot
create the needed loop device nodes. Passing ``--old-chroot`` will use the old
system where ``/dev/loop*`` is setup for you.


How it works
------------

Lorax uses `dnf <https://github.com/rpm-software-management/dnf>`_ to install
packages into a temporary directory, sets up configuration files, it then
removes unneeded files to save space, and creates a squashfs filesystem of the
files.  The iso is then built using a generic initramfs and the kernel from the
selected repositories.

To drive these processes Lorax uses a custom template system, based on `Mako
templates <http://www.makotemplates.org/>`_ with the addition of custom
commands (documented in :class:`pylorax.ltmpl.LoraxTemplateRunner`). Mako
supports ``%if/%endif`` blocks as well as free-form python code inside ``<%
%>`` tags and variable substitution with ``${}``. The default templates are
shipped with lorax in ``/usr/share/lorax/templates.d/99-generic/`` and use the
``.tmpl`` extension.


runtime-install.tmpl
~~~~~~~~~~~~~~~~~~~~

The ``runtime-install.tmpl`` template lists packages to be installed using the
``installpkg`` command.  This template is fairly simple, installing common packages and
architecture specific packages. It must end with the ``run_pkg_transaction``
command which tells dnf to download and install the packages.


runtime-postinstall.tmpl
~~~~~~~~~~~~~~~~~~~~~~~~

The ``runtime-postinstall.tmpl`` template is where the system configuration
happens. The installer environment is similar to a normal running system, but
needs some special handling. Configuration files are setup, systemd is told to
start the anaconda.target instead of a default system target, and a number of
unneeded services are disabled, some of which can interfere with the
installation. A number of template commands are used here:

* :func:`append <pylorax.ltmpl.LoraxTemplateRunner.append>` to add text to a file.
* :func:`chmod <pylorax.ltmpl.LoraxTemplateRunner.chmod>` changes the file's mode.
* :func:`install <pylorax.ltmpl.LoraxTemplateRunner.install>` to install a file into the installroot.
* :func:`mkdir <pylorax.ltmpl.LoraxTemplateRunner.mkdir>` makes a new directory.
* :func:`move <pylorax.ltmpl.LoraxTemplateRunner.move>` to move a file into the installroot
* :func:`replace <pylorax.ltmpl.LoraxTemplateRunner.replace>` does text substitution in a file
* :func:`remove <pylorax.ltmpl.LoraxTemplateRunner.remove>` deletes a file
* :func:`runcmd <pylorax.ltmpl.LoraxTemplateRunner.runcmd>` run arbitrary commands.
* :func:`symlink <pylorax.ltmpl.LoraxTemplateRunner.symlink>` creates a symlink
* :func:`systemctl <pylorax.ltmpl.LoraxTemplateRunner.systemctl>` runs systemctl in the installroot


runtime-cleanup.tmpl
~~~~~~~~~~~~~~~~~~~~

The ``runtime-cleanup.tmpl`` template is used to remove files that aren't strictly needed
by the installation environment. In addition to the ``remove`` template command it uses:

* :func:`removepkg <pylorax.ltmpl.LoraxTemplateRunner.removepkg>`
  remove all of a specific package's contents. A package may be pulled in as a dependency, but
  not really used. eg. sound support.
* :func:`removefrom <pylorax.ltmpl.LoraxTemplateRunner.removefrom>`
  Removes some files from a package. A file glob can be used, or the --allbut option to 
  remove everything except a select few.
* :func:`removekmod <pylorax.ltmpl.LoraxTemplateRunner.removekmod>`
  Removes kernel modules


The squashfs filesystem
~~~~~~~~~~~~~~~~~~~~~~~

After ``runtime-*.tmpl`` templates have finished their work lorax creates an
empty ext4 filesystem, copies the remaining files to it, and makes a squashfs
filesystem of it. This file is the / of the boot.iso's installer environment
and is what is in the LiveOS/squashfs.img file on the iso.


iso creation
~~~~~~~~~~~~

The iso creation is handled by another set of templates. The one used depends
on the architecture that the iso is being created for. They are also stored in
``/usr/share/lorax/templates.d/99-generic`` and are named after the arch, like
``x86.tmpl`` and ``aarch64.tmpl``. They handle creation of the tree, copying
configuration template files, configuration variable substitution, treeinfo
metadata (via the :func:`treeinfo <pylorax.ltmpl.LoraxTemplateRunner.treeinfo>`
template command). Kernel and initrd are copied from the installroot to their
final locations and then mkisofs is run to create the boot.iso


Custom Templates
----------------

The default set of templates and configuration files from the lorax-generic-templates package
are shipped in the ``/usr/share/lorax/templates.d/99-generic/`` directory. You can
make a copy of them and place them into another directory under ``templates.d``
and they will be used instead if their sort order is below all other directories. This
allows multiple packages to ship lorax templates without conflict. You can (and probably
should) select the specific template directory by passing ``--sharedir`` to lorax.

Product and Updates Images
==========================

Lorax now supports creation of product.img and updates.img as part of the build
process. This is implemented using the installimg template command which will
take the contents of a directory and create a compressed archive from it. The
directory must be created by one of the packages installed by
runtime-install.tmpl or by passing ``--installpkgs <pkgname>`` to lorax at
runtime.  The x86, ppc, ppc64le and aarch64 templates all look for
/usr/share/lorax/product/ and /usr/share/lorax/updates/ directories in the
install chroot while creating the final install tree. If there are files in
those directories lorax will create images/product.img and/or
images/updates.img

These archives are just like an anaconda updates image -- their contents are
copied over the top of the filesystem at boot time so that you can drop in
files to add to or replace anything on the filesystem.

Anaconda has several places that it looks for updates, the one for product.img
is in /run/install/product.  So for example, to add an installclass to Anaconda
you would put your custom class here:

``/usr/share/lorax/product/run/install/product/pyanaconda/installclasses/custom.py``

If the packages containing the product/updates files are not included as part
of normal dependencies you can add specific packages with the ``--installpkgs``
command or the installpkgs paramater of :class:`pylorax.treebuilder.RuntimeBuilder`
Requests-FTP
============

Requests-FTP is an implementation of a very stupid FTP transport adapter for
use with the awesome `Requests`_ Python library.

This library is *not* intended to be an example of Transport Adapters best
practices. This library was cowboyed together in about 4 hours of total work,
has no tests, and relies on a few ugly hacks. Instead, it is intended as both
a starting point for future development and a useful example for how to
implement transport adapters.

Here's how you use it:

.. code-block:: pycon

    >>> import requests
    >>> import requests_ftp
    >>> requests_ftp.monkeypatch_session()
    >>> s = requests.Session()
    >>> resp = s.list('ftp://127.0.0.1/', auth=('Lukasa', 'notmypass'))
    >>> resp.status_code
    '226'
    >>> print resp.content
    ...snip...
    >>> resp = s.stor('ftp://127.0.0.1/test.txt', auth=('Lukasa', 'notmypass'),
                       files={'file': open('report.txt', 'rb')})


Features
--------

Almost none!

- Adds the FTP LIST, STOR, RETR and NLST verbs via a new FTP transport adapter.
- Provides a function that monkeypatches the Requests Session object, exposing
  helper methods much like the current ``Session.get()`` and ``Session.post()``
  methods.
- Piggybacks on standard Requests idioms: uses normal Requests models and
  access methods, including the tuple form of authentication.

Does not provide:

- Connection pooling! One new connection and multiple commands for each
  request, including authentication. **Super** inefficient.
- SFTP. Security is for the weak.
- Less common commands.

Important Notes
---------------

Many corners have been cut in my rush to get this code finished. The most
obvious problem is that this code does not have *any* tests. This is my highest
priority for fixing.

More notably, we have the following important caveats:

- The design of the Requests Transport Adapater means that the STOR method
  has to un-encode a multipart form-data encoded body to get the file. This is
  painful, and I haven't tested this thoroughly, so it might not work.
- **Massive** assumptions have been made in the use of the STOR method. This
  code assumes that there will only be one file included in the files argument.
  It also requires that you provide the filename to save as as part of the URL.
  This is single-handedly the most brittle part of this adapter.
- This code is not optimised for performance AT ALL. There is some low-hanging
  fruit here: we should be able to connection pool relatively easily, and we
  can probably avoid making some of the requests we do.

Contributing
------------

Please do! I would love for this to be developed further by anyone who is
interested. Wherever possible, please provide unit tests for your work (yes,
this is very much a 'do as I say, not as I do' kind of moment). Don't forget
to add your name to AUTHORS.

License
-------

To maximise compatibility with Requests, this code is licensed under the Apache
license. See LICENSE for more details.

.. _`Requests`: https://github.com/kennethreitz/requestsRequests-File
=============

Requests-File is a transport adapter for use with the `Requests`_ Python
library to allow local filesystem access via file:\/\/ URLs.

To use:

.. code-block:: python

    import requests
    from requests_file import FileAdapter

    s = requests.Session()
    s.mount('file://', FileAdapter())

    resp = s.get('file:///path/to/file')

Features
--------

- Will open and read local files
- Might set a Content-Length header
- That's about it

No encoding information is set in the response object, so be careful using
Response.text: the chardet library will be used to convert the file to a
unicode type and it may not detect what you actually want.

EACCES is converted to a 403 status code, and ENOENT is converted to a
404. All other IOError types are converted to a 400.

Contributing
------------

Contributions welcome! Feel free to open a pull request against
https://github.com/dashea/requests-file

License
-------

To maximise compatibility with Requests, this code is licensed under the Apache
license. See LICENSE for more details.

.. _`Requests`: https://github.com/kennethreitz/requests
pid
===

.. image:: https://travis-ci.org/trbs/pid.svg?branch=master
    :target: https://travis-ci.org/trbs/pid

.. image:: https://coveralls.io/repos/trbs/pid/badge.png
    :target: https://coveralls.io/r/trbs/pid

.. image:: https://pypip.in/v/pid/badge.png
    :target: https://pypi.python.org/pypi/pid/
    :alt: Latest PyPI version

.. image:: https://pypip.in/d/pid/badge.png
    :target: https://pypi.python.org/pypi/pid/
    :alt: Number of PyPI downloads

PidFile class featuring:

 - stale detection
 - pidfile locking (fcntl)
 - chmod (default is 0o644)
 - chown
 - custom exceptions

Context Manager
---------------

PidFile can be used as a context manager::

  from pid import PidFile
  
  with PidFile():
    do_something()


Decorator
---------

PidFile can also be used a a decorator::

  from pid.decorator import pidfile
  
  @pidfile()
  def main():
    pass

  if __name__ == "__main__":
    main()


Exception Order
---------------

In default mode PidFile will try to acquire a file lock before anything else.
This means that normally you get a PidFileAlreadyLockedError instead of the
PidFileAlreadyRunningError when running a program twice.

If you just want to know if a program is already running its easiest to catch
just PidFileError since it will capture all possible PidFile exceptions.

Behaviour
---------

Changes in version 2.0.0 and going forward:

* pid is now friendly with daemon context managers such as 
  `python-daemon <https://pypi.python.org/pypi/python-daemon/>`_ where
  the PidFile context manager is passed as a parameter. The
  new corrected behaviour will ensure the process environment is
  determinde at the time of acquiring/checking the lock. Prior
  behaviour would determine the process environment when
  instancing the class which may result in incorrect determination
  of the PID in the case of a process forking after instancing
  PidFile.

\

* Cleanup of pidfile on termination is done using `atexit` module.
  The default SIGTERM handler doesn't cleanly exit and therefore
  the atexit registered functions will not execute. A custom
  handler which triggers the atexit registered functions for cleanup
  will override the default SIGTERM handler. If a prior signal handler
  has been configured, then it will not be overridden.
Jinja Changelog
===============


Version 2.10.1
--------------

Released 2019-04-06

-   ``SandboxedEnvironment`` securely handles ``str.format_map`` in
    order to prevent code execution through untrusted format strings.
    The sandbox already handled ``str.format``.


Version 2.10
------------

released on November 8th 2017

- Added a new extension node called ``OverlayScope`` which can be used to
  create an unoptimized scope that will look up all variables from a
  derived context.
- Added an ``in`` test that works like the in operator.  This can be used
  in combination with ``reject`` and ``select``.
- Added ``previtem`` and ``nextitem`` to loop contexts, providing access to the
  previous/next item in the loop. If such an item does not exist, the value is
  undefined.
- Added ``changed(*values)`` to loop contexts, providing an easy way of
  checking whether a value has changed since the last iteration (or rather
  since the last call of the method)
- Added a ``namespace`` function that creates a special object which allows
  attribute assignment using the ``set`` tag.  This can be used to carry data
  across scopes, e.g. from a loop body to code that comes after the loop.
- Added a ``trimmed`` modifier to ``{% trans %}`` to strip linebreaks and
  surrounding whitespace. Also added a new policy to enable this for all
  ``trans`` blocks.
- The ``random`` filter is no longer incorrectly constant folded and will
  produce a new random choice each time the template is rendered. (`#478`_)
- Added a ``unique`` filter. (`#469`_)
- Added ``min`` and ``max`` filters. (`#475`_)
- Added tests for all comparison operators: ``eq``, ``ne``, ``lt``, ``le``,
  ``gt``, ``ge``. (`#665`_)
- ``import`` statement cannot end with a trailing comma. (`#617`_, `#618`_)
- ``indent`` filter will not indent blank lines by default. (`#685`_)
- Add ``reverse`` argument for ``dictsort`` filter. (`#692`_)
- Add a ``NativeEnvironment`` that renders templates to native Python types
  instead of strings. (`#708`_)
- Added filter support to the block ``set`` tag. (`#489`_)
- ``tojson`` filter marks output as safe to match documented behavior.
  (`#718`_)
- Resolved a bug where getting debug locals for tracebacks could
  modify template context.
- Fixed a bug where having many ``{% elif ... %}`` blocks resulted in a
  "too many levels of indentation" error.  These blocks now compile to
  native ``elif ..:`` instead of ``else: if ..:`` (`#759`_)

.. _#469: https://github.com/pallets/jinja/pull/469
.. _#475: https://github.com/pallets/jinja/pull/475
.. _#478: https://github.com/pallets/jinja/pull/478
.. _#489: https://github.com/pallets/jinja/pull/489
.. _#617: https://github.com/pallets/jinja/pull/617
.. _#618: https://github.com/pallets/jinja/pull/618
.. _#665: https://github.com/pallets/jinja/pull/665
.. _#685: https://github.com/pallets/jinja/pull/685
.. _#692: https://github.com/pallets/jinja/pull/692
.. _#708: https://github.com/pallets/jinja/pull/708
.. _#718: https://github.com/pallets/jinja/pull/718
.. _#759: https://github.com/pallets/jinja/pull/759


Version 2.9.6
-------------

(bugfix release, released on April 3rd 2017)

- Fixed custom context behavior in fast resolve mode (#675)


Version 2.9.5
-------------

(bugfix release, released on January 28th 2017)

- Restored the original repr of the internal ``_GroupTuple`` because this
  caused issues with ansible and it was an unintended change.  (#654)
- Added back support for custom contexts that override the old ``resolve``
  method since it was hard for people to spot that this could cause a
  regression.
- Correctly use the buffer for the else block of for loops.  This caused
  invalid syntax errors to be caused on 2.x and completely wrong behavior
  on Python 3 (#669)
- Resolve an issue where the ``{% extends %}`` tag could not be used with
  async environments. (#668)
- Reduce memory footprint slightly by reducing our unicode database dump
  we use for identifier matching on Python 3 (#666)
- Fixed autoescaping not working for macros in async compilation mode. (#671)


Version 2.9.4
-------------

(bugfix release, released on January 10th 2017)

- Solved some warnings for string literals.  (#646)
- Increment the bytecode cache version which was not done due to an
  oversight before.
- Corrected bad code generation and scoping for filtered loops.  (#649)
- Resolved an issue where top-level output silencing after known extend
  blocks could generate invalid code when blocks where contained in if
  statements.  (#651)
- Made the ``truncate.leeway`` default configurable to improve compatibility
  with older templates.


Version 2.9.3
-------------

(bugfix release, released on January 8th 2017)

- Restored the use of blocks in macros to the extend that was possible
  before.  On Python 3 it would render a generator repr instead of
  the block contents. (#645)
- Set a consistent behavior for assigning of variables in inner scopes
  when the variable is also read from an outer scope.  This now sets the
  intended behavior in all situations however it does not restore the
  old behavior where limited assignments to outer scopes was possible.
  For more information and a discussion see #641
- Resolved an issue where ``block scoped`` would not take advantage of the
  new scoping rules.  In some more exotic cases a variable overriden in a
  local scope would not make it into a block.
- Change the code generation of the ``with`` statement to be in line with the
  new scoping rules.  This resolves some unlikely bugs in edge cases.  This
  also introduces a new internal ``With`` node that can be used by extensions.


Version 2.9.2
-------------

(bugfix release, released on January 8th 2017)

- Fixed a regression that caused for loops to not be able to use the same
  variable for the target as well as source iterator.  (#640)
- Add support for a previously unknown behavior of macros.  It used to be
  possible in some circumstances to explicitly provide a caller argument
  to macros.  While badly buggy and unintended it turns out that this is a
  common case that gets copy pasted around.  To not completely break backwards
  compatibility with the most common cases it's now possible to provide an
  explicit keyword argument for caller if it's given an explicit default.
  (#642)


Version 2.9.1
-------------

(bugfix release, released on January 7th 2017)

- Resolved a regression with call block scoping for macros.  Nested caller
  blocks that used the same identifiers as outer macros could refer to the
  wrong variable incorrectly.


Version 2.9
-----------
(codename Derivation, released on January 7th 2017)

- Change cache key definition in environment. This fixes a performance
  regression introduced in 2.8.
- Added support for ``generator_stop`` on supported Python versions
  (Python 3.5 and later)
- Corrected a long standing issue with operator precedence of math operations
  not being what was expected.
- Added support for Python 3.6 async iterators through a new async mode.
- Added policies for filter defaults and similar things.
- urlize now sets "rel noopener" by default.
- Support attribute fallback for old-style classes in 2.x.
- Support toplevel set statements in extend situations.
- Restored behavior of Cycler for Python 3 users.
- Subtraction now follows the same behavior as other operators on undefined
  values.
- ``map`` and friends will now give better error messages if you forgot to
  quote the parameter.
- Depend on MarkupSafe 0.23 or higher.
- Improved the ``truncate`` filter to support better truncation in case
  the string is barely truncated at all.
- Change the logic for macro autoescaping to be based on the runtime
  autoescaping information at call time instead of macro define time.
- Ported a modified version of the ``tojson`` filter from Flask to Jinja2
  and hooked it up with the new policy framework.
- Block sets are now marked ``safe`` by default.
- On Python 2 the asciification of ASCII strings can now be disabled with
  the ``compiler.ascii_str`` policy.
- Tests now no longer accept an arbitrary expression as first argument but
  a restricted one.  This means that you can now properly use multiple
  tests in one expression without extra parentheses.  In particular you can
  now write ``foo is divisibleby 2 or foo is divisibleby 3``
  as you would expect.
- Greatly changed the scoping system to be more consistent with what template
  designers and developers expect.  There is now no more magic difference
  between the different include and import constructs.  Context is now always
  propagated the same way.  The only remaining differences is the defaults
  for ``with context`` and ``without context``.
- The ``with`` and ``autoescape`` tags are now built-in.
- Added the new ``select_autoescape`` function which helps configuring better
  autoescaping easier.
- Fixed a runtime error in the sandbox when attributes of async generators
  were accessed.


Version 2.8.1
-------------

(bugfix release, released on December 29th 2016)

- Fixed the ``for_qs`` flag for ``urlencode``.
- Fixed regression when applying ``int`` to non-string values.
- SECURITY: if the sandbox mode is used format expressions are now sandboxed
  with the same rules as in Jinja.  This solves various information leakage
  problems that can occur with format strings.


Version 2.8
-----------

(codename Replacement, released on July 26th 2015)

- Added ``target`` parameter to urlize function.
- Added support for ``followsymlinks`` to the file system loader.
- The truncate filter now counts the length.
- Added equalto filter that helps with select filters.
- Changed cache keys to use absolute file names if available
  instead of load names.
- Fixed loop length calculation for some iterators.
- Changed how Jinja2 enforces strings to be native strings in
  Python 2 to work when people break their default encoding.
- Added :func:`make_logging_undefined` which returns an undefined
  object that logs failures into a logger.
- If unmarshalling of cached data fails the template will be
  reloaded now.
- Implemented a block ``set`` tag.
- Default cache size was increased to 400 from a low 50.
- Fixed ``is number`` test to accept long integers in all Python versions.
- Changed ``is number`` to accept Decimal as a number.
- Added a check for default arguments followed by non-default arguments. This
  change makes ``{% macro m(x, y=1, z) %}...{% endmacro %}`` a syntax error.
  The previous behavior for this code was broken anyway (resulting in the
  default value being applied to ``y``).
- Add ability to use custom subclasses of ``jinja2.compiler.CodeGenerator`` and
  ``jinja2.runtime.Context`` by adding two new attributes to the environment
  (``code_generator_class`` and ``context_class``) (pull request ``#404``).
- added support for context/environment/evalctx decorator functions on
  the finalize callback of the environment.
- escape query strings for urlencode properly.  Previously slashes were not
  escaped in that place.
- Add 'base' parameter to 'int' filter.


Version 2.7.3
-------------

(bugfix release, released on June 6th 2014)

- Security issue: Corrected the security fix for the cache folder.  This
  fix was provided by RedHat.


Version 2.7.2
-------------

(bugfix release, released on January 10th 2014)

- Prefix loader was not forwarding the locals properly to
  inner loaders.  This is now fixed.
- Security issue: Changed the default folder for the filesystem cache to be
  user specific and read and write protected on UNIX systems.  See
  `Debian bug 734747`_ for more information.

.. _Debian bug 734747: http://bugs.debian.org/cgi-bin/bugreport.cgi?bug=734747


Version 2.7.1
-------------

(bugfix release, released on August 7th 2013)

- Fixed a bug with ``call_filter`` not working properly on environment
  and context filters.
- Fixed lack of Python 3 support for bytecode caches.
- Reverted support for defining blocks in included templates as this
  broke existing templates for users.
- Fixed some warnings with hashing of undefineds and nodes if Python
  is run with warnings for Python 3.
- Added support for properly hashing undefined objects.
- Fixed a bug with the title filter not working on already uppercase
  strings.


Version 2.7
-----------

(codename Translation, released on May 20th 2013)

- Choice and prefix loaders now dispatch source and template lookup
  separately in order to work in combination with module loaders as
  advertised.
- Fixed filesizeformat.
- Added a non-silent option for babel extraction.
- Added ``urlencode`` filter that automatically quotes values for
  URL safe usage with utf-8 as only supported encoding.  If applications
  want to change this encoding they can override the filter.
- Added ``keep-trailing-newline`` configuration to environments and
  templates to optionally preserve the final trailing newline.
- Accessing ``last`` on the loop context no longer causes the iterator
  to be consumed into a list.
- Python requirement changed: 2.6, 2.7 or >= 3.3 are required now,
  supported by same source code, using the "six" compatibility library.
- Allow ``contextfunction`` and other decorators to be applied to ``__call__``.
- Added support for changing from newline to different signs in the ``wordwrap``
  filter.
- Added support for ignoring memcache errors silently.
- Added support for keeping the trailing newline in templates.
- Added finer grained support for stripping whitespace on the left side
  of blocks.
- Added ``map``, ``select``, ``reject``, ``selectattr`` and ``rejectattr``
  filters.
- Added support for ``loop.depth`` to figure out how deep inside a recursive
  loop the code is.
- Disabled py_compile for pypy and python 3.


Version 2.6
-----------

(codename Convolution, released on July 24th 2011)

- internal attributes now raise an internal attribute error now instead
  of returning an undefined.  This fixes problems when passing undefined
  objects to Python semantics expecting APIs.
- traceback support now works properly for PyPy.  (Tested with 1.4)
- implemented operator intercepting for sandboxed environments.  This
  allows application developers to disable builtin operators for better
  security.  (For instance limit the mathematical operators to actual
  integers instead of longs)
- groupby filter now supports dotted notation for grouping by attributes
  of attributes.
- scoped blocks now properly treat toplevel assignments and imports.
  Previously an import suddenly "disappeared" in a scoped block.
- automatically detect newer Python interpreter versions before loading code
  from bytecode caches to prevent segfaults on invalid opcodes.  The segfault
  in earlier Jinja2 versions here was not a Jinja2 bug but a limitation in
  the underlying Python interpreter.  If you notice Jinja2 segfaulting in
  earlier versions after an upgrade of the Python interpreter you don't have
  to upgrade, it's enough to flush the bytecode cache.  This just no longer
  makes this necessary, Jinja2 will automatically detect these cases now.
- the sum filter can now sum up values by attribute.  This is a backwards
  incompatible change.  The argument to the filter previously was the
  optional starting index which defaults to zero.  This now became the
  second argument to the function because it's rarely used.
- like sum, sort now also makes it possible to order items by attribute.
- like sum and sort, join now also is able to join attributes of objects
  as string.
- the internal eval context now has a reference to the environment.
- added a mapping test to see if an object is a dict or an object with
  a similar interface.


Version 2.5.5
-------------

(re-release of 2.5.4 with built documentation removed for filesize.
 Released on October 18th 2010)

- built documentation is no longer part of release.


Version 2.5.4
-------------

(bugfix release, released on October 17th 2010)

- Fixed extensions not loading properly with overlays.
- Work around a bug in cpython for the debugger that causes segfaults
  on 64bit big-endian architectures.


Version 2.5.3
-------------

(bugfix release, released on October 17th 2010)

- fixed an operator precedence error introduced in 2.5.2.  Statements
  like "-foo.bar" had their implicit parentheses applied around the
  first part of the expression ("(-foo).bar") instead of the more
  correct "-(foo.bar)".


Version 2.5.2
-------------
(bugfix release, released on August 18th 2010)

- improved setup.py script to better work with assumptions people
  might still have from it (``--with-speedups``).
- fixed a packaging error that excluded the new debug support.


Version 2.5.1
-------------

(bugfix release, released on August 17th 2010)

- StopIteration exceptions raised by functions called from templates
  are now intercepted and converted to undefineds.  This solves a
  lot of debugging grief.  (StopIteration is used internally to
  abort template execution)
- improved performance of macro calls slightly.
- babel extraction can now properly extract newstyle gettext calls.
- using the variable ``num`` in newstyle gettext for something else
  than the pluralize count will no longer raise a :exc:`KeyError`.
- removed builtin markup class and switched to markupsafe.  For backwards
  compatibility the pure Python implementation still exists but is
  pulled from markupsafe by the Jinja2 developers.  The debug support
  went into a separate feature called "debugsupport" and is disabled
  by default because it is only relevant for Python 2.4
- fixed an issue with unary operators having the wrong precedence.


Version 2.5
-----------

(codename Incoherence, released on May 29th 2010)

- improved the sort filter (should have worked like this for a
  long time) by adding support for case insensitive searches.
- fixed a bug for getattribute constant folding.
- support for newstyle gettext translations which result in a
  nicer in-template user interface and more consistent
  catalogs. (:ref:`newstyle-gettext`)
- it's now possible to register extensions after an environment
  was created.


Version 2.4.1
-------------

(bugfix release, released on April 20th 2010)

- fixed an error reporting bug for undefineds.


Version 2.4
-----------

(codename Correlation, released on April 13th 2010)

- the environment template loading functions now transparently
  pass through a template object if it was passed to it.  This
  makes it possible to import or extend from a template object
  that was passed to the template.
- added a :class:`ModuleLoader` that can load templates from
  precompiled sources.  The environment now features a method
  to compile the templates from a configured loader into a zip
  file or folder.
- the _speedups C extension now supports Python 3.
- added support for autoescaping toggling sections and support
  for evaluation contexts (:ref:`eval-context`).
- extensions have a priority now.


Version 2.3.1
-------------

(bugfix release, released on February 19th 2010)

- fixed an error reporting bug on all python versions
- fixed an error reporting bug on Python 2.4


Version 2.3
-----------

(codename 3000 Pythons, released on February 10th 2010)

- fixes issue with code generator that causes unbound variables
  to be generated if set was used in if-blocks and other small
  identifier problems.
- include tags are now able to select between multiple templates
  and take the first that exists, if a list of templates is
  given.
- fixed a problem with having call blocks in outer scopes that
  have an argument that is also used as local variable in an
  inner frame (#360).
- greatly improved error message reporting (#339)
- implicit tuple expressions can no longer be totally empty.
  This change makes ``{% if %}...{% endif %}`` a syntax error
  now. (#364)
- added support for translator comments if extracted via babel.
- added with-statement extension.
- experimental Python 3 support.


Version 2.2.1
-------------

(bugfix release, released on September 14th 2009)

- fixes some smaller problems for Jinja2 on Jython.


Version 2.2
-----------

(codename Kong, released on September 13th 2009)

- Include statements can now be marked with ``ignore missing`` to skip
  non existing templates.
- Priority of ``not`` raised.  It's now possible to write `not foo in bar`
  as an alias to `foo not in bar` like in python.  Previously the grammar
  required parentheses (`not (foo in bar)`) which was odd.
- Fixed a bug that caused syntax errors when defining macros or using the
  `{% call %}` tag inside loops.
- Fixed a bug in the parser that made ``{{ foo[1, 2] }}`` impossible.
- Made it possible to refer to names from outer scopes in included templates
  that were unused in the callers frame (#327)
- Fixed a bug that caused internal errors if names where used as iteration
  variable and regular variable *after* the loop if that variable was unused
  *before* the loop.  (#331)
- Added support for optional ``scoped`` modifier to blocks.
- Added support for line-comments.
- Added the ``meta`` module.
- Renamed (undocumented) attribute "overlay" to "overlayed" on the
  environment because it was clashing with a method of the same name.
- speedup extension is now disabled by default.


Version 2.1.1
-------------

(bugfix release, released on December 25th 2008)

- Fixed a translation error caused by looping over empty recursive loops.


Version 2.1
-----------

(codename Yasuzō, released on November 23rd 2008)

- fixed a bug with nested loops and the special loop variable.  Before the
  change an inner loop overwrote the loop variable from the outer one after
  iteration.
- fixed a bug with the i18n extension that caused the explicit pluralization
  block to look up the wrong variable.
- fixed a limitation in the lexer that made ``{{ foo.0.0 }}`` impossible.
- index based subscribing of variables with a constant value returns an
  undefined object now instead of raising an index error.  This was a bug
  caused by eager optimizing.
- the i18n extension looks up ``foo.ugettext`` now followed by ``foo.gettext``
  if an translations object is installed.  This makes dealing with custom
  translations classes easier.
- fixed a confusing behavior with conditional extending.  loops were partially
  executed under some conditions even though they were not part of a visible
  area.
- added ``sort`` filter that works like ``dictsort`` but for arbitrary sequences.
- fixed a bug with empty statements in macros.
- implemented a bytecode cache system.  (:ref:`bytecode-cache`)
- the template context is now weakref-able
- inclusions and imports "with context" forward all variables now, not only
  the initial context.
- added a cycle helper called ``cycler``.
- added a joining helper called ``joiner``.
- added a ``compile_expression`` method to the environment that allows compiling
  of Jinja expressions into callable Python objects.
- fixed an escaping bug in urlize


Version 2.0
-----------

(codename jinjavitus, released on July 17th 2008)

- the subscribing of objects (looking up attributes and items) changed from
  slightly.  It's now possible to give attributes or items a higher priority
  by either using dot-notation lookup or the bracket syntax.  This also
  changed the AST slightly.  ``Subscript`` is gone and was replaced with
  :class:`~jinja2.nodes.Getitem` and :class:`~jinja2.nodes.Getattr`.

  For more information see :ref:`the implementation details <notes-on-subscriptions>`.
- added support for preprocessing and token stream filtering for extensions.
  This would allow extensions to allow simplified gettext calls in template
  data and something similar.
- added :meth:`jinja2.environment.TemplateStream.dump`.
- added missing support for implicit string literal concatenation.
  ``{{ "foo" "bar" }}`` is equivalent to ``{{ "foobar" }}``
- ``else`` is optional for conditional expressions.  If not given it evaluates
  to ``false``.
- improved error reporting for undefined values by providing a position.
- ``filesizeformat`` filter uses decimal prefixes now per default and can be
  set to binary mode with the second parameter.
- fixed bug in finalizer


Version 2.0rc1
--------------

(no codename, released on June 9th 2008)

- first release of Jinja2
Run these examples from the root directory of pycparser.

Please note that most realistic C code samples would require running the C
preprocessor before passing the code to **pycparser**; see the `README file
<https://github.com/eliben/pycparser/blob/master/README.rst>`_ and
`this blog post
<https://eli.thegreenplace.net/2015/on-parsing-c-type-declarations-and-fake-headers>`_
more details.
pyca/cryptography
=================

.. image:: https://img.shields.io/pypi/v/cryptography.svg
    :target: https://pypi.org/project/cryptography/
    :alt: Latest Version

.. image:: https://readthedocs.org/projects/cryptography/badge/?version=latest
    :target: https://cryptography.io
    :alt: Latest Docs

.. image:: https://github.com/pyca/cryptography/workflows/CI/badge.svg?branch=master
    :target: https://github.com/pyca/cryptography/actions?query=workflow%3ACI+branch%3Amaster

.. image:: https://codecov.io/github/pyca/cryptography/coverage.svg?branch=master
    :target: https://codecov.io/github/pyca/cryptography?branch=master


``cryptography`` is a package which provides cryptographic recipes and
primitives to Python developers.  Our goal is for it to be your "cryptographic
standard library". It supports Python 2.7, Python 3.6+, and PyPy 5.4+.

``cryptography`` includes both high level recipes and low level interfaces to
common cryptographic algorithms such as symmetric ciphers, message digests, and
key derivation functions. For example, to encrypt something with
``cryptography``'s high level symmetric encryption recipe:

.. code-block:: pycon

    >>> from cryptography.fernet import Fernet
    >>> # Put this somewhere safe!
    >>> key = Fernet.generate_key()
    >>> f = Fernet(key)
    >>> token = f.encrypt(b"A really secret message. Not for prying eyes.")
    >>> token
    '...'
    >>> f.decrypt(token)
    'A really secret message. Not for prying eyes.'

You can find more information in the `documentation`_.

You can install ``cryptography`` with:

.. code-block:: console

    $ pip install cryptography

For full details see `the installation documentation`_.

Discussion
~~~~~~~~~~

If you run into bugs, you can file them in our `issue tracker`_.

We maintain a `cryptography-dev`_ mailing list for development discussion.

You can also join ``#cryptography-dev`` on Freenode to ask questions or get
involved.

Security
~~~~~~~~

Need to report a security issue? Please consult our `security reporting`_
documentation.


.. _`documentation`: https://cryptography.io/
.. _`the installation documentation`: https://cryptography.io/en/latest/installation.html
.. _`issue tracker`: https://github.com/pyca/cryptography/issues
.. _`cryptography-dev`: https://mail.python.org/mailman/listinfo/cryptography-dev
.. _`security reporting`: https://cryptography.io/en/latest/security.html
API stability
=============

From its first release, ``cryptography`` has had a strong API stability
policy.

What does this policy cover?
----------------------------

This policy includes any API or behavior that is documented in this
documentation.

What does "stable" mean?
------------------------

* Public APIs will not be removed or renamed without providing a compatibility
  alias.
* The behavior of existing APIs will not change.

What doesn't this policy cover?
-------------------------------

* We may add new features, things like the result of ``dir(obj))`` or the
  contents of ``obj.__dict__`` may change.
* Objects are not guaranteed to be pickleable, and pickled objects from one
  version of ``cryptography`` may not be loadable in future versions.
* Development versions of ``cryptography``. Before a feature is in a release,
  it is not covered by this policy and may change.

Security
~~~~~~~~

One exception to our API stability policy is for security. We will violate this
policy as necessary in order to resolve a security issue or harden
``cryptography`` against a possible attack.

Versioning
----------

This project uses a custom versioning scheme as described below.

Given a version ``cryptography X.Y.Z``,

* ``X.Y`` is a decimal number that is incremented for
  potentially-backwards-incompatible releases.

  * This increases like a standard decimal.
    In other words, 0.9 is the ninth release, and 1.0 is the tenth (not 0.10).
    The dividing decimal point can effectively be ignored.

* ``Z`` is an integer that is incremented for backward-compatible releases.

Deprecation
~~~~~~~~~~~

From time to time we will want to change the behavior of an API or remove it
entirely. In that case, here's how the process will work:

* In ``cryptography X.Y`` the feature exists.
* In ``cryptography X.Y + 0.1`` using that feature will emit a
  ``UserWarning``.
* In ``cryptography X.Y + 0.2`` using that feature will emit a
  ``UserWarning``.
* In ``cryptography X.Y + 0.3`` the feature will be removed or changed.

In short, code that runs without warnings will always continue to work for a
period of two releases.

From time to time, we may decide to deprecate an API that is particularly
widely used. In these cases, we may decide to provide an extended deprecation
period, at our discretion.
.. include:: ../CHANGELOG.rst
Community
=========

You can find ``cryptography`` all over the web:

* `Mailing list`_
* `Source code`_
* `Issue tracker`_
* `Documentation`_
* IRC: ``#cryptography-dev`` on ``irc.freenode.net``

Wherever we interact, we adhere to the `Python Community Code of Conduct`_.


.. _`Mailing list`: https://mail.python.org/mailman/listinfo/cryptography-dev
.. _`Source code`: https://github.com/pyca/cryptography
.. _`Issue tracker`: https://github.com/pyca/cryptography/issues
.. _`Documentation`: https://cryptography.io/
.. _`Python Community Code of Conduct`: https://www.python.org/psf/codeofconduct/
C bindings
==========

C bindings are bindings to C libraries, using cffi_ whenever possible.

.. _cffi: https://cffi.readthedocs.io

Bindings live in ``cryptography.hazmat.bindings``.

When modifying the bindings you will need to recompile the C extensions to
test the changes. This can be accomplished with ``pip install -e .`` in the
project root. If you do not do this a ``RuntimeError`` will be raised.

Style guide
-----------

Don't name parameters:

.. code-block:: c

    /* Good */
    long f(long);
    /* Bad */
    long f(long x);

...unless they're inside a struct:

.. code-block:: c

    struct my_struct {
        char *name;
        int number;
        ...;
    };

Include ``void`` if the function takes no arguments:

.. code-block:: c

    /* Good */
    long f(void);
    /* Bad */
    long f();

Wrap lines at 80 characters like so:

.. code-block:: c

    /* Pretend this went to 80 characters */
    long f(long, long,
           int *)

Include a space after commas between parameters:

.. code-block:: c

    /* Good */
    long f(int, char *)
    /* Bad */
    long f(int,char *)

Use C-style ``/* */`` comments instead of C++-style ``//``:

.. code-block:: c

    // Bad
    /* Good */

Values set by ``#define`` should be assigned the appropriate type. If you see
this:

.. code-block:: c

    #define SOME_INTEGER_LITERAL 0x0;
    #define SOME_UNSIGNED_INTEGER_LITERAL 0x0001U;
    #define SOME_STRING_LITERAL "hello";

...it should be added to the bindings like so:

.. code-block:: c

    static const int SOME_INTEGER_LITERAL;
    static const unsigned int SOME_UNSIGNED_INTEGER_LITERAL;
    static const char *const SOME_STRING_LITERAL;

Adding constant, types, functions...
------------------------------------

You can create bindings for any name that exists in some version of
the library you're binding against. However, the project also has to
keep supporting older versions of the library. In order to achieve this,
binding modules have a ``CUSTOMIZATIONS`` constant, and there is a
``CONDITIONAL_NAMES`` constants in
``src/cryptography/hazmat/bindings/openssl/_conditional.py``.

Let's say you want to enable quantum transmogrification. The upstream
library implements this as the following API::

    static const int QM_TRANSMOGRIFICATION_ALIGNMENT_LEFT;
    static const int QM_TRANSMOGRIFICATION_ALIGNMENT_RIGHT;
    typedef ... QM_TRANSMOGRIFICATION_CTX;
    int QM_transmogrify(QM_TRANSMOGRIFICATION_CTX *, int);

To start, create a new constant that defines if the *actual* library
has the feature you want, and add it to ``TYPES``::

    static const long Cryptography_HAS_QUANTUM_TRANSMOGRIFICATION;

This should start with ``Cryptography_``, since we're adding it in
this library. This prevents namespace collisions.

Then, define the actual features (constants, types, functions...) you
want to expose. If it's a constant, just add it to ``TYPES``::

    static const int QM_TRANSMOGRIFICATION_ALIGNMENT_LEFT;
    static const int QM_TRANSMOGRIFICATION_ALIGNMENT_RIGHT;

If it's a struct, add it to ``TYPES`` as well. The following is an
opaque struct::

    typedef ... QM_TRANSMOGRIFICATION_CTX;

... but you can also make some or all items in the struct accessible::

    typedef struct {
        /* Fundamental constant k for your particular universe */
        BIGNUM *k;
        ...;
    } QM_TRANSMOGRIFICATION_CTX;

For functions just add the signature to ``FUNCTIONS``::

    int QM_transmogrify(QM_TRANSMOGRIFICATION_CTX *, int);

Then, we define the ``CUSTOMIZATIONS`` entry. To do that, we have to
come up with a C preprocessor expression that decides whether or not a
feature exists in the library. For example::

    #ifdef QM_transmogrify

Then, we set the flag that signifies the feature exists::

    static const long Cryptography_HAS_QUANTUM_TRANSMOGRIFICATION = 1;

Otherwise, we set that flag to 0::

    #else
    static const long Cryptography_HAS_QUANTUM_TRANSMOGRIFICATION = 0;

Then, in that ``#else`` block, we define the names that aren't
available as dummy values. For an integer constant, use 0::

    static const int QM_TRANSMOGRIFICATION_ALIGNMENT_LEFT = 0;
    static const int QM_TRANSMOGRIFICATION_ALIGNMENT_RIGHT = 0;

For a function, it's a bit trickier. You have to define a function
pointer of the appropriate type to be NULL::

    int (*QM_transmogrify)(QM_TRANSMOGRIFICATION_CTX *, int) = NULL;

(To do that, copy the signature, put a ``*`` in front of the function
name and wrap it in parentheses, and then put ``= NULL`` at the end).

Note how types don't need to be conditionally defined, as long as all
the necessarily type definitions are in place.

Finally, add an entry to ``CONDITIONAL_NAMES`` with all of the things
you want to conditionally export::

    def cryptography_has_quantum_transmogrification():
        return [
            "QM_TRANSMOGRIFICATION_ALIGNMENT_LEFT",
            "QM_TRANSMOGRIFICATION_ALIGNMENT_RIGHT",
            "QM_transmogrify",
        ]


    CONDITIONAL_NAMES = {
        ...
        "Cryptography_HAS_QUANTUM_TRANSMOGRIFICATION": (
            cryptography_has_quantum_transmogrification
        ),
    }


Caveats
~~~~~~~

Sometimes, a set of loosely related features are added in the same
version, and it's impractical to create ``#ifdef`` statements for each
one. In that case, it may make sense to either check for a particular
version. For example, to check for OpenSSL 1.1.1 or newer::

    #if CRYPTOGRAPHY_OPENSSL_111_OR_GREATER

Sometimes, the version of a library on a particular platform will have
features that you thought it wouldn't, based on its version.
Occasionally, packagers appear to ship arbitrary VCS checkouts. As a
result, sometimes you may have to add separate ``#ifdef`` statements
for particular features. This kind of issue is typically only caught
by running the tests on a wide variety of systems, which is the job of
our continuous integration infrastructure.
ARC4 vector creation
====================

This page documents the code that was used to generate the ARC4 test
vectors for key lengths not available in :rfc:`6229`. All the vectors
were generated using OpenSSL and verified with Go.

Creation
--------

``cryptography`` was modified to support ARC4 key lengths not listed
in :rfc:`6229`. Then the following Python script was run to generate the
vector files.

.. literalinclude:: /development/custom-vectors/arc4/generate_arc4.py

Download link: :download:`generate_arc4.py
</development/custom-vectors/arc4/generate_arc4.py>`


Verification
------------

The following Go code was used to verify the vectors.

.. literalinclude:: /development/custom-vectors/arc4/verify_arc4.go
    :language: go

Download link: :download:`verify_arc4.go
</development/custom-vectors/arc4/verify_arc4.go>`
CAST5 vector creation
=====================

This page documents the code that was used to generate the CAST5 CBC, CFB, OFB,
and CTR test vectors as well as the code used to verify them against another
implementation. The CBC, CFB, and OFB vectors were generated using OpenSSL and
the CTR vectors were generated using Apple's CommonCrypto. All the generated
vectors were verified with Go.

Creation
--------

``cryptography`` was modified to support CAST5 in CBC, CFB, and OFB modes. Then
the following Python script was run to generate the vector files.

.. literalinclude:: /development/custom-vectors/cast5/generate_cast5.py

Download link: :download:`generate_cast5.py
</development/custom-vectors/cast5/generate_cast5.py>`


Verification
------------

The following Go code was used to verify the vectors.

.. literalinclude:: /development/custom-vectors/cast5/verify_cast5.go
    :language: go

Download link: :download:`verify_cast5.go
</development/custom-vectors/cast5/verify_cast5.go>`
HKDF vector creation
====================

This page documents the code that was used to generate a longer
HKDF test vector (1200 bytes) than is available in :rfc:`5869`. All
the vectors were generated using OpenSSL and verified with Go.

Creation
--------

The following Python script was run to generate the vector files.

.. literalinclude:: /development/custom-vectors/hkdf/generate_hkdf.py

Download link: :download:`generate_hkdf.py
</development/custom-vectors/hkdf/generate_hkdf.py>`


Verification
------------

The following Go code was used to verify the vectors.

.. literalinclude:: /development/custom-vectors/hkdf/verify_hkdf.go
    :language: go

Download link: :download:`verify_hkdf.go
</development/custom-vectors/hkdf/verify_hkdf.go>`
IDEA vector creation
=====================

This page documents the code that was used to generate the IDEA CBC, CFB, and
OFB test vectors as well as the code used to verify them against another
implementation. The vectors were generated using OpenSSL and verified with
`Botan`_.

Creation
--------

``cryptography`` was modified to support IDEA in CBC, CFB, and OFB modes. Then
the following python script was run to generate the vector files.

.. literalinclude:: /development/custom-vectors/idea/generate_idea.py

Download link: :download:`generate_idea.py
</development/custom-vectors/idea/generate_idea.py>`


Verification
------------

The following Python code was used to verify the vectors using the `Botan`_
project's Python bindings.

.. literalinclude:: /development/custom-vectors/idea/verify_idea.py

Download link: :download:`verify_idea.py
</development/custom-vectors/idea/verify_idea.py>`

.. _`Botan`: https://botan.randombit.net
RSA OAEP SHA2 vector creation
=============================

This page documents the code that was used to generate the RSA OAEP SHA2
test vectors as well as code used to verify them against another
implementation.


Creation
--------

``cryptography`` was modified to allow the use of SHA2 in OAEP encryption. Then
the following python script was run to generate the vector files.

.. literalinclude:: /development/custom-vectors/rsa-oaep-sha2/generate_rsa_oaep_sha2.py

Download link: :download:`generate_rsa_oaep_sha2.py
</development/custom-vectors/rsa-oaep-sha2/generate_rsa_oaep_sha2.py>`


Verification
------------

A Java 8 program was written using `Bouncy Castle`_ to load and verify the test
vectors.


.. literalinclude:: /development/custom-vectors/rsa-oaep-sha2/VerifyRSAOAEPSHA2.java

Download link: :download:`VerifyRSAOAEPSHA2.java
</development/custom-vectors/rsa-oaep-sha2/VerifyRSAOAEPSHA2.java>`

Using the Verifier
------------------

Download and install the `Java 8 SDK`_. Initial verification was performed
using ``jdk-8u77-macosx-x64.dmg``.

Download the latest `Bouncy Castle`_ JAR.  Initial verification was performed
using ``bcprov-jdk15on-154.jar``.

Set the ``-classpath`` to include the Bouncy Castle jar and the path to
``VerifyRSAOAEPSHA2.java`` and compile the program.

.. code-block:: console

    $ javac -classpath ~/Downloads/bcprov-jdk15on-154.jar:./ VerifyRSAOAEPSHA2.java

Finally, run the program with the path to the SHA-2 vectors:

.. code-block:: console

    $ java -classpath ~/Downloads/bcprov-jdk15on-154.jar:./ VerifyRSAOAEPSHA2

.. _`Bouncy Castle`: https://www.bouncycastle.org/
.. _`Java 8 SDK`: https://www.oracle.com/technetwork/java/javase/downloads/index.html
SECP256K1 vector creation
=========================

This page documents the code that was used to generate the SECP256K1 elliptic
curve test vectors as well as code used to verify them against another
implementation.


Creation
--------

The vectors are generated using a `pure Python ecdsa`_ implementation. The test
messages and combinations of algorithms are derived from the NIST vector data.

.. literalinclude:: /development/custom-vectors/secp256k1/generate_secp256k1.py

Download link: :download:`generate_secp256k1.py
</development/custom-vectors/secp256k1/generate_secp256k1.py>`


Verification
------------

``cryptography`` was modified to support the SECP256K1 curve. Then
the following python script was run to generate the vector files.

.. literalinclude:: /development/custom-vectors/secp256k1/verify_secp256k1.py

Download link: :download:`verify_secp256k1.py
</development/custom-vectors/secp256k1/verify_secp256k1.py>`

.. _`pure Python ecdsa`: https://pypi.org/project/ecdsa/
SEED vector creation
=====================

This page documents the code that was used to generate the SEED CFB and OFB
test vectors as well as the code used to verify them against another
implementation. The vectors were generated using OpenSSL and verified
with `Botan`_.

Creation
--------

``cryptography`` was modified to support SEED in CFB and OFB modes. Then
the following python script was run to generate the vector files.

.. literalinclude:: /development/custom-vectors/seed/generate_seed.py

Download link: :download:`generate_seed.py
</development/custom-vectors/seed/generate_seed.py>`


Verification
------------

The following Python code was used to verify the vectors using the `Botan`_
project's Python bindings.

.. literalinclude:: /development/custom-vectors/seed/verify_seed.py

Download link: :download:`verify_seed.py
</development/custom-vectors/seed/verify_seed.py>`

.. _`Botan`: https://botan.randombit.net
Getting started
===============

Development dependencies
------------------------
Working on ``cryptography`` requires the installation of a small number of
development dependencies in addition to the dependencies for
:doc:`/installation`. These are listed in ``dev-requirements.txt`` and they can
be installed in a `virtualenv`_ using `pip`_. Before you install them, follow
the **build** instructions in :doc:`/installation` (be sure to stop before
actually installing ``cryptography``). Once you've done that, install the
development dependencies, and then install ``cryptography`` in ``editable``
mode. For example:

.. code-block:: console

    $ # Create a virtualenv and activate it
    $ # Set up your cryptography build environment
    $ pip install --requirement dev-requirements.txt
    $ pip install --editable .

Make sure that ``pip install --requirement ...`` has installed the Python
package ``vectors/`` and packages on ``tests/`` . If it didn't, you may
install them manually by using ``pip`` on each directory.

You will also need to install ``enchant`` using your system's package manager
to check spelling in the documentation.

.. note::
    There is an upstream bug in ``enchant`` that prevents its installation on
    Windows with 64-bit Python. See `this Github issue`_ for more information.
    The easiest workaround is to use 32-bit Python for ``cryptography``
    development, even on 64-bit Windows.

You are now ready to run the tests and build the documentation.

OpenSSL on macOS
~~~~~~~~~~~~~~~~

You must have installed `OpenSSL`_ via `Homebrew`_ or `MacPorts`_ and must set
``CFLAGS`` and ``LDFLAGS`` environment variables before installing the
``dev-requirements.txt`` otherwise pip will fail with include errors.

For example, with `Homebrew`_:

.. code-block:: console

    $ env LDFLAGS="-L$(brew --prefix openssl@1.1)/lib" \
        CFLAGS="-I$(brew --prefix openssl@1.1)/include" \
        pip install --requirement ./dev-requirements.txt

Alternatively for a static build you can specify
``CRYPTOGRAPHY_SUPPRESS_LINK_FLAGS=1`` and ensure ``LDFLAGS`` points to the
absolute path for the `OpenSSL`_ libraries before calling pip.

.. tip::
    You will also need to set these values when `Building documentation`_.

Running tests
-------------

``cryptography`` unit tests are found in the ``tests/`` directory and are
designed to be run using `pytest`_. `pytest`_ will discover the tests
automatically, so all you have to do is:

.. code-block:: console

    $ pytest
    ...
    62746 passed in 220.43 seconds

This runs the tests with the default Python interpreter.

You can also verify that the tests pass on other supported Python interpreters.
For this we use `tox`_, which will automatically create a `virtualenv`_ for
each supported Python version and run the tests. For example:

.. code-block:: console

    $ tox
    ...
     py27: commands succeeded
    ERROR:   pypy: InterpreterNotFound: pypy
     py38: commands succeeded
     docs: commands succeeded
     pep8: commands succeeded

You may not have all the required Python versions installed, in which case you
will see one or more ``InterpreterNotFound`` errors.


Building documentation
----------------------

``cryptography`` documentation is stored in the ``docs/`` directory. It is
written in `reStructured Text`_ and rendered using `Sphinx`_.

Use `tox`_ to build the documentation. For example:

.. code-block:: console

    $ tox -e docs
    ...
    docs: commands succeeded
    congratulations :)

The HTML documentation index can now be found at
``docs/_build/html/index.html``.

.. _`Homebrew`: https://brew.sh
.. _`MacPorts`: https://www.macports.org
.. _`OpenSSL`: https://www.openssl.org
.. _`pytest`: https://pypi.org/project/pytest/
.. _`tox`: https://pypi.org/project/tox/
.. _`virtualenv`: https://pypi.org/project/virtualenv/
.. _`pip`: https://pypi.org/project/pip/
.. _`sphinx`: https://pypi.org/project/Sphinx/
.. _`reStructured Text`: https://www.sphinx-doc.org/en/master/usage/restructuredtext/basics.html
.. _`this Github issue`: https://github.com/rfk/pyenchant/issues/42
Development
===========

As an open source project, ``cryptography`` welcomes contributions of all
forms. The sections below will help you get started.

File bugs and feature requests on our issue tracker on `GitHub`_. If it is a
bug check out `what to put in your bug report`_.

.. toctree::
    :maxdepth: 2

    getting-started
    submitting-patches
    reviewing-patches
    test-vectors
    c-bindings

.. _`GitHub`: https://github.com/pyca/cryptography
.. _`what to put in your bug report`: https://www.contribution-guide.org/#what-to-put-in-your-bug-report
Reviewing and merging patches
=============================

Everyone is encouraged to review open pull requests. We only ask that you try
and think carefully, ask questions and are `excellent to one another`_. Code
review is our opportunity to share knowledge, design ideas and make friends.

When reviewing a patch try to keep each of these concepts in mind:

Intent
------

* What is the change being proposed?
* Do we want this feature or is the bug they're fixing really a bug?

Architecture
------------

* Is the proposed change being made in the correct place? Is it a fix in a
  backend when it should be in the primitives?

Implementation
--------------

* Does the change do what the author claims?
* Are there sufficient tests?
* Has it been documented?
* Will this change introduce new bugs?

Grammar and style
-----------------

These are small things that are not caught by the automated style checkers.

* Does a variable need a better name?
* Should this be a keyword argument?

Merge requirements
------------------

Because cryptography is so complex, and the implications of getting it wrong so
devastating, ``cryptography`` has a strict merge policy for committers:

* Patches must *never* be pushed directly to ``master``, all changes (even the
  most trivial typo fixes!) must be submitted as a pull request.
* A committer may *never* merge their own pull request, a second party must
  merge their changes. If multiple people work on a pull request, it must be
  merged by someone who did not work on it.
* A patch that breaks tests, or introduces regressions by changing or removing
  existing tests should not be merged. Tests must always be passing on
  ``master``.
* If somehow the tests get into a failing state on ``master`` (such as by a
  backwards incompatible release of a dependency) no pull requests may be
  merged until this is rectified.
* All merged patches must have 100% test coverage.

The purpose of these policies is to minimize the chances we merge a change
that jeopardizes our users' security.

.. _`excellent to one another`: https://speakerdeck.com/ohrite/better-code-review
Submitting patches
==================

* Always make a new branch for your work.
* Patches should be small to facilitate easier review. `Studies have shown`_
  that review quality falls off as patch size grows. Sometimes this will result
  in many small PRs to land a single large feature.
* Larger changes should be discussed on `our mailing list`_ before submission.
* New features and significant bug fixes should be documented in the
  :doc:`/changelog`.
* You must have legal permission to distribute any code you contribute to
  ``cryptography``, and it must be available under both the BSD and Apache
  Software License Version 2.0 licenses.

If you believe you've identified a security issue in ``cryptography``, please
follow the directions on the :doc:`security page </security>`.

Code
----

When in doubt, refer to :pep:`8` for Python code. You can check if your code
meets our automated requirements by formatting it with ``black`` and running
``flake8`` against it. If you've installed the development requirements this
will automatically use our configuration. You can also run the ``tox`` job with
``tox -e pep8``.

`Write comments as complete sentences.`_

Class names which contains acronyms or initialisms should always be
capitalized. A class should be named ``HTTPClient``, not ``HttpClient``.

Every code file must start with the boilerplate licensing notice:

.. code-block:: python

    # This file is dual licensed under the terms of the Apache License, Version
    # 2.0, and the BSD License. See the LICENSE file in the root of this repository
    # for complete details.

Additionally, every Python code file must contain

.. code-block:: python

    from __future__ import absolute_import, division, print_function

API considerations
~~~~~~~~~~~~~~~~~~

Most projects' APIs are designed with a philosophy of "make easy things easy,
and make hard things possible". One of the perils of writing cryptographic code
is that secure code looks just like insecure code, and its results are almost
always indistinguishable. As a result, ``cryptography`` has, as a design
philosophy: "make it hard to do insecure things". Here are a few strategies for
API design that should be both followed, and should inspire other API choices:

If it is necessary to compare a user provided value with a computed value (for
example, verifying a signature), there should be an API provided that performs
the verification in a secure way (for example, using a constant time
comparison), rather than requiring the user to perform the comparison
themselves.

If it is incorrect to ignore the result of a method, it should raise an
exception, and not return a boolean ``True``/``False`` flag. For example, a
method to verify a signature should raise ``InvalidSignature``, and not return
whether the signature was valid.

.. code-block:: python

    # This is bad.
    def verify(sig):
        # ...
        return is_valid

    # Good!
    def verify(sig):
        # ...
        if not is_valid:
            raise InvalidSignature

Every recipe should include a version or algorithmic marker of some sort in its
output in order to allow transparent upgrading of the algorithms in use, as
the algorithms or parameters needed to achieve a given security margin evolve.

APIs at the :doc:`/hazmat/primitives/index` and recipes layer should
automatically use the :func:`~cryptography.hazmat.backends.default_backend`,
but optionally allow specifying a different backend.

C bindings
~~~~~~~~~~

More information on C bindings can be found in :doc:`the dedicated
section of the documentation <c-bindings>`.

Tests
-----

All code changes must be accompanied by unit tests with 100% code coverage (as
measured by the combined metrics across our build matrix).

When implementing a new primitive or recipe ``cryptography`` requires that you
provide a set of test vectors. See :doc:`/development/test-vectors` for more
details.

Documentation
-------------

All features should be documented with prose in the ``docs`` section. To ensure
it builds and passes `doc8`_ style checks you can run ``tox -e docs``.

Because of the inherent challenges in implementing correct cryptographic
systems, we want to make our documentation point people in the right directions
as much as possible. To that end:

* When documenting a generic interface, use a strong algorithm in examples.
  (e.g. when showing a hashing example, don't use
  :class:`~cryptography.hazmat.primitives.hashes.MD5`)
* When giving prescriptive advice, always provide references and supporting
  material.
* When there is real disagreement between cryptographic experts, represent both
  sides of the argument and describe the trade-offs clearly.

When documenting a new module in the ``hazmat`` package, its documentation
should begin with the "Hazardous Materials" warning:

.. code-block:: rest

    .. hazmat::

Always prefer terminology that is most broadly accepted. For example:

* When referring to class instances use "an instance of ``Foo``"
  instead of "a ``Foo`` provider".

When referring to a hypothetical individual (such as "a person receiving an
encrypted message") use gender neutral pronouns (they/them/their).

Docstrings are typically only used when writing abstract classes, but should
be written like this if required:

.. code-block:: python

    def some_function(some_arg):
        """
        Does some things.

        :param some_arg: Some argument.
        """

So, specifically:

* Always use three double quotes.
* Put the three double quotes on their own line.
* No blank line at the end.
* Use Sphinx parameter/attribute documentation `syntax`_.


.. _`Write comments as complete sentences.`: https://nedbatchelder.com/blog/201401/comments_should_be_sentences.html
.. _`syntax`: https://www.sphinx-doc.org/en/master/usage/restructuredtext/domains.html#info-field-lists
.. _`Studies have shown`: https://smartbear.com/learn/code-review/best-practices-for-peer-code-review/
.. _`our mailing list`: https://mail.python.org/mailman/listinfo/cryptography-dev
.. _`doc8`: https://github.com/openstack/doc8
Test vectors
============

Testing the correctness of the primitives implemented in each ``cryptography``
backend requires trusted test vectors. Where possible these vectors are
obtained from official sources such as `NIST`_ or `IETF`_ RFCs. When this is
not possible ``cryptography`` has chosen to create a set of custom vectors
using an official vector file as input to verify consistency between
implemented backends.

Vectors are kept in the ``cryptography_vectors`` package rather than within our
main test suite.

Sources
-------

Project Wycheproof
~~~~~~~~~~~~~~~~~~

We run vectors from `Project Wycheproof`_ -- a collection of known edge-cases
for various cryptographic algorithms. These are not included in the repository
(or ``cryptography_vectors`` package), but rather cloned from Git in our
continuous integration environments.

We have ensured all test vectors are used as of commit
``2196000605e45d91097147c9c71f26b72af58003``.

Asymmetric ciphers
~~~~~~~~~~~~~~~~~~

* RSA PKCS #1 from the RSA FTP site (ftp://ftp.rsasecurity.com/pub/pkcs/pkcs-1/
  and ftp://ftp.rsa.com/pub/rsalabs/tmp/).
* RSA FIPS 186-2 and PKCS1 v1.5 vulnerability test vectors from `NIST CAVP`_.
* FIPS 186-2 and FIPS 186-3 DSA test vectors from `NIST CAVP`_.
* FIPS 186-2 and FIPS 186-3 ECDSA test vectors from `NIST CAVP`_.
* DH and ECDH and ECDH+KDF(17.4) test vectors from `NIST CAVP`_.
* Ed25519 test vectors from the `Ed25519 website_`.
* OpenSSL PEM RSA serialization vectors from the `OpenSSL example key`_ and
  `GnuTLS key parsing tests`_.
* OpenSSL PEM DSA serialization vectors from the `GnuTLS example keys`_.
* PKCS #8 PEM serialization vectors from

  * GnuTLS: `enc-rsa-pkcs8.pem`_, `enc2-rsa-pkcs8.pem`_,
    `unenc-rsa-pkcs8.pem`_, `pkcs12_s2k_pem.c`_. The encoding error in
    `unenc-rsa-pkcs8.pem`_ was fixed, and the contents of `enc-rsa-pkcs8.pem`_
    was re-encrypted to include it. The contents of `enc2-rsa-pkcs8.pem`_
    was re-encrypted using a stronger PKCS#8 cipher.
  * `Botan's ECC private keys`_.
* `asymmetric/public/PKCS1/dsa.pub.pem`_ is a PKCS1 DSA public key from the
  Ruby test suite.
* X25519 and X448 test vectors from :rfc:`7748`.
* RSA OAEP with custom label from the `BoringSSL evp tests`_.
* Ed448 test vectors from :rfc:`8032`.


Custom asymmetric vectors
~~~~~~~~~~~~~~~~~~~~~~~~~

.. toctree::
    :maxdepth: 1

    custom-vectors/secp256k1
    custom-vectors/rsa-oaep-sha2

* ``asymmetric/PEM_Serialization/ec_private_key.pem`` and
  ``asymmetric/DER_Serialization/ec_private_key.der`` - Contains an Elliptic
  Curve key generated by OpenSSL from the curve ``secp256r1``.
* ``asymmetric/PEM_Serialization/ec_private_key_encrypted.pem`` and
  ``asymmetric/DER_Serialization/ec_private_key_encrypted.der``- Contains the
  same Elliptic Curve key as ``ec_private_key.pem``, except that it is
  encrypted with AES-128 with the password "123456".
* ``asymmetric/PEM_Serialization/ec_public_key.pem`` and
  ``asymmetric/DER_Serialization/ec_public_key.der``- Contains the public key
  corresponding to ``ec_private_key.pem``, generated using OpenSSL.
* ``asymmetric/PEM_Serialization/rsa_private_key.pem`` - Contains an RSA 2048
  bit key generated using OpenSSL, protected by the secret "123456" with DES3
  encryption.
* ``asymmetric/PEM_Serialization/rsa_public_key.pem`` and
  ``asymmetric/DER_Serialization/rsa_public_key.der``- Contains an RSA 2048
  bit public generated using OpenSSL from ``rsa_private_key.pem``.
* ``asymmetric/PEM_Serialization/dsa_4096.pem`` - Contains a 4096-bit DSA
  private key generated using OpenSSL.
* ``asymmetric/PEM_Serialization/dsaparam.pem`` - Contains 2048-bit DSA
  parameters generated using OpenSSL; contains no keys.
* ``asymmetric/PEM_Serialization/dsa_private_key.pem`` - Contains a DSA 2048
  bit key generated using OpenSSL from the parameters in ``dsaparam.pem``,
  protected by the secret "123456" with DES3 encryption.
* ``asymmetric/PEM_Serialization/dsa_public_key.pem`` and
  ``asymmetric/DER_Serialization/dsa_public_key.der`` - Contains a DSA 2048 bit
  key generated using OpenSSL from ``dsa_private_key.pem``.
* ``asymmetric/DER_Serialization/dsa_public_key_no_params.der`` - Contains a
  DSA public key with the optional parameters removed.
* ``asymmetric/DER_Serialization/dsa_public_key_invalid_bit_string.der`` -
  Contains a DSA public key with the bit string padding value set to 2 rather
  than the required 0.
* ``asymmetric/PKCS8/unenc-dsa-pkcs8.pem`` and
  ``asymmetric/DER_Serialization/unenc-dsa-pkcs8.der`` - Contains a DSA 1024
  bit key generated using OpenSSL.
* ``asymmetric/PKCS8/unenc-dsa-pkcs8.pub.pem`` and
  ``asymmetric/DER_Serialization/unenc-dsa-pkcs8.pub.der`` - Contains a DSA
  2048 bit public key generated using OpenSSL from ``unenc-dsa-pkcs8.pem``.
* DER conversions of the `GnuTLS example keys`_ for DSA as well as the
  `OpenSSL example key`_ for RSA.
* DER conversions of `enc-rsa-pkcs8.pem`_, `enc2-rsa-pkcs8.pem`_, and
  `unenc-rsa-pkcs8.pem`_.
* ``asymmetric/public/PKCS1/rsa.pub.pem`` and
  ``asymmetric/public/PKCS1/rsa.pub.der`` are PKCS1 conversions of the public
  key from ``asymmetric/PKCS8/unenc-rsa-pkcs8.pem`` using PEM and DER encoding.
* ``x509/custom/ca/ca_key.pem`` - An unencrypted PCKS8 ``secp256r1`` key. It is
  the private key for the certificate ``x509/custom/ca/ca.pem``. This key is
  encoded in several of the PKCS12 custom vectors.
* ``x509/custom/ca/rsa_key.pem`` - An unencrypted PCKS8 4096 bit RSA key. It is
  the private key for the certificate ``x509/custom/ca/rsa_ca.pem``.
* ``asymmetric/EC/compressed_points.txt`` - Contains compressed public points
  generated using OpenSSL.
* ``asymmetric/X448/x448-pkcs8-enc.pem`` and
  ``asymmetric/X448/x448-pkcs8-enc.der`` contain an X448 key encrypted with
  AES 256 CBC with the password ``password``.
* ``asymmetric/X448/x448-pkcs8.pem`` and ``asymmetric/X448/x448-pkcs8.der``
  contain an unencrypted X448 key.
* ``asymmetric/X448/x448-pub.pem`` and ``asymmetric/X448/x448-pub.der`` contain
  an X448 public key.
* ``asymmetric/Ed25519/ed25519-pkcs8-enc.pem`` and
  ``asymmetric/Ed25519/ed25519-pkcs8-enc.der`` contain an Ed25519 key encrypted
  with AES 256 CBC with the password ``password``.
* ``asymmetric/Ed25519/ed25519-pkcs8.pem`` and
  ``asymmetric/Ed25519/ed25519-pkcs8.der`` contain an unencrypted Ed25519 key.
* ``asymmetric/Ed25519/ed25519-pub.pem`` and
  ``asymmetric/Ed25519/ed25519-pub.der`` contain an Ed25519 public key.
* ``asymmetric/X25519/x25519-pkcs8-enc.pem`` and
  ``asymmetric/X25519/x25519-pkcs8-enc.der`` contain an X25519 key encrypted
  with AES 256 CBC with the password ``password``.
* ``asymmetric/X25519/x25519-pkcs8.pem`` and
  ``asymmetric/X25519/x25519-pkcs8.der`` contain an unencrypted X25519 key.
* ``asymmetric/X25519/x25519-pub.pem`` and ``asymmetric/X25519/x25519-pub.der``
  contain an X25519 public key.
* ``asymmetric/Ed448/ed448-pkcs8-enc.pem`` and
  ``asymmetric/Ed448/ed448-pkcs8-enc.der`` contain an Ed448 key encrypted
  with AES 256 CBC with the password ``password``.
* ``asymmetric/Ed448/ed448-pkcs8.pem`` and
  ``asymmetric/Ed448/ed448-pkcs8.der`` contain an unencrypted Ed448 key.
* ``asymmetric/Ed448/ed448-pub.pem`` and ``asymmetric/Ed448/ed448-pub.der``
  contain an Ed448 public key.


Key exchange
~~~~~~~~~~~~

* ``vectors/cryptography_vectors/asymmetric/DH/rfc3526.txt`` contains
  several standardized Diffie-Hellman groups from :rfc:`3526`.

* ``vectors/cryptography_vectors/asymmetric/DH/RFC5114.txt`` contains
  Diffie-Hellman examples from appendix A.1, A.2 and A.3 of :rfc:`5114`.

* ``vectors/cryptography_vectors/asymmetric/DH/vec.txt`` contains
  Diffie-Hellman examples from `botan`_.

* ``vectors/cryptography_vectors/asymmetric/DH/bad_exchange.txt`` contains
  Diffie-Hellman vector pairs that were generated using OpenSSL
  ``DH_generate_parameters_ex`` and ``DH_generate_key``.

* ``vectors/cryptography_vectors/asymmetric/DH/dhp.pem``,
  ``vectors/cryptography_vectors/asymmetric/DH/dhkey.pem`` and
  ``vectors/cryptography_vectors/asymmetric/DH/dhpub.pem`` contains
  Diffie-Hellman parameters and key respectively. The keys were
  generated using OpenSSL following `DHKE`_ guide.
  ``vectors/cryptography_vectors/asymmetric/DH/dhkey.txt`` contains
  all parameter in text.
  ``vectors/cryptography_vectors/asymmetric/DH/dhp.der``,
  ``vectors/cryptography_vectors/asymmetric/DH/dhkey.der`` and
  ``vectors/cryptography_vectors/asymmetric/DH/dhpub.der`` contains
  are the above parameters and keys in DER format.

* ``vectors/cryptography_vectors/asymmetric/DH/dhp_rfc5114_2.pem``,
  ``vectors/cryptography_vectors/asymmetric/DH/dhkey_rfc5114_2.pem`` and
  ``vectors/cryptography_vectors/asymmetric/DH/dhpub_rfc5114_2.pem`` contains
  Diffie-Hellman parameters and key respectively. The keys were
  generated using OpenSSL following `DHKE`_ guide. When creating the
  parameters we added the `-pkeyopt dh_rfc5114:2` option to use
  :rfc:`5114` 2048 bit DH parameters with 224 bit subgroup.
  ``vectors/cryptography_vectors/asymmetric/DH/dhkey_rfc5114_2.txt`` contains
  all parameter in text.
  ``vectors/cryptography_vectors/asymmetric/DH/dhp_rfc5114_2.der``,
  ``vectors/cryptography_vectors/asymmetric/DH/dhkey_rfc5114_2.der`` and
  ``vectors/cryptography_vectors/asymmetric/DH/dhpub_rfc5114_2.der`` contains
  are the above parameters and keys in DER format.
* ``vectors/cryptography_vectors/asymmetric/DH/dh_key_256.pem`` contains
  a PEM PKCS8 encoded DH key with a 256-bit key size.

* ``vectors/cryptoraphy_vectors/asymmetric/ECDH/brainpool.txt`` contains
  Brainpool vectors from :rfc:`7027`.

X.509
~~~~~

* PKITS test suite from `NIST PKI Testing`_.
* ``v1_cert.pem`` from the OpenSSL source tree (`testx509.pem`_).
* ``ecdsa_root.pem`` - `DigiCert Global Root G3`_, a ``secp384r1`` ECDSA root
  certificate.
* ``verisign-md2-root.pem`` - A legacy Verisign public root signed using the
  MD2 algorithm. This is a PEM conversion of the `root data`_ in the NSS source
  tree.
* ``cryptography.io.pem`` - A leaf certificate issued by RapidSSL for the
  cryptography website.
* ``rapidssl_sha256_ca_g3.pem`` - The intermediate CA that issued the
  ``cryptography.io.pem`` certificate.
* ``cryptography.io.precert.pem`` - A pre-certificate with the CT poison
  extension for the cryptography website.
* ``cryptography-scts.io.pem`` - A leaf certificate issued by Let's Encrypt for
  the cryptography website which contains signed certificate timestamps.
* ``wildcard_san.pem`` - A leaf certificate issued by a public CA for
  ``langui.sh`` that contains wildcard entries in the SAN extension.
* ``san_edipartyname.der`` - A DSA certificate from a `Mozilla bug`_
  containing a SAN extension with an ``ediPartyName`` general name.
* ``san_x400address.der`` - A DSA certificate from a `Mozilla bug`_ containing
  a SAN extension with an ``x400Address`` general name.
* ``department-of-state-root.pem`` - The intermediary CA for the Department of
  State, issued by the United States Federal Government's Common Policy CA.
  Notably has a ``critical`` policy constraints extensions.
* ``e-trust.ru.der`` - A certificate from a `Russian CA`_ signed using the GOST
  cipher and containing numerous unusual encodings such as NUMERICSTRING in
  the subject DN.
* ``alternate-rsa-sha1-oid.pem`` - A certificate from an
  `unknown signature OID`_ Mozilla bug that uses an alternate signature OID for
  RSA with SHA1.
* ``badssl-sct.pem`` - A certificate with the certificate transparency signed
  certificate timestamp extension.
* ``bigoid.pem`` - A certificate with a rather long OID in the
  Certificate Policies extension.  We need to make sure we can parse
  long OIDs.
* ``wosign-bc-invalid.pem`` - A certificate issued by WoSign that contains
  a basic constraints extension with CA set to false and a path length of zero
  in violation of :rfc:`5280`.
* ``tls-feature-ocsp-staple.pem`` - A certificate issued by Let's Encrypt that
  contains a TLS Feature extension with the ``status_request`` feature
  (commonly known as OCSP Must-Staple).
* ``unique-identifier.pem`` - A certificate containing
  a distinguished name with an ``x500UniqueIdentifier``.
* ``utf8-dnsname.pem`` - A certificate containing non-ASCII characters in the
  DNS name entries of the SAN extension.
* ``badasn1time.pem`` - A certificate containing an incorrectly specified
  UTCTime in its validity->not_after.
* ``letsencryptx3.pem`` - A subordinate certificate used by Let's Encrypt to
  issue end entity certificates.
* ``ed25519-rfc8410.pem`` - A certificate containing an X25519 public key with
  an ``ed25519`` signature taken from :rfc:`8410`.
* ``root-ed25519.pem`` - An ``ed25519`` root certificate (``ed25519`` signature
  with ``ed25519`` public key) from the OpenSSL test suite.
  (`root-ed25519.pem`_)
* ``server-ed25519-cert.pem`` - An ``ed25519`` server certificate (RSA
  signature with ``ed25519`` public key) from the OpenSSL test suite.
  (`server-ed25519-cert.pem`_)
* ``server-ed448-cert.pem`` - An ``ed448`` server certificate (RSA
  signature with ``ed448`` public key) from the OpenSSL test suite.
  (`server-ed448-cert.pem`_)

Custom X.509 Vectors
~~~~~~~~~~~~~~~~~~~~

* ``invalid_version.pem`` - Contains an RSA 2048 bit certificate with the
  X.509 version field set to ``0x7``.
* ``post2000utctime.pem`` - Contains an RSA 2048 bit certificate with the
  ``notBefore`` and ``notAfter`` fields encoded as post-2000 ``UTCTime``.
* ``dsa_selfsigned_ca.pem`` - Contains a DSA self-signed CA certificate
  generated using OpenSSL.
* ``ec_no_named_curve.pem`` - Contains an ECDSA certificate that does not have
  an embedded OID defining the curve.
* ``all_supported_names.pem`` - An RSA 2048 bit certificate generated using
  OpenSSL that contains a subject and issuer that have two of each supported
  attribute type from :rfc:`5280`.
* ``unsupported_subject_name.pem`` - An RSA 2048 bit self-signed CA certificate
  generated using OpenSSL that contains the unsupported "initials" name.
* ``utf8_common_name.pem`` - An RSA 2048 bit self-signed CA certificate
  generated using OpenSSL that contains a UTF8String common name with the value
  "We heart UTF8!™".
* ``two_basic_constraints.pem`` - An RSA 2048 bit self-signed certificate
  containing two basic constraints extensions.
* ``basic_constraints_not_critical.pem`` - An RSA 2048 bit self-signed
  certificate containing a basic constraints extension that is not marked as
  critical.
* ``bc_path_length_zero.pem`` - An RSA 2048 bit self-signed
  certificate containing a basic constraints extension with a path length of
  zero.
* ``unsupported_extension.pem`` - An RSA 2048 bit self-signed certificate
  containing an unsupported extension type. The OID was encoded as
  "1.2.3.4" with an ``extnValue`` of "value".
* ``unsupported_extension_2.pem`` - A ``secp256r1`` certificate
  containing two unsupported extensions. The OIDs are ``1.3.6.1.4.1.41482.2``
  with an ``extnValue`` of ``1.3.6.1.4.1.41482.1.2`` and
  ``1.3.6.1.4.1.45724.2.1.1`` with an ``extnValue`` of ``\x03\x02\x040``
* ``unsupported_extension_critical.pem`` - An RSA 2048 bit self-signed
  certificate containing an unsupported extension type marked critical. The OID
  was encoded as "1.2.3.4" with an ``extnValue`` of "value".
* ``san_email_dns_ip_dirname_uri.pem`` - An RSA 2048 bit self-signed
  certificate containing a subject alternative name extension with the
  following general names: ``rfc822Name``, ``dNSName``, ``iPAddress``,
  ``directoryName``, and ``uniformResourceIdentifier``.
* ``san_empty_hostname.pem`` - An RSA 2048 bit self-signed certificate
  containing a subject alternative extension with an empty ``dNSName``
  general name.
* ``san_other_name.pem`` - An RSA 2048 bit self-signed certificate containing
  a subject alternative name extension with the ``otherName`` general name.
* ``san_registered_id.pem`` - An RSA 1024 bit certificate containing a
  subject alternative name extension with the ``registeredID`` general name.
* ``all_key_usages.pem`` - An RSA 2048 bit self-signed certificate containing
  a key usage extension with all nine purposes set to true.
* ``extended_key_usage.pem`` - An RSA 2048 bit self-signed certificate
  containing an extended key usage extension with eight usages.
* ``san_idna_names.pem`` - An RSA 2048 bit self-signed certificate containing
  a subject alternative name extension with ``rfc822Name``, ``dNSName``, and
  ``uniformResourceIdentifier`` general names with IDNA (:rfc:`5895`) encoding.
* ``san_wildcard_idna.pem`` - An RSA 2048 bit self-signed certificate
  containing a subject alternative name extension with a ``dNSName`` general
  name with a wildcard IDNA (:rfc:`5895`) domain.
* ``san_idna2003_dnsname.pem`` - An RSA 2048 bit self-signed certificate
  containing a subject alternative name extension with an IDNA 2003
  (:rfc:`3490`) ``dNSName``.
* ``san_rfc822_names.pem`` - An RSA 2048 bit self-signed certificate containing
  a subject alternative name extension with various ``rfc822Name`` values.
* ``san_rfc822_idna.pem`` - An RSA 2048 bit self-signed certificate containing
  a subject alternative name extension with an IDNA ``rfc822Name``.
* ``san_uri_with_port.pem`` - An RSA 2048 bit self-signed certificate
  containing a subject alternative name extension with various
  ``uniformResourceIdentifier`` values.
* ``san_ipaddr.pem`` - An RSA 2048 bit self-signed certificate containing a
  subject alternative name extension with an ``iPAddress`` value.
* ``san_dirname.pem`` - An RSA 2048 bit self-signed certificate containing a
  subject alternative name extension with a ``directoryName`` value.
* ``inhibit_any_policy_5.pem`` - An RSA 2048 bit self-signed certificate
  containing an inhibit any policy extension with the value 5.
* ``inhibit_any_policy_negative.pem`` - An RSA 2048 bit self-signed certificate
  containing an inhibit any policy extension with the value -1.
* ``authority_key_identifier.pem`` - An RSA 2048 bit self-signed certificate
  containing an authority key identifier extension with key identifier,
  authority certificate issuer, and authority certificate serial number fields.
* ``authority_key_identifier_no_keyid.pem`` - An RSA 2048 bit self-signed
  certificate containing an authority key identifier extension with authority
  certificate issuer and authority certificate serial number fields.
* ``aia_ocsp_ca_issuers.pem`` - An RSA 2048 bit self-signed certificate
  containing an authority information access extension with two OCSP and one
  CA issuers entry.
* ``aia_ocsp.pem`` - An RSA 2048 bit self-signed certificate
  containing an authority information access extension with an OCSP entry.
* ``aia_ca_issuers.pem`` - An RSA 2048 bit self-signed certificate
  containing an authority information access extension with a CA issuers entry.
* ``cdp_empty_hostname.pem`` - An RSA 2048 bit self-signed certificate
  containing a CRL distribution point extension with ``fullName`` URI without
  a hostname.
* ``cdp_fullname_reasons_crl_issuer.pem`` - An RSA 1024 bit certificate
  containing a CRL distribution points extension with ``fullName``,
  ``cRLIssuer``, and ``reasons`` data.
* ``cdp_crl_issuer.pem`` - An RSA 1024 bit certificate containing a CRL
  distribution points extension with ``cRLIssuer`` data.
* ``cdp_all_reasons.pem`` - An RSA 1024 bit certificate containing a CRL
  distribution points extension with all ``reasons`` bits set.
* ``cdp_reason_aa_compromise.pem`` - An RSA 1024 bit certificate containing a
  CRL distribution points extension with the ``AACompromise`` ``reasons`` bit
  set.
* ``nc_permitted_excluded.pem`` - An RSA 2048 bit self-signed certificate
  containing a name constraints extension with both permitted and excluded
  elements. Contains ``IPv4`` and ``IPv6`` addresses with network mask as well
  as ``dNSName`` with a leading period.
* ``nc_permitted_excluded_2.pem`` - An RSA 2048 bit self-signed certificate
  containing a name constraints extension with both permitted and excluded
  elements. Unlike ``nc_permitted_excluded.pem``, the general names do not
  contain any name constraints specific values.
* ``nc_permitted.pem`` - An RSA 2048 bit self-signed certificate containing a
  name constraints extension with permitted elements.
* ``nc_permitted_2.pem`` - An RSA 2048 bit self-signed certificate containing a
  name constraints extension with permitted elements that do not contain any
  name constraints specific values.
* ``nc_excluded.pem`` - An RSA 2048 bit self-signed certificate containing a
  name constraints extension with excluded elements.
* ``nc_invalid_ip_netmask.pem`` - An RSA 2048 bit self-signed certificate
  containing a name constraints extension with a permitted element that has an
  ``IPv6`` IP and an invalid network mask.
* ``nc_single_ip_netmask.pem`` - An RSA 2048 bit self-signed certificate
  containing a name constraints extension with a permitted element that has two
  IPs with ``/32`` and ``/128`` network masks.
* ``cp_user_notice_with_notice_reference.pem`` - An RSA 2048 bit self-signed
  certificate containing a certificate policies extension with a
  notice reference in the user notice.
* ``cp_user_notice_with_explicit_text.pem`` - An RSA 2048 bit self-signed
  certificate containing a certificate policies extension with explicit
  text and no notice reference.
* ``cp_cps_uri.pem`` - An RSA 2048 bit self-signed certificate containing a
  certificate policies extension with a CPS URI and no user notice.
* ``cp_user_notice_no_explicit_text.pem`` - An RSA 2048 bit self-signed
  certificate containing a certificate policies extension with a user notice
  with no explicit text.
* ``cp_invalid.pem`` - An RSA 2048 bit self-signed certificate containing a
  certificate policies extension with invalid data.
* ``ian_uri.pem`` - An RSA 2048 bit certificate containing an issuer
  alternative name extension with a ``URI`` general name.
* ``ocsp_nocheck.pem`` - An RSA 2048 bit self-signed certificate containing
  an ``OCSPNoCheck`` extension.
* ``pc_inhibit_require.pem`` - An RSA 2048 bit self-signed certificate
  containing a policy constraints extension with both inhibit policy mapping
  and require explicit policy elements.
* ``pc_inhibit.pem`` - An RSA 2048 bit self-signed certificate containing a
  policy constraints extension with an inhibit policy mapping element.
* ``pc_require.pem`` - An RSA 2048 bit self-signed certificate containing a
  policy constraints extension with a require explicit policy element.
* ``unsupported_subject_public_key_info.pem`` - A certificate whose public key
  is an unknown OID (``1.3.6.1.4.1.8432.1.1.2``).
* ``policy_constraints_explicit.pem`` - A self-signed certificate containing
  a ``policyConstraints`` extension with a ``requireExplicitPolicy`` value.
* ``freshestcrl.pem`` - A self-signed certificate containing a ``freshestCRL``
  extension.
* ``sia.pem`` - An RSA 2048 bit self-signed certificate containing a subject
  information access extension with both a CA repository entry and a custom
  OID entry.
* ``ca/ca.pem`` - A self-signed certificate with ``basicConstraints`` set to
  true. Its private key is ``ca/ca_key.pem``. This certificate is encoded in
  several of the PKCS12 custom vectors.
* ``negative_serial.pem`` - A certificate with a serial number that is a
  negative number.
* ``rsa_pss.pem`` - A certificate with an RSA PSS signature.
* ``root-ed448.pem`` - An ``ed448`` self-signed CA certificate
  using ``ed448-pkcs8.pem`` as key.
* ``ca/rsa_ca.pem`` - A self-signed RSA certificate with ``basicConstraints``
  set to true. Its private key is ``ca/rsa_key.pem``.

Custom X.509 Request Vectors
~~~~~~~~~~~~~~~~~~~~~~~~~~~~

* ``dsa_sha1.pem`` and ``dsa_sha1.der`` - Contain a certificate request using
  1024-bit DSA parameters and SHA1 generated using OpenSSL.
* ``rsa_md4.pem`` and ``rsa_md4.der`` - Contain a certificate request using
  2048 bit RSA and MD4 generated using OpenSSL.
* ``rsa_sha1.pem`` and ``rsa_sha1.der`` - Contain a certificate request using
  2048 bit RSA and SHA1 generated using OpenSSL.
* ``rsa_sha256.pem`` and ``rsa_sha256.der`` - Contain a certificate request
  using 2048 bit RSA and SHA256 generated using OpenSSL.
* ``ec_sha256.pem`` and ``ec_sha256.der`` - Contain a certificate request
  using EC (``secp384r1``) and SHA256 generated using OpenSSL.
* ``san_rsa_sha1.pem`` and ``san_rsa_sha1.der`` - Contain a certificate
  request using RSA and SHA1 with a subject alternative name extension
  generated using OpenSSL.
* ``two_basic_constraints.pem`` - A certificate signing request
  for an RSA 2048 bit key containing two basic constraints extensions.
* ``unsupported_extension.pem`` - A certificate signing request
  for an RSA 2048 bit key containing containing an unsupported
  extension type. The OID was encoded as "1.2.3.4" with an
  ``extnValue`` of "value".
* ``unsupported_extension_critical.pem`` - A certificate signing
  request for an RSA 2048 bit key containing containing an unsupported
  extension type marked critical. The OID was encoded as "1.2.3.4"
  with an ``extnValue`` of "value".
* ``basic_constraints.pem`` - A certificate signing request for an RSA
  2048 bit key containing a basic constraints extension marked as
  critical.
* ``invalid_signature.pem`` - A certificate signing request for an RSA
  1024 bit key containing an invalid signature with correct padding.
* ``challenge.pem`` - A certificate signing request for an RSA 2048 bit key
  containing a challenge password.
* ``challenge-invalid.der`` - A certificate signing request for an RSA 2048 bit
  key containing a challenge password attribute that has been encoded as an
  ASN.1 integer rather than a string.
* ``challenge-unstructured.pem`` - A certificate signing request for an RSA
  2048 bit key containing a challenge password attribute and an unstructured
  name attribute.

Custom X.509 Certificate Revocation List Vectors
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

* ``crl_all_reasons.pem`` - Contains a CRL with 12 revoked certificates, whose
  serials match their list position. It includes one revocation without
  any entry extensions, 10 revocations with every supported reason code and one
  revocation with an unsupported, non-critical entry extension with the OID
  value set to "1.2.3.4".
* ``crl_dup_entry_ext.pem`` - Contains a CRL with one revocation which has a
  duplicate entry extension.
* ``crl_md2_unknown_crit_entry_ext.pem`` - Contains a CRL with one revocation
  which contains an unsupported critical entry extension with the OID value set
  to "1.2.3.4". The CRL uses an unsupported MD2 signature algorithm.
* ``crl_unsupported_reason.pem`` - Contains a CRL with one revocation which has
  an unsupported reason code.
* ``crl_inval_cert_issuer_entry_ext.pem`` - Contains a CRL with one revocation
  which has one entry extension for certificate issuer with an empty value.
* ``crl_empty.pem`` - Contains a CRL with no revoked certificates.
* ``crl_ian_aia_aki.pem`` - Contains a CRL with ``IssuerAlternativeName``,
  ``AuthorityInformationAccess``, ``AuthorityKeyIdentifier`` and ``CRLNumber``
  extensions.
* ``valid_signature.pem`` - Contains a CRL with the public key which was used
  to generate it.
* ``invalid_signature.pem`` - Contains a CRL with the last signature byte
  incremented by 1 to produce an invalid signature, and the public key which
  was used to generate it.
* ``crl_delta_crl_indicator.pem`` - Contains a CRL with the
  ``DeltaCRLIndicator`` extension.
* ``crl_idp_fullname_only.pem`` - Contains a CRL with an
  ``IssuingDistributionPoints`` extension with only a ``fullname`` for the
  distribution point.
* ``crl_idp_only_ca.pem`` - Contains a CRL with an
  ``IssuingDistributionPoints`` extension that is only valid for CA certificate
  revocation.
* ``crl_idp_fullname_only_aa.pem`` - Contains a CRL with an
  ``IssuingDistributionPoints`` extension that sets a ``fullname`` and is only
  valid for attribute certificate revocation.
* ``crl_idp_fullname_only_user.pem`` - Contains a CRL with an
  ``IssuingDistributionPoints`` extension that sets a ``fullname`` and is only
  valid for user certificate revocation.
* ``crl_idp_fullname_indirect_crl.pem`` - Contains a CRL with an
  ``IssuingDistributionPoints`` extension that sets a ``fullname`` and the
  indirect CRL flag.
* ``crl_idp_reasons_only.pem`` - Contains a CRL with an
  ``IssuingDistributionPoints`` extension that is only valid for revocations
  with the ``keyCompromise`` reason.
* ``crl_idp_relative_user_all_reasons.pem`` - Contains a CRL with an
  ``IssuingDistributionPoints`` extension that sets all revocation reasons as
  allowed.
* ``crl_idp_relativename_only.pem`` - Contains a CRL with an
  ``IssuingDistributionPoints`` extension with only a ``relativename`` for
  the distribution point.

X.509 OCSP Test Vectors
~~~~~~~~~~~~~~~~~~~~~~~
* ``x509/ocsp/resp-sha256.der`` - An OCSP response for ``cryptography.io`` with
  a SHA256 signature.
* ``x509/ocsp/resp-unauthorized.der`` - An OCSP response with an unauthorized
  status.
* ``x509/ocsp/resp-revoked.der`` - An OCSP response for ``revoked.badssl.com``
  with a revoked status.
* ``x509/ocsp/resp-delegate-unknown-cert.der`` - An OCSP response for an
  unknown cert from ``AC Camerafirma``. This response also contains a delegate
  certificate.
* ``x509/ocsp/resp-responder-key-hash.der`` - An OCSP response from the
  ``DigiCert`` OCSP responder that uses a key hash for the responder ID.
* ``x509/ocsp/resp-revoked-reason.der`` - An OCSP response from the
  ``QuoVadis`` OCSP responder that contains a revoked certificate with a
  revocation reason.
* ``x509/ocsp/resp-revoked-no-next-update.der`` - An OCSP response that
  contains a revoked certificate and no ``nextUpdate`` value.
* ``x509/ocsp/resp-invalid-signature-oid.der`` - An OCSP response that was
  modified to contain an MD2 signature algorithm object identifier.
* ``x509/ocsp/resp-single-extension-reason.der`` - An OCSP response that
  contains a ``CRLReason`` single extension.
* ``x509/ocsp/resp-sct-extension.der`` - An OCSP response containing a
  ``CT Certificate SCTs`` single extension, from the SwissSign OCSP responder.
* ``x509/ocsp/ocsp-army.deps.mil-resp.der`` - An OCSP response containing
  multiple ``SINGLERESP`` values.

Custom X.509 OCSP Test Vectors
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
* ``x509/ocsp/req-sha1.der`` - An OCSP request containing a single request and
  using SHA1 as the hash algorithm.
* ``x509/ocsp/req-multi-sha1.der`` - An OCSP request containing multiple
  requests.
* ``x509/ocsp/req-invalid-hash-alg.der`` - An OCSP request containing an
  invalid hash algorithm OID.
* ``x509/ocsp/req-ext-nonce.der`` - An OCSP request containing a nonce
  extension.

Custom PKCS12 Test Vectors
~~~~~~~~~~~~~~~~~~~~~~~~~~
* ``pkcs12/cert-key-aes256cbc.p12`` - A PKCS12 file containing a cert
  (``x509/custom/ca/ca.pem``) and key (``x509/custom/ca/ca_key.pem``)
  both encrypted with AES 256 CBC with the password ``cryptography``.
* ``pkcs12/cert-none-key-none.p12`` - A PKCS12 file containing a cert
  (``x509/custom/ca/ca.pem``) and key (``x509/custom/ca/ca_key.pem``)
  with no encryption. The password (used for integrity checking only) is
  ``cryptography``.
* ``pkcs12/cert-rc2-key-3des.p12`` - A PKCS12 file containing a cert
  (``x509/custom/ca/ca.pem``) encrypted with RC2 and key
  (``x509/custom/ca/ca_key.pem``) encrypted via 3DES with the password
  ``cryptography``.
* ``pkcs12/no-password.p12`` - A PKCS12 file containing a cert
  (``x509/custom/ca/ca.pem``) and key (``x509/custom/ca/ca_key.pem``) with no
  encryption and no password.
* ``pkcs12/no-cert-key-aes256cbc.p12`` - A PKCS12 file containing a key
  (``x509/custom/ca/ca_key.pem``) encrypted via AES 256 CBC with the
  password ``cryptography`` and no certificate.
* ``pkcs12/cert-aes256cbc-no-key.p12`` - A PKCS12 file containing a cert
  (``x509/custom/ca/ca.pem``) encrypted via AES 256 CBC with the
  password ``cryptography`` and no private key.

Custom PKCS7 Test Vectors
~~~~~~~~~~~~~~~~~~~~~~~~~
* ``pkcs7/isrg.pem`` - A PEM encoded PKCS7 file containing the ISRG X1 root
  CA.
* ``pkcs7/amazon-roots.p7b`` - A DER encoded PCKS7 file containing Amazon Root
  CA 2 and 3.
* ``pkcs7/enveloped.pem`` - A PEM encoded PKCS7 file with enveloped data.

Custom OpenSSH Test Vectors
~~~~~~~~~~~~~~~~~~~~~~~~~~~

Generated by
``asymmetric/OpenSSH/gen.sh``
using command-line tools from OpenSSH_7.6p1 package.

* ``dsa-nopsw.key``, ``dsa-nopsw.key.pub``, ``dsa-nopsw.key-cert.pub`` -
  DSA-1024 private key; and corresponding public key in plain format
  and with self-signed certificate.
* ``dsa-psw.key``, ``dsa-psw.key.pub`` -
  Password-protected DSA-1024 private key and corresponding public key.
  Password is "password".
* ``ecdsa-nopsw.key``, ``ecdsa-nopsw.key.pub``,
  ``ecdsa-nopsw.key-cert.pub`` -
  SECP256R1 private key; and corresponding public key in plain format
  and with self-signed certificate.
* ``ecdsa-psw.key``, ``ecdsa-psw.key.pub`` -
  Password-protected SECP384R1 private key and corresponding public key.
  Password is "password".
* ``ed25519-nopsw.key``, ``ed25519-nopsw.key.pub``,
  ``ed25519-nopsw.key-cert.pub`` -
  Ed25519 private key; and corresponding public key in plain format
  and with self-signed certificate.
* ``ed25519-psw.key``, ``ed25519-psw.key.pub`` -
  Password-protected Ed25519 private key and corresponding public key.
  Password is "password".
* ``rsa-nopsw.key``, ``rsa-nopsw.key.pub``,
  ``rsa-nopsw.key-cert.pub`` -
  RSA-2048 private key; and corresponding public key in plain format
  and with self-signed certificate.
* ``rsa-psw.key``, ``rsa-psw.key.pub`` -
  Password-protected RSA-2048 private key and corresponding public key.
  Password is "password".

Hashes
~~~~~~

* MD5 from :rfc:`1321`.
* RIPEMD160 from the `RIPEMD website`_.
* SHA1 from `NIST CAVP`_.
* SHA2 (224, 256, 384, 512, 512/224, 512/256) from `NIST CAVP`_.
* SHA3 (224, 256, 384, 512) from `NIST CAVP`_.
* SHAKE (128, 256) from `NIST CAVP`_.
* Blake2s and Blake2b from OpenSSL `test/evptests.txt`_.

HMAC
~~~~

* HMAC-MD5 from :rfc:`2202`.
* HMAC-SHA1 from :rfc:`2202`.
* HMAC-RIPEMD160 from :rfc:`2286`.
* HMAC-SHA2 (224, 256, 384, 512) from :rfc:`4231`.

Key derivation functions
~~~~~~~~~~~~~~~~~~~~~~~~

* HKDF (SHA1, SHA256) from :rfc:`5869`.
* PBKDF2 (HMAC-SHA1) from :rfc:`6070`.
* scrypt from the `draft RFC`_.
* X9.63 KDF from `NIST CAVP`_.
* SP 800-108 Counter Mode KDF (HMAC-SHA1, HMAC-SHA224, HMAC-SHA256,
  HMAC-SHA384, HMAC-SHA512) from `NIST CAVP`_.

Key wrapping
~~~~~~~~~~~~

* AES key wrap (AESKW) and 3DES key wrap test vectors from `NIST CAVP`_.
* AES key wrap with padding vectors from `Botan's key wrap vectors`_.

Recipes
~~~~~~~

* Fernet from its `specification repository`_.

Symmetric ciphers
~~~~~~~~~~~~~~~~~

* AES (CBC, CFB, ECB, GCM, OFB, CCM) from `NIST CAVP`_.
* AES CTR from :rfc:`3686`.
* 3DES (CBC, CFB, ECB, OFB) from `NIST CAVP`_.
* ARC4 (KEY-LENGTH: 40, 56, 64, 80, 128, 192, 256) from :rfc:`6229`.
* ARC4 (KEY-LENGTH: 160) generated by this project.
  See: :doc:`/development/custom-vectors/arc4`
* Blowfish (CBC, CFB, ECB, OFB) from `Bruce Schneier's vectors`_.
* Camellia (ECB) from NTT's `Camellia page`_ as linked by `CRYPTREC`_.
* Camellia (CBC, CFB, OFB) from `OpenSSL's test vectors`_.
* CAST5 (ECB) from :rfc:`2144`.
* CAST5 (CBC, CFB, OFB) generated by this project.
  See: :doc:`/development/custom-vectors/cast5`
* ChaCha20 from :rfc:`7539`.
* ChaCha20Poly1305 from :rfc:`7539`, `OpenSSL's evpciph.txt`_, and the
  `BoringSSL ChaCha20Poly1305 tests`_.
* IDEA (ECB) from the `NESSIE IDEA vectors`_ created by `NESSIE`_.
* IDEA (CBC, CFB, OFB) generated by this project.
  See: :doc:`/development/custom-vectors/idea`
* SEED (ECB) from :rfc:`4269`.
* SEED (CBC) from :rfc:`4196`.
* SEED (CFB, OFB) generated by this project.
  See: :doc:`/development/custom-vectors/seed`

Two factor authentication
~~~~~~~~~~~~~~~~~~~~~~~~~

* HOTP from :rfc:`4226`
* TOTP from :rfc:`6238` (Note that an `errata`_ for the test vectors in RFC
  6238 exists)

CMAC
~~~~

* AES-128, AES-192, AES-256, 3DES from `NIST SP-800-38B`_

Poly1305
~~~~~~~~

* Test vectors from :rfc:`7539`.

Creating test vectors
---------------------

When official vectors are unavailable ``cryptography`` may choose to build
its own using existing vectors as source material.

Created Vectors
~~~~~~~~~~~~~~~

.. toctree::
    :maxdepth: 1

    custom-vectors/arc4
    custom-vectors/cast5
    custom-vectors/idea
    custom-vectors/seed
    custom-vectors/hkdf


If official test vectors appear in the future the custom generated vectors
should be discarded.

Any vectors generated by this method must also be prefixed with the following
header format (substituting the correct information):

.. code-block:: python

    # CAST5 CBC vectors built for https://github.com/pyca/cryptography
    # Derived from the AESVS MMT test data for CBC
    # Verified against the CommonCrypto and Go crypto packages
    # Key Length : 128

.. _`NIST`: https://www.nist.gov/
.. _`IETF`: https://www.ietf.org/
.. _`Project Wycheproof`: https://github.com/google/wycheproof
.. _`NIST CAVP`: https://csrc.nist.gov/projects/cryptographic-algorithm-validation-program
.. _`Bruce Schneier's vectors`: https://www.schneier.com/wp-content/uploads/2015/12/vectors-2.txt
.. _`Camellia page`: https://info.isl.ntt.co.jp/crypt/eng/camellia/
.. _`CRYPTREC`: https://www.cryptrec.go.jp
.. _`OpenSSL's test vectors`: https://github.com/openssl/openssl/blob/97cf1f6c2854a3a955fd7dd3a1f113deba00c9ef/crypto/evp/evptests.txt#L232
.. _`OpenSSL's evpciph.txt`: https://github.com/openssl/openssl/blob/5a7bc0be97dee9ac715897fe8180a08e211bc6ea/test/evpciph.txt#L2362
.. _`BoringSSL ChaCha20Poly1305 tests`: https://boringssl.googlesource.com/boringssl/+/2e2a226ac9201ac411a84b5e79ac3a7333d8e1c9/crypto/cipher_extra/test/chacha20_poly1305_tests.txt
.. _`BoringSSL evp tests`: https://boringssl.googlesource.com/boringssl/+/ce3773f9fe25c3b54390bc51d72572f251c7d7e6/crypto/evp/evp_tests.txt
.. _`RIPEMD website`: https://homes.esat.kuleuven.be/~bosselae/ripemd160.html
.. _`draft RFC`: https://tools.ietf.org/html/draft-josefsson-scrypt-kdf-01
.. _`Specification repository`: https://github.com/fernet/spec
.. _`errata`: https://www.rfc-editor.org/errata_search.php?rfc=6238
.. _`OpenSSL example key`: https://github.com/openssl/openssl/blob/d02b48c63a58ea4367a0e905979f140b7d090f86/test/testrsa.pem
.. _`GnuTLS key parsing tests`: https://gitlab.com/gnutls/gnutls/commit/f16ef39ef0303b02d7fa590a37820440c466ce8d
.. _`enc-rsa-pkcs8.pem`: https://gitlab.com/gnutls/gnutls/blob/f8d943b38bf74eaaa11d396112daf43cb8aa82ae/tests/pkcs8-decode/encpkcs8.pem
.. _`enc2-rsa-pkcs8.pem`: https://gitlab.com/gnutls/gnutls/blob/f8d943b38bf74eaaa11d396112daf43cb8aa82ae/tests/pkcs8-decode/enc2pkcs8.pem
.. _`unenc-rsa-pkcs8.pem`: https://gitlab.com/gnutls/gnutls/blob/f8d943b38bf74eaaa11d396112daf43cb8aa82ae/tests/pkcs8-decode/unencpkcs8.pem
.. _`pkcs12_s2k_pem.c`: https://gitlab.com/gnutls/gnutls/blob/f8d943b38bf74eaaa11d396112daf43cb8aa82ae/tests/pkcs12_s2k_pem.c
.. _`Botan's ECC private keys`: https://github.com/randombit/botan/tree/4917f26a2b154e841cd27c1bcecdd41d2bdeb6ce/src/tests/data/ecc
.. _`GnuTLS example keys`: https://gitlab.com/gnutls/gnutls/commit/ad2061deafdd7db78fd405f9d143b0a7c579da7b
.. _`NESSIE IDEA vectors`: https://www.cosic.esat.kuleuven.be/nessie/testvectors/bc/idea/Idea-128-64.verified.test-vectors
.. _`NESSIE`: https://en.wikipedia.org/wiki/NESSIE
.. _`Ed25519 website`: https://ed25519.cr.yp.to/software.html
.. _`NIST SP-800-38B`: https://csrc.nist.gov/publications/detail/sp/800-38b/archive/2005-05-01
.. _`NIST PKI Testing`: https://csrc.nist.gov/Projects/PKI-Testing
.. _`testx509.pem`: https://github.com/openssl/openssl/blob/master/test/testx509.pem
.. _`DigiCert Global Root G3`: http://cacerts.digicert.com/DigiCertGlobalRootG3.crt
.. _`root data`: https://hg.mozilla.org/projects/nss/file/25b2922cc564/security/nss/lib/ckfw/builtins/certdata.txt#l2053
.. _`asymmetric/public/PKCS1/dsa.pub.pem`: https://github.com/ruby/ruby/blob/4ccb387f3bc436a08fc6d72c4931994f5de95110/test/openssl/test_pkey_dsa.rb#L53
.. _`Mozilla bug`: https://bugzilla.mozilla.org/show_bug.cgi?id=233586
.. _`Russian CA`: https://e-trust.gosuslugi.ru/
.. _`test/evptests.txt`: https://github.com/openssl/openssl/blob/2d0b44126763f989a4cbffbffe9d0c7518158bb7/test/evptests.txt
.. _`unknown signature OID`: https://bugzilla.mozilla.org/show_bug.cgi?id=405966
.. _`botan`: https://github.com/randombit/botan/blob/57789bdfc55061002b2727d0b32587612829a37c/src/tests/data/pubkey/dh.vec
.. _`DHKE`: https://sandilands.info/sgordon/diffie-hellman-secret-key-exchange-with-openssl
.. _`Botan's key wrap vectors`: https://github.com/randombit/botan/blob/737f33c09a18500e044dca3e2ae13bd2c08bafdd/src/tests/data/keywrap/nist_key_wrap.vec
.. _`root-ed25519.pem`: https://github.com/openssl/openssl/blob/2a1e2fe145c6eb8e75aa2e1b3a8c3a49384b2852/test/certs/root-ed25519.pem
.. _`server-ed25519-cert.pem`: https://github.com/openssl/openssl/blob/2a1e2fe145c6eb8e75aa2e1b3a8c3a49384b2852/test/certs/server-ed25519-cert.pem
.. _`server-ed448-cert.pem`: https://github.com/openssl/openssl/blob/2a1e2fe145c6eb8e75aa2e1b3a8c3a49384b2852/test/certs/server-ed448-cert.pem
Doing a release
===============

Doing a release of ``cryptography`` requires a few steps.

Security Releases
-----------------

In addition to the other steps described below, for a release which fixes a
security vulnerability, you should also include the following steps:

* Request a `CVE from MITRE`_. Once you have received the CVE, it should be
  included in the :doc:`changelog`. Ideally you should request the CVE before
  starting the release process so that the CVE is available at the time of the
  release.
* Ensure that the :doc:`changelog` entry credits whoever reported the issue.
* The release should be announced on the `oss-security`_ mailing list, in
  addition to the regular announcement lists.

Verifying OpenSSL version
-------------------------

The release process creates wheels bundling OpenSSL for Windows, macOS, and
Linux. Check that the Windows, macOS, and Linux builders (both
``pyca/cryptography-manylinux1`` and ``pyca/cryptography-manylinux2010``) have
the latest OpenSSL. If anything is out of date follow the instructions for
upgrading OpenSSL.

Upgrading OpenSSL
-----------------

Use the `upgrading OpenSSL issue template`_.

Bumping the version number
--------------------------

The next step in doing a release is bumping the version number in the
software.

* Update the version number in ``src/cryptography/__about__.py``.
* Update the version number in ``vectors/cryptography_vectors/__about__.py``.
* Set the release date in the :doc:`/changelog`.
* Do a commit indicating this.
* Send a pull request with this.
* Wait for it to be merged.

Performing the release
----------------------

The commit that merged the version number bump is now the official release
commit for this release. You will need to have ``gpg`` installed and a ``gpg``
key in order to do a release. Once this has happened:

* Run ``python release.py {version}``.

The release should now be available on PyPI and a tag should be available in
the repository.

Verifying the release
---------------------

You should verify that ``pip install cryptography`` works correctly:

.. code-block:: pycon

    >>> import cryptography
    >>> cryptography.__version__
    '...'
    >>> import cryptography_vectors
    >>> cryptography_vectors.__version__
    '...'

Verify that this is the version you just released.

For the Windows wheels check the builds for the ``cryptography-wheel-builder``
job and verify that the final output for each build shows it loaded and linked
the expected OpenSSL version.

Post-release tasks
------------------

* Update the version number to the next major (e.g. ``0.5.dev1``) in
  ``src/cryptography/__about__.py`` and
  ``vectors/cryptography_vectors/__about__.py``.
* Close the `milestone`_ for the previous release on GitHub.
* Add new :doc:`/changelog` entry with next version and note that it is under
  active development
* Send a pull request with these items
* Check for any outstanding code undergoing a deprecation cycle by looking in
  ``cryptography.utils`` for ``DeprecatedIn**`` definitions. If any exist open
  a ticket to increment them for the next release.
* Send an email to the `mailing list`_ and `python-announce`_ announcing the
  release.

.. _`CVE from MITRE`: https://cveform.mitre.org/
.. _`oss-security`: https://www.openwall.com/lists/oss-security/
.. _`upgrading OpenSSL issue template`: https://github.com/pyca/cryptography/issues/new?template=openssl-release.md
.. _`milestone`: https://github.com/pyca/cryptography/milestones
.. _`mailing list`: https://mail.python.org/mailman/listinfo/cryptography-dev
.. _`python-announce`: https://mail.python.org/mailman/listinfo/python-announce-list
Exceptions
==========

.. currentmodule:: cryptography.exceptions


.. class:: UnsupportedAlgorithm

    Raised when the requested algorithm, or combination of algorithms is not
    supported.


.. class:: AlreadyFinalized

    This is raised when a context is used after being finalized.


.. class:: InvalidSignature

    This is raised when signature verification fails. This can occur with
    HMAC or asymmetric key signature validation.


.. class:: NotYetFinalized

    This is raised when the AEAD tag property is accessed on a context
    before it is finalized.


.. class:: AlreadyUpdated

    This is raised when additional data is added to a context after update
    has already been called.


.. class:: InvalidKey

    This is raised when the verify method of a key derivation function's
    computed key does not match the expected key.
Frequently asked questions
==========================

.. _faq-howto-handle-deprecation-warning:

I cannot suppress the deprecation warning that ``cryptography`` emits on import
-------------------------------------------------------------------------------

.. hint::

   The deprecation warning emitted on import does not inherit
   :py:exc:`DeprecationWarning` but inherits :py:exc:`UserWarning`
   instead.

If your pytest setup follows the best practices of failing on
emitted warnings (``filterwarnings = error``), you may ignore it
by adding the following line at the end of the list::

   ignore:Python 2 is no longer supported by the Python core team. Support for it is now deprecated in cryptography, and will be removed in a future release.:UserWarning

**Note:** Using ``cryptography.utils.CryptographyDeprecationWarning``
is not possible here because specifying it triggers
``import cryptography`` internally that emits the warning before
the ignore rule even kicks in.

Ref: https://github.com/pytest-dev/pytest/issues/7524

The same applies when you use :py:func:`~warnings.filterwarnings` in
your code or invoke CPython with :std:option:`-W` command line option.

``cryptography`` failed to install!
-----------------------------------

If you are having issues installing ``cryptography`` the first troubleshooting
step is to upgrade ``pip`` and then try to install again. For most users this will
take the form of ``pip install -U pip``, but on Windows you should do
``python -m pip install -U pip``. If you are still seeing errors after upgrading
and trying ``pip install cryptography`` again, please see the :doc:`/installation`
documentation.

How does ``cryptography`` compare to NaCl (Networking and Cryptography Library)?
--------------------------------------------------------------------------------

While ``cryptography`` and `NaCl`_ both share the goal of making cryptography
easier, and safer, to use for developers, ``cryptography`` is designed to be a
general purpose library, interoperable with existing systems, while NaCl
features a collection of hand selected algorithms.

``cryptography``'s :ref:`recipes <cryptography-layout>` layer has similar goals
to NaCl.

If you prefer NaCl's design, we highly recommend `PyNaCl`_, which is also
maintained by the PyCA team.

Why use ``cryptography``?
-------------------------

If you've done cryptographic work in Python before you have likely encountered
other libraries in Python such as *M2Crypto*, *PyCrypto*, or *PyOpenSSL*. In
building ``cryptography`` we wanted to address a few issues we observed in the
legacy libraries:

* Extremely error prone APIs and insecure defaults.
* Use of poor implementations of algorithms (i.e. ones with known side-channel
  attacks).
* Lack of maintenance.
* Lack of high level APIs.
* Lack of PyPy and Python 3 support.
* Absence of algorithms such as
  :class:`AES-GCM <cryptography.hazmat.primitives.ciphers.modes.GCM>` and
  :class:`~cryptography.hazmat.primitives.kdf.hkdf.HKDF`.

Installing ``cryptography`` produces a ``fatal error: 'openssl/opensslv.h' file not found`` error
-------------------------------------------------------------------------------------------------

``cryptography`` provides wheels which include a statically linked copy of
OpenSSL. If you see this error it is likely because your copy of ``pip`` is too
old to find our wheel files. Upgrade your ``pip`` with ``pip install -U pip``
and then try to install ``cryptography`` again.

Users on PyPy, unusual CPU architectures, or distributions of Linux using
``musl`` (like Alpine) will need to compile ``cryptography`` themselves. Please
view our :doc:`/installation` documentation.

``cryptography`` raised an ``InternalError`` and I'm not sure what to do?
-------------------------------------------------------------------------

Frequently ``InternalError`` is raised when there are errors on the OpenSSL
error stack that were placed there by other libraries that are also using
OpenSSL. Try removing the other libraries and see if the problem persists.
If you have no other libraries using OpenSSL in your process, or they do not
appear to be at fault, it's possible that this is a bug in ``cryptography``.
Please file an `issue`_ with instructions on how to reproduce it.

error: ``-Werror=sign-conversion``: No option ``-Wsign-conversion`` during installation
---------------------------------------------------------------------------------------

The compiler you are using is too old and not supported by ``cryptography``.
Please upgrade to a more recent version. If you are running OpenBSD 6.1 or
earlier the default compiler is extremely old. Use ``pkg_add`` to install a
newer ``gcc`` and then install ``cryptography`` using
``CC=/path/to/newer/gcc pip install cryptography``.

Installing ``cryptography`` fails with ``Invalid environment marker: python_version < '3'``
-------------------------------------------------------------------------------------------

Your ``pip`` and/or ``setuptools`` are outdated. Please upgrade to the latest
versions with ``pip install -U pip setuptools`` (or on Windows
``python -m pip install -U pip setuptools``).

Installing cryptography with OpenSSL 0.9.8, 1.0.0, 1.0.1, 1.0.2 fails
---------------------------------------------------------------------

The OpenSSL project has dropped support for the 0.9.8, 1.0.0, 1.0.1, and 1.0.2
release series. Since they are no longer receiving security patches from
upstream, ``cryptography`` is also dropping support for them. To fix this issue
you should upgrade to a newer version of OpenSSL (1.1.0 or later). This may
require you to upgrade to a newer operating system.

Why are there no wheels for my Python3.x version?
-------------------------------------------------

Our Python3 wheels are ``abi3`` wheels. This means they support multiple
versions of Python. The ``abi3`` wheel can be used with any version of Python
greater than or equal to the version it specifies. Recent versions of ``pip``
will automatically install ``abi3`` wheels.

Why can't I import my PEM file?
-------------------------------

PEM is a format (defined by several RFCs, but originally :rfc:`1421`) for
encoding keys, certificates and others cryptographic data into a regular form.
The data is encoded as base64 and wrapped with a header and footer.

If you are having trouble importing PEM files, make sure your file fits
the following rules:

* has a one-line header like this: ``-----BEGIN [FILE TYPE]-----``
  (where ``[FILE TYPE]`` is ``CERTIFICATE``, ``PUBLIC KEY``, ``PRIVATE KEY``,
  etc.)

* has a one-line footer like this: ``-----END [FILE TYPE]-----``

* all lines, except for the final one, must consist of exactly 64
  characters.

For example, this is a PEM file for a RSA Public Key: ::

   -----BEGIN PUBLIC KEY-----
   MIIBIjANBgkqhkiG9w0BAQEFAAOCAQ8AMIIBCgKCAQEA7CsKFSzq20NLb2VQDXma
   9DsDXtKADv0ziI5hT1KG6Bex5seE9pUoEcUxNv4uXo2jzAUgyRweRl/DLU8SoN8+
   WWd6YWik4GZvNv7j0z28h9Q5jRySxy4dmElFtIRHGiKhqd1Z06z4AzrmKEzgxkOk
   LJjY9cvwD+iXjpK2oJwNNyavvjb5YZq6V60RhpyNtKpMh2+zRLgIk9sROEPQeYfK
   22zj2CnGBMg5Gm2uPOsGDltl/I/Fdh1aO3X4i1GXwCuPf1kSAg6lPJD0batftkSG
   v0X0heUaV0j1HSNlBWamT4IR9+iJfKJHekOqvHQBcaCu7Ja4kXzx6GZ3M2j/Ja3A
   2QIDAQAB
   -----END PUBLIC KEY-----


.. _`NaCl`: https://nacl.cr.yp.to/
.. _`PyNaCl`: https://pynacl.readthedocs.io
.. _`WSGIApplicationGroup`: https://modwsgi.readthedocs.io/en/develop/configuration-directives/WSGIApplicationGroup.html
.. _`issue`: https://github.com/pyca/cryptography/issues
Fernet (symmetric encryption)
=============================

.. currentmodule:: cryptography.fernet

Fernet guarantees that a message encrypted using it cannot be
manipulated or read without the key. `Fernet`_ is an implementation of
symmetric (also known as "secret key") authenticated cryptography. Fernet also
has support for implementing key rotation via :class:`MultiFernet`.

.. class:: Fernet(key)

    This class provides both encryption and decryption facilities.

    .. doctest::

        >>> from cryptography.fernet import Fernet
        >>> key = Fernet.generate_key()
        >>> f = Fernet(key)
        >>> token = f.encrypt(b"my deep dark secret")
        >>> token
        b'...'
        >>> f.decrypt(token)
        b'my deep dark secret'

    :param key: A URL-safe base64-encoded 32-byte key. This **must** be
                kept secret. Anyone with this key is able to create and
                read messages.
    :type key: bytes or str

    .. classmethod:: generate_key()

        Generates a fresh fernet key. Keep this some place safe! If you lose it
        you'll no longer be able to decrypt messages; if anyone else gains
        access to it, they'll be able to decrypt all of your messages, and
        they'll also be able forge arbitrary messages that will be
        authenticated and decrypted.

    .. method:: encrypt(data)

        Encrypts data passed. The result of this encryption is known as a
        "Fernet token" and has strong privacy and authenticity guarantees.

        :param bytes data: The message you would like to encrypt.
        :returns bytes: A secure message that cannot be read or altered
                        without the key. It is URL-safe base64-encoded. This is
                        referred to as a "Fernet token".
        :raises TypeError: This exception is raised if ``data`` is not
                           ``bytes``.

        .. note::

            The encrypted message contains the current time when it was
            generated in *plaintext*, the time a message was created will
            therefore be visible to a possible attacker.

    .. method:: encrypt_at_time(data, current_time)

       .. versionadded:: 3.0

       Encrypts data passed using explicitly passed current time. See
       :meth:`encrypt` for the documentation of the ``data`` parameter, the
       return type and the exceptions raised.

       The motivation behind this method is for the client code to be able to
       test token expiration. Since this method can be used in an insecure
       manner one should make sure the correct time (``int(time.time())``)
       is passed as ``current_time`` outside testing.

       :param int current_time: The current time.

       .. note::

            Similarly to :meth:`encrypt` the encrypted message contains the
            timestamp in *plaintext*, in this case the timestamp is the value
            of the ``current_time`` parameter.


    .. method:: decrypt(token, ttl=None)

        Decrypts a Fernet token. If successfully decrypted you will receive the
        original plaintext as the result, otherwise an exception will be
        raised. It is safe to use this data immediately as Fernet verifies
        that the data has not been tampered with prior to returning it.

        :param bytes token: The Fernet token. This is the result of calling
                            :meth:`encrypt`.
        :param int ttl: Optionally, the number of seconds old a message may be
                        for it to be valid. If the message is older than
                        ``ttl`` seconds (from the time it was originally
                        created) an exception will be raised. If ``ttl`` is not
                        provided (or is ``None``), the age of the message is
                        not considered.
        :returns bytes: The original plaintext.
        :raises cryptography.fernet.InvalidToken: If the ``token`` is in any
                                                  way invalid, this exception
                                                  is raised. A token may be
                                                  invalid for a number of
                                                  reasons: it is older than the
                                                  ``ttl``, it is malformed, or
                                                  it does not have a valid
                                                  signature.
        :raises TypeError: This exception is raised if ``token`` is not
                           ``bytes``.

    .. method:: decrypt_at_time(token, ttl, current_time)

       .. versionadded:: 3.0

       Decrypts a token using explicitly passed current time. See
       :meth:`decrypt` for the documentation of the ``token`` and ``ttl``
       parameters (``ttl`` is required here), the return type and the exceptions
       raised.

       The motivation behind this method is for the client code to be able to
       test token expiration. Since this method can be used in an insecure
       manner one should make sure the correct time (``int(time.time())``)
       is passed as ``current_time`` outside testing.

       :param int current_time: The current time.


    .. method:: extract_timestamp(token)

        .. versionadded:: 2.3

        Returns the timestamp for the token. The caller can then decide if
        the token is about to expire and, for example, issue a new token.

        :param bytes token: The Fernet token. This is the result of calling
                            :meth:`encrypt`.
        :returns int: The UNIX timestamp of the token.
        :raises cryptography.fernet.InvalidToken: If the ``token``'s signature
                                                  is invalid this exception
                                                  is raised.
        :raises TypeError: This exception is raised if ``token`` is not
                           ``bytes``.


.. class:: MultiFernet(fernets)

    .. versionadded:: 0.7

    This class implements key rotation for Fernet. It takes a ``list`` of
    :class:`Fernet` instances and implements the same API with the exception
    of one additional method: :meth:`MultiFernet.rotate`:

    .. doctest::

        >>> from cryptography.fernet import Fernet, MultiFernet
        >>> key1 = Fernet(Fernet.generate_key())
        >>> key2 = Fernet(Fernet.generate_key())
        >>> f = MultiFernet([key1, key2])
        >>> token = f.encrypt(b"Secret message!")
        >>> token
        b'...'
        >>> f.decrypt(token)
        b'Secret message!'

    MultiFernet performs all encryption options using the *first* key in the
    ``list`` provided. MultiFernet attempts to decrypt tokens with each key in
    turn. A :class:`cryptography.fernet.InvalidToken` exception is raised if
    the correct key is not found in the ``list`` provided.

    Key rotation makes it easy to replace old keys. You can add your new key at
    the front of the list to start encrypting new messages, and remove old keys
    as they are no longer needed.

    Token rotation as offered by :meth:`MultiFernet.rotate` is a best practice
    and manner of cryptographic hygiene designed to limit damage in the event of
    an undetected event and to increase the difficulty of attacks. For example,
    if an employee who had access to your company's fernet keys leaves, you'll
    want to generate new fernet key, rotate all of the tokens currently deployed
    using that new key, and then retire the old fernet key(s) to which the
    employee had access.

    .. method:: rotate(msg)

        .. versionadded:: 2.2

        Rotates a token by re-encrypting it under the :class:`MultiFernet`
        instance's primary key. This preserves the timestamp that was originally
        saved with the token. If a token has successfully been rotated then the
        rotated token will be returned. If rotation fails this will raise an
        exception.

        .. doctest::

           >>> from cryptography.fernet import Fernet, MultiFernet
           >>> key1 = Fernet(Fernet.generate_key())
           >>> key2 = Fernet(Fernet.generate_key())
           >>> f = MultiFernet([key1, key2])
           >>> token = f.encrypt(b"Secret message!")
           >>> token
           b'...'
           >>> f.decrypt(token)
           b'Secret message!'
           >>> key3 = Fernet(Fernet.generate_key())
           >>> f2 = MultiFernet([key3, key1, key2])
           >>> rotated = f2.rotate(token)
           >>> f2.decrypt(rotated)
           b'Secret message!'

        :param bytes msg: The token to re-encrypt.
        :returns bytes: A secure message that cannot be read or altered without
           the key. This is URL-safe base64-encoded. This is referred to as a
           "Fernet token".
        :raises cryptography.fernet.InvalidToken: If a ``token`` is in any
           way invalid this exception is raised.
        :raises TypeError: This exception is raised if the ``msg`` is not
           ``bytes``.


.. class:: InvalidToken

    See :meth:`Fernet.decrypt` for more information.


Using passwords with Fernet
---------------------------

It is possible to use passwords with Fernet. To do this, you need to run the
password through a key derivation function such as
:class:`~cryptography.hazmat.primitives.kdf.pbkdf2.PBKDF2HMAC`, bcrypt or
:class:`~cryptography.hazmat.primitives.kdf.scrypt.Scrypt`.

.. doctest::

    >>> import base64
    >>> import os
    >>> from cryptography.fernet import Fernet
    >>> from cryptography.hazmat.primitives import hashes
    >>> from cryptography.hazmat.primitives.kdf.pbkdf2 import PBKDF2HMAC
    >>> password = b"password"
    >>> salt = os.urandom(16)
    >>> kdf = PBKDF2HMAC(
    ...     algorithm=hashes.SHA256(),
    ...     length=32,
    ...     salt=salt,
    ...     iterations=100000,
    ... )
    >>> key = base64.urlsafe_b64encode(kdf.derive(password))
    >>> f = Fernet(key)
    >>> token = f.encrypt(b"Secret message!")
    >>> token
    b'...'
    >>> f.decrypt(token)
    b'Secret message!'

In this scheme, the salt has to be stored in a retrievable location in order
to derive the same key from the password in the future.

The iteration count used should be adjusted to be as high as your server can
tolerate. A good default is at least 100,000 iterations which is what Django
recommended in 2014.

Implementation
--------------

Fernet is built on top of a number of standard cryptographic primitives.
Specifically it uses:

* :class:`~cryptography.hazmat.primitives.ciphers.algorithms.AES` in
  :class:`~cryptography.hazmat.primitives.ciphers.modes.CBC` mode with a
  128-bit key for encryption; using
  :class:`~cryptography.hazmat.primitives.padding.PKCS7` padding.
* :class:`~cryptography.hazmat.primitives.hmac.HMAC` using
  :class:`~cryptography.hazmat.primitives.hashes.SHA256` for authentication.
* Initialization vectors are generated using ``os.urandom()``.

For complete details consult the `specification`_.

Limitations
-----------

Fernet is ideal for encrypting data that easily fits in memory. As a design
feature it does not expose unauthenticated bytes. This means that the complete
message contents must be available in memory, making Fernet generally
unsuitable for very large files at this time.


.. _`Fernet`: https://github.com/fernet/spec/
.. _`specification`: https://github.com/fernet/spec/blob/master/Spec.md
Glossary
========

.. glossary::
    :sorted:

    plaintext
        User-readable data you care about.

    ciphertext
        The encoded data, it's not user readable. Potential attackers are able
        to see this.

    encryption
        The process of converting plaintext to ciphertext.

    decryption
        The process of converting ciphertext to plaintext.

    key
        Secret data is encoded with a function using this key. Sometimes
        multiple keys are used. These **must** be kept secret, if a key is
        exposed to an attacker, any data encrypted with it will be exposed.

    symmetric cryptography
        Cryptographic operations where encryption and decryption use the same
        key.

    public-key cryptography
    asymmetric cryptography
        Cryptographic operations where encryption and decryption use different
        keys. There are separate encryption and decryption keys. Typically
        encryption is performed using a :term:`public key`, and it can then be
        decrypted using a :term:`private key`. Asymmetric cryptography can also
        be used to create signatures, which can be generated with a
        :term:`private key` and verified with a :term:`public key`.

    public key
        This is one of two keys involved in :term:`public-key cryptography`. It
        can be used to encrypt messages for someone possessing the
        corresponding :term:`private key` and to verify signatures created with
        the corresponding :term:`private key`. This can be distributed
        publicly, hence the name.

    private key
        This is one of two keys involved in :term:`public-key cryptography`. It
        can be used to decrypt messages which were encrypted with the
        corresponding :term:`public key`, as well as to create signatures,
        which can be verified with the corresponding :term:`public key`. These
        **must** be kept secret, if they are exposed, all encrypted messages
        are compromised, and an attacker will be able to forge signatures.

    authentication
        The process of verifying that a message was created by a specific
        individual (or program). Like encryption, authentication can be either
        symmetric or asymmetric. Authentication is necessary for effective
        encryption.

    ciphertext indistinguishability
        This is a property of encryption systems whereby two encrypted messages
        aren't distinguishable without knowing the encryption key. This is
        considered a basic, necessary property for a working encryption system.

    text
        This type corresponds to ``unicode`` on Python 2 and ``str`` on Python
        3.  This is equivalent to ``six.text_type``.

    nonce
        A nonce is a **n**\ umber used **once**. Nonces are used in many
        cryptographic protocols. Generally, a nonce does not have to be secret
        or unpredictable, but it must be unique. A nonce is often a random
        or pseudo-random number (see :doc:`Random number generation
        </random-numbers>`). Since a nonce does not have to be unpredictable,
        it can also take a form of a counter.

    opaque key
        An opaque key is a type of key that allows you to perform cryptographic
        operations such as encryption, decryption, signing, and verification,
        but does not allow access to the key itself. Typically an opaque key is
        loaded from a `hardware security module`_ (HSM).

    A-label
        The ASCII compatible encoded (ACE) representation of an
        internationalized (unicode) domain name. A-labels begin with the
        prefix ``xn--``. To create an A-label from a unicode domain string use
        a library like `idna`_.

    bits
        A bit is binary value -- a value that has only two possible states.
        Typically binary values are represented visually as 0 or 1, but
        remember that their actual value is not a printable character. A byte
        on modern computers is 8 bits and represents 256 possible values. In
        cryptographic applications when you see something say it requires a 128
        bit key, you can calculate the number of bytes by dividing by 8. 128
        divided by 8 is 16, so a 128 bit key is a 16 byte key.

    bytes-like
        A bytes-like object contains binary data and supports the
        `buffer protocol`_. This includes ``bytes``, ``bytearray``, and
        ``memoryview`` objects.

    U-label
        The presentational unicode form of an internationalized domain
        name. U-labels use unicode characters outside the ASCII range and
        are encoded as A-labels when stored in certificates.

.. _`hardware security module`: https://en.wikipedia.org/wiki/Hardware_security_module
.. _`idna`: https://pypi.org/project/idna/
.. _`buffer protocol`: https://docs.python.org/3/c-api/buffer.html
.. hazmat::

Backends
========

Getting a backend
-----------------

.. currentmodule:: cryptography.hazmat.backends

``cryptography`` was designed to support multiple cryptographic backends, but
consumers rarely need this flexibility. Starting with version 3.1 ``backend``
arguments are optional and the default backend will automatically be selected
if none is specified.

On older versions you can get the default backend by calling
:func:`~default_backend`.


.. function:: default_backend()

    :returns: An object that provides at least
        :class:`~interfaces.CipherBackend`, :class:`~interfaces.HashBackend`, and
        :class:`~interfaces.HMACBackend`.

Individual backends
-------------------

.. toctree::
    :maxdepth: 1

    openssl
    interfaces
.. hazmat::

Backend interfaces
==================

.. currentmodule:: cryptography.hazmat.backends.interfaces


Backend implementations may provide a number of interfaces to support
operations such as :doc:`/hazmat/primitives/symmetric-encryption`,
:doc:`/hazmat/primitives/cryptographic-hashes`, and
:doc:`/hazmat/primitives/mac/hmac`.

A specific ``backend`` may provide one or more of these interfaces.


.. class:: CipherBackend

    A backend that provides methods for using ciphers for encryption
    and decryption.

    The following backends implement this interface:

    * :doc:`/hazmat/backends/openssl`

    .. method:: cipher_supported(cipher, mode)

        Check if a ``cipher`` and ``mode`` combination is supported by
        this backend.

        :param cipher: An instance of
            :class:`~cryptography.hazmat.primitives.ciphers.CipherAlgorithm`.

        :param mode: An instance of
            :class:`~cryptography.hazmat.primitives.ciphers.modes.Mode`.

        :returns: ``True`` if the specified ``cipher`` and ``mode`` combination
            is supported by this backend, otherwise ``False``


    .. method:: create_symmetric_encryption_ctx(cipher, mode)

        Create a
        :class:`~cryptography.hazmat.primitives.ciphers.CipherContext` that
        can be used for encrypting data with the symmetric ``cipher`` using
        the given ``mode``.

        :param cipher: An instance of
            :class:`~cryptography.hazmat.primitives.ciphers.CipherAlgorithm`.

        :param mode: An instance of
            :class:`~cryptography.hazmat.primitives.ciphers.modes.Mode`.

        :returns:
            :class:`~cryptography.hazmat.primitives.ciphers.CipherContext`

        :raises ValueError: When tag is not None in an AEAD mode


    .. method:: create_symmetric_decryption_ctx(cipher, mode)

        Create a
        :class:`~cryptography.hazmat.primitives.ciphers.CipherContext` that
        can be used for decrypting data with the symmetric ``cipher`` using
        the given ``mode``.

        :param cipher: An instance of
            :class:`~cryptography.hazmat.primitives.ciphers.CipherAlgorithm`.

        :param mode: An instance of
            :class:`~cryptography.hazmat.primitives.ciphers.modes.Mode`.

        :returns:
            :class:`~cryptography.hazmat.primitives.ciphers.CipherContext`

        :raises ValueError: When tag is None in an AEAD mode


.. class:: HashBackend

    A backend with methods for using cryptographic hash functions.

    The following backends implement this interface:

    * :doc:`/hazmat/backends/openssl`

    .. method:: hash_supported(algorithm)

        Check if the specified ``algorithm`` is supported by this backend.

        :param algorithm: An instance of
            :class:`~cryptography.hazmat.primitives.hashes.HashAlgorithm`.

        :returns: ``True`` if the specified ``algorithm`` is supported by this
            backend, otherwise ``False``.


    .. method:: create_hash_ctx(algorithm)

        Create a
        :class:`~cryptography.hazmat.primitives.hashes.HashContext` that
        uses the specified ``algorithm`` to calculate a message digest.

        :param algorithm: An instance of
            :class:`~cryptography.hazmat.primitives.hashes.HashAlgorithm`.

        :returns:
            :class:`~cryptography.hazmat.primitives.hashes.HashContext`


.. class:: HMACBackend

    A backend with methods for using cryptographic hash functions as message
    authentication codes.

    The following backends implement this interface:

    * :doc:`/hazmat/backends/openssl`

    .. method:: hmac_supported(algorithm)

        Check if the specified ``algorithm`` is supported by this backend.

        :param algorithm: An instance of
            :class:`~cryptography.hazmat.primitives.hashes.HashAlgorithm`.

        :returns: ``True`` if the specified ``algorithm`` is supported for HMAC
            by this backend, otherwise ``False``.

    .. method:: create_hmac_ctx(key, algorithm)

        Create a
        :class:`~cryptography.hazmat.primitives.hashes.HashContext` that
        uses the specified ``algorithm`` to calculate a hash-based message
        authentication code.

        :param bytes key: Secret key as ``bytes``.

        :param algorithm: An instance of
            :class:`~cryptography.hazmat.primitives.hashes.HashAlgorithm`.

        :returns:
            :class:`~cryptography.hazmat.primitives.hashes.HashContext`


.. class:: CMACBackend

    .. versionadded:: 0.4

    A backend with methods for using CMAC

    .. method:: cmac_algorithm_supported(algorithm)

        :param algorithm: An instance of
            :class:`~cryptography.hazmat.primitives.ciphers.BlockCipherAlgorithm`.

        :return: Returns True if the block cipher is supported for CMAC by this backend

    .. method:: create_cmac_ctx(algorithm)

        Create a
        context that
        uses the specified ``algorithm`` to calculate a message authentication code.

        :param algorithm: An instance of
            :class:`~cryptography.hazmat.primitives.ciphers.BlockCipherAlgorithm`.

        :returns: CMAC object.


.. class:: PBKDF2HMACBackend

    .. versionadded:: 0.2

    A backend with methods for using PBKDF2 using HMAC as a PRF.

    The following backends implement this interface:

    * :doc:`/hazmat/backends/openssl`

    .. method:: pbkdf2_hmac_supported(algorithm)

        Check if the specified ``algorithm`` is supported by this backend.

        :param algorithm: An instance of
            :class:`~cryptography.hazmat.primitives.hashes.HashAlgorithm`.

        :returns: ``True`` if the specified ``algorithm`` is supported for
            PBKDF2 HMAC by this backend, otherwise ``False``.

    .. method:: derive_pbkdf2_hmac(self, algorithm, length, salt, iterations, key_material)

        :param algorithm: An instance of
            :class:`~cryptography.hazmat.primitives.hashes.HashAlgorithm`.

        :param int length: The desired length of the derived key. Maximum is
            (2\ :sup:`32` - 1) * ``algorithm.digest_size``

        :param bytes salt: A salt.

        :param int iterations: The number of iterations to perform of the hash
            function. This can be used to control the length of time the
            operation takes. Higher numbers help mitigate brute force attacks
            against derived keys.

        :param bytes key_material: The key material to use as a basis for
            the derived key. This is typically a password.

        :return bytes: Derived key.


.. class:: RSABackend

    .. versionadded:: 0.2

    A backend with methods for using RSA.

    .. method:: generate_rsa_private_key(public_exponent, key_size)

        :param int public_exponent: The public exponent of the new key.
            Often one of the small Fermat primes 3, 5, 17, 257 or 65537.

        :param int key_size: The length in bits of the modulus. Should be
            at least 2048.

        :return: A new instance of
            :class:`~cryptography.hazmat.primitives.asymmetric.rsa.RSAPrivateKey`.

        :raises ValueError: If the public_exponent is not valid.

    .. method:: rsa_padding_supported(padding)

        Check if the specified ``padding`` is supported by the backend.

        :param padding: An instance of
            :class:`~cryptography.hazmat.primitives.asymmetric.padding.AsymmetricPadding`.

        :returns: ``True`` if the specified ``padding`` is supported by this
            backend, otherwise ``False``.

    .. method:: generate_rsa_parameters_supported(public_exponent, key_size)

        Check if the specified parameters are supported for key generation by
        the backend.

        :param int public_exponent: The public exponent.

        :param int key_size: The bit length of the generated modulus.

    .. method:: load_rsa_private_numbers(numbers)

        :param numbers: An instance of
            :class:`~cryptography.hazmat.primitives.asymmetric.rsa.RSAPrivateNumbers`.

        :returns: An instance of
            :class:`~cryptography.hazmat.primitives.asymmetric.rsa.RSAPrivateKey`.

        :raises ValueError: This is raised when the values of ``p``, ``q``,
            ``private_exponent``, ``public_exponent``, or ``modulus`` do not
            match the bounds specified in :rfc:`3447`.

        :raises cryptography.exceptions.UnsupportedAlgorithm: This is raised
            when any backend specific criteria are not met.

    .. method:: load_rsa_public_numbers(numbers)

        :param numbers: An instance of
            :class:`~cryptography.hazmat.primitives.asymmetric.rsa.RSAPublicNumbers`.

        :returns: An instance of
            :class:`~cryptography.hazmat.primitives.asymmetric.rsa.RSAPublicKey`.

        :raises ValueError: This is raised when the values of
            ``public_exponent`` or ``modulus`` do not match the bounds
            specified in :rfc:`3447`.

        :raises cryptography.exceptions.UnsupportedAlgorithm: This is raised
            when any backend specific criteria are not met.


.. class:: DSABackend

    .. versionadded:: 0.4

    A backend with methods for using DSA.

    .. method:: generate_dsa_parameters(key_size)

        :param int key_size: The length of the modulus in bits. It should be
            either 1024, 2048 or 3072. For keys generated in 2015 this should
            be at least 2048.
            Note that some applications (such as SSH) have not yet gained
            support for larger key sizes specified in FIPS 186-3 and are still
            restricted to only the 1024-bit keys specified in FIPS 186-2.

        :return: A new instance of
            :class:`~cryptography.hazmat.primitives.asymmetric.dsa.DSAParameters`.

    .. method:: generate_dsa_private_key(parameters)

        :param parameters: An instance of
            :class:`~cryptography.hazmat.primitives.asymmetric.dsa.DSAParameters`.

        :return: A new instance of
            :class:`~cryptography.hazmat.primitives.asymmetric.dsa.DSAPrivateKey`.

        :raises ValueError: This is raised if the key size is not one of 1024,
            2048, or 3072.

    .. method:: generate_dsa_private_key_and_parameters(key_size)

        :param int key_size: The length of the modulus in bits. It should be
            either 1024, 2048 or 3072. For keys generated in 2015 this should
            be at least 2048.
            Note that some applications (such as SSH) have not yet gained
            support for larger key sizes specified in FIPS 186-3 and are still
            restricted to only the 1024-bit keys specified in FIPS 186-2.

        :return: A new instance of
            :class:`~cryptography.hazmat.primitives.asymmetric.dsa.DSAPrivateKey`.

        :raises ValueError: This is raised if the key size is not supported
            by the backend.

    .. method:: dsa_hash_supported(algorithm)

        :param algorithm: An instance of
            :class:`~cryptography.hazmat.primitives.hashes.HashAlgorithm`.

        :returns: ``True`` if the specified ``algorithm`` is supported by this
            backend, otherwise ``False``.

    .. method:: dsa_parameters_supported(p, q, g)

        :param int p: The p value of a DSA key.

        :param int q: The q value of a DSA key.

        :param int g: The g value of a DSA key.

        :returns: ``True`` if the given values of ``p``, ``q``, and ``g`` are
            supported by this backend, otherwise ``False``.

    .. method:: load_dsa_parameter_numbers(numbers)

        :param numbers: An instance of
            :class:`~cryptography.hazmat.primitives.asymmetric.dsa.DSAParameterNumbers`.

        :returns: An instance of
            :class:`~cryptography.hazmat.primitives.asymmetric.dsa.DSAParameters`.

        :raises cryptography.exceptions.UnsupportedAlgorithm: This is raised
            when any backend specific criteria are not met.

    .. method:: load_dsa_private_numbers(numbers)

        :param numbers: An instance of
            :class:`~cryptography.hazmat.primitives.asymmetric.dsa.DSAPrivateNumbers`.

        :returns: An instance of
            :class:`~cryptography.hazmat.primitives.asymmetric.dsa.DSAPrivateKey`.

        :raises cryptography.exceptions.UnsupportedAlgorithm: This is raised
            when any backend specific criteria are not met.

    .. method:: load_dsa_public_numbers(numbers)

        :param numbers: An instance of
            :class:`~cryptography.hazmat.primitives.asymmetric.dsa.DSAPublicNumbers`.

        :returns: An instance of
            :class:`~cryptography.hazmat.primitives.asymmetric.dsa.DSAPublicKey`.

        :raises cryptography.exceptions.UnsupportedAlgorithm: This is raised
            when any backend specific criteria are not met.


.. class:: EllipticCurveBackend

    .. versionadded:: 0.5

    .. method:: elliptic_curve_supported(curve)

        :param curve: An instance of
            :class:`~cryptography.hazmat.primitives.asymmetric.ec.EllipticCurve`.

        :returns: True if the elliptic curve is supported by this backend.

    .. method:: elliptic_curve_signature_algorithm_supported(signature_algorithm, curve)

        :param signature_algorithm: An instance of
            :class:`~cryptography.hazmat.primitives.asymmetric.ec.EllipticCurveSignatureAlgorithm`.

        :param curve: An instance of
            :class:`~cryptography.hazmat.primitives.asymmetric.ec.EllipticCurve`.

        :returns: True if the signature algorithm and curve are supported by this backend.

    .. method:: generate_elliptic_curve_private_key(curve)

        :param curve: An instance of
            :class:`~cryptography.hazmat.primitives.asymmetric.ec.EllipticCurve`.

    .. method:: load_elliptic_curve_private_numbers(numbers)

        :param numbers: An instance of
            :class:`~cryptography.hazmat.primitives.asymmetric.ec.EllipticCurvePrivateNumbers`.

        :returns: An instance of
            :class:`~cryptography.hazmat.primitives.asymmetric.ec.EllipticCurvePrivateKey`.

    .. method:: load_elliptic_curve_public_numbers(numbers)

        :param numbers: An instance of
            :class:`~cryptography.hazmat.primitives.asymmetric.ec.EllipticCurvePublicNumbers`.

        :returns: An instance of
            :class:`~cryptography.hazmat.primitives.asymmetric.ec.EllipticCurvePublicKey`.

    .. method:: derive_elliptic_curve_private_key(private_value, curve)

        :param private_value: A secret scalar value.

        :param curve: An instance of
            :class:`~cryptography.hazmat.primitives.asymmetric.ec.EllipticCurve`.

        :returns: An instance of
            :class:`~cryptography.hazmat.primitives.asymmetric.ec.EllipticCurvePrivateKey`.

.. class:: PEMSerializationBackend

    .. versionadded:: 0.6

    A backend with methods for working with any PEM encoded keys.

    .. method:: load_pem_private_key(data, password)

        :param bytes data: PEM data to load.
        :param bytes password: The password to use if the data is encrypted.
            Should be ``None`` if the data is not encrypted.
        :return: A new instance of the appropriate type of private key that the
            serialized data contains.
        :raises ValueError: If the data could not be deserialized.
        :raises cryptography.exceptions.UnsupportedAlgorithm: If the data is
            encrypted with an unsupported algorithm.

    .. method:: load_pem_public_key(data)

        :param bytes data: PEM data to load.
        :return: A new instance of the appropriate type of public key
            serialized data contains.
        :raises ValueError: If the data could not be deserialized.

    .. method:: load_pem_parameters(data)

        .. versionadded:: 2.0

        :param bytes data: PEM data to load.
        :return: A new instance of the appropriate type of asymmetric
            parameters the serialized data contains.
        :raises ValueError: If the data could not be deserialized.

.. class:: DERSerializationBackend

    .. versionadded:: 0.8

    A backend with methods for working with DER encoded keys.

    .. method:: load_der_private_key(data, password)

        :param bytes data: DER data to load.
        :param bytes password: The password to use if the data is encrypted.
            Should be ``None`` if the data is not encrypted.
        :return: A new instance of the appropriate type of private key that the
            serialized data contains.
        :raises ValueError: If the data could not be deserialized.
        :raises cryptography.exceptions.UnsupportedAlgorithm: If the data is
            encrypted with an unsupported algorithm.

    .. method:: load_der_public_key(data)

        :param bytes data: DER data to load.
        :return: A new instance of the appropriate type of public key
            serialized data contains.
        :raises ValueError: If the data could not be deserialized.

    .. method:: load_der_parameters(data)

        .. versionadded:: 2.0

        :param bytes data: DER data to load.
        :return: A new instance of the appropriate type of asymmetric
            parameters the serialized data contains.
        :raises ValueError: If the data could not be deserialized.


.. class:: X509Backend

    .. versionadded:: 0.7

    A backend with methods for working with X.509 objects.

    .. method:: load_pem_x509_certificate(data)

        :param bytes data: PEM formatted certificate data.

        :returns: An instance of :class:`~cryptography.x509.Certificate`.

    .. method:: load_der_x509_certificate(data)

        :param bytes data: DER formatted certificate data.

        :returns: An instance of :class:`~cryptography.x509.Certificate`.

    .. method:: load_pem_x509_csr(data)

        .. versionadded:: 0.9

        :param bytes data: PEM formatted certificate signing request data.

        :returns: An instance of
            :class:`~cryptography.x509.CertificateSigningRequest`.

    .. method:: load_der_x509_csr(data)

        .. versionadded:: 0.9

        :param bytes data: DER formatted certificate signing request data.

        :returns: An instance of
            :class:`~cryptography.x509.CertificateSigningRequest`.

    .. method:: create_x509_csr(builder, private_key, algorithm)

        .. versionadded:: 1.0

        :param builder: An instance of
            :class:`~cryptography.x509.CertificateSigningRequestBuilder`.

        :param private_key: The
            :class:`~cryptography.hazmat.primitives.asymmetric.rsa.RSAPrivateKey`,
            :class:`~cryptography.hazmat.primitives.asymmetric.dsa.DSAPrivateKey` or
            :class:`~cryptography.hazmat.primitives.asymmetric.ec.EllipticCurvePrivateKey`
            that will be used to sign the request.  When the request is
            signed by a certificate authority, the private key's associated
            public key will be stored in the resulting certificate.

        :param algorithm: The
            :class:`~cryptography.hazmat.primitives.hashes.HashAlgorithm`
            that will be used to generate the request signature.

        :returns: A new instance of
            :class:`~cryptography.x509.CertificateSigningRequest`.

    .. method:: create_x509_certificate(builder, private_key, algorithm)

        .. versionadded:: 1.0

        :param builder: An instance of
            :class:`~cryptography.x509.CertificateBuilder`.

        :param private_key: The
            :class:`~cryptography.hazmat.primitives.asymmetric.rsa.RSAPrivateKey`,
            :class:`~cryptography.hazmat.primitives.asymmetric.dsa.DSAPrivateKey` or
            :class:`~cryptography.hazmat.primitives.asymmetric.ec.EllipticCurvePrivateKey`
            that will be used to sign the certificate.

        :param algorithm: The
            :class:`~cryptography.hazmat.primitives.hashes.HashAlgorithm`
            that will be used to generate the certificate signature.

        :returns: A new instance of :class:`~cryptography.x509.Certificate`.

    .. method:: create_x509_crl(builder, private_key, algorithm)

        .. versionadded:: 1.2

        :param builder: An instance of
            :class:`~cryptography.x509.CertificateRevocationListBuilder`.

        :param private_key: The
            :class:`~cryptography.hazmat.primitives.asymmetric.rsa.RSAPrivateKey`,
            :class:`~cryptography.hazmat.primitives.asymmetric.dsa.DSAPrivateKey` or
            :class:`~cryptography.hazmat.primitives.asymmetric.ec.EllipticCurvePrivateKey`
            that will be used to sign the CRL.

        :param algorithm: The
            :class:`~cryptography.hazmat.primitives.hashes.HashAlgorithm`
            that will be used to generate the CRL signature.

        :returns: A new instance of
            :class:`~cryptography.x509.CertificateRevocationList`.

    .. method:: create_x509_revoked_certificate(builder)

        .. versionadded:: 1.2

        :param builder: An instance of RevokedCertificateBuilder.

        :returns: A new instance of
            :class:`~cryptography.x509.RevokedCertificate`.

    .. method:: x509_name_bytes(name)

        .. versionadded:: 1.6

        :param name: An instance of :class:`~cryptography.x509.Name`.

        :return bytes: The DER encoded bytes.

.. class:: DHBackend

    .. versionadded:: 0.9

    A backend with methods for doing Diffie-Hellman key exchange.

    .. method:: generate_dh_parameters(generator, key_size)

        :param int generator: The generator to use. Often 2 or 5.

        :param int key_size: The bit length of the prime modulus to generate.

        :return: A new instance of
            :class:`~cryptography.hazmat.primitives.asymmetric.dh.DHParameters`.

        :raises ValueError: If ``key_size`` is not at least 512.

    .. method:: generate_dh_private_key(parameters)

        :param parameters: An instance of
            :class:`~cryptography.hazmat.primitives.asymmetric.dh.DHParameters`.

        :return: A new instance of
            :class:`~cryptography.hazmat.primitives.asymmetric.dh.DHPrivateKey`.

    .. method:: generate_dh_private_key_and_parameters(generator, key_size)

        :param int generator: The generator to use. Often 2 or 5.

        :param int key_size: The bit length of the prime modulus to generate.

        :return: A new instance of
            :class:`~cryptography.hazmat.primitives.asymmetric.dh.DHPrivateKey`.

        :raises ValueError: If ``key_size`` is not at least 512.

    .. method:: load_dh_private_numbers(numbers)

        :param numbers: A
            :class:`~cryptography.hazmat.primitives.asymmetric.dh.DHPrivateNumbers`
            instance.

        :return: A new instance of
            :class:`~cryptography.hazmat.primitives.asymmetric.dh.DHPrivateKey`.

        :raises cryptography.exceptions.UnsupportedAlgorithm: This is raised
            when any backend specific criteria are not met.

    .. method:: load_dh_public_numbers(numbers)

        :param numbers: A
            :class:`~cryptography.hazmat.primitives.asymmetric.dh.DHPublicNumbers`
            instance.

        :return: A new instance of
            :class:`~cryptography.hazmat.primitives.asymmetric.dh.DHPublicKey`.

        :raises cryptography.exceptions.UnsupportedAlgorithm: This is raised
            when any backend specific criteria are not met.

    .. method:: load_dh_parameter_numbers(numbers)

        :param numbers: A
            :class:`~cryptography.hazmat.primitives.asymmetric.dh.DHParameterNumbers`
            instance.

        :return: A new instance of
            :class:`~cryptography.hazmat.primitives.asymmetric.dh.DHParameters`.

        :raises cryptography.exceptions.UnsupportedAlgorithm: This is raised
            when any backend specific criteria are not met.

    .. method:: dh_parameters_supported(p, g, q=None)

        :param int p: The p value of the DH key.

        :param int g: The g value of the DH key.

        :param int q: The q value of the DH key.

        :returns: ``True`` if the given values of ``p``, ``g`` and ``q``
            are supported by this backend, otherwise ``False``.

    .. versionadded:: 1.8

    .. method:: dh_x942_serialization_supported()

        :returns: True if serialization of DH objects with
            subgroup order (q) is supported by this backend.


.. class:: ScryptBackend

    .. versionadded:: 1.6

    A backend with methods for using Scrypt.

    The following backends implement this interface:

    * :doc:`/hazmat/backends/openssl`

    .. method:: derive_scrypt(self, key_material, salt, length, n, r, p)

        :param bytes key_material: The key material to use as a basis for
            the derived key. This is typically a password.

        :param bytes salt: A salt.

        :param int length: The desired length of the derived key.

        :param int n: CPU/Memory cost parameter. It must be larger than 1 and be a
            power of 2.

        :param int r: Block size parameter.

        :param int p: Parallelization parameter.

        :return bytes: Derived key.

.. hazmat::

OpenSSL backend
===============

The `OpenSSL`_ C library. Cryptography supports OpenSSL version 1.1.0 and
greater.

.. data:: cryptography.hazmat.backends.openssl.backend

    This is the exposed API for the OpenSSL backend.

    It implements the following interfaces:

    * :class:`~cryptography.hazmat.backends.interfaces.CipherBackend`
    * :class:`~cryptography.hazmat.backends.interfaces.CMACBackend`
    * :class:`~cryptography.hazmat.backends.interfaces.DERSerializationBackend`
    * :class:`~cryptography.hazmat.backends.interfaces.DHBackend`
    * :class:`~cryptography.hazmat.backends.interfaces.DSABackend`
    * :class:`~cryptography.hazmat.backends.interfaces.EllipticCurveBackend`
    * :class:`~cryptography.hazmat.backends.interfaces.HashBackend`
    * :class:`~cryptography.hazmat.backends.interfaces.HMACBackend`
    * :class:`~cryptography.hazmat.backends.interfaces.PBKDF2HMACBackend`
    * :class:`~cryptography.hazmat.backends.interfaces.RSABackend`
    * :class:`~cryptography.hazmat.backends.interfaces.PEMSerializationBackend`
    * :class:`~cryptography.hazmat.backends.interfaces.X509Backend`

    It also implements the following interface for OpenSSL versions ``1.1.0``
    and above.

    * :class:`~cryptography.hazmat.backends.interfaces.ScryptBackend`

    It also exposes the following:

    .. attribute:: name

        The string name of this backend: ``"openssl"``

    .. method:: openssl_version_text()

        :return text: The friendly string name of the loaded OpenSSL library.
            This is not necessarily the same version as it was compiled against.

    .. method:: openssl_version_number()

        .. versionadded:: 1.8

        :return int: The integer version of the loaded OpenSSL library. This is
            defined in ``opensslv.h`` as ``OPENSSL_VERSION_NUMBER`` and is
            typically shown in hexadecimal (e.g. ``0x1010003f``). This is
            not necessarily the same version as it was compiled against.

    .. method:: activate_osrandom_engine()

        Activates the OS random engine. This will effectively disable OpenSSL's
        default CSPRNG.

    .. method:: osrandom_engine_implementation()

        .. versionadded:: 1.7

        Returns the implementation of OS random engine.

    .. method:: activate_builtin_random()

        This will activate the default OpenSSL CSPRNG.

OS random engine
----------------

.. note::

    As of OpenSSL 1.1.1d its CSPRNG is fork-safe by default.
    ``cryptography`` does not compile or load the custom engine on
    these versions.

By default OpenSSL uses a user-space CSPRNG that is seeded from system random (
``/dev/urandom`` or ``CryptGenRandom``). This CSPRNG is not reseeded
automatically when a process calls ``fork()``. This can result in situations
where two different processes can return similar or identical keys and
compromise the security of the system.

The approach this project has chosen to mitigate this vulnerability is to
include an engine that replaces the OpenSSL default CSPRNG with one that
sources its entropy from ``/dev/urandom`` on UNIX-like operating systems and
uses ``CryptGenRandom`` on Windows. This method of pulling from the system pool
allows us to avoid potential issues with `initializing the RNG`_ as well as
protecting us from the ``fork()`` weakness.

This engine is **active** by default when importing the OpenSSL backend. When
active this engine will be used to generate all the random data OpenSSL
requests.

When importing only the binding it is added to the engine list but
**not activated**.


OS random sources
-----------------

On macOS and FreeBSD ``/dev/urandom`` is an alias for ``/dev/random``. The
implementation on macOS uses the `Yarrow`_ algorithm. FreeBSD uses the
`Fortuna`_ algorithm.

On Windows the implementation of ``CryptGenRandom`` depends on which version of
the operation system you are using. See the `Microsoft documentation`_ for more
details.

Linux uses its own PRNG design. ``/dev/urandom`` is a non-blocking source
seeded from the same pool as ``/dev/random``.

+------------------------------------------+------------------------------+
| Windows                                  | ``CryptGenRandom()``         |
+------------------------------------------+------------------------------+
| Linux >= 3.17 with working               | ``getrandom()``              |
| ``SYS_getrandom`` syscall                |                              |
+------------------------------------------+------------------------------+
| OpenBSD >= 5.6                           | ``getentropy()``             |
+------------------------------------------+------------------------------+
| BSD family (including macOS 10.12+) with | ``getentropy()``             |
| ``SYS_getentropy`` in ``sys/syscall.h``  |                              |
+------------------------------------------+------------------------------+
| fallback                                 | ``/dev/urandom`` with        |
|                                          | cached file descriptor       |
+------------------------------------------+------------------------------+


.. _`OpenSSL`: https://www.openssl.org/
.. _`initializing the RNG`: https://en.wikipedia.org/wiki/OpenSSL#Predictable_private_keys_.28Debian-specific.29
.. _`Fortuna`: https://en.wikipedia.org/wiki/Fortuna_(PRNG)
.. _`Yarrow`: https://en.wikipedia.org/wiki/Yarrow_algorithm
.. _`Microsoft documentation`: https://docs.microsoft.com/en-us/windows/desktop/api/wincrypt/nf-wincrypt-cryptgenrandom
.. hazmat::


Authenticated encryption
========================

.. module:: cryptography.hazmat.primitives.ciphers.aead

Authenticated encryption with associated data (AEAD) are encryption schemes
which provide both confidentiality and integrity for their ciphertext. They
also support providing integrity for associated data which is not encrypted.

.. class:: ChaCha20Poly1305(key)

    .. versionadded:: 2.0

    The ChaCha20Poly1305 construction is defined in :rfc:`7539` section 2.8.
    It is a stream cipher combined with a MAC that offers strong integrity
    guarantees.

    :param key: A 32-byte key. This **must** be kept secret.
    :type key: :term:`bytes-like`

    :raises cryptography.exceptions.UnsupportedAlgorithm: If the version of
        OpenSSL does not support ChaCha20Poly1305.

    .. doctest::

        >>> import os
        >>> from cryptography.hazmat.primitives.ciphers.aead import ChaCha20Poly1305
        >>> data = b"a secret message"
        >>> aad = b"authenticated but unencrypted data"
        >>> key = ChaCha20Poly1305.generate_key()
        >>> chacha = ChaCha20Poly1305(key)
        >>> nonce = os.urandom(12)
        >>> ct = chacha.encrypt(nonce, data, aad)
        >>> chacha.decrypt(nonce, ct, aad)
        b'a secret message'

    .. classmethod:: generate_key()

        Securely generates a random ChaCha20Poly1305 key.

        :returns bytes: A 32 byte key.

    .. method:: encrypt(nonce, data, associated_data)

        .. warning::

            Reuse of a ``nonce`` with a given ``key`` compromises the security
            of any message with that ``nonce`` and ``key`` pair.

        Encrypts the ``data`` provided and authenticates the
        ``associated_data``.  The output of this can be passed directly
        to the ``decrypt`` method.

        :param nonce: A 12 byte value. **NEVER REUSE A NONCE** with a key.
        :type nonce: :term:`bytes-like`
        :param bytes data: The data to encrypt.
        :param bytes associated_data: Additional data that should be
            authenticated with the key, but does not need to be encrypted. Can
            be ``None``.
        :returns bytes: The ciphertext bytes with the 16 byte tag appended.
        :raises OverflowError: If ``data`` or ``associated_data`` is larger
            than 2\ :sup:`32` bytes.

    .. method:: decrypt(nonce, data, associated_data)

        Decrypts the ``data`` and authenticates the ``associated_data``. If you
        called encrypt with ``associated_data`` you must pass the same
        ``associated_data`` in decrypt or the integrity check will fail.

        :param nonce: A 12 byte value. **NEVER REUSE A NONCE** with a
            key.
        :type nonce: :term:`bytes-like`
        :param bytes data: The data to decrypt (with tag appended).
        :param bytes associated_data: Additional data to authenticate. Can be
            ``None`` if none was passed during encryption.
        :returns bytes: The original plaintext.
        :raises cryptography.exceptions.InvalidTag: If the authentication tag
            doesn't validate this exception will be raised. This will occur
            when the ciphertext has been changed, but will also occur when the
            key, nonce, or associated data are wrong.

.. class:: AESGCM(key)

    .. versionadded:: 2.0

    The AES-GCM construction is composed of the
    :class:`~cryptography.hazmat.primitives.ciphers.algorithms.AES` block
    cipher utilizing Galois Counter Mode (GCM).

    :param key: A 128, 192, or 256-bit key. This **must** be kept secret.
    :type key: :term:`bytes-like`

    .. doctest::

        >>> import os
        >>> from cryptography.hazmat.primitives.ciphers.aead import AESGCM
        >>> data = b"a secret message"
        >>> aad = b"authenticated but unencrypted data"
        >>> key = AESGCM.generate_key(bit_length=128)
        >>> aesgcm = AESGCM(key)
        >>> nonce = os.urandom(12)
        >>> ct = aesgcm.encrypt(nonce, data, aad)
        >>> aesgcm.decrypt(nonce, ct, aad)
        b'a secret message'

    .. classmethod:: generate_key(bit_length)

        Securely generates a random AES-GCM key.

        :param bit_length: The bit length of the key to generate. Must be
            128, 192, or 256.

        :returns bytes: The generated key.

    .. method:: encrypt(nonce, data, associated_data)

        .. warning::

            Reuse of a ``nonce`` with a given ``key`` compromises the security
            of any message with that ``nonce`` and ``key`` pair.

        Encrypts and authenticates the ``data`` provided as well as
        authenticating the ``associated_data``.  The output of this can be
        passed directly to the ``decrypt`` method.

        :param nonce: NIST `recommends a 96-bit IV length`_ for best
            performance but it can be up to 2\ :sup:`64` - 1 :term:`bits`.
            **NEVER REUSE A NONCE** with a key.
        :type nonce: :term:`bytes-like`
        :param bytes data: The data to encrypt.
        :param bytes associated_data: Additional data that should be
            authenticated with the key, but is not encrypted. Can be ``None``.
        :returns bytes: The ciphertext bytes with the 16 byte tag appended.
        :raises OverflowError: If ``data`` or ``associated_data`` is larger
            than 2\ :sup:`32` bytes.

    .. method:: decrypt(nonce, data, associated_data)

        Decrypts the ``data`` and authenticates the ``associated_data``. If you
        called encrypt with ``associated_data`` you must pass the same
        ``associated_data`` in decrypt or the integrity check will fail.

        :param nonce: NIST `recommends a 96-bit IV length`_ for best
            performance but it can be up to 2\ :sup:`64` - 1 :term:`bits`.
            **NEVER REUSE A NONCE** with a key.
        :type nonce: :term:`bytes-like`
        :param bytes data: The data to decrypt (with tag appended).
        :param bytes associated_data: Additional data to authenticate. Can be
            ``None`` if none was passed during encryption.
        :returns bytes: The original plaintext.
        :raises cryptography.exceptions.InvalidTag: If the authentication tag
            doesn't validate this exception will be raised. This will occur
            when the ciphertext has been changed, but will also occur when the
            key, nonce, or associated data are wrong.

.. class:: AESCCM(key, tag_length=16)

    .. versionadded:: 2.0

    .. note:

        AES-CCM is provided largely for compatibility with existing protocols.
        Due to its construction it is not as computationally efficient as
        other AEAD ciphers.

    The AES-CCM construction is composed of the
    :class:`~cryptography.hazmat.primitives.ciphers.algorithms.AES` block
    cipher utilizing Counter with CBC-MAC (CCM) (specified in :rfc:`3610`).

    :param key: A 128, 192, or 256-bit key. This **must** be kept secret.
    :type key: :term:`bytes-like`
    :param int tag_length: The length of the authentication tag. This
        defaults to 16 bytes and it is **strongly** recommended that you
        do not make it shorter unless absolutely necessary. Valid tag
        lengths are 4, 6, 8, 10, 12, 14, and 16.

    :raises cryptography.exceptions.UnsupportedAlgorithm: If the version of
        OpenSSL does not support AES-CCM.

    .. doctest::

        >>> import os
        >>> from cryptography.hazmat.primitives.ciphers.aead import AESCCM
        >>> data = b"a secret message"
        >>> aad = b"authenticated but unencrypted data"
        >>> key = AESCCM.generate_key(bit_length=128)
        >>> aesccm = AESCCM(key)
        >>> nonce = os.urandom(13)
        >>> ct = aesccm.encrypt(nonce, data, aad)
        >>> aesccm.decrypt(nonce, ct, aad)
        b'a secret message'

    .. classmethod:: generate_key(bit_length)

        Securely generates a random AES-CCM key.

        :param bit_length: The bit length of the key to generate. Must be
            128, 192, or 256.

        :returns bytes: The generated key.

    .. method:: encrypt(nonce, data, associated_data)

        .. warning::

            Reuse of a ``nonce`` with a given ``key`` compromises the security
            of any message with that ``nonce`` and ``key`` pair.

        Encrypts and authenticates the ``data`` provided as well as
        authenticating the ``associated_data``.  The output of this can be
        passed directly to the ``decrypt`` method.

        :param nonce: A value of between 7 and 13 bytes. The maximum
            length is determined by the length of the ciphertext you are
            encrypting and must satisfy the condition:
            ``len(data) < 2 ** (8 * (15 - len(nonce)))``
            **NEVER REUSE A NONCE** with a key.
        :type nonce: :term:`bytes-like`
        :param bytes data: The data to encrypt.
        :param bytes associated_data: Additional data that should be
            authenticated with the key, but is not encrypted. Can be ``None``.
        :returns bytes: The ciphertext bytes with the tag appended.
        :raises OverflowError: If ``data`` or ``associated_data`` is larger
            than 2\ :sup:`32` bytes.

    .. method:: decrypt(nonce, data, associated_data)

        Decrypts the ``data`` and authenticates the ``associated_data``. If you
        called encrypt with ``associated_data`` you must pass the same
        ``associated_data`` in decrypt or the integrity check will fail.

        :param nonce: A value of between 7 and 13 bytes. This
            is the same value used when you originally called encrypt.
            **NEVER REUSE A NONCE** with a key.
        :type nonce: :term:`bytes-like`
        :param bytes data: The data to decrypt (with tag appended).
        :param bytes associated_data: Additional data to authenticate. Can be
            ``None`` if none was passed during encryption.
        :returns bytes: The original plaintext.
        :raises cryptography.exceptions.InvalidTag: If the authentication tag
            doesn't validate this exception will be raised. This will occur
            when the ciphertext has been changed, but will also occur when the
            key, nonce, or associated data are wrong.

.. _`recommends a 96-bit IV length`: https://csrc.nist.gov/publications/detail/sp/800-38d/final
.. hazmat::

Diffie-Hellman key exchange
===========================

.. currentmodule:: cryptography.hazmat.primitives.asymmetric.dh

.. note::
    For security and performance reasons we suggest using
    :class:`~cryptography.hazmat.primitives.asymmetric.ec.ECDH` instead of DH
    where possible.


`Diffie-Hellman key exchange`_ (D–H) is a method that allows two parties
to jointly agree on a shared secret using an insecure channel.


Exchange Algorithm
~~~~~~~~~~~~~~~~~~

For most applications the ``shared_key`` should be passed to a key
derivation function. This allows mixing of additional information into the
key, derivation of multiple keys, and destroys any structure that may be
present.

.. warning::

    This example does not give `forward secrecy`_ and is only provided as a
    demonstration of the basic Diffie-Hellman construction. For real world
    applications always use the ephemeral form described after this example.

.. code-block:: pycon

    >>> from cryptography.hazmat.primitives import hashes
    >>> from cryptography.hazmat.primitives.asymmetric import dh
    >>> from cryptography.hazmat.primitives.kdf.hkdf import HKDF
    >>> # Generate some parameters. These can be reused.
    >>> parameters = dh.generate_parameters(generator=2, key_size=2048)
    >>> # Generate a private key for use in the exchange.
    >>> server_private_key = parameters.generate_private_key()
    >>> # In a real handshake the peer is a remote client. For this
    >>> # example we'll generate another local private key though. Note that in
    >>> # a DH handshake both peers must agree on a common set of parameters.
    >>> peer_private_key = parameters.generate_private_key()
    >>> shared_key = server_private_key.exchange(peer_private_key.public_key())
    >>> # Perform key derivation.
    >>> derived_key = HKDF(
    ...     algorithm=hashes.SHA256(),
    ...     length=32,
    ...     salt=None,
    ...     info=b'handshake data',
    ... ).derive(shared_key)
    >>> # And now we can demonstrate that the handshake performed in the
    >>> # opposite direction gives the same final value
    >>> same_shared_key = peer_private_key.exchange(
    ...     server_private_key.public_key()
    ... )
    >>> same_derived_key = HKDF(
    ...     algorithm=hashes.SHA256(),
    ...     length=32,
    ...     salt=None,
    ...     info=b'handshake data',
    ... ).derive(same_shared_key)
    >>> derived_key == same_derived_key

DHE (or EDH), the ephemeral form of this exchange, is **strongly
preferred** over simple DH and provides `forward secrecy`_ when used.  You must
generate a new private key using :func:`~DHParameters.generate_private_key` for
each :meth:`~DHPrivateKey.exchange` when performing an DHE key exchange. An
example of the ephemeral form:

.. code-block:: pycon

    >>> from cryptography.hazmat.primitives import hashes
    >>> from cryptography.hazmat.primitives.asymmetric import dh
    >>> from cryptography.hazmat.primitives.kdf.hkdf import HKDF
    >>> # Generate some parameters. These can be reused.
    >>> parameters = dh.generate_parameters(generator=2, key_size=2048)
    >>> # Generate a private key for use in the exchange.
    >>> private_key = parameters.generate_private_key()
    >>> # In a real handshake the peer_public_key will be received from the
    >>> # other party. For this example we'll generate another private key and
    >>> # get a public key from that. Note that in a DH handshake both peers
    >>> # must agree on a common set of parameters.
    >>> peer_public_key = parameters.generate_private_key().public_key()
    >>> shared_key = private_key.exchange(peer_public_key)
    >>> # Perform key derivation.
    >>> derived_key = HKDF(
    ...     algorithm=hashes.SHA256(),
    ...     length=32,
    ...     salt=None,
    ...     info=b'handshake data',
    ... ).derive(shared_key)
    >>> # For the next handshake we MUST generate another private key, but
    >>> # we can reuse the parameters.
    >>> private_key_2 = parameters.generate_private_key()
    >>> peer_public_key_2 = parameters.generate_private_key().public_key()
    >>> shared_key_2 = private_key_2.exchange(peer_public_key_2)
    >>> derived_key_2 = HKDF(
    ...     algorithm=hashes.SHA256(),
    ...     length=32,
    ...     salt=None,
    ...     info=b'handshake data',
    ... ).derive(shared_key_2)

To assemble a :class:`~DHParameters` and a :class:`~DHPublicKey` from
primitive integers, you must first create the
:class:`~DHParameterNumbers` and :class:`~DHPublicNumbers` objects. For
example, if **p**, **g**, and **y** are :class:`int` objects received from a
peer::

    pn = dh.DHParameterNumbers(p, g)
    parameters = pn.parameters()
    peer_public_numbers = dh.DHPublicNumbers(y, pn)
    peer_public_key = peer_public_numbers.public_key()


See also the :class:`~cryptography.hazmat.backends.interfaces.DHBackend`
API for additional functionality.

Group parameters
~~~~~~~~~~~~~~~~

.. function:: generate_parameters(generator, key_size, backend=None)

    .. versionadded:: 1.7

    Generate a new DH parameter group for use with ``backend``.

    :param generator: The :class:`int` to use as a generator. Must be
        2 or 5.

    :param key_size: The bit length of the prime modulus to generate.

    :param backend: An optional
        :class:`~cryptography.hazmat.backends.interfaces.DHBackend`
        instance.

    :returns: DH parameters as a new instance of
        :class:`~cryptography.hazmat.primitives.asymmetric.dh.DHParameters`.

    :raises ValueError: If ``key_size`` is not at least 512.


.. class:: DHParameters

    .. versionadded:: 1.7


    .. method:: generate_private_key()

        Generate a DH private key. This method can be used to generate many
        new private keys from a single set of parameters.

        :return: An instance of
            :class:`~cryptography.hazmat.primitives.asymmetric.dh.DHPrivateKey`.

    .. method:: parameter_numbers()

        Return the numbers that make up this set of parameters.

        :return: A :class:`~cryptography.hazmat.primitives.asymmetric.dh.DHParameterNumbers`.

    .. method:: parameter_bytes(encoding, format)

        .. versionadded:: 2.0

        Allows serialization of the parameters to bytes. Encoding (
        :attr:`~cryptography.hazmat.primitives.serialization.Encoding.PEM` or
        :attr:`~cryptography.hazmat.primitives.serialization.Encoding.DER`) and
        format (
        :attr:`~cryptography.hazmat.primitives.serialization.ParameterFormat.PKCS3`)
        are chosen to define the exact serialization.

        :param encoding: A value from the
            :class:`~cryptography.hazmat.primitives.serialization.Encoding` enum.

        :param format: A value from the
            :class:`~cryptography.hazmat.primitives.serialization.ParameterFormat`
            enum. At the moment only ``PKCS3`` is supported.

        :return bytes: Serialized parameters.

.. class:: DHParametersWithSerialization

    .. versionadded:: 1.7

    Alias for :class:`DHParameters`.


Key interfaces
~~~~~~~~~~~~~~

.. class:: DHPrivateKey

    .. versionadded:: 1.7

    A DH private key that is not an :term:`opaque key` also implements
    :class:`DHPrivateKeyWithSerialization` to provide serialization methods.

    .. attribute:: key_size

        The bit length of the prime modulus.

    .. method:: public_key()

        Return the public key associated with this private key.

        :return: A :class:`~cryptography.hazmat.primitives.asymmetric.dh.DHPublicKey`.

    .. method:: parameters()

        Return the parameters associated with this private key.

        :return: A :class:`~cryptography.hazmat.primitives.asymmetric.dh.DHParameters`.

    .. method:: exchange(peer_public_key)

        .. versionadded:: 1.7

        :param DHPublicKey peer_public_key: The public key for
            the peer.

        :return bytes: The agreed key. The bytes are ordered in 'big' endian.


.. class:: DHPrivateKeyWithSerialization

    .. versionadded:: 1.7

    This interface contains additional methods relating to serialization.
    Any object with this interface also has all the methods from
    :class:`DHPrivateKey`.

    .. method:: private_numbers()

        Return the numbers that make up this private key.

        :return: A :class:`~cryptography.hazmat.primitives.asymmetric.dh.DHPrivateNumbers`.

    .. method:: private_bytes(encoding, format, encryption_algorithm)

        .. versionadded:: 1.8

        Allows serialization of the key to bytes. Encoding (
        :attr:`~cryptography.hazmat.primitives.serialization.Encoding.PEM` or
        :attr:`~cryptography.hazmat.primitives.serialization.Encoding.DER`),
        format (
        :attr:`~cryptography.hazmat.primitives.serialization.PrivateFormat.PKCS8`)
        and encryption algorithm (such as
        :class:`~cryptography.hazmat.primitives.serialization.BestAvailableEncryption`
        or :class:`~cryptography.hazmat.primitives.serialization.NoEncryption`)
        are chosen to define the exact serialization.

        :param encoding: A value from the
            :class:`~cryptography.hazmat.primitives.serialization.Encoding` enum.

        :param format: A value from the
            :class:`~cryptography.hazmat.primitives.serialization.PrivateFormat`
            enum.

        :param encryption_algorithm: An instance of an object conforming to the
            :class:`~cryptography.hazmat.primitives.serialization.KeySerializationEncryption`
            interface.

        :return bytes: Serialized key.


.. class:: DHPublicKey

    .. versionadded:: 1.7

    .. attribute:: key_size

        The bit length of the prime modulus.

    .. method:: parameters()

        Return the parameters associated with this private key.

        :return: A :class:`~cryptography.hazmat.primitives.asymmetric.dh.DHParameters`.

    .. method:: public_numbers()

        Return the numbers that make up this public key.

        :return: A :class:`~cryptography.hazmat.primitives.asymmetric.dh.DHPublicNumbers`.

    .. method:: public_bytes(encoding, format)

        .. versionadded:: 1.8

        Allows serialization of the key to bytes. Encoding (
        :attr:`~cryptography.hazmat.primitives.serialization.Encoding.PEM` or
        :attr:`~cryptography.hazmat.primitives.serialization.Encoding.DER`) and
        format (
        :attr:`~cryptography.hazmat.primitives.serialization.PublicFormat.SubjectPublicKeyInfo`)
        are chosen to define the exact serialization.

        :param encoding: A value from the
            :class:`~cryptography.hazmat.primitives.serialization.Encoding` enum.

        :param format: A value from the
            :class:`~cryptography.hazmat.primitives.serialization.PublicFormat` enum.

        :return bytes: Serialized key.

.. class:: DHPublicKeyWithSerialization

    .. versionadded:: 1.7

    Alias for :class:`DHPublicKey`.


Numbers
~~~~~~~

.. class:: DHParameterNumbers(p, g, q=None)

    .. versionadded:: 0.8

    The collection of integers that define a Diffie-Hellman group.

    .. attribute:: p

        :type: int

        The prime modulus value.

    .. attribute:: g

        :type: int

        The generator value. Must be 2 or greater.

    .. attribute:: q

        .. versionadded:: 1.8

        :type: int

        p subgroup order value.

    .. method:: parameters(backend=None)

        .. versionadded:: 1.7

        :param backend: An optional instance of
            :class:`~cryptography.hazmat.backends.interfaces.DHBackend`.

        :returns: A new instance of :class:`DHParameters`.

.. class:: DHPrivateNumbers(x, public_numbers)

    .. versionadded:: 0.8

    The collection of integers that make up a Diffie-Hellman private key.

    .. attribute:: public_numbers

        :type: :class:`~cryptography.hazmat.primitives.asymmetric.dh.DHPublicNumbers`

        The :class:`DHPublicNumbers` which makes up the DH public
        key associated with this DH private key.

    .. attribute:: x

        :type: int

        The private value.

    .. method:: private_key(backend=None)

        .. versionadded:: 1.7

        :param backend: An optional instance of
            :class:`~cryptography.hazmat.backends.interfaces.DHBackend`.

        :returns: A new instance of :class:`DHPrivateKey`.


.. class:: DHPublicNumbers(y, parameter_numbers)

    .. versionadded:: 0.8

    The collection of integers that make up a Diffie-Hellman public key.

     .. attribute:: parameter_numbers

        :type: :class:`~cryptography.hazmat.primitives.asymmetric.dh.DHParameterNumbers`

        The parameters for this DH group.

    .. attribute:: y

        :type: int

        The public value.

    .. method:: public_key(backend=None)

        .. versionadded:: 1.7

        :param backend: An optional instance of
            :class:`~cryptography.hazmat.backends.interfaces.DHBackend`.

        :returns: A new instance of :class:`DHPublicKey`.


.. _`Diffie-Hellman key exchange`: https://en.wikipedia.org/wiki/Diffie%E2%80%93Hellman_key_exchange
.. _`forward secrecy`: https://en.wikipedia.org/wiki/Forward_secrecy
.. hazmat::

DSA
===

.. module:: cryptography.hazmat.primitives.asymmetric.dsa

.. note::

    DSA is a **legacy algorithm** and should generally be avoided in favor of
    choices like
    :doc:`EdDSA using curve25519</hazmat/primitives/asymmetric/ed25519>` or
    :doc:`ECDSA</hazmat/primitives/asymmetric/ec>`.

`DSA`_ is a `public-key`_ algorithm for signing messages.

Generation
~~~~~~~~~~

.. function:: generate_private_key(key_size, backend=None)

    .. versionadded:: 0.5

    .. versionchanged:: 3.0

        Added support for 4096-bit keys for some legacy applications that
        continue to use DSA despite the wider cryptographic community's
        `ongoing protestations`_.

    Generate a DSA private key from the given key size. This function will
    generate a new set of parameters and key in one step.

    :param int key_size: The length of the modulus in :term:`bits`. It should
        be either 1024, 2048, 3072, or 4096. For keys generated in 2015 this
        should be `at least 2048`_ (See page 41).

    :param backend: An optional instance of
        :class:`~cryptography.hazmat.backends.interfaces.DSABackend`.

    :return: An instance of
        :class:`~cryptography.hazmat.primitives.asymmetric.dsa.DSAPrivateKey`.

    :raises cryptography.exceptions.UnsupportedAlgorithm: This is raised if
        the provided ``backend`` does not implement
        :class:`~cryptography.hazmat.backends.interfaces.DSABackend`

.. function:: generate_parameters(key_size, backend=None)

    .. versionadded:: 0.5

    .. versionchanged:: 3.0

        Added support for 4096-bit keys for some legacy applications that
        continue to use DSA despite the wider cryptographic community's
        `ongoing protestations`_.

    Generate DSA parameters using the provided ``backend``.

    :param int key_size: The length of :attr:`~DSAParameterNumbers.q`. It
        should be either 1024, 2048, 3072, or 4096. For keys generated in 2015
        this should be `at least 2048`_ (See page 41).

    :param backend: An optional instance of
        :class:`~cryptography.hazmat.backends.interfaces.DSABackend`.

    :return: An instance of
        :class:`~cryptography.hazmat.primitives.asymmetric.dsa.DSAParameters`.

    :raises cryptography.exceptions.UnsupportedAlgorithm: This is raised if
        the provided ``backend`` does not implement
        :class:`~cryptography.hazmat.backends.interfaces.DSABackend`

Signing
~~~~~~~

Using a :class:`~cryptography.hazmat.primitives.asymmetric.dsa.DSAPrivateKey`
instance.

.. doctest::

    >>> from cryptography.hazmat.primitives import hashes
    >>> from cryptography.hazmat.primitives.asymmetric import dsa
    >>> private_key = dsa.generate_private_key(
    ...     key_size=1024,
    ... )
    >>> data = b"this is some data I'd like to sign"
    >>> signature = private_key.sign(
    ...     data,
    ...     hashes.SHA256()
    ... )

The ``signature`` is a ``bytes`` object, whose contents is DER encoded as
described in :rfc:`3279`. This can be decoded using
:func:`~cryptography.hazmat.primitives.asymmetric.utils.decode_dss_signature`.

If your data is too large to be passed in a single call, you can hash it
separately and pass that value using
:class:`~cryptography.hazmat.primitives.asymmetric.utils.Prehashed`.

.. doctest::

    >>> from cryptography.hazmat.primitives.asymmetric import utils
    >>> chosen_hash = hashes.SHA256()
    >>> hasher = hashes.Hash(chosen_hash)
    >>> hasher.update(b"data & ")
    >>> hasher.update(b"more data")
    >>> digest = hasher.finalize()
    >>> sig = private_key.sign(
    ...     digest,
    ...     utils.Prehashed(chosen_hash)
    ... )

Verification
~~~~~~~~~~~~

Verification is performed using a
:class:`~cryptography.hazmat.primitives.asymmetric.dsa.DSAPublicKey` instance.
You can get a public key object with
:func:`~cryptography.hazmat.primitives.serialization.load_pem_public_key`,
:func:`~cryptography.hazmat.primitives.serialization.load_der_public_key`,
:meth:`~cryptography.hazmat.primitives.asymmetric.dsa.DSAPublicNumbers.public_key`
, or
:meth:`~cryptography.hazmat.primitives.asymmetric.dsa.DSAPrivateKey.public_key`.

.. doctest::

    >>> public_key = private_key.public_key()
    >>> public_key.verify(
    ...     signature,
    ...     data,
    ...     hashes.SHA256()
    ... )

``verify()`` takes the signature in the same format as is returned by
``sign()``.

``verify()`` will raise an :class:`~cryptography.exceptions.InvalidSignature`
exception if the signature isn't valid.

If your data is too large to be passed in a single call, you can hash it
separately and pass that value using
:class:`~cryptography.hazmat.primitives.asymmetric.utils.Prehashed`.

.. doctest::

    >>> chosen_hash = hashes.SHA256()
    >>> hasher = hashes.Hash(chosen_hash)
    >>> hasher.update(b"data & ")
    >>> hasher.update(b"more data")
    >>> digest = hasher.finalize()
    >>> public_key.verify(
    ...     sig,
    ...     digest,
    ...     utils.Prehashed(chosen_hash)
    ... )

Numbers
~~~~~~~

.. class:: DSAParameterNumbers(p, q, g)

    .. versionadded:: 0.5

    The collection of integers that make up a set of DSA parameters.

    .. attribute:: p

        :type: int

        The public modulus.

    .. attribute:: q

        :type: int

        The sub-group order.

    .. attribute:: g

        :type: int

        The generator.

    .. method:: parameters(backend=None)

        :param backend: An optional instance of
            :class:`~cryptography.hazmat.backends.interfaces.DSABackend`.

        :returns: A new instance of
            :class:`~cryptography.hazmat.primitives.asymmetric.dsa.DSAParameters`.

.. class:: DSAPublicNumbers(y, parameter_numbers)

    .. versionadded:: 0.5

    The collection of integers that make up a DSA public key.

    .. attribute:: y

        :type: int

        The public value ``y``.

    .. attribute:: parameter_numbers

        :type: :class:`~cryptography.hazmat.primitives.asymmetric.dsa.DSAParameterNumbers`

        The :class:`~cryptography.hazmat.primitives.asymmetric.dsa.DSAParameterNumbers`
        associated with the public key.

    .. method:: public_key(backend=None)

        :param backend: An optional instance of
            :class:`~cryptography.hazmat.backends.interfaces.DSABackend`.

        :returns: A new instance of
            :class:`~cryptography.hazmat.primitives.asymmetric.dsa.DSAPublicKey`.

.. class:: DSAPrivateNumbers(x, public_numbers)

    .. versionadded:: 0.5

    The collection of integers that make up a DSA private key.

    .. warning::

        Revealing the value of ``x`` will compromise the security of any
        cryptographic operations performed.

    .. attribute:: x

        :type: int

        The private value ``x``.

    .. attribute:: public_numbers

        :type: :class:`~cryptography.hazmat.primitives.asymmetric.dsa.DSAPublicNumbers`

        The :class:`~cryptography.hazmat.primitives.asymmetric.dsa.DSAPublicNumbers`
        associated with the private key.

    .. method:: private_key(backend=None)

        :param backend: An optional instance of
            :class:`~cryptography.hazmat.backends.interfaces.DSABackend`.

        :returns: A new instance of
            :class:`~cryptography.hazmat.primitives.asymmetric.dsa.DSAPrivateKey`.

Key interfaces
~~~~~~~~~~~~~~

.. class:: DSAParameters

    .. versionadded:: 0.3

    `DSA`_ parameters.

    .. method:: generate_private_key()

        .. versionadded:: 0.5

        Generate a DSA private key. This method can be used to generate many
        new private keys from a single set of parameters.

        :return: An instance of
            :class:`~cryptography.hazmat.primitives.asymmetric.dsa.DSAPrivateKey`.


.. class:: DSAParametersWithNumbers

    .. versionadded:: 0.5

    Extends :class:`DSAParameters`.

    .. method:: parameter_numbers()

        Create a
        :class:`~cryptography.hazmat.primitives.asymmetric.dsa.DSAParameterNumbers`
        object.

        :returns: A
            :class:`~cryptography.hazmat.primitives.asymmetric.dsa.DSAParameterNumbers`
            instance.


.. class:: DSAPrivateKey

    .. versionadded:: 0.3

    A `DSA`_ private key. A DSA private key that is not an
    :term:`opaque key` also implements :class:`DSAPrivateKeyWithSerialization`
    to provide serialization methods.

    .. method:: public_key()

        :return: :class:`~cryptography.hazmat.primitives.asymmetric.dsa.DSAPublicKey`

        An DSA public key object corresponding to the values of the private key.

    .. method:: parameters()

        :return: :class:`~cryptography.hazmat.primitives.asymmetric.dsa.DSAParameters`

        The DSAParameters object associated with this private key.

    .. attribute:: key_size

        :type: int

        The bit length of :attr:`~DSAParameterNumbers.q`.

    .. method:: sign(data, algorithm)

        .. versionadded:: 1.5
        .. versionchanged:: 1.6
            :class:`~cryptography.hazmat.primitives.asymmetric.utils.Prehashed`
            can now be used as an ``algorithm``.

        Sign one block of data which can be verified later by others using the
        public key.

        :param bytes data: The message string to sign.

        :param algorithm: An instance of
            :class:`~cryptography.hazmat.primitives.hashes.HashAlgorithm` or
            :class:`~cryptography.hazmat.primitives.asymmetric.utils.Prehashed`
            if the ``data`` you want to sign has already been hashed.

        :return bytes: Signature.


.. class:: DSAPrivateKeyWithSerialization

    .. versionadded:: 0.8

    This interface contains additional methods relating to serialization.
    Any object with this interface also has all the methods from
    :class:`DSAPrivateKey`.

    .. method:: private_numbers()

        Create a
        :class:`~cryptography.hazmat.primitives.asymmetric.dsa.DSAPrivateNumbers`
        object.

        :returns: A
            :class:`~cryptography.hazmat.primitives.asymmetric.dsa.DSAPrivateNumbers`
            instance.

    .. method:: private_bytes(encoding, format, encryption_algorithm)

        Allows serialization of the key to bytes. Encoding (
        :attr:`~cryptography.hazmat.primitives.serialization.Encoding.PEM` or
        :attr:`~cryptography.hazmat.primitives.serialization.Encoding.DER`),
        format (
        :attr:`~cryptography.hazmat.primitives.serialization.PrivateFormat.TraditionalOpenSSL`,
        :attr:`~cryptography.hazmat.primitives.serialization.PrivateFormat.OpenSSH`
        or
        :attr:`~cryptography.hazmat.primitives.serialization.PrivateFormat.PKCS8`)
        and encryption algorithm (such as
        :class:`~cryptography.hazmat.primitives.serialization.BestAvailableEncryption`
        or :class:`~cryptography.hazmat.primitives.serialization.NoEncryption`)
        are chosen to define the exact serialization.

        :param encoding: A value from the
            :class:`~cryptography.hazmat.primitives.serialization.Encoding` enum.

        :param format: A value from the
            :class:`~cryptography.hazmat.primitives.serialization.PrivateFormat`
            enum.

        :param encryption_algorithm: An instance of an object conforming to the
            :class:`~cryptography.hazmat.primitives.serialization.KeySerializationEncryption`
            interface.

        :return bytes: Serialized key.


.. class:: DSAPublicKey

    .. versionadded:: 0.3

    A `DSA`_ public key.

    .. attribute:: key_size

        :type: int

        The bit length of :attr:`~DSAParameterNumbers.q`.

    .. method:: parameters()

        :return: :class:`~cryptography.hazmat.primitives.asymmetric.dsa.DSAParameters`

        The DSAParameters object associated with this public key.

    .. method:: public_numbers()

        Create a
        :class:`~cryptography.hazmat.primitives.asymmetric.dsa.DSAPublicNumbers`
        object.

        :returns: A
            :class:`~cryptography.hazmat.primitives.asymmetric.dsa.DSAPublicNumbers`
            instance.

    .. method:: public_bytes(encoding, format)

        Allows serialization of the key to bytes. Encoding (
        :attr:`~cryptography.hazmat.primitives.serialization.Encoding.PEM` or
        :attr:`~cryptography.hazmat.primitives.serialization.Encoding.DER`) and
        format (
        :attr:`~cryptography.hazmat.primitives.serialization.PublicFormat.SubjectPublicKeyInfo`)
        are chosen to define the exact serialization.

        :param encoding: A value from the
            :class:`~cryptography.hazmat.primitives.serialization.Encoding` enum.

        :param format: A value from the
            :class:`~cryptography.hazmat.primitives.serialization.PublicFormat` enum.

        :return bytes: Serialized key.

    .. method:: verify(signature, data, algorithm)

        .. versionadded:: 1.5
        .. versionchanged:: 1.6
            :class:`~cryptography.hazmat.primitives.asymmetric.utils.Prehashed`
            can now be used as an ``algorithm``.

        Verify one block of data was signed by the private key
        associated with this public key.

        :param bytes signature: The signature to verify.

        :param bytes data: The message string that was signed.

        :param algorithm: An instance of
            :class:`~cryptography.hazmat.primitives.hashes.HashAlgorithm` or
            :class:`~cryptography.hazmat.primitives.asymmetric.utils.Prehashed`
            if the ``data`` you want to sign has already been hashed.

        :raises cryptography.exceptions.InvalidSignature: If the signature does
            not validate.


.. class:: DSAPublicKeyWithSerialization

    .. versionadded:: 0.8

    Alias for :class:`DSAPublicKey`.


.. _`DSA`: https://en.wikipedia.org/wiki/Digital_Signature_Algorithm
.. _`public-key`: https://en.wikipedia.org/wiki/Public-key_cryptography
.. _`FIPS 186-4`: https://csrc.nist.gov/publications/detail/fips/186/4/final
.. _`at least 2048`: https://www.cosic.esat.kuleuven.be/ecrypt/ecrypt2/documents/D.SPA.20.pdf
.. _`ongoing protestations`: https://buttondown.email/cryptography-dispatches/archive/cryptography-dispatches-dsa-is-past-its-prime/
.. hazmat::

Elliptic curve cryptography
===========================

.. module:: cryptography.hazmat.primitives.asymmetric.ec


.. function:: generate_private_key(curve, backend=None)

    .. versionadded:: 0.5

    Generate a new private key on ``curve`` for use with ``backend``.

    :param curve: An instance of :class:`EllipticCurve`.

    :param backend: An optional instance of
        :class:`~cryptography.hazmat.backends.interfaces.EllipticCurveBackend`.

    :returns: A new instance of :class:`EllipticCurvePrivateKey`.


.. function:: derive_private_key(private_value, curve, backend=None)

    .. versionadded:: 1.6

    Derive a private key from ``private_value`` on ``curve`` for use with
    ``backend``.

    :param int private_value: The secret scalar value.

    :param curve: An instance of :class:`EllipticCurve`.

    :param backend: An optional instance of
        :class:`~cryptography.hazmat.backends.interfaces.EllipticCurveBackend`.

    :returns: A new instance of :class:`EllipticCurvePrivateKey`.


Elliptic Curve Signature Algorithms
-----------------------------------

.. class:: ECDSA(algorithm)

    .. versionadded:: 0.5

    The ECDSA signature algorithm first standardized in NIST publication
    `FIPS 186-3`_, and later in `FIPS 186-4`_.

    Note that while elliptic curve keys can be used for both signing and key
    exchange, this is `bad cryptographic practice`_. Instead, users should
    generate separate signing and ECDH keys.

    :param algorithm: An instance of
        :class:`~cryptography.hazmat.primitives.hashes.HashAlgorithm`.

    .. doctest::

        >>> from cryptography.hazmat.primitives import hashes
        >>> from cryptography.hazmat.primitives.asymmetric import ec
        >>> private_key = ec.generate_private_key(
        ...     ec.SECP384R1()
        ... )
        >>> data = b"this is some data I'd like to sign"
        >>> signature = private_key.sign(
        ...     data,
        ...     ec.ECDSA(hashes.SHA256())
        ... )

    The ``signature`` is a ``bytes`` object, whose contents are DER encoded as
    described in :rfc:`3279`. This can be decoded using
    :func:`~cryptography.hazmat.primitives.asymmetric.utils.decode_dss_signature`.

    If your data is too large to be passed in a single call, you can hash it
    separately and pass that value using
    :class:`~cryptography.hazmat.primitives.asymmetric.utils.Prehashed`.

    .. doctest::

        >>> from cryptography.hazmat.primitives.asymmetric import utils
        >>> chosen_hash = hashes.SHA256()
        >>> hasher = hashes.Hash(chosen_hash)
        >>> hasher.update(b"data & ")
        >>> hasher.update(b"more data")
        >>> digest = hasher.finalize()
        >>> sig = private_key.sign(
        ...     digest,
        ...     ec.ECDSA(utils.Prehashed(chosen_hash))
        ... )


    Verification requires the public key, the DER-encoded signature itself, the
    signed data, and knowledge of the hashing algorithm that was used when
    producing the signature:

    >>> public_key = private_key.public_key()
    >>> public_key.verify(signature, data, ec.ECDSA(hashes.SHA256()))

    As above, the ``signature`` is a ``bytes`` object whose contents are DER
    encoded as described in :rfc:`3279`. It can be created from a raw ``(r,s)``
    pair by using
    :func:`~cryptography.hazmat.primitives.asymmetric.utils.encode_dss_signature`.

    If the signature is not valid, an
    :class:`~cryptography.exceptions.InvalidSignature` exception will be raised.

    If your data is too large to be passed in a single call, you can hash it
    separately and pass that value using
    :class:`~cryptography.hazmat.primitives.asymmetric.utils.Prehashed`.

    .. doctest::

        >>> chosen_hash = hashes.SHA256()
        >>> hasher = hashes.Hash(chosen_hash)
        >>> hasher.update(b"data & ")
        >>> hasher.update(b"more data")
        >>> digest = hasher.finalize()
        >>> public_key.verify(
        ...     sig,
        ...     digest,
        ...     ec.ECDSA(utils.Prehashed(chosen_hash))
        ... )

    .. note::
        Although in this case the public key was derived from the private one,
        in a typical setting you will not possess the private key. The
        `Key loading`_ section explains how to load the public key from other
        sources.


.. class:: EllipticCurvePrivateNumbers(private_value, public_numbers)

    .. versionadded:: 0.5

    The collection of integers that make up an EC private key.

    .. attribute:: public_numbers

        :type: :class:`~cryptography.hazmat.primitives.asymmetric.ec.EllipticCurvePublicNumbers`

        The :class:`EllipticCurvePublicNumbers` which makes up the EC public
        key associated with this EC private key.

    .. attribute:: private_value

        :type: int

        The private value.

    .. method:: private_key(backend=None)

        Convert a collection of numbers into a private key suitable for doing
        actual cryptographic operations.

        :param backend: An optional instance of
            :class:`~cryptography.hazmat.backends.interfaces.EllipticCurveBackend`.

        :returns: A new instance of :class:`EllipticCurvePrivateKey`.


.. class:: EllipticCurvePublicNumbers(x, y, curve)

    .. warning::
        The point represented by this object is not validated in any way until
        :meth:`EllipticCurvePublicNumbers.public_key` is called and may not
        represent a valid point on the curve. You should not attempt to perform
        any computations using the values from this class until you have either
        validated it yourself or called ``public_key()`` successfully.

    .. versionadded:: 0.5

    The collection of integers that make up an EC public key.

     .. attribute:: curve

        :type: :class:`EllipticCurve`

        The elliptic curve for this key.

    .. attribute:: x

        :type: int

        The affine x component of the public point used for verifying.

    .. attribute:: y

        :type: int

        The affine y component of the public point used for verifying.

    .. method:: public_key(backend=None)

        Convert a collection of numbers into a public key suitable for doing
        actual cryptographic operations.

        :param backend: An optional instance of
            :class:`~cryptography.hazmat.backends.interfaces.EllipticCurveBackend`.

        :raises ValueError: Raised if the point is invalid for the curve.
        :returns: A new instance of :class:`EllipticCurvePublicKey`.

    .. method:: encode_point()

        .. warning::

            This method is deprecated as of version 2.5. Callers should migrate
            to using
            :meth:`~cryptography.hazmat.primitives.asymmetric.ec.EllipticCurvePublicKey.public_bytes`.

        .. versionadded:: 1.1

        Encodes an elliptic curve point to a byte string as described in
        `SEC 1 v2.0`_ section 2.3.3. This method only supports uncompressed
        points.

        :return bytes: The encoded point.

    .. classmethod:: from_encoded_point(curve, data)

        .. versionadded:: 1.1

        .. note::

            This has been deprecated in favor of
            :meth:`~cryptography.hazmat.primitives.asymmetric.ec.EllipticCurvePublicKey.from_encoded_point`

        Decodes a byte string as described in `SEC 1 v2.0`_ section 2.3.3 and
        returns an :class:`EllipticCurvePublicNumbers`. This method only
        supports uncompressed points.

        :param curve: An
            :class:`~cryptography.hazmat.primitives.asymmetric.ec.EllipticCurve`
            instance.

        :param bytes data: The serialized point byte string.

        :returns: An :class:`EllipticCurvePublicNumbers` instance.

        :raises ValueError: Raised on invalid point type or data length.

        :raises TypeError: Raised when curve is not an
            :class:`~cryptography.hazmat.primitives.asymmetric.ec.EllipticCurve`.

Elliptic Curve Key Exchange algorithm
-------------------------------------

.. class:: ECDH()

    .. versionadded:: 1.1

    The Elliptic Curve Diffie-Hellman Key Exchange algorithm first standardized
    in NIST publication `800-56A`_, and later in `800-56Ar2`_.

    For most applications the ``shared_key`` should be passed to a key
    derivation function. This allows mixing of additional information into the
    key, derivation of multiple keys, and destroys any structure that may be
    present.

    Note that while elliptic curve keys can be used for both signing and key
    exchange, this is `bad cryptographic practice`_. Instead, users should
    generate separate signing and ECDH keys.

    .. warning::

        This example does not give `forward secrecy`_ and is only provided as a
        demonstration of the basic Diffie-Hellman construction. For real world
        applications always use the ephemeral form described after this example.

    .. doctest::

        >>> from cryptography.hazmat.primitives import hashes
        >>> from cryptography.hazmat.primitives.asymmetric import ec
        >>> from cryptography.hazmat.primitives.kdf.hkdf import HKDF
        >>> # Generate a private key for use in the exchange.
        >>> server_private_key = ec.generate_private_key(
        ...     ec.SECP384R1()
        ... )
        >>> # In a real handshake the peer is a remote client. For this
        >>> # example we'll generate another local private key though.
        >>> peer_private_key = ec.generate_private_key(
        ...     ec.SECP384R1()
        ... )
        >>> shared_key = server_private_key.exchange(
        ...     ec.ECDH(), peer_private_key.public_key())
        >>> # Perform key derivation.
        >>> derived_key = HKDF(
        ...     algorithm=hashes.SHA256(),
        ...     length=32,
        ...     salt=None,
        ...     info=b'handshake data',
        ... ).derive(shared_key)
        >>> # And now we can demonstrate that the handshake performed in the
        >>> # opposite direction gives the same final value
        >>> same_shared_key = peer_private_key.exchange(
        ...     ec.ECDH(), server_private_key.public_key())
        >>> # Perform key derivation.
        >>> same_derived_key = HKDF(
        ...     algorithm=hashes.SHA256(),
        ...     length=32,
        ...     salt=None,
        ...     info=b'handshake data',
        ... ).derive(same_shared_key)
        >>> derived_key == same_derived_key
        True

    ECDHE (or EECDH), the ephemeral form of this exchange, is **strongly
    preferred** over simple ECDH and provides `forward secrecy`_ when used.
    You must generate a new private key using :func:`generate_private_key` for
    each :meth:`~EllipticCurvePrivateKey.exchange` when performing an ECDHE key
    exchange. An example of the ephemeral form:

    .. doctest::

        >>> from cryptography.hazmat.primitives import hashes
        >>> from cryptography.hazmat.primitives.asymmetric import ec
        >>> from cryptography.hazmat.primitives.kdf.hkdf import HKDF
        >>> # Generate a private key for use in the exchange.
        >>> private_key = ec.generate_private_key(
        ...     ec.SECP384R1()
        ... )
        >>> # In a real handshake the peer_public_key will be received from the
        >>> # other party. For this example we'll generate another private key
        >>> # and get a public key from that.
        >>> peer_public_key = ec.generate_private_key(
        ...     ec.SECP384R1()
        ... ).public_key()
        >>> shared_key = private_key.exchange(ec.ECDH(), peer_public_key)
        >>> # Perform key derivation.
        >>> derived_key = HKDF(
        ...     algorithm=hashes.SHA256(),
        ...     length=32,
        ...     salt=None,
        ...     info=b'handshake data',
        ... ).derive(shared_key)
        >>> # For the next handshake we MUST generate another private key.
        >>> private_key_2 = ec.generate_private_key(
        ...     ec.SECP384R1()
        ... )
        >>> peer_public_key_2 = ec.generate_private_key(
        ...     ec.SECP384R1()
        ... ).public_key()
        >>> shared_key_2 = private_key_2.exchange(ec.ECDH(), peer_public_key_2)
        >>> derived_key_2 = HKDF(
        ...     algorithm=hashes.SHA256(),
        ...     length=32,
        ...     salt=None,
        ...     info=b'handshake data',
        ... ).derive(shared_key_2)

Elliptic Curves
---------------

Elliptic curves provide equivalent security at much smaller key sizes than
other asymmetric cryptography systems such as RSA or DSA. For many operations
elliptic curves are also significantly faster; `elliptic curve diffie-hellman
is faster than diffie-hellman`_.

.. note::
    Curves with a size of `less than 224 bits`_ should not be used. You should
    strongly consider using curves of at least 224 :term:`bits`.

Generally the NIST prime field ("P") curves are significantly faster than the
other types suggested by NIST at both signing and verifying with ECDSA.

Prime fields also `minimize the number of security concerns for elliptic-curve
cryptography`_. However, there is `some concern`_ that both the prime field and
binary field ("B") NIST curves may have been weakened during their generation.

Currently `cryptography` only supports NIST curves, none of which are
considered "safe" by the `SafeCurves`_ project run by Daniel J. Bernstein and
Tanja Lange.

All named curves are instances of :class:`EllipticCurve`.

.. class:: SECP256R1

    .. versionadded:: 0.5

    SECG curve ``secp256r1``. Also called NIST P-256.


.. class:: SECP384R1

    .. versionadded:: 0.5

    SECG curve ``secp384r1``. Also called NIST P-384.


.. class:: SECP521R1

    .. versionadded:: 0.5

    SECG curve ``secp521r1``. Also called NIST P-521.


.. class:: SECP224R1

    .. versionadded:: 0.5

    SECG curve ``secp224r1``. Also called NIST P-224.


.. class:: SECP192R1

    .. versionadded:: 0.5

    SECG curve ``secp192r1``. Also called NIST P-192.


.. class:: SECP256K1

    .. versionadded:: 0.9

    SECG curve ``secp256k1``.


.. class:: BrainpoolP256R1

    .. versionadded:: 2.2

    Brainpool curve specified in :rfc:`5639`. These curves are discouraged
    for new systems.

.. class:: BrainpoolP384R1

    .. versionadded:: 2.2

    Brainpool curve specified in :rfc:`5639`. These curves are discouraged
    for new systems.

.. class:: BrainpoolP512R1

    .. versionadded:: 2.2

    Brainpool curve specified in :rfc:`5639`. These curves are discouraged
    for new systems.

.. class:: SECT571K1

    .. versionadded:: 0.5

    SECG curve ``sect571k1``. Also called NIST K-571. These binary curves are
    discouraged for new systems.


.. class:: SECT409K1

    .. versionadded:: 0.5

    SECG curve ``sect409k1``. Also called NIST K-409. These binary curves are
    discouraged for new systems.


.. class:: SECT283K1

    .. versionadded:: 0.5

    SECG curve ``sect283k1``. Also called NIST K-283. These binary curves are
    discouraged for new systems.


.. class:: SECT233K1

    .. versionadded:: 0.5

    SECG curve ``sect233k1``. Also called NIST K-233. These binary curves are
    discouraged for new systems.


.. class:: SECT163K1

    .. versionadded:: 0.5

    SECG curve ``sect163k1``. Also called NIST K-163. These binary curves are
    discouraged for new systems.


.. class:: SECT571R1

    .. versionadded:: 0.5

    SECG curve ``sect571r1``. Also called NIST B-571. These binary curves are
    discouraged for new systems.


.. class:: SECT409R1

    .. versionadded:: 0.5

    SECG curve ``sect409r1``. Also called NIST B-409. These binary curves are
    discouraged for new systems.


.. class:: SECT283R1

    .. versionadded:: 0.5

    SECG curve ``sect283r1``. Also called NIST B-283. These binary curves are
    discouraged for new systems.


.. class:: SECT233R1

    .. versionadded:: 0.5

    SECG curve ``sect233r1``. Also called NIST B-233. These binary curves are
    discouraged for new systems.


.. class:: SECT163R2

    .. versionadded:: 0.5

    SECG curve ``sect163r2``. Also called NIST B-163. These binary curves are
    discouraged for new systems.




Key Interfaces
~~~~~~~~~~~~~~

.. class:: EllipticCurve

    .. versionadded:: 0.5

    A named elliptic curve.

    .. attribute:: name

        :type: str

        The name of the curve. Usually the name used for the ASN.1 OID such as
        ``secp256k1``.

    .. attribute:: key_size

        :type: int

        Size (in :term:`bits`) of a secret scalar for the curve (as generated
        by :func:`generate_private_key`).


.. class:: EllipticCurveSignatureAlgorithm

    .. versionadded:: 0.5
    .. versionchanged:: 1.6
        :class:`~cryptography.hazmat.primitives.asymmetric.utils.Prehashed`
        can now be used as an ``algorithm``.

    A signature algorithm for use with elliptic curve keys.

    .. attribute:: algorithm

        :type: :class:`~cryptography.hazmat.primitives.hashes.HashAlgorithm` or
            :class:`~cryptography.hazmat.primitives.asymmetric.utils.Prehashed`

        The digest algorithm to be used with the signature scheme.


.. class:: EllipticCurvePrivateKey

    .. versionadded:: 0.5

    An elliptic curve private key for use with an algorithm such as `ECDSA`_ or
    `EdDSA`_. An elliptic curve private key that is not an
    :term:`opaque key` also implements
    :class:`EllipticCurvePrivateKeyWithSerialization` to provide serialization
    methods.

    .. method:: exchange(algorithm, peer_public_key)

        .. versionadded:: 1.1

        Performs a key exchange operation using the provided algorithm with
        the peer's public key.

        For most applications the ``shared_key`` should be passed to a key
        derivation function. This allows mixing of additional information into the
        key, derivation of multiple keys, and destroys any structure that may be
        present.

        :param algorithm: The key exchange algorithm, currently only
            :class:`~cryptography.hazmat.primitives.asymmetric.ec.ECDH` is
            supported.
        :param EllipticCurvePublicKey peer_public_key: The public key for the
            peer.

        :returns bytes: A shared key.

    .. method:: public_key()

        :return: :class:`EllipticCurvePublicKey`

        The EllipticCurvePublicKey object for this private key.

    .. method:: sign(data, signature_algorithm)

        .. versionadded:: 1.5

        Sign one block of data which can be verified later by others using the
        public key.

        :param bytes data: The message string to sign.

        :param signature_algorithm: An instance of
            :class:`EllipticCurveSignatureAlgorithm`, such as :class:`ECDSA`.

        :return bytes: The signature as a ``bytes`` object, whose contents are
            DER encoded as described in :rfc:`3279`. This can be decoded using
            :func:`~cryptography.hazmat.primitives.asymmetric.utils.decode_dss_signature`,
            which returns the decoded tuple ``(r, s)``.

    .. attribute:: curve

        :type: :class:`EllipticCurve`

        The EllipticCurve that this key is on.

    .. attribute:: key_size

        .. versionadded:: 1.9

        :type: int

        Size (in :term:`bits`) of a secret scalar for the curve (as generated
        by :func:`generate_private_key`).


.. class:: EllipticCurvePrivateKeyWithSerialization

    .. versionadded:: 0.8

    This interface contains additional methods relating to serialization.
    Any object with this interface also has all the methods from
    :class:`EllipticCurvePrivateKey`.

    .. method:: private_numbers()

        Create a :class:`EllipticCurvePrivateNumbers` object.

        :returns: An :class:`EllipticCurvePrivateNumbers` instance.

    .. method:: private_bytes(encoding, format, encryption_algorithm)

        Allows serialization of the key to bytes. Encoding (
        :attr:`~cryptography.hazmat.primitives.serialization.Encoding.PEM` or
        :attr:`~cryptography.hazmat.primitives.serialization.Encoding.DER`),
        format (
        :attr:`~cryptography.hazmat.primitives.serialization.PrivateFormat.TraditionalOpenSSL`,
        :attr:`~cryptography.hazmat.primitives.serialization.PrivateFormat.OpenSSH`
        or
        :attr:`~cryptography.hazmat.primitives.serialization.PrivateFormat.PKCS8`)
        and encryption algorithm (such as
        :class:`~cryptography.hazmat.primitives.serialization.BestAvailableEncryption`
        or :class:`~cryptography.hazmat.primitives.serialization.NoEncryption`)
        are chosen to define the exact serialization.

        :param encoding: A value from the
            :class:`~cryptography.hazmat.primitives.serialization.Encoding` enum.

        :param format: A value from the
            :class:`~cryptography.hazmat.primitives.serialization.PrivateFormat` enum.

        :param encryption_algorithm: An instance of an object conforming to the
            :class:`~cryptography.hazmat.primitives.serialization.KeySerializationEncryption`
            interface.

        :return bytes: Serialized key.


.. class:: EllipticCurvePublicKey

    .. versionadded:: 0.5

    An elliptic curve public key.

     .. attribute:: curve

        :type: :class:`EllipticCurve`

        The elliptic curve for this key.

    .. method:: public_numbers()

        Create a :class:`EllipticCurvePublicNumbers` object.

        :returns: An :class:`EllipticCurvePublicNumbers` instance.

    .. method:: public_bytes(encoding, format)

        Allows serialization of the key data to bytes. When encoding the public
        key the encodings (
        :attr:`~cryptography.hazmat.primitives.serialization.Encoding.PEM`,
        :attr:`~cryptography.hazmat.primitives.serialization.Encoding.DER`) and
        format (
        :attr:`~cryptography.hazmat.primitives.serialization.PublicFormat.SubjectPublicKeyInfo`)
        are chosen to define the exact serialization. When encoding the point
        the encoding
        :attr:`~cryptography.hazmat.primitives.serialization.Encoding.X962`
        should be used with the formats (
        :attr:`~cryptography.hazmat.primitives.serialization.PublicFormat.UncompressedPoint`
        or
        :attr:`~cryptography.hazmat.primitives.serialization.PublicFormat.CompressedPoint`
        ).

        :param encoding: A value from the
            :class:`~cryptography.hazmat.primitives.serialization.Encoding` enum.

        :param format: A value from the
            :class:`~cryptography.hazmat.primitives.serialization.PublicFormat` enum.

        :return bytes: Serialized data.

    .. method:: verify(signature, data, signature_algorithm)

        .. versionadded:: 1.5

        Verify one block of data was signed by the private key associated
        with this public key.

        :param bytes signature: The DER-encoded signature to verify.
            A raw signature may be DER-encoded by splitting it into the ``r``
            and ``s`` components and passing them into
            :func:`~cryptography.hazmat.primitives.asymmetric.utils.encode_dss_signature`.

        :param bytes data: The message string that was signed.

        :param signature_algorithm: An instance of
            :class:`EllipticCurveSignatureAlgorithm`.

        :raises cryptography.exceptions.InvalidSignature: If the signature does
            not validate.

    .. attribute:: key_size

        .. versionadded:: 1.9

        :type: int

        Size (in :term:`bits`) of a secret scalar for the curve (as generated
        by :func:`generate_private_key`).

    .. classmethod:: from_encoded_point(curve, data)

        .. versionadded:: 2.5

        Decodes a byte string as described in `SEC 1 v2.0`_ section 2.3.3 and
        returns an :class:`EllipticCurvePublicKey`. This class method supports
        compressed points.

        :param curve: An
            :class:`~cryptography.hazmat.primitives.asymmetric.ec.EllipticCurve`
            instance.

        :param bytes data: The serialized point byte string.

        :returns: An :class:`EllipticCurvePublicKey` instance.

        :raises ValueError: Raised when an invalid point is supplied.

        :raises TypeError: Raised when curve is not an
            :class:`~cryptography.hazmat.primitives.asymmetric.ec.EllipticCurve`.


.. class:: EllipticCurvePublicKeyWithSerialization

    .. versionadded:: 0.6

    Alias for :class:`EllipticCurvePublicKey`.



Serialization
~~~~~~~~~~~~~

This sample demonstrates how to generate a private key and serialize it.


.. doctest::

    >>> from cryptography.hazmat.primitives import serialization
    >>> from cryptography.hazmat.primitives.asymmetric import ec

    >>> private_key = ec.generate_private_key(ec.SECP384R1())

    >>> serialized_private = private_key.private_bytes(
    ...     encoding=serialization.Encoding.PEM,
    ...     format=serialization.PrivateFormat.PKCS8,
    ...     encryption_algorithm=serialization.BestAvailableEncryption(b'testpassword')
    ... )
    >>> serialized_private.splitlines()[0]
    b'-----BEGIN ENCRYPTED PRIVATE KEY-----'

You can also serialize the key without a password, by relying on
:class:`~cryptography.hazmat.primitives.serialization.NoEncryption`.

The public key is serialized as follows:


.. doctest::

    >>> public_key = private_key.public_key()
    >>> serialized_public = public_key.public_bytes(
    ...     encoding=serialization.Encoding.PEM,
    ...     format=serialization.PublicFormat.SubjectPublicKeyInfo
    ... )
    >>> serialized_public.splitlines()[0]
    b'-----BEGIN PUBLIC KEY-----'

This is the part that you would normally share with the rest of the world.


Key loading
~~~~~~~~~~~

This extends the sample in the previous section, assuming that the variables
``serialized_private`` and ``serialized_public`` contain the respective keys
in PEM format.

.. doctest::

    >>> loaded_public_key = serialization.load_pem_public_key(
    ...     serialized_public,
    ... )

    >>> loaded_private_key = serialization.load_pem_private_key(
    ...     serialized_private,
    ...     # or password=None, if in plain text
    ...     password=b'testpassword',
    ... )


Elliptic Curve Object Identifiers
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

.. class:: EllipticCurveOID

    .. versionadded:: 2.4

    .. attribute:: SECP192R1

        Corresponds to the dotted string ``"1.2.840.10045.3.1.1"``.

    .. attribute:: SECP224R1

        Corresponds to the dotted string ``"1.3.132.0.33"``.

    .. attribute:: SECP256K1

        Corresponds to the dotted string ``"1.3.132.0.10"``.

    .. attribute:: SECP256R1

        Corresponds to the dotted string ``"1.2.840.10045.3.1.7"``.

    .. attribute:: SECP384R1

        Corresponds to the dotted string ``"1.3.132.0.34"``.

    .. attribute:: SECP521R1

        Corresponds to the dotted string ``"1.3.132.0.35"``.

    .. attribute:: BRAINPOOLP256R1

        .. versionadded:: 2.5

        Corresponds to the dotted string ``"1.3.36.3.3.2.8.1.1.7"``.

    .. attribute:: BRAINPOOLP384R1

        .. versionadded:: 2.5

        Corresponds to the dotted string ``"1.3.36.3.3.2.8.1.1.11"``.

    .. attribute:: BRAINPOOLP512R1

        .. versionadded:: 2.5

        Corresponds to the dotted string ``"1.3.36.3.3.2.8.1.1.13"``.

    .. attribute:: SECT163K1

        .. versionadded:: 2.5

        Corresponds to the dotted string ``"1.3.132.0.1"``.

    .. attribute:: SECT163R2

        .. versionadded:: 2.5

        Corresponds to the dotted string ``"1.3.132.0.15"``.

    .. attribute:: SECT233K1

        .. versionadded:: 2.5

        Corresponds to the dotted string ``"1.3.132.0.26"``.

    .. attribute:: SECT233R1

        .. versionadded:: 2.5

        Corresponds to the dotted string ``"1.3.132.0.27"``.

    .. attribute:: SECT283K1

        .. versionadded:: 2.5

        Corresponds to the dotted string ``"1.3.132.0.16"``.

    .. attribute:: SECT283R1

        .. versionadded:: 2.5

        Corresponds to the dotted string ``"1.3.132.0.17"``.

    .. attribute:: SECT409K1

        .. versionadded:: 2.5

        Corresponds to the dotted string ``"1.3.132.0.36"``.

    .. attribute:: SECT409R1

        .. versionadded:: 2.5

        Corresponds to the dotted string ``"1.3.132.0.37"``.

    .. attribute:: SECT571K1

        .. versionadded:: 2.5

        Corresponds to the dotted string ``"1.3.132.0.38"``.

    .. attribute:: SECT571R1

        .. versionadded:: 2.5

        Corresponds to the dotted string ``"1.3.132.0.39"``.

.. function:: get_curve_for_oid(oid)

    .. versionadded:: 2.6

    A function that takes an :class:`~cryptography.x509.ObjectIdentifier`
    and returns the associated elliptic curve class.

    :param oid: An instance of
        :class:`~cryptography.x509.ObjectIdentifier`.

    :returns: The matching elliptic curve class. The returned class conforms
        to the :class:`EllipticCurve` interface.

    :raises LookupError: Raised if no elliptic curve is found that matches
        the provided object identifier.

.. _`FIPS 186-3`: https://csrc.nist.gov/csrc/media/publications/fips/186/3/archive/2009-06-25/documents/fips_186-3.pdf
.. _`FIPS 186-4`: https://csrc.nist.gov/publications/detail/fips/186/4/final
.. _`800-56A`: https://csrc.nist.gov/publications/detail/sp/800-56a/revised/archive/2007-03-14
.. _`800-56Ar2`: https://csrc.nist.gov/publications/detail/sp/800-56a/rev-2/final
.. _`some concern`: https://crypto.stackexchange.com/questions/10263/should-we-trust-the-nist-recommended-ecc-parameters
.. _`less than 224 bits`: https://www.cosic.esat.kuleuven.be/ecrypt/ecrypt2/documents/D.SPA.20.pdf
.. _`elliptic curve diffie-hellman is faster than diffie-hellman`: https://digitalcommons.unl.edu/cgi/viewcontent.cgi?article=1100&context=cseconfwork
.. _`minimize the number of security concerns for elliptic-curve cryptography`: https://cr.yp.to/ecdh/curve25519-20060209.pdf
.. _`SafeCurves`: https://safecurves.cr.yp.to/
.. _`ECDSA`: https://en.wikipedia.org/wiki/ECDSA
.. _`EdDSA`: https://en.wikipedia.org/wiki/EdDSA
.. _`forward secrecy`: https://en.wikipedia.org/wiki/Forward_secrecy
.. _`SEC 1 v2.0`: https://www.secg.org/sec1-v2.pdf
.. _`bad cryptographic practice`: https://crypto.stackexchange.com/a/3313
.. hazmat::

Ed25519 signing
===============

.. currentmodule:: cryptography.hazmat.primitives.asymmetric.ed25519


Ed25519 is an elliptic curve signing algorithm using `EdDSA`_ and
`Curve25519`_. If you do not have legacy interoperability concerns then you
should strongly consider using this signature algorithm.


Signing & Verification
~~~~~~~~~~~~~~~~~~~~~~

.. doctest::

    >>> from cryptography.hazmat.primitives.asymmetric.ed25519 import Ed25519PrivateKey
    >>> private_key = Ed25519PrivateKey.generate()
    >>> signature = private_key.sign(b"my authenticated message")
    >>> public_key = private_key.public_key()
    >>> # Raises InvalidSignature if verification fails
    >>> public_key.verify(signature, b"my authenticated message")

Key interfaces
~~~~~~~~~~~~~~

.. class:: Ed25519PrivateKey

    .. versionadded:: 2.6

    .. classmethod:: generate()

        Generate an Ed25519 private key.

        :returns: :class:`Ed25519PrivateKey`

    .. classmethod:: from_private_bytes(data)

        :param data: 32 byte private key.
        :type data: :term:`bytes-like`

        :returns: :class:`Ed25519PrivateKey`

        .. doctest::

            >>> from cryptography.hazmat.primitives import serialization
            >>> from cryptography.hazmat.primitives.asymmetric import ed25519
            >>> private_key = ed25519.Ed25519PrivateKey.generate()
            >>> private_bytes = private_key.private_bytes(
            ...     encoding=serialization.Encoding.Raw,
            ...     format=serialization.PrivateFormat.Raw,
            ...     encryption_algorithm=serialization.NoEncryption()
            ... )
            >>> loaded_private_key = ed25519.Ed25519PrivateKey.from_private_bytes(private_bytes)


    .. method:: public_key()

        :returns: :class:`Ed25519PublicKey`

    .. method:: sign(data)

        :param bytes data: The data to sign.

        :returns bytes: The 64 byte signature.

    .. method:: private_bytes(encoding, format, encryption_algorithm)

        Allows serialization of the key to bytes. Encoding (
        :attr:`~cryptography.hazmat.primitives.serialization.Encoding.PEM`,
        :attr:`~cryptography.hazmat.primitives.serialization.Encoding.DER`, or
        :attr:`~cryptography.hazmat.primitives.serialization.Encoding.Raw`) and
        format (
        :attr:`~cryptography.hazmat.primitives.serialization.PrivateFormat.PKCS8`,
        :attr:`~cryptography.hazmat.primitives.serialization.PrivateFormat.OpenSSH`
        or
        :attr:`~cryptography.hazmat.primitives.serialization.PrivateFormat.Raw`
        ) are chosen to define the exact serialization.

        :param encoding: A value from the
            :class:`~cryptography.hazmat.primitives.serialization.Encoding` enum.

        :param format: A value from the
            :class:`~cryptography.hazmat.primitives.serialization.PrivateFormat`
            enum. If the ``encoding`` is
            :attr:`~cryptography.hazmat.primitives.serialization.Encoding.Raw`
            then ``format`` must be
            :attr:`~cryptography.hazmat.primitives.serialization.PrivateFormat.Raw`
            , otherwise it must be
            :attr:`~cryptography.hazmat.primitives.serialization.PrivateFormat.PKCS8` or
            :attr:`~cryptography.hazmat.primitives.serialization.PrivateFormat.OpenSSH`.

        :param encryption_algorithm: An instance of an object conforming to the
            :class:`~cryptography.hazmat.primitives.serialization.KeySerializationEncryption`
            interface.

        :return bytes: Serialized key.

.. class:: Ed25519PublicKey

    .. versionadded:: 2.6

    .. classmethod:: from_public_bytes(data)

        :param bytes data: 32 byte public key.

        :returns: :class:`Ed25519PublicKey`

        .. doctest::

            >>> from cryptography.hazmat.primitives import serialization
            >>> from cryptography.hazmat.primitives.asymmetric import ed25519
            >>> private_key = ed25519.Ed25519PrivateKey.generate()
            >>> public_key = private_key.public_key()
            >>> public_bytes = public_key.public_bytes(
            ...     encoding=serialization.Encoding.Raw,
            ...     format=serialization.PublicFormat.Raw
            ... )
            >>> loaded_public_key = ed25519.Ed25519PublicKey.from_public_bytes(public_bytes)

    .. method:: public_bytes(encoding, format)

        Allows serialization of the key to bytes. Encoding (
        :attr:`~cryptography.hazmat.primitives.serialization.Encoding.PEM`,
        :attr:`~cryptography.hazmat.primitives.serialization.Encoding.DER`,
        :attr:`~cryptography.hazmat.primitives.serialization.Encoding.OpenSSH`,
        or
        :attr:`~cryptography.hazmat.primitives.serialization.Encoding.Raw`) and
        format (
        :attr:`~cryptography.hazmat.primitives.serialization.PublicFormat.SubjectPublicKeyInfo`,
        :attr:`~cryptography.hazmat.primitives.serialization.PublicFormat.OpenSSH`
        , or
        :attr:`~cryptography.hazmat.primitives.serialization.PublicFormat.Raw`
        ) are chosen to define the exact serialization.

        :param encoding: A value from the
            :class:`~cryptography.hazmat.primitives.serialization.Encoding` enum.

        :param format: A value from the
            :class:`~cryptography.hazmat.primitives.serialization.PublicFormat`
            enum. If the ``encoding`` is
            :attr:`~cryptography.hazmat.primitives.serialization.Encoding.Raw`
            then ``format`` must be
            :attr:`~cryptography.hazmat.primitives.serialization.PublicFormat.Raw`.
            If ``encoding`` is
            :attr:`~cryptography.hazmat.primitives.serialization.Encoding.OpenSSH`
            then ``format`` must be
            :attr:`~cryptography.hazmat.primitives.serialization.PublicFormat.OpenSSH`.
            In all other cases ``format`` must be
            :attr:`~cryptography.hazmat.primitives.serialization.PublicFormat.SubjectPublicKeyInfo`.

        :returns bytes: The public key bytes.

    .. method:: verify(signature, data)

        :param bytes signature: The signature to verify.

        :param bytes data: The data to verify.

        :raises cryptography.exceptions.InvalidSignature: Raised when the
            signature cannot be verified.



.. _`EdDSA`: https://en.wikipedia.org/wiki/EdDSA
.. _`Curve25519`: https://en.wikipedia.org/wiki/Curve25519
.. hazmat::

Ed448 signing
=============

.. currentmodule:: cryptography.hazmat.primitives.asymmetric.ed448


Ed448 is an elliptic curve signing algorithm using `EdDSA`_.


Signing & Verification
~~~~~~~~~~~~~~~~~~~~~~

.. doctest::

    >>> from cryptography.hazmat.primitives.asymmetric.ed448 import Ed448PrivateKey
    >>> private_key = Ed448PrivateKey.generate()
    >>> signature = private_key.sign(b"my authenticated message")
    >>> public_key = private_key.public_key()
    >>> # Raises InvalidSignature if verification fails
    >>> public_key.verify(signature, b"my authenticated message")

Key interfaces
~~~~~~~~~~~~~~

.. class:: Ed448PrivateKey

    .. versionadded:: 2.6

    .. classmethod:: generate()

        Generate an Ed448 private key.

        :returns: :class:`Ed448PrivateKey`

    .. classmethod:: from_private_bytes(data)

        :param data: 57 byte private key.
        :type data: :term:`bytes-like`

        :returns: :class:`Ed448PrivateKey`

    .. method:: public_key()

        :returns: :class:`Ed448PublicKey`

    .. method:: sign(data)

        :param bytes data: The data to sign.

        :returns bytes: The 114 byte signature.

    .. method:: private_bytes(encoding, format, encryption_algorithm)

        Allows serialization of the key to bytes. Encoding (
        :attr:`~cryptography.hazmat.primitives.serialization.Encoding.PEM`,
        :attr:`~cryptography.hazmat.primitives.serialization.Encoding.DER`, or
        :attr:`~cryptography.hazmat.primitives.serialization.Encoding.Raw`) and
        format (
        :attr:`~cryptography.hazmat.primitives.serialization.PrivateFormat.PKCS8`
        or
        :attr:`~cryptography.hazmat.primitives.serialization.PrivateFormat.Raw`
        ) are chosen to define the exact serialization.

        :param encoding: A value from the
            :class:`~cryptography.hazmat.primitives.serialization.Encoding` enum.

        :param format: A value from the
            :class:`~cryptography.hazmat.primitives.serialization.PrivateFormat`
            enum. If the ``encoding`` is
            :attr:`~cryptography.hazmat.primitives.serialization.Encoding.Raw`
            then ``format`` must be
            :attr:`~cryptography.hazmat.primitives.serialization.PrivateFormat.Raw`
            , otherwise it must be
            :attr:`~cryptography.hazmat.primitives.serialization.PrivateFormat.PKCS8`.

        :param encryption_algorithm: An instance of an object conforming to the
            :class:`~cryptography.hazmat.primitives.serialization.KeySerializationEncryption`
            interface.

        :return bytes: Serialized key.

.. class:: Ed448PublicKey

    .. versionadded:: 2.6

    .. classmethod:: from_public_bytes(data)

        :param bytes data: 57 byte public key.

        :returns: :class:`Ed448PublicKey`

    .. method:: public_bytes(encoding, format)

        Allows serialization of the key to bytes. Encoding (
        :attr:`~cryptography.hazmat.primitives.serialization.Encoding.PEM`,
        :attr:`~cryptography.hazmat.primitives.serialization.Encoding.DER`, or
        :attr:`~cryptography.hazmat.primitives.serialization.Encoding.Raw`) and
        format (
        :attr:`~cryptography.hazmat.primitives.serialization.PublicFormat.SubjectPublicKeyInfo`
        or
        :attr:`~cryptography.hazmat.primitives.serialization.PublicFormat.Raw`
        ) are chosen to define the exact serialization.

        :param encoding: A value from the
            :class:`~cryptography.hazmat.primitives.serialization.Encoding` enum.

        :param format: A value from the
            :class:`~cryptography.hazmat.primitives.serialization.PublicFormat`
            enum. If the ``encoding`` is
            :attr:`~cryptography.hazmat.primitives.serialization.Encoding.Raw`
            then ``format`` must be
            :attr:`~cryptography.hazmat.primitives.serialization.PublicFormat.Raw`
            , otherwise it must be
            :attr:`~cryptography.hazmat.primitives.serialization.PublicFormat.SubjectPublicKeyInfo`.

        :returns bytes: The public key bytes.

    .. method:: verify(signature, data)

        :param bytes signature: The signature to verify.

        :param bytes data: The data to verify.

        :raises cryptography.exceptions.InvalidSignature: Raised when the
            signature cannot be verified.



.. _`EdDSA`: https://en.wikipedia.org/wiki/EdDSA
.. hazmat::

Asymmetric algorithms
=====================

Asymmetric cryptography is a branch of cryptography where a secret key can be
divided into two parts, a :term:`public key` and a :term:`private key`. The
public key can be given to anyone, trusted or not, while the private key must
be kept secret (just like the key in symmetric cryptography).

Asymmetric cryptography has two primary use cases: authentication and
confidentiality. Using asymmetric cryptography, messages can be signed with a
private key, and then anyone with the public key is able to verify that the
message was created by someone possessing the corresponding private key. This
can be combined with a `proof of identity`_ system to know what entity (person
or group) actually owns that private key, providing authentication.

Encryption with asymmetric cryptography works in a slightly different way from
symmetric encryption. Someone with the public key is able to encrypt a message,
providing confidentiality, and then only the person in possession of the
private key is able to decrypt it.

.. toctree::
    :maxdepth: 1

    ed25519
    x25519
    ed448
    x448
    ec
    rsa
    dh
    dsa
    serialization
    utils


.. _`proof of identity`: https://en.wikipedia.org/wiki/Public-key_infrastructure
.. hazmat::

RSA
===

.. module:: cryptography.hazmat.primitives.asymmetric.rsa

`RSA`_ is a `public-key`_ algorithm for encrypting and signing messages.

Generation
~~~~~~~~~~

Unlike symmetric cryptography, where the key is typically just a random series
of bytes, RSA keys have a complex internal structure with `specific
mathematical properties`_.

.. function:: generate_private_key(public_exponent, key_size, backend=None)

    .. versionadded:: 0.5

    .. versionchanged:: 3.0

        Tightened restrictions on ``public_exponent``.

    Generates a new RSA private key using the provided ``backend``.
    ``key_size`` describes how many :term:`bits` long the key should be. Larger
    keys provide more security; currently ``1024`` and below are considered
    breakable while ``2048`` or ``4096`` are reasonable default key sizes for
    new keys. The ``public_exponent`` indicates what one mathematical property
    of the key generation will be. Unless you have a specific reason to do
    otherwise, you should always `use 65537`_.

    .. doctest::

        >>> from cryptography.hazmat.primitives.asymmetric import rsa
        >>> private_key = rsa.generate_private_key(
        ...     public_exponent=65537,
        ...     key_size=2048,
        ... )

    :param int public_exponent: The public exponent of the new key.
        Either 65537 or 3 (for legacy purposes). Almost everyone should
        `use 65537`_.

    :param int key_size: The length of the modulus in :term:`bits`. For keys
        generated in 2015 it is strongly recommended to be
        `at least 2048`_ (See page 41). It must not be less than 512.
        Some backends may have additional limitations.

    :param backend: An optional backend which implements
        :class:`~cryptography.hazmat.backends.interfaces.RSABackend`.

    :return: An instance of
        :class:`~cryptography.hazmat.primitives.asymmetric.rsa.RSAPrivateKey`.

    :raises cryptography.exceptions.UnsupportedAlgorithm: This is raised if
        the provided ``backend`` does not implement
        :class:`~cryptography.hazmat.backends.interfaces.RSABackend`

Key loading
~~~~~~~~~~~

If you already have an on-disk key in the PEM format (which are recognizable by
the distinctive ``-----BEGIN {format}-----`` and ``-----END {format}-----``
markers), you can load it:

.. code-block:: pycon

    >>> from cryptography.hazmat.primitives import serialization

    >>> with open("path/to/key.pem", "rb") as key_file:
    ...     private_key = serialization.load_pem_private_key(
    ...         key_file.read(),
    ...         password=None,
    ...     )

Serialized keys may optionally be encrypted on disk using a password. In this
example we loaded an unencrypted key, and therefore we did not provide a
password. If the key is encrypted we can pass a ``bytes`` object as the
``password`` argument.

There is also support for :func:`loading public keys in the SSH format
<cryptography.hazmat.primitives.serialization.load_ssh_public_key>`.

Key serialization
~~~~~~~~~~~~~~~~~

If you have a private key that you've loaded or generated which implements the
:class:`~cryptography.hazmat.primitives.asymmetric.rsa.RSAPrivateKeyWithSerialization`
interface you can use
:meth:`~cryptography.hazmat.primitives.asymmetric.rsa.RSAPrivateKeyWithSerialization.private_bytes`
to serialize the key.

.. doctest::

    >>> from cryptography.hazmat.primitives import serialization
    >>> pem = private_key.private_bytes(
    ...    encoding=serialization.Encoding.PEM,
    ...    format=serialization.PrivateFormat.PKCS8,
    ...    encryption_algorithm=serialization.BestAvailableEncryption(b'mypassword')
    ... )
    >>> pem.splitlines()[0]
    b'-----BEGIN ENCRYPTED PRIVATE KEY-----'

It is also possible to serialize without encryption using
:class:`~cryptography.hazmat.primitives.serialization.NoEncryption`.

.. doctest::

    >>> pem = private_key.private_bytes(
    ...    encoding=serialization.Encoding.PEM,
    ...    format=serialization.PrivateFormat.TraditionalOpenSSL,
    ...    encryption_algorithm=serialization.NoEncryption()
    ... )
    >>> pem.splitlines()[0]
    b'-----BEGIN RSA PRIVATE KEY-----'

For public keys you can use
:meth:`~cryptography.hazmat.primitives.asymmetric.rsa.RSAPublicKey.public_bytes`
to serialize the key.

.. doctest::

    >>> from cryptography.hazmat.primitives import serialization
    >>> public_key = private_key.public_key()
    >>> pem = public_key.public_bytes(
    ...    encoding=serialization.Encoding.PEM,
    ...    format=serialization.PublicFormat.SubjectPublicKeyInfo
    ... )
    >>> pem.splitlines()[0]
    b'-----BEGIN PUBLIC KEY-----'

Signing
~~~~~~~

A private key can be used to sign a message. This allows anyone with the public
key to verify that the message was created by someone who possesses the
corresponding private key. RSA signatures require a specific hash function, and
padding to be used. Here is an example of signing ``message`` using RSA, with a
secure hash function and padding:

.. doctest::

    >>> from cryptography.hazmat.primitives import hashes
    >>> from cryptography.hazmat.primitives.asymmetric import padding
    >>> message = b"A message I want to sign"
    >>> signature = private_key.sign(
    ...     message,
    ...     padding.PSS(
    ...         mgf=padding.MGF1(hashes.SHA256()),
    ...         salt_length=padding.PSS.MAX_LENGTH
    ...     ),
    ...     hashes.SHA256()
    ... )

Valid paddings for signatures are
:class:`~cryptography.hazmat.primitives.asymmetric.padding.PSS` and
:class:`~cryptography.hazmat.primitives.asymmetric.padding.PKCS1v15`. ``PSS``
is the recommended choice for any new protocols or applications, ``PKCS1v15``
should only be used to support legacy protocols.

If your data is too large to be passed in a single call, you can hash it
separately and pass that value using
:class:`~cryptography.hazmat.primitives.asymmetric.utils.Prehashed`.

.. doctest::

    >>> from cryptography.hazmat.primitives.asymmetric import utils
    >>> chosen_hash = hashes.SHA256()
    >>> hasher = hashes.Hash(chosen_hash)
    >>> hasher.update(b"data & ")
    >>> hasher.update(b"more data")
    >>> digest = hasher.finalize()
    >>> sig = private_key.sign(
    ...     digest,
    ...     padding.PSS(
    ...         mgf=padding.MGF1(hashes.SHA256()),
    ...         salt_length=padding.PSS.MAX_LENGTH
    ...     ),
    ...     utils.Prehashed(chosen_hash)
    ... )

Verification
~~~~~~~~~~~~

The previous section describes what to do if you have a private key and want to
sign something. If you have a public key, a message, a signature, and the
signing algorithm that was used you can check that the private key associated
with a given public key was used to sign that specific message.  You can obtain
a public key to use in verification using
:func:`~cryptography.hazmat.primitives.serialization.load_pem_public_key`,
:func:`~cryptography.hazmat.primitives.serialization.load_der_public_key`,
:meth:`~cryptography.hazmat.primitives.asymmetric.rsa.RSAPublicNumbers.public_key`
, or
:meth:`~cryptography.hazmat.primitives.asymmetric.rsa.RSAPrivateKey.public_key`.

.. doctest::

    >>> public_key = private_key.public_key()
    >>> public_key.verify(
    ...     signature,
    ...     message,
    ...     padding.PSS(
    ...         mgf=padding.MGF1(hashes.SHA256()),
    ...         salt_length=padding.PSS.MAX_LENGTH
    ...     ),
    ...     hashes.SHA256()
    ... )

If the signature does not match, ``verify()`` will raise an
:class:`~cryptography.exceptions.InvalidSignature` exception.

If your data is too large to be passed in a single call, you can hash it
separately and pass that value using
:class:`~cryptography.hazmat.primitives.asymmetric.utils.Prehashed`.

.. doctest::

    >>> chosen_hash = hashes.SHA256()
    >>> hasher = hashes.Hash(chosen_hash)
    >>> hasher.update(b"data & ")
    >>> hasher.update(b"more data")
    >>> digest = hasher.finalize()
    >>> public_key.verify(
    ...     sig,
    ...     digest,
    ...     padding.PSS(
    ...         mgf=padding.MGF1(hashes.SHA256()),
    ...         salt_length=padding.PSS.MAX_LENGTH
    ...     ),
    ...     utils.Prehashed(chosen_hash)
    ... )

Encryption
~~~~~~~~~~

RSA encryption is interesting because encryption is performed using the
**public** key, meaning anyone can encrypt data. The data is then decrypted
using the **private** key.

Like signatures, RSA supports encryption with several different padding
options. Here's an example using a secure padding and hash function:

.. doctest::

    >>> message = b"encrypted data"
    >>> ciphertext = public_key.encrypt(
    ...     message,
    ...     padding.OAEP(
    ...         mgf=padding.MGF1(algorithm=hashes.SHA256()),
    ...         algorithm=hashes.SHA256(),
    ...         label=None
    ...     )
    ... )

Valid paddings for encryption are
:class:`~cryptography.hazmat.primitives.asymmetric.padding.OAEP` and
:class:`~cryptography.hazmat.primitives.asymmetric.padding.PKCS1v15`. ``OAEP``
is the recommended choice for any new protocols or applications, ``PKCS1v15``
should only be used to support legacy protocols.


Decryption
~~~~~~~~~~

Once you have an encrypted message, it can be decrypted using the private key:

.. doctest::

    >>> plaintext = private_key.decrypt(
    ...     ciphertext,
    ...     padding.OAEP(
    ...         mgf=padding.MGF1(algorithm=hashes.SHA256()),
    ...         algorithm=hashes.SHA256(),
    ...         label=None
    ...     )
    ... )
    >>> plaintext == message
    True

Padding
~~~~~~~

.. module:: cryptography.hazmat.primitives.asymmetric.padding

.. class:: AsymmetricPadding

    .. versionadded:: 0.2

    .. attribute:: name

.. class:: PSS(mgf, salt_length)

    .. versionadded:: 0.3

    .. versionchanged:: 0.4
        Added ``salt_length`` parameter.

    PSS (Probabilistic Signature Scheme) is a signature scheme defined in
    :rfc:`3447`. It is more complex than PKCS1 but possesses a `security proof`_.
    This is the `recommended padding algorithm`_ for RSA signatures. It cannot
    be used with RSA encryption.

    :param mgf: A mask generation function object. At this time the only
        supported MGF is :class:`MGF1`.

    :param int salt_length: The length of the salt. It is recommended that this
        be set to ``PSS.MAX_LENGTH``.

    .. attribute:: MAX_LENGTH

        Pass this attribute to ``salt_length`` to get the maximum salt length
        available.

.. class:: OAEP(mgf, algorithm, label)

    .. versionadded:: 0.4

    OAEP (Optimal Asymmetric Encryption Padding) is a padding scheme defined in
    :rfc:`3447`. It provides probabilistic encryption and is `proven secure`_
    against several attack types. This is the `recommended padding algorithm`_
    for RSA encryption. It cannot be used with RSA signing.

    :param mgf: A mask generation function object. At this time the only
        supported MGF is :class:`MGF1`.

    :param algorithm: An instance of
        :class:`~cryptography.hazmat.primitives.hashes.HashAlgorithm`.

    :param bytes label: A label to apply. This is a rarely used field and
        should typically be set to ``None`` or ``b""``, which are equivalent.

.. class:: PKCS1v15()

    .. versionadded:: 0.3

    PKCS1 v1.5 (also known as simply PKCS1) is a simple padding scheme
    developed for use with RSA keys. It is defined in :rfc:`3447`. This padding
    can be used for signing and encryption.

    It is not recommended that ``PKCS1v15`` be used for new applications,
    :class:`OAEP` should be preferred for encryption and :class:`PSS` should be
    preferred for signatures.

    .. warning::

        Our implementation of PKCS1 v1.5 decryption is not constant time. See
        :doc:`/limitations` for details.


.. function:: calculate_max_pss_salt_length(key, hash_algorithm)

    .. versionadded:: 1.5

    :param key: An RSA public or private key.
    :param hash_algorithm: A
        :class:`cryptography.hazmat.primitives.hashes.HashAlgorithm`.
    :returns int: The computed salt length.

    Computes the length of the salt that :class:`PSS` will use if
    :data:`PSS.MAX_LENGTH` is used.


Mask generation functions
-------------------------

.. class:: MGF1(algorithm)

    .. versionadded:: 0.3

    .. versionchanged:: 0.6
        Removed the deprecated ``salt_length`` parameter.

    MGF1 (Mask Generation Function 1) is used as the mask generation function
    in :class:`PSS` and :class:`OAEP` padding. It takes a hash algorithm.

    :param algorithm: An instance of
        :class:`~cryptography.hazmat.primitives.hashes.HashAlgorithm`.

Numbers
~~~~~~~

.. currentmodule:: cryptography.hazmat.primitives.asymmetric.rsa

These classes hold the constituent components of an RSA key. They are useful
only when more traditional :doc:`/hazmat/primitives/asymmetric/serialization`
is unavailable.

.. class:: RSAPublicNumbers(e, n)

    .. versionadded:: 0.5

    The collection of integers that make up an RSA public key.

    .. attribute:: n

        :type: int

        The public modulus.

    .. attribute:: e

        :type: int

        The public exponent.

    .. method:: public_key(backend=None)

        :param backend: An optional instance of
            :class:`~cryptography.hazmat.backends.interfaces.RSABackend`.

        :returns: A new instance of
            :class:`~cryptography.hazmat.primitives.asymmetric.rsa.RSAPublicKey`.

.. class:: RSAPrivateNumbers(p, q, d, dmp1, dmq1, iqmp, public_numbers)

    .. versionadded:: 0.5

    The collection of integers that make up an RSA private key.

    .. warning::

        With the exception of the integers contained in the
        :class:`RSAPublicNumbers` all attributes of this class must be kept
        secret. Revealing them will compromise the security of any
        cryptographic operations performed with a key loaded from them.

    .. attribute:: public_numbers

        :type: :class:`~cryptography.hazmat.primitives.asymmetric.rsa.RSAPublicNumbers`

        The :class:`RSAPublicNumbers` which makes up the RSA public key
        associated with this RSA private key.

    .. attribute:: p

        :type: int

        ``p``, one of the two primes composing ``n``.

    .. attribute:: q

        :type: int

        ``q``, one of the two primes composing ``n``.

    .. attribute:: d

        :type: int

        The private exponent.

    .. attribute:: dmp1

        :type: int

        A `Chinese remainder theorem`_ coefficient used to speed up RSA
        operations. Calculated as: d mod (p-1)

    .. attribute:: dmq1

        :type: int

        A `Chinese remainder theorem`_ coefficient used to speed up RSA
        operations. Calculated as: d mod (q-1)

    .. attribute:: iqmp

        :type: int

        A `Chinese remainder theorem`_ coefficient used to speed up RSA
        operations. Calculated as: q\ :sup:`-1` mod p

    .. method:: private_key(backend=None)

        :param backend: An optional instance of
            :class:`~cryptography.hazmat.backends.interfaces.RSABackend`.

        :returns: An instance of
            :class:`~cryptography.hazmat.primitives.asymmetric.rsa.RSAPrivateKey`.

Handling partial RSA private keys
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

If you are trying to load RSA private keys yourself you may find that not all
parameters required by ``RSAPrivateNumbers`` are available. In particular the
`Chinese Remainder Theorem`_ (CRT) values ``dmp1``, ``dmq1``, ``iqmp`` may be
missing or present in a different form. For example, `OpenPGP`_ does not include
the ``iqmp``, ``dmp1`` or ``dmq1`` parameters.

The following functions are provided for users who want to work with keys like
this without having to do the math themselves.

.. function:: rsa_crt_iqmp(p, q)

    .. versionadded:: 0.4

    Computes the ``iqmp`` (also known as ``qInv``) parameter from the RSA
    primes ``p`` and ``q``.

.. function:: rsa_crt_dmp1(private_exponent, p)

    .. versionadded:: 0.4

    Computes the ``dmp1`` parameter from the RSA private exponent (``d``) and
    prime ``p``.

.. function:: rsa_crt_dmq1(private_exponent, q)

    .. versionadded:: 0.4

    Computes the ``dmq1`` parameter from the RSA private exponent (``d``) and
    prime ``q``.

.. function:: rsa_recover_prime_factors(n, e, d)

    .. versionadded:: 0.8

    Computes the prime factors ``(p, q)`` given the modulus, public exponent,
    and private exponent.

    .. note::

        When recovering prime factors this algorithm will always return ``p``
        and ``q`` such that ``p > q``. Note: before 1.5, this function always
        returned ``p`` and ``q`` such that ``p < q``. It was changed because
        libraries commonly require ``p > q``.

    :return: A tuple ``(p, q)``


Key interfaces
~~~~~~~~~~~~~~

.. class:: RSAPrivateKey

    .. versionadded:: 0.2

    An `RSA`_ private key. An RSA private key that is not an
    :term:`opaque key` also implements :class:`RSAPrivateKeyWithSerialization`
    to provide serialization methods.

    .. method:: decrypt(ciphertext, padding)

        .. versionadded:: 0.4

        Decrypt data that was encrypted with the public key.

        :param bytes ciphertext: The ciphertext to decrypt.

        :param padding: An instance of
            :class:`~cryptography.hazmat.primitives.asymmetric.padding.AsymmetricPadding`.

        :return bytes: Decrypted data.

    .. method:: public_key()

        :return: :class:`~cryptography.hazmat.primitives.asymmetric.rsa.RSAPublicKey`

        An RSA public key object corresponding to the values of the private key.

    .. attribute:: key_size

        :type: int

        The bit length of the modulus.

    .. method:: sign(data, padding, algorithm)

        .. versionadded:: 1.4
        .. versionchanged:: 1.6
            :class:`~cryptography.hazmat.primitives.asymmetric.utils.Prehashed`
            can now be used as an ``algorithm``.

        Sign one block of data which can be verified later by others using the
        public key.

        :param bytes data: The message string to sign.

        :param padding: An instance of
            :class:`~cryptography.hazmat.primitives.asymmetric.padding.AsymmetricPadding`.

        :param algorithm: An instance of
            :class:`~cryptography.hazmat.primitives.hashes.HashAlgorithm` or
            :class:`~cryptography.hazmat.primitives.asymmetric.utils.Prehashed`
            if the ``data`` you want to sign has already been hashed.

        :return bytes: Signature.


.. class:: RSAPrivateKeyWithSerialization

    .. versionadded:: 0.8

    This interface contains additional methods relating to serialization.
    Any object with this interface also has all the methods from
    :class:`RSAPrivateKey`.

    .. method:: private_numbers()

        Create a
        :class:`~cryptography.hazmat.primitives.asymmetric.rsa.RSAPrivateNumbers`
        object.

        :returns: An
            :class:`~cryptography.hazmat.primitives.asymmetric.rsa.RSAPrivateNumbers`
            instance.

    .. method:: private_bytes(encoding, format, encryption_algorithm)

        Allows serialization of the key to bytes. Encoding (
        :attr:`~cryptography.hazmat.primitives.serialization.Encoding.PEM` or
        :attr:`~cryptography.hazmat.primitives.serialization.Encoding.DER`),
        format (
        :attr:`~cryptography.hazmat.primitives.serialization.PrivateFormat.TraditionalOpenSSL`,
        :attr:`~cryptography.hazmat.primitives.serialization.PrivateFormat.OpenSSH`
        or
        :attr:`~cryptography.hazmat.primitives.serialization.PrivateFormat.PKCS8`)
        and encryption algorithm (such as
        :class:`~cryptography.hazmat.primitives.serialization.BestAvailableEncryption`
        or :class:`~cryptography.hazmat.primitives.serialization.NoEncryption`)
        are chosen to define the exact serialization.

        :param encoding: A value from the
            :class:`~cryptography.hazmat.primitives.serialization.Encoding` enum.

        :param format: A value from the
            :class:`~cryptography.hazmat.primitives.serialization.PrivateFormat`
            enum.

        :param encryption_algorithm: An instance of an object conforming to the
            :class:`~cryptography.hazmat.primitives.serialization.KeySerializationEncryption`
            interface.

        :return bytes: Serialized key.


.. class:: RSAPublicKey

    .. versionadded:: 0.2

    An `RSA`_ public key.

    .. method:: encrypt(plaintext, padding)

        .. versionadded:: 0.4

        Encrypt data with the public key.

        :param bytes plaintext: The plaintext to encrypt.

        :param padding: An instance of
            :class:`~cryptography.hazmat.primitives.asymmetric.padding.AsymmetricPadding`.

        :return bytes: Encrypted data.

    .. attribute:: key_size

        :type: int

        The bit length of the modulus.

    .. method:: public_numbers()

        Create a
        :class:`~cryptography.hazmat.primitives.asymmetric.rsa.RSAPublicNumbers`
        object.

        :returns: An
            :class:`~cryptography.hazmat.primitives.asymmetric.rsa.RSAPublicNumbers`
            instance.

    .. method:: public_bytes(encoding, format)

        Allows serialization of the key to bytes. Encoding (
        :attr:`~cryptography.hazmat.primitives.serialization.Encoding.PEM` or
        :attr:`~cryptography.hazmat.primitives.serialization.Encoding.DER`) and
        format (
        :attr:`~cryptography.hazmat.primitives.serialization.PublicFormat.SubjectPublicKeyInfo`
        or
        :attr:`~cryptography.hazmat.primitives.serialization.PublicFormat.PKCS1`)
        are chosen to define the exact serialization.

        :param encoding: A value from the
            :class:`~cryptography.hazmat.primitives.serialization.Encoding` enum.

        :param format: A value from the
            :class:`~cryptography.hazmat.primitives.serialization.PublicFormat` enum.

        :return bytes: Serialized key.

    .. method:: verify(signature, data, padding, algorithm)

        .. versionadded:: 1.4
        .. versionchanged:: 1.6
            :class:`~cryptography.hazmat.primitives.asymmetric.utils.Prehashed`
            can now be used as an ``algorithm``.

        Verify one block of data was signed by the private key
        associated with this public key.

        :param bytes signature: The signature to verify.

        :param bytes data: The message string that was signed.

        :param padding: An instance of
            :class:`~cryptography.hazmat.primitives.asymmetric.padding.AsymmetricPadding`.

        :param algorithm: An instance of
            :class:`~cryptography.hazmat.primitives.hashes.HashAlgorithm` or
            :class:`~cryptography.hazmat.primitives.asymmetric.utils.Prehashed`
            if the ``data`` you want to verify has already been hashed.

        :raises cryptography.exceptions.InvalidSignature: If the signature does
            not validate.

    .. method:: recover_data_from_signature(signature, padding, algorithm)

        .. versionadded:: 3.3

        Recovers the signed data from the signature. The data contains the
        digest of the original message string. The ``padding`` and
        ``algorithm`` parameters must match the ones used when the signature
        was created for the recovery to succeed.

        The ``algorithm`` parameter can also be set to ``None`` to recover all
        the data present in the signature, without regard to its format or the
        hash algorithm used for its creation.

        For
        :class:`~cryptography.hazmat.primitives.asymmetric.padding.PKCS1v15`
        padding, this returns the data after removing the padding layer. For
        standard signatures the data contains the full ``DigestInfo`` structure.
        For non-standard signatures, any data can be returned, including zero-
        length data.

        Normally you should use the
        :meth:`~cryptography.hazmat.primitives.asymmetric.rsa.RSAPublicKey.verify`
        function to validate the signature. But for some non-standard signature
        formats you may need to explicitly recover and validate the signed
        data. Following are some examples:

        - Some old Thawte and Verisign timestamp certificates without ``DigestInfo``.
        - Signed MD5/SHA1 hashes in TLS 1.1 or earlier (RFC 4346, section 4.7).
        - IKE version 1 signatures without ``DigestInfo`` (RFC 2409, section 5.1).

        :param bytes signature: The signature.

        :param padding: An instance of
            :class:`~cryptography.hazmat.primitives.asymmetric.padding.AsymmetricPadding`.
            Recovery is only supported with some of the padding types. (Currently
            only with
            :class:`~cryptography.hazmat.primitives.asymmetric.padding.PKCS1v15`).

        :param algorithm: An instance of
            :class:`~cryptography.hazmat.primitives.hashes.HashAlgorithm`.
            Can be ``None`` to return the all the data present in the signature.

        :return bytes: The signed data.

        :raises cryptography.exceptions.InvalidSignature: If the signature is
            invalid.

        :raises cryptography.exceptions.UnsupportedAlgorithm: If signature
            data recovery is not supported with the provided ``padding`` type.

.. class:: RSAPublicKeyWithSerialization

    .. versionadded:: 0.8

    Alias for :class:`RSAPublicKey`.


.. _`RSA`: https://en.wikipedia.org/wiki/RSA_(cryptosystem)
.. _`public-key`: https://en.wikipedia.org/wiki/Public-key_cryptography
.. _`specific mathematical properties`: https://en.wikipedia.org/wiki/RSA_(cryptosystem)#Key_generation
.. _`use 65537`: https://www.daemonology.net/blog/2009-06-11-cryptographic-right-answers.html
.. _`at least 2048`: https://www.cosic.esat.kuleuven.be/ecrypt/ecrypt2/documents/D.SPA.20.pdf
.. _`OpenPGP`: https://en.wikipedia.org/wiki/Pretty_Good_Privacy
.. _`Chinese Remainder Theorem`: https://en.wikipedia.org/wiki/RSA_%28cryptosystem%29#Using_the_Chinese_remainder_algorithm
.. _`security proof`: https://eprint.iacr.org/2001/062.pdf
.. _`recommended padding algorithm`: https://www.daemonology.net/blog/2009-06-11-cryptographic-right-answers.html
.. _`proven secure`: https://cseweb.ucsd.edu/~mihir/papers/oae.pdf
.. hazmat::

Key Serialization
=================

.. module:: cryptography.hazmat.primitives.serialization

.. testsetup::

    import base64

    pem_data = b"""
    -----BEGIN RSA PRIVATE KEY-----
    MIICXgIBAAKBgQDn09PV9KPE7Q+N5K5UtNLT1DLl8z/pKM2pP5tXqWx2OsEw00lC
    kDHdHESwzS050s/8rtkERKKyusCzCm9+vC1pQzUlmtibfF4PQAQc1pJL6KHqlidg
    Hw49atYmnC25CaeXt65pAYXoIacOZ8k5X7FW3Eagex8nG0iMw4ObOtg6CwIDAQAB
    AoGBAL31l/4YYN1rNrSZLrQgGyUSGsbLxJHEKolFon95R3O1fzoH117gkstQb4TE
    Cwv3jw/JIfBaYUq8tku/AE9D2Jx51x7kYaCuQIMTavKIgkXKfxTQCQDjSEfkvXMW
    4WOIj5sYdSCNbzLbaeFsWG32bSsBTy/sSheDIlCEFnqDuqwBAkEA+wYfJEMDf5nS
    VCQd9VKGM4HVeTWBioaWBFCflFdhc1Vb65dsNDp8iIMZgAHC2LEX5dMUmgqXk7AT
    lwFlIeW4CwJBAOxsSfuIVMuPKyx1xQ6ebpC7zeVxIOdswcM8ain91MSGDdKZw6pF
    ioFh3kUbKHw4yqqHbdRmUDAJ1mcgGJQOxgECQQCmQaGylKfmhWymyd0FtIip6J4I
    z4ViyEznwrZOu6kRiEF/QiUqWmpMx/fFrmTsvC5Fy43jkIxgBsiSxRvEXa+NAkB+
    5m0bhwTEslchKSGZhC6inzuYAQ4BSh4C1mXBnk5bIf0/Ymtk9KiwY8CzZS1o5+7Y
    c5LfI/+8mTss5UxsBDYBAkEA6NqhcsNWndIJZiWUU4u+RjFUQXqH8WCyJmEDCNxs
    7SGRS1DTUGX4Y70m9dQpguy6Zg+gpHC+o+ERZR06uEQr+w==
    -----END RSA PRIVATE KEY-----
    """.strip()
    public_pem_data = b"""
    -----BEGIN PUBLIC KEY-----
    MIGfMA0GCSqGSIb3DQEBAQUAA4GNADCBiQKBgQDn09PV9KPE7Q+N5K5UtNLT1DLl
    8z/pKM2pP5tXqWx2OsEw00lCkDHdHESwzS050s/8rtkERKKyusCzCm9+vC1pQzUl
    mtibfF4PQAQc1pJL6KHqlidgHw49atYmnC25CaeXt65pAYXoIacOZ8k5X7FW3Eag
    ex8nG0iMw4ObOtg6CwIDAQAB
    -----END PUBLIC KEY-----
    """.strip()
    der_data = base64.b64decode(
        b"MIICdwIBADANBgkqhkiG9w0BAQEFAASCAmEwggJdAgEAAoGBALskegl+DrI3Msw5Z63x"
        b"nj1rgoPR0KykwBi+jZgAwHv/B0TJyhy6NuEnaf+x442L7lepOqoWQzlUGXyuaSQU9mT/"
        b"vHTGZ2xM8QJJaccr4eGho0MU9HePyNCFWjWVrGKpwSEAd6CLlzC0Wiy4kC9IoAUoS/IP"
        b"jeyLTQNCddatgcARAgMBAAECgYAA/LlKJgeJUStTcpHgGD6mXjHvnAwWJELQKDP5+tA8"
        b"VAQGwBX1G5qzJDGrPGtHQ7DSqdwF4YFZtgTpZmGq1wsAjz3lv6L4XiVsHiIPtP1B4gMx"
        b"X9ogxcDzVQ7hyezXPioMAcp7Isus9Csn8HhftcL56BRabn6GvWqbIAy6zJcgEQJBAMlZ"
        b"nymKW5/jKth+wkCfqEXlPhGNPO1uq87QZUbYxwdjtSM09J9+HMfH+WXR9ARCOL46DJ0I"
        b"JfyjcdmuDDlh9IkCQQDt76up1Tmc7lkb/89IRBu2MudGJPMEf96VCG11nmcXulyk1OLi"
        b"TXfO62YpxZbgYrvlrNxEYlSG7WQMztBgA51JAkBU2RhyJ+S+drsaaigvlVgSxCyotszi"
        b"/Q0XZMgY18bfPUwanvkqsLkuEv3sw1HB7an9t3aTQdjIIpQad/acw8OJAkEAjvmnCK21"
        b"KgTbjQShtQYgNNLPwImxcjG4OYvP4o6l2k9FHlNCZsQwSymOwWkXKYyK5g+CaKFBs7Zw"
        b"mXWpJxjk6QJBAInqbm1w3yVfGD9I2mMQi/6oDJQP3pdWU4mU4h4sdDyRgTQLpkD4yypg"
        b"jOACt4mTzxifSVT9fT+a79SkT8FFmZE="
    )
    public_der_data = base64.b64decode(
        b"MIGfMA0GCSqGSIb3DQEBAQUAA4GNADCBiQKBgQC7JHoJfg6yNzLMOWet8Z49a4KD0dCs"
        b"pMAYvo2YAMB7/wdEycocujbhJ2n/seONi+5XqTqqFkM5VBl8rmkkFPZk/7x0xmdsTPEC"
        b"SWnHK+HhoaNDFPR3j8jQhVo1laxiqcEhAHegi5cwtFosuJAvSKAFKEvyD43si00DQnXW"
        b"rYHAEQIDAQAB"
    )
    message = b""

    def sign_with_rsa_key(key, message):
        return b""

    def sign_with_dsa_key(key, message):
        return b""

    parameters_pem_data = b"""
    -----BEGIN DH PARAMETERS-----
    MIGHAoGBALsrWt44U1ojqTy88o0wfjysBE51V6Vtarjm2+5BslQK/RtlndHde3gx
    +ccNs+InANszcuJFI8AHt4743kGRzy5XSlul4q4dDJENOHoyqYxueFuFVJELEwLQ
    XrX/McKw+hS6GPVQnw6tZhgGo9apdNdYgeLQeQded8Bum8jqzP3rAgEC
    -----END DH PARAMETERS-----
    """.strip()

    parameters_der_data = base64.b64decode(
        b"MIGHAoGBALsrWt44U1ojqTy88o0wfjysBE51V6Vtarjm2+5BslQK/RtlndHde3gx+ccNs+In"
        b"ANsz\ncuJFI8AHt4743kGRzy5XSlul4q4dDJENOHoyqYxueFuFVJELEwLQXrX/McKw+hS6GP"
        b"VQnw6tZhgG\no9apdNdYgeLQeQded8Bum8jqzP3rAgEC"
    )

There are several common schemes for serializing asymmetric private and public
keys to bytes. They generally support encryption of private keys and additional
key metadata.

Many serialization formats support multiple different types of asymmetric keys
and will return an instance of the appropriate type. You should check that
the returned key matches the type your application expects when using these
methods.

    .. doctest::

        >>> from cryptography.hazmat.primitives.asymmetric import dsa, rsa
        >>> from cryptography.hazmat.primitives.serialization import load_pem_private_key
        >>> key = load_pem_private_key(pem_data, password=None)
        >>> if isinstance(key, rsa.RSAPrivateKey):
        ...     signature = sign_with_rsa_key(key, message)
        ... elif isinstance(key, dsa.DSAPrivateKey):
        ...     signature = sign_with_dsa_key(key, message)
        ... else:
        ...     raise TypeError

Key dumping
~~~~~~~~~~~

The ``serialization`` module contains functions for loading keys from
``bytes``. To dump a ``key`` object to ``bytes``, you must call the appropriate
method on the key object. Documentation for these methods in found in the
:mod:`~cryptography.hazmat.primitives.asymmetric.rsa`,
:mod:`~cryptography.hazmat.primitives.asymmetric.dsa`, and
:mod:`~cryptography.hazmat.primitives.asymmetric.ec` module documentation.

PEM
~~~

PEM is an encapsulation format, meaning keys in it can actually be any of
several different key types. However these are all self-identifying, so you
don't need to worry about this detail. PEM keys are recognizable because they
all begin with ``-----BEGIN {format}-----`` and end with ``-----END
{format}-----``.

.. note::

    A PEM block which starts with ``-----BEGIN CERTIFICATE-----`` is not a
    public or private key, it's an :doc:`X.509 Certificate </x509/index>`. You
    can load it using :func:`~cryptography.x509.load_pem_x509_certificate` and
    extract the public key with
    :meth:`Certificate.public_key <cryptography.x509.Certificate.public_key>`.

.. function:: load_pem_private_key(data, password, backend=None)

    .. versionadded:: 0.6

    Deserialize a private key from PEM encoded data to one of the supported
    asymmetric private key types.

    :param data: The PEM encoded key data.
    :type data: :term:`bytes-like`

    :param password: The password to use to decrypt the data. Should
        be ``None`` if the private key is not encrypted.
    :type data: :term:`bytes-like`

    :param backend: An optional instance of
        :class:`~cryptography.hazmat.backends.interfaces.PEMSerializationBackend`.

    :returns: One of
        :class:`~cryptography.hazmat.primitives.asymmetric.rsa.RSAPrivateKey`,
        :class:`~cryptography.hazmat.primitives.asymmetric.dsa.DSAPrivateKey`,
        :class:`~cryptography.hazmat.primitives.asymmetric.dh.DHPrivateKey`,
        or
        :class:`~cryptography.hazmat.primitives.asymmetric.ec.EllipticCurvePrivateKey`
        depending on the contents of ``data``.

    :raises ValueError: If the PEM data could not be decrypted or if its
        structure could not be decoded successfully.

    :raises TypeError: If a ``password`` was given and the private key was
        not encrypted. Or if the key was encrypted but no
        password was supplied.

    :raises cryptography.exceptions.UnsupportedAlgorithm: If the serialized key
        is of a type that is not supported by the backend.

.. function:: load_pem_public_key(data, backend=None)

    .. versionadded:: 0.6

    Deserialize a public key from PEM encoded data to one of the supported
    asymmetric public key types. The PEM encoded data is typically a
    ``subjectPublicKeyInfo`` payload as specified in :rfc:`5280`.

    .. doctest::

        >>> from cryptography.hazmat.primitives.serialization import load_pem_public_key
        >>> key = load_pem_public_key(public_pem_data)
        >>> isinstance(key, rsa.RSAPublicKey)
        True

    :param bytes data: The PEM encoded key data.

    :param backend: An optional instance of
        :class:`~cryptography.hazmat.backends.interfaces.PEMSerializationBackend`.


    :returns: One of
        :class:`~cryptography.hazmat.primitives.asymmetric.rsa.RSAPublicKey`,
        :class:`~cryptography.hazmat.primitives.asymmetric.dsa.DSAPublicKey`,
        :class:`~cryptography.hazmat.primitives.asymmetric.dh.DHPublicKey`,
        or
        :class:`~cryptography.hazmat.primitives.asymmetric.ec.EllipticCurvePublicKey`
        depending on the contents of ``data``.

    :raises ValueError: If the PEM data's structure could not be decoded
        successfully.

    :raises cryptography.exceptions.UnsupportedAlgorithm: If the serialized key
        is of a type that is not supported by the backend.

.. function:: load_pem_parameters(data, backend=None)

    .. versionadded:: 2.0

    Deserialize parameters from PEM encoded data to one of the supported
    asymmetric parameters types.

    .. doctest::

        >>> from cryptography.hazmat.primitives.serialization import load_pem_parameters
        >>> from cryptography.hazmat.primitives.asymmetric import dh
        >>> parameters = load_pem_parameters(parameters_pem_data)
        >>> isinstance(parameters, dh.DHParameters)
        True

    :param bytes data: The PEM encoded parameters data.

    :param backend: An optional instance of
        :class:`~cryptography.hazmat.backends.interfaces.PEMSerializationBackend`.


    :returns: Currently only
        :class:`~cryptography.hazmat.primitives.asymmetric.dh.DHParameters`
        supported.

    :raises ValueError: If the PEM data's structure could not be decoded
        successfully.

    :raises cryptography.exceptions.UnsupportedAlgorithm: If the serialized parameters
        is of a type that is not supported by the backend.

DER
~~~

DER is an ASN.1 encoding type. There are no encapsulation boundaries and the
data is binary. DER keys may be in a variety of formats, but as long as you
know whether it is a public or private key the loading functions will handle
the rest.

.. function:: load_der_private_key(data, password, backend=None)

    .. versionadded:: 0.8

    Deserialize a private key from DER encoded data to one of the supported
    asymmetric private key types.

    :param data: The DER encoded key data.
    :type data: :term:`bytes-like`

    :param password: The password to use to decrypt the data. Should
        be ``None`` if the private key is not encrypted.
    :type password: :term:`bytes-like`

    :param backend: An optional instance of
        :class:`~cryptography.hazmat.backends.interfaces.DERSerializationBackend`.

    :returns: One of
        :class:`~cryptography.hazmat.primitives.asymmetric.rsa.RSAPrivateKey`,
        :class:`~cryptography.hazmat.primitives.asymmetric.dsa.DSAPrivateKey`,
        :class:`~cryptography.hazmat.primitives.asymmetric.dh.DHPrivateKey`,
        or
        :class:`~cryptography.hazmat.primitives.asymmetric.ec.EllipticCurvePrivateKey`
        depending on the contents of ``data``.

    :raises ValueError: If the DER data could not be decrypted or if its
        structure could not be decoded successfully.

    :raises TypeError: If a ``password`` was given and the private key was
        not encrypted. Or if the key was encrypted but no
        password was supplied.

    :raises cryptography.exceptions.UnsupportedAlgorithm: If the serialized key
        is of a type that is not supported by the backend.

    .. doctest::

        >>> from cryptography.hazmat.primitives.asymmetric import rsa
        >>> from cryptography.hazmat.primitives.serialization import load_der_private_key
        >>> key = load_der_private_key(der_data, password=None)
        >>> isinstance(key, rsa.RSAPrivateKey)
        True

.. function:: load_der_public_key(data, backend=None)

    .. versionadded:: 0.8

    Deserialize a public key from DER encoded data to one of the supported
    asymmetric public key types. The DER encoded data is typically a
    ``subjectPublicKeyInfo`` payload as specified in :rfc:`5280`.

    :param bytes data: The DER encoded key data.

    :param backend: An optional instance of
        :class:`~cryptography.hazmat.backends.interfaces.DERSerializationBackend`.

    :returns: One of
        :class:`~cryptography.hazmat.primitives.asymmetric.rsa.RSAPublicKey`,
        :class:`~cryptography.hazmat.primitives.asymmetric.dsa.DSAPublicKey`,
        :class:`~cryptography.hazmat.primitives.asymmetric.dh.DHPublicKey`,
        or
        :class:`~cryptography.hazmat.primitives.asymmetric.ec.EllipticCurvePublicKey`
        depending on the contents of ``data``.

    :raises ValueError: If the DER data's structure could not be decoded
        successfully.

    :raises cryptography.exceptions.UnsupportedAlgorithm: If the serialized key is of a type that
        is not supported by the backend.

    .. doctest::

        >>> from cryptography.hazmat.primitives.asymmetric import rsa
        >>> from cryptography.hazmat.primitives.serialization import load_der_public_key
        >>> key = load_der_public_key(public_der_data)
        >>> isinstance(key, rsa.RSAPublicKey)
        True

.. function:: load_der_parameters(data, backend=None)

    .. versionadded:: 2.0

    Deserialize parameters from DER encoded data to one of the supported
    asymmetric parameters types.

    :param bytes data: The DER encoded parameters data.

    :param backend: An optional instance of
        :class:`~cryptography.hazmat.backends.interfaces.DERSerializationBackend`.

    :returns: Currently only
        :class:`~cryptography.hazmat.primitives.asymmetric.dh.DHParameters`
        supported.

    :raises ValueError: If the DER data's structure could not be decoded
        successfully.

    :raises cryptography.exceptions.UnsupportedAlgorithm: If the serialized key is of a type that
        is not supported by the backend.

    .. doctest::

        >>> from cryptography.hazmat.primitives.asymmetric import dh
        >>> from cryptography.hazmat.primitives.serialization import load_der_parameters
        >>> parameters = load_der_parameters(parameters_der_data)
        >>> isinstance(parameters, dh.DHParameters)
        True


OpenSSH Public Key
~~~~~~~~~~~~~~~~~~

The format used by OpenSSH to store public keys, as specified in :rfc:`4253`.

An example RSA key in OpenSSH format (line breaks added for formatting
purposes)::

    ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABAQDDu/XRP1kyK6Cgt36gts9XAk
    FiiuJLW6RU0j3KKVZSs1I7Z3UmU9/9aVh/rZV43WQG8jaR6kkcP4stOR0DEtll
    PDA7ZRBnrfiHpSQYQ874AZaAoIjgkv7DBfsE6gcDQLub0PFjWyrYQUJhtOLQEK
    vY/G0vt2iRL3juawWmCFdTK3W3XvwAdgGk71i6lHt+deOPNEPN2H58E4odrZ2f
    sxn/adpDqfb2sM0kPwQs0aWvrrKGvUaustkivQE4XWiSFnB0oJB/lKK/CKVKuy
    ///ImSCGHQRvhwariN2tvZ6CBNSLh3iQgeB0AkyJlng7MXB2qYq/Ci2FUOryCX
    2MzHvnbv testkey@localhost

DSA keys look almost identical but begin with ``ssh-dss`` rather than
``ssh-rsa``. ECDSA keys have a slightly different format, they begin with
``ecdsa-sha2-{curve}``.

.. function:: load_ssh_public_key(data, backend=None)

    .. versionadded:: 0.7

    Deserialize a public key from OpenSSH (:rfc:`4253` and
    `PROTOCOL.certkeys`_) encoded data to an
    instance of the public key type for the specified backend.

    :param data: The OpenSSH encoded key data.
    :type data: :term:`bytes-like`

    :param backend: An optional backend which implements
        :class:`~cryptography.hazmat.backends.interfaces.RSABackend`,
        :class:`~cryptography.hazmat.backends.interfaces.DSABackend`, or
        :class:`~cryptography.hazmat.backends.interfaces.EllipticCurveBackend`
        depending on the key's type.

    :returns: One of
        :class:`~cryptography.hazmat.primitives.asymmetric.rsa.RSAPublicKey`,
        :class:`~cryptography.hazmat.primitives.asymmetric.dsa.DSAPublicKey`,
        :class:`~cryptography.hazmat.primitives.asymmetric.ec.EllipticCurvePublicKey`
        , or
        :class:`~cryptography.hazmat.primitives.asymmetric.ed25519.Ed25519PublicKey`,
        depending on the contents of ``data``.

    :raises ValueError: If the OpenSSH data could not be properly decoded or
        if the key is not in the proper format.

    :raises cryptography.exceptions.UnsupportedAlgorithm: If the serialized
        key is of a type that is not supported.

OpenSSH Private Key
~~~~~~~~~~~~~~~~~~~

The format used by OpenSSH to store private keys, as approximately specified
in `PROTOCOL.key`_.

An example ECDSA key in OpenSSH format::

    -----BEGIN OPENSSH PRIVATE KEY-----
    b3BlbnNzaC1rZXktdjEAAAAABG5vbmUAAAAEbm9uZQAAAAAAAAABAAAAaAAAABNlY2RzYS
    1zaGEyLW5pc3RwMjU2AAAACG5pc3RwMjU2AAAAQQRI0fWnI1CxX7qYqp0ih6bxjhGmUrZK
    /Axf8vhM8Db3oH7CFR+JdL715lUdu4XCWvQZKVf60/h3kBFhuxQC23XjAAAAqKPzVaOj81
    WjAAAAE2VjZHNhLXNoYTItbmlzdHAyNTYAAAAIbmlzdHAyNTYAAABBBEjR9acjULFfupiq
    nSKHpvGOEaZStkr8DF/y+EzwNvegfsIVH4l0vvXmVR27hcJa9BkpV/rT+HeQEWG7FALbde
    MAAAAga/VGV2asRlL3kXXao0aochQ59nXHA2xEGeAoQd952r0AAAAJbWFya29AdmZmAQID
    BAUGBw==
    -----END OPENSSH PRIVATE KEY-----

.. function:: load_ssh_private_key(data, password, backend=None)

    .. versionadded:: 3.0

    Deserialize a private key from OpenSSH encoded data to an
    instance of the private key type for the specified backend.

    :param data: The PEM encoded OpenSSH private key data.
    :type data: :term:`bytes-like`

    :param bytes password: Password bytes to use to decrypt
        password-protected key. Or ``None`` if not needed.

    :param backend: An optional backend which implements
        :class:`~cryptography.hazmat.backends.interfaces.RSABackend`,
        :class:`~cryptography.hazmat.backends.interfaces.DSABackend`, or
        :class:`~cryptography.hazmat.backends.interfaces.EllipticCurveBackend`
        depending on the key's type.

    :returns: One of
        :class:`~cryptography.hazmat.primitives.asymmetric.rsa.RSAPublicKey`,
        :class:`~cryptography.hazmat.primitives.asymmetric.dsa.DSAPublicKey`,
        :class:`~cryptography.hazmat.primitives.asymmetric.ec.EllipticCurvePublicKey`
        or
        :class:`~cryptography.hazmat.primitives.asymmetric.ed25519.Ed25519PublicKey`,
        depending on the contents of ``data``.

    :raises ValueError: If the OpenSSH data could not be properly decoded,
        if the key is not in the proper format or the incorrect password
        was provided.

    :raises cryptography.exceptions.UnsupportedAlgorithm: If the serialized
        key is of a type that is not supported.

PKCS12
~~~~~~

.. currentmodule:: cryptography.hazmat.primitives.serialization.pkcs12

PKCS12 is a binary format described in :rfc:`7292`. It can contain
certificates, keys, and more. PKCS12 files commonly have a ``pfx`` or ``p12``
file suffix.

.. note::

    ``cryptography`` only supports a single private key and associated
    certificates when parsing PKCS12 files at this time.

.. function:: load_key_and_certificates(data, password, backend=None)

    .. versionadded:: 2.5

    Deserialize a PKCS12 blob.

    :param data: The binary data.
    :type data: :term:`bytes-like`

    :param password: The password to use to decrypt the data. ``None``
        if the PKCS12 is not encrypted.
    :type password: :term:`bytes-like`

    :param backend: An optional backend instance.

    :returns: A tuple of
        ``(private_key, certificate, additional_certificates)``.
        ``private_key`` is a private key type or ``None``, ``certificate``
        is either the :class:`~cryptography.x509.Certificate` whose public key
        matches the private key in the PKCS 12 object or ``None``, and
        ``additional_certificates`` is a list of all other
        :class:`~cryptography.x509.Certificate` instances in the PKCS12 object.

.. function:: serialize_key_and_certificates(name, key, cert, cas, encryption_algorithm)

    .. versionadded:: 3.0

    .. warning::

        PKCS12 encryption is not secure and should not be used as a security
        mechanism. Wrap a PKCS12 blob in a more secure envelope if you need
        to store or send it safely. Encryption is provided for compatibility
        reasons only.

    Serialize a PKCS12 blob.

    .. note::

        Due to `a bug in Firefox`_ it's not possible to load unencrypted PKCS12
        blobs in Firefox.

    :param name: The friendly name to use for the supplied certificate and key.
    :type name: bytes

    :param key: The private key to include in the structure.
    :type key: An
        :class:`~cryptography.hazmat.primitives.asymmetric.rsa.RSAPrivateKeyWithSerialization`
        ,
        :class:`~cryptography.hazmat.primitives.asymmetric.ec.EllipticCurvePrivateKeyWithSerialization`
        , or
        :class:`~cryptography.hazmat.primitives.asymmetric.dsa.DSAPrivateKeyWithSerialization`
        object.

    :param cert: The certificate associated with the private key.
    :type cert: :class:`~cryptography.x509.Certificate` or ``None``

    :param cas: An optional set of certificates to also include in the structure.
    :type cas: list of :class:`~cryptography.x509.Certificate` or ``None``

    :param encryption_algorithm: The encryption algorithm that should be used
        for the key and certificate. An instance of an object conforming to the
        :class:`~cryptography.hazmat.primitives.serialization.KeySerializationEncryption`
        interface. PKCS12 encryption is **very weak** and should not be used
        as a security boundary.

    :return bytes: Serialized PKCS12.

PKCS7
~~~~~

.. currentmodule:: cryptography.hazmat.primitives.serialization.pkcs7

PKCS7 is a format described in :rfc:`2315`, among other specifications. It can
contain certificates, CRLs, and much more. PKCS7 files commonly have a ``p7b``,
``p7m``, or ``p7s`` file suffix but other suffixes are also seen in the wild.

.. note::

    ``cryptography`` only supports parsing certificates from PKCS7 files at
    this time.

.. function:: load_pem_pkcs7_certificates(data)

    .. versionadded:: 3.1

    Deserialize a PEM encoded PKCS7 blob to a list of certificates. PKCS7 can
    contain many other types of data, including CRLs, but this function will
    ignore everything except certificates.

    :param data: The data.
    :type data: bytes

    :returns: A list of :class:`~cryptography.x509.Certificate`.

    :raises ValueError: If the PKCS7 data could not be loaded.

    :raises cryptography.exceptions.UnsupportedAlgorithm: If the PKCS7 data
        is of a type that is not supported.

.. function:: load_der_pkcs7_certificates(data)

    .. versionadded:: 3.1

    Deserialize a DER encoded PKCS7 blob to a list of certificates. PKCS7 can
    contain many other types of data, including CRLs, but this function will
    ignore everything except certificates.

    :param data: The data.
    :type data: bytes

    :returns: A list of :class:`~cryptography.x509.Certificate`.

    :raises ValueError: If the PKCS7 data could not be loaded.

    :raises cryptography.exceptions.UnsupportedAlgorithm: If the PKCS7 data
        is of a type that is not supported.

.. testsetup::

    ca_key = b"""
    -----BEGIN PRIVATE KEY-----
    MIGHAgEAMBMGByqGSM49AgEGCCqGSM49AwEHBG0wawIBAQQgA8Zqz5vLeR0ePZUe
    jBfdyMmnnI4U5uAJApWTsMn/RuWhRANCAAQY/8+7+Tm49d3D7sBAiwZ1BqtPzdgs
    UiROH+AQRme1XxW5Yr07zwxvvhr3tKEPtLnLboazUPlsUb/Bgte+xfkF
    -----END PRIVATE KEY-----
    """.strip()

    ca_cert = b"""
    -----BEGIN CERTIFICATE-----
    MIIBUTCB96ADAgECAgIDCTAKBggqhkjOPQQDAjAnMQswCQYDVQQGEwJVUzEYMBYG
    A1UEAwwPY3J5cHRvZ3JhcGh5IENBMB4XDTE3MDEwMTEyMDEwMFoXDTM4MTIzMTA4
    MzAwMFowJzELMAkGA1UEBhMCVVMxGDAWBgNVBAMMD2NyeXB0b2dyYXBoeSBDQTBZ
    MBMGByqGSM49AgEGCCqGSM49AwEHA0IABBj/z7v5Obj13cPuwECLBnUGq0/N2CxS
    JE4f4BBGZ7VfFblivTvPDG++Gve0oQ+0uctuhrNQ+WxRv8GC177F+QWjEzARMA8G
    A1UdEwEB/wQFMAMBAf8wCgYIKoZIzj0EAwIDSQAwRgIhANES742XWm64tkGnz8Dn
    pG6u2lHkZFQr3oaVvPcemvlbAiEA0WGGzmYx5C9UvfXIK7NEziT4pQtyESE0uRVK
    Xw4nMqk=
    -----END CERTIFICATE-----
    """.strip()


.. class:: PKCS7SignatureBuilder

    The PKCS7 signature builder can create both basic PKCS7 signed messages as
    well as S/MIME messages, which are commonly used in email. S/MIME has
    multiple versions, but this implements a subset of :rfc:`2632`, also known
    as S/MIME Version 3.

    .. versionadded:: 3.2

    .. doctest::

        >>> from cryptography import x509
        >>> from cryptography.hazmat.primitives import hashes, serialization
        >>> from cryptography.hazmat.primitives.serialization import pkcs7
        >>> cert = x509.load_pem_x509_certificate(ca_cert)
        >>> key = serialization.load_pem_private_key(ca_key, None)
        >>> options = [pkcs7.PKCS7Options.DetachedSignature]
        >>> pkcs7.PKCS7SignatureBuilder().set_data(
        ...     b"data to sign"
        ... ).add_signer(
        ...     cert, key, hashes.SHA256()
        ... ).sign(
        ...     serialization.Encoding.SMIME, options
        ... )
        b'...'

    .. method:: set_data(data)

        :param data: The data to be hashed and signed.
        :type data: :term:`bytes-like`

    .. method:: add_signer(certificate, private_key, hash_algorithm)

        :param certificate: The :class:`~cryptography.x509.Certificate`.

        :param private_key: The
            :class:`~cryptography.hazmat.primitives.asymmetric.rsa.RSAPrivateKey` or
            :class:`~cryptography.hazmat.primitives.asymmetric.ec.EllipticCurvePrivateKey`
            associated with the certificate provided.

        :param hash_algorithm: The
            :class:`~cryptography.hazmat.primitives.hashes.HashAlgorithm` that
            will be used to generate the signature. This must be an instance of
            :class:`~cryptography.hazmat.primitives.hashes.SHA1`,
            :class:`~cryptography.hazmat.primitives.hashes.SHA224`,
            :class:`~cryptography.hazmat.primitives.hashes.SHA256`,
            :class:`~cryptography.hazmat.primitives.hashes.SHA384`, or
            :class:`~cryptography.hazmat.primitives.hashes.SHA512`.

    .. method:: add_certificate(certificate)

        Add an additional certificate (typically used to help build a
        verification chain) to the PKCS7 structure. This method may
        be called multiple times to add as many certificates as desired.

        :param certificate: The :class:`~cryptography.x509.Certificate` to add.

    .. method:: sign(encoding, options, backend=None)

        :param encoding: :attr:`~cryptography.hazmat.primitives.serialization.Encoding.PEM`,
            :attr:`~cryptography.hazmat.primitives.serialization.Encoding.DER`,
            or :attr:`~cryptography.hazmat.primitives.serialization.Encoding.SMIME`.

        :param options: A list of
            :class:`~cryptography.hazmat.primitives.serialization.pkcs7.PKCS7Options`.

        :return bytes: The signed PKCS7 message.

        :param backend: An optional backend.


.. class:: PKCS7Options

    .. versionadded:: 3.2

    An enumeration of options for PKCS7 signature creation.

    .. attribute:: Text

        The text option adds ``text/plain`` headers to an S/MIME message when
        serializing to
        :attr:`~cryptography.hazmat.primitives.serialization.Encoding.SMIME`.
        This option is disallowed with ``DER`` serialization.

    .. attribute:: Binary

        Signing normally converts line endings (LF to CRLF). When
        passing this option the data will not be converted.

    .. attribute:: DetachedSignature

        Don't embed the signed data within the ASN.1. When signing with
        :attr:`~cryptography.hazmat.primitives.serialization.Encoding.SMIME`
        this also results in the data being added as clear text before the
        PEM encoded structure.

    .. attribute:: NoCapabilities

        PKCS7 structures contain a ``MIMECapabilities`` section inside the
        ``authenticatedAttributes``. Passing this as an option removes
        ``MIMECapabilities``.

    .. attribute:: NoAttributes

        PKCS7 structures contain an ``authenticatedAttributes`` section.
        Passing this as an option removes that section. Note that if you
        pass ``NoAttributes`` you can't pass ``NoCapabilities`` since
        ``NoAttributes`` removes ``MIMECapabilities`` and more.

    .. attribute:: NoCerts

        Don't include the signer's certificate in the PKCS7 structure. This can
        reduce the size of the signature but requires that the recipient can
        obtain the signer's certificate by other means (for example from a
        previously signed message).

Serialization Formats
~~~~~~~~~~~~~~~~~~~~~

.. currentmodule:: cryptography.hazmat.primitives.serialization

.. class:: PrivateFormat

    .. versionadded:: 0.8

    An enumeration for private key formats. Used with the ``private_bytes``
    method available on
    :class:`~cryptography.hazmat.primitives.asymmetric.rsa.RSAPrivateKeyWithSerialization`
    ,
    :class:`~cryptography.hazmat.primitives.asymmetric.ec.EllipticCurvePrivateKeyWithSerialization`
    , :class:`~cryptography.hazmat.primitives.asymmetric.dh.DHPrivateKeyWithSerialization`
    and
    :class:`~cryptography.hazmat.primitives.asymmetric.dsa.DSAPrivateKeyWithSerialization`.

    .. attribute:: TraditionalOpenSSL

        Frequently known as PKCS#1 format. Still a widely used format, but
        generally considered legacy.

        A PEM encoded RSA key will look like::

            -----BEGIN RSA PRIVATE KEY-----
            ...
            -----END RSA PRIVATE KEY-----

    .. attribute:: PKCS8

        A more modern format for serializing keys which allows for better
        encryption. Choose this unless you have explicit legacy compatibility
        requirements.

        A PEM encoded key will look like::

            -----BEGIN PRIVATE KEY-----
            ...
            -----END PRIVATE KEY-----

    .. attribute:: Raw

        .. versionadded:: 2.5

        A raw format used by :doc:`/hazmat/primitives/asymmetric/x448`. It is a
        binary format and is invalid for other key types.

    .. attribute:: OpenSSH

        .. versionadded:: 3.0

        Custom private key format for OpenSSH, internals are based on SSH protocol
        and not ASN1.  Requires
        :attr:`~cryptography.hazmat.primitives.serialization.Encoding.PEM`
        encoding.

        A PEM encoded OpenSSH key will look like::

            -----BEGIN OPENSSH PRIVATE KEY-----
            ...
            -----END OPENSSH PRIVATE KEY-----


.. class:: PublicFormat

    .. versionadded:: 0.8

    An enumeration for public key formats. Used with the ``public_bytes``
    method available on
    :class:`~cryptography.hazmat.primitives.asymmetric.rsa.RSAPublicKeyWithSerialization`
    ,
    :class:`~cryptography.hazmat.primitives.asymmetric.ec.EllipticCurvePublicKeyWithSerialization`
    , :class:`~cryptography.hazmat.primitives.asymmetric.dh.DHPublicKeyWithSerialization`
    , and
    :class:`~cryptography.hazmat.primitives.asymmetric.dsa.DSAPublicKeyWithSerialization`.

    .. attribute:: SubjectPublicKeyInfo

        This is the typical public key format. It consists of an algorithm
        identifier and the public key as a bit string. Choose this unless
        you have specific needs.

        A PEM encoded key will look like::

            -----BEGIN PUBLIC KEY-----
            ...
            -----END PUBLIC KEY-----

    .. attribute:: PKCS1

        Just the public key elements (without the algorithm identifier). This
        format is RSA only, but is used by some older systems.

        A PEM encoded key will look like::

            -----BEGIN RSA PUBLIC KEY-----
            ...
            -----END RSA PUBLIC KEY-----

    .. attribute:: OpenSSH

        .. versionadded:: 1.4

        The public key format used by OpenSSH (e.g. as found in
        ``~/.ssh/id_rsa.pub`` or ``~/.ssh/authorized_keys``).

    .. attribute:: Raw

        .. versionadded:: 2.5

        A raw format used by :doc:`/hazmat/primitives/asymmetric/x448`. It is a
        binary format and is invalid for other key types.

    .. attribute:: CompressedPoint

        .. versionadded:: 2.5

        A compressed elliptic curve public key as defined in ANSI X9.62 section
        4.3.6 (as well as `SEC 1 v2.0`_).

    .. attribute:: UncompressedPoint

        .. versionadded:: 2.5

        An uncompressed elliptic curve public key as defined in ANSI X9.62
        section 4.3.6 (as well as `SEC 1 v2.0`_).

.. class:: ParameterFormat

    .. versionadded:: 2.0

    An enumeration for parameters formats. Used with the ``parameter_bytes``
    method available on
    :class:`~cryptography.hazmat.primitives.asymmetric.dh.DHParametersWithSerialization`.

    .. attribute:: PKCS3

        ASN1 DH parameters sequence as defined in `PKCS3`_.

Serialization Encodings
~~~~~~~~~~~~~~~~~~~~~~~

.. class:: Encoding

    An enumeration for encoding types. Used with the ``private_bytes`` method
    available on
    :class:`~cryptography.hazmat.primitives.asymmetric.rsa.RSAPrivateKeyWithSerialization`
    ,
    :class:`~cryptography.hazmat.primitives.asymmetric.ec.EllipticCurvePrivateKeyWithSerialization`
    , :class:`~cryptography.hazmat.primitives.asymmetric.dh.DHPrivateKeyWithSerialization`,
    :class:`~cryptography.hazmat.primitives.asymmetric.dsa.DSAPrivateKeyWithSerialization`,
    and
    :class:`~cryptography.hazmat.primitives.asymmetric.x448.X448PrivateKey`
    as well as ``public_bytes`` on
    :class:`~cryptography.hazmat.primitives.asymmetric.rsa.RSAPublicKey`,
    :class:`~cryptography.hazmat.primitives.asymmetric.dh.DHPublicKey`,
    :class:`~cryptography.hazmat.primitives.asymmetric.ec.EllipticCurvePublicKey`,
    and
    :class:`~cryptography.hazmat.primitives.asymmetric.x448.X448PublicKey`.

    .. attribute:: PEM

        .. versionadded:: 0.8

        For PEM format. This is a base64 format with delimiters.

    .. attribute:: DER

        .. versionadded:: 0.9

        For DER format. This is a binary format.

    .. attribute:: OpenSSH

        .. versionadded:: 1.4

        The format used by OpenSSH public keys. This is a text format.

    .. attribute:: Raw

        .. versionadded:: 2.5

        A raw format used by :doc:`/hazmat/primitives/asymmetric/x448`. It is a
        binary format and is invalid for other key types.

    .. attribute:: X962

        .. versionadded:: 2.5

        The format used by elliptic curve point encodings. This is a binary
        format.

    .. attribute:: SMIME

        .. versionadded:: 3.2

        An output format used for PKCS7. This is a text format.


Serialization Encryption Types
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

.. class:: KeySerializationEncryption

    Objects with this interface are usable as encryption types with methods
    like ``private_bytes`` available on
    :class:`~cryptography.hazmat.primitives.asymmetric.rsa.RSAPrivateKeyWithSerialization`
    ,
    :class:`~cryptography.hazmat.primitives.asymmetric.ec.EllipticCurvePrivateKeyWithSerialization`
    , :class:`~cryptography.hazmat.primitives.asymmetric.dh.DHPrivateKeyWithSerialization`
    and
    :class:`~cryptography.hazmat.primitives.asymmetric.dsa.DSAPrivateKeyWithSerialization`.
    All other classes in this section represent the available choices for
    encryption and have this interface. They are used with
    :class:`~cryptography.hazmat.primitives.asymmetric.rsa.RSAPrivateKeyWithSerialization.private_bytes`.

.. class:: BestAvailableEncryption(password)

    Encrypt using the best available encryption for a given key's backend.
    This is a curated encryption choice and the algorithm may change over
    time.

    :param bytes password: The password to use for encryption.

.. class:: NoEncryption

    Do not encrypt.


.. _`a bug in Firefox`: https://bugzilla.mozilla.org/show_bug.cgi?id=773111
.. _`PKCS3`: https://www.teletrust.de/fileadmin/files/oid/oid_pkcs-3v1-4.pdf
.. _`SEC 1 v2.0`: https://www.secg.org/sec1-v2.pdf
.. _`PROTOCOL.key`: https://github.com/openssh/openssh-portable/blob/master/PROTOCOL.key
.. _`PROTOCOL.certkeys`: https://github.com/openssh/openssh-portable/blob/master/PROTOCOL.certkeys
.. hazmat::

Asymmetric Utilities
====================

.. currentmodule:: cryptography.hazmat.primitives.asymmetric.utils


.. function:: decode_dss_signature(signature)

    Takes in signatures generated by the DSA/ECDSA signers and returns a
    tuple ``(r, s)``. These signatures are ASN.1 encoded ``Dss-Sig-Value``
    sequences (as defined in :rfc:`3279`)

    :param bytes signature: The signature to decode.

    :returns: The decoded tuple ``(r, s)``.

    :raises ValueError: Raised if the signature is malformed.

.. function:: encode_dss_signature(r, s)

    Creates an ASN.1 encoded ``Dss-Sig-Value`` (as defined in :rfc:`3279`) from
    raw ``r`` and ``s`` values.

    :param int r: The raw signature value ``r``.

    :param int s: The raw signature value ``s``.

    :return bytes: The encoded signature.

.. class:: Prehashed(algorithm)

    .. versionadded:: 1.6

    ``Prehashed`` can be passed as the ``algorithm`` in the RSA
    :meth:`~cryptography.hazmat.primitives.asymmetric.rsa.RSAPrivateKey.sign`
    and
    :meth:`~cryptography.hazmat.primitives.asymmetric.rsa.RSAPublicKey.verify`
    as well as DSA
    :meth:`~cryptography.hazmat.primitives.asymmetric.dsa.DSAPrivateKey.sign`
    and
    :meth:`~cryptography.hazmat.primitives.asymmetric.dsa.DSAPublicKey.verify`
    methods.

    For elliptic curves it can be passed as the ``algorithm`` in
    :class:`~cryptography.hazmat.primitives.asymmetric.ec.ECDSA` and then used
    with
    :meth:`~cryptography.hazmat.primitives.asymmetric.ec.EllipticCurvePrivateKey.sign`
    and
    :meth:`~cryptography.hazmat.primitives.asymmetric.ec.EllipticCurvePublicKey.verify`
    .

    :param algorithm: An instance of
        :class:`~cryptography.hazmat.primitives.hashes.HashAlgorithm`.

    .. doctest::

        >>> import hashlib
        >>> from cryptography.hazmat.primitives import hashes
        >>> from cryptography.hazmat.primitives.asymmetric import (
        ...    padding, rsa, utils
        ... )
        >>> private_key = rsa.generate_private_key(
        ...     public_exponent=65537,
        ...     key_size=2048,
        ... )
        >>> prehashed_msg = hashlib.sha256(b"A message I want to sign").digest()
        >>> signature = private_key.sign(
        ...     prehashed_msg,
        ...     padding.PSS(
        ...         mgf=padding.MGF1(hashes.SHA256()),
        ...         salt_length=padding.PSS.MAX_LENGTH
        ...     ),
        ...     utils.Prehashed(hashes.SHA256())
        ... )
        >>> public_key = private_key.public_key()
        >>> public_key.verify(
        ...     signature,
        ...     prehashed_msg,
        ...     padding.PSS(
        ...         mgf=padding.MGF1(hashes.SHA256()),
        ...         salt_length=padding.PSS.MAX_LENGTH
        ...     ),
        ...     utils.Prehashed(hashes.SHA256())
        ... )
.. hazmat::

X25519 key exchange
===================

.. currentmodule:: cryptography.hazmat.primitives.asymmetric.x25519


X25519 is an elliptic curve `Diffie-Hellman key exchange`_ using `Curve25519`_.
It allows two parties to jointly agree on a shared secret using an insecure
channel.


Exchange Algorithm
~~~~~~~~~~~~~~~~~~

For most applications the ``shared_key`` should be passed to a key
derivation function. This allows mixing of additional information into the
key, derivation of multiple keys, and destroys any structure that may be
present.

.. doctest::

    >>> from cryptography.hazmat.primitives import hashes
    >>> from cryptography.hazmat.primitives.asymmetric.x25519 import X25519PrivateKey
    >>> from cryptography.hazmat.primitives.kdf.hkdf import HKDF
    >>> # Generate a private key for use in the exchange.
    >>> private_key = X25519PrivateKey.generate()
    >>> # In a real handshake the peer_public_key will be received from the
    >>> # other party. For this example we'll generate another private key and
    >>> # get a public key from that. Note that in a DH handshake both peers
    >>> # must agree on a common set of parameters.
    >>> peer_public_key = X25519PrivateKey.generate().public_key()
    >>> shared_key = private_key.exchange(peer_public_key)
    >>> # Perform key derivation.
    >>> derived_key = HKDF(
    ...     algorithm=hashes.SHA256(),
    ...     length=32,
    ...     salt=None,
    ...     info=b'handshake data',
    ... ).derive(shared_key)
    >>> # For the next handshake we MUST generate another private key.
    >>> private_key_2 = X25519PrivateKey.generate()
    >>> peer_public_key_2 = X25519PrivateKey.generate().public_key()
    >>> shared_key_2 = private_key_2.exchange(peer_public_key_2)
    >>> derived_key_2 = HKDF(
    ...     algorithm=hashes.SHA256(),
    ...     length=32,
    ...     salt=None,
    ...     info=b'handshake data',
    ... ).derive(shared_key_2)

Key interfaces
~~~~~~~~~~~~~~

.. class:: X25519PrivateKey

    .. versionadded:: 2.0

    .. classmethod:: generate()

        Generate an X25519 private key.

        :returns: :class:`X25519PrivateKey`

    .. classmethod:: from_private_bytes(data)

        .. versionadded:: 2.5

        A class method for loading an X25519 key encoded as
        :attr:`~cryptography.hazmat.primitives.serialization.Encoding.Raw`.

        :param bytes data: 32 byte private key.

        :returns: :class:`X25519PrivateKey`

        .. doctest::

            >>> from cryptography.hazmat.primitives import serialization
            >>> from cryptography.hazmat.primitives.asymmetric import x25519
            >>> private_key = x25519.X25519PrivateKey.generate()
            >>> private_bytes = private_key.private_bytes(
            ...     encoding=serialization.Encoding.Raw,
            ...     format=serialization.PrivateFormat.Raw,
            ...     encryption_algorithm=serialization.NoEncryption()
            ... )
            >>> loaded_private_key = x25519.X25519PrivateKey.from_private_bytes(private_bytes)

    .. method:: public_key()

        :returns: :class:`X25519PublicKey`

    .. method:: exchange(peer_public_key)

        :param X25519PublicKey peer_public_key: The public key for the
            peer.

        :returns bytes: A shared key.

    .. method:: private_bytes(encoding, format, encryption_algorithm)

        .. versionadded:: 2.5

        Allows serialization of the key to bytes. Encoding (
        :attr:`~cryptography.hazmat.primitives.serialization.Encoding.PEM`,
        :attr:`~cryptography.hazmat.primitives.serialization.Encoding.DER`, or
        :attr:`~cryptography.hazmat.primitives.serialization.Encoding.Raw`) and
        format (
        :attr:`~cryptography.hazmat.primitives.serialization.PrivateFormat.PKCS8`
        or
        :attr:`~cryptography.hazmat.primitives.serialization.PrivateFormat.Raw`
        ) are chosen to define the exact serialization.

        :param encoding: A value from the
            :class:`~cryptography.hazmat.primitives.serialization.Encoding` enum.

        :param format: A value from the
            :class:`~cryptography.hazmat.primitives.serialization.PrivateFormat`
            enum. If the ``encoding`` is
            :attr:`~cryptography.hazmat.primitives.serialization.Encoding.Raw`
            then ``format`` must be
            :attr:`~cryptography.hazmat.primitives.serialization.PrivateFormat.Raw`
            , otherwise it must be
            :attr:`~cryptography.hazmat.primitives.serialization.PrivateFormat.PKCS8`.

        :param encryption_algorithm: An instance of an object conforming to the
            :class:`~cryptography.hazmat.primitives.serialization.KeySerializationEncryption`
            interface.

        :return bytes: Serialized key.

.. class:: X25519PublicKey

    .. versionadded:: 2.0

    .. classmethod:: from_public_bytes(data)

        :param bytes data: 32 byte public key.

        :returns: :class:`X25519PublicKey`

        .. doctest::

            >>> from cryptography.hazmat.primitives.asymmetric import x25519
            >>> private_key = x25519.X25519PrivateKey.generate()
            >>> public_key = private_key.public_key()
            >>> public_bytes = public_key.public_bytes(
            ...     encoding=serialization.Encoding.Raw,
            ...     format=serialization.PublicFormat.Raw
            ... )
            >>> loaded_public_key = x25519.X25519PublicKey.from_public_bytes(public_bytes)

    .. method:: public_bytes(encoding, format)

        Allows serialization of the key to bytes. Encoding (
        :attr:`~cryptography.hazmat.primitives.serialization.Encoding.PEM`,
        :attr:`~cryptography.hazmat.primitives.serialization.Encoding.DER`, or
        :attr:`~cryptography.hazmat.primitives.serialization.Encoding.Raw`) and
        format (
        :attr:`~cryptography.hazmat.primitives.serialization.PublicFormat.SubjectPublicKeyInfo`
        or
        :attr:`~cryptography.hazmat.primitives.serialization.PublicFormat.Raw`
        ) are chosen to define the exact serialization.

        :param encoding: A value from the
            :class:`~cryptography.hazmat.primitives.serialization.Encoding` enum.

        :param format: A value from the
            :class:`~cryptography.hazmat.primitives.serialization.PublicFormat`
            enum. If the ``encoding`` is
            :attr:`~cryptography.hazmat.primitives.serialization.Encoding.Raw`
            then ``format`` must be
            :attr:`~cryptography.hazmat.primitives.serialization.PublicFormat.Raw`
            , otherwise it must be
            :attr:`~cryptography.hazmat.primitives.serialization.PublicFormat.SubjectPublicKeyInfo`.

        :returns bytes: The public key bytes.


.. _`Diffie-Hellman key exchange`: https://en.wikipedia.org/wiki/Diffie%E2%80%93Hellman_key_exchange
.. _`Curve25519`: https://en.wikipedia.org/wiki/Curve25519
.. hazmat::

X448 key exchange
===================

.. currentmodule:: cryptography.hazmat.primitives.asymmetric.x448


X448 is an elliptic curve `Diffie-Hellman key exchange`_ using `Curve448`_.
It allows two parties to jointly agree on a shared secret using an insecure
channel.


Exchange Algorithm
~~~~~~~~~~~~~~~~~~

For most applications the ``shared_key`` should be passed to a key
derivation function. This allows mixing of additional information into the
key, derivation of multiple keys, and destroys any structure that may be
present.

.. doctest::

    >>> from cryptography.hazmat.primitives import hashes
    >>> from cryptography.hazmat.primitives.asymmetric.x448 import X448PrivateKey
    >>> from cryptography.hazmat.primitives.kdf.hkdf import HKDF
    >>> # Generate a private key for use in the exchange.
    >>> private_key = X448PrivateKey.generate()
    >>> # In a real handshake the peer_public_key will be received from the
    >>> # other party. For this example we'll generate another private key and
    >>> # get a public key from that. Note that in a DH handshake both peers
    >>> # must agree on a common set of parameters.
    >>> peer_public_key = X448PrivateKey.generate().public_key()
    >>> shared_key = private_key.exchange(peer_public_key)
    >>> # Perform key derivation.
    >>> derived_key = HKDF(
    ...     algorithm=hashes.SHA256(),
    ...     length=32,
    ...     salt=None,
    ...     info=b'handshake data',
    ... ).derive(shared_key)
    >>> # For the next handshake we MUST generate another private key.
    >>> private_key_2 = X448PrivateKey.generate()
    >>> peer_public_key_2 = X448PrivateKey.generate().public_key()
    >>> shared_key_2 = private_key_2.exchange(peer_public_key_2)
    >>> derived_key_2 = HKDF(
    ...     algorithm=hashes.SHA256(),
    ...     length=32,
    ...     salt=None,
    ...     info=b'handshake data',
    ... ).derive(shared_key_2)

Key interfaces
~~~~~~~~~~~~~~

.. class:: X448PrivateKey

    .. versionadded:: 2.5

    .. classmethod:: generate()

        Generate an X448 private key.

        :returns: :class:`X448PrivateKey`

    .. classmethod:: from_private_bytes(data)

        :param data: 56 byte private key.
        :type data: :term:`bytes-like`

        :returns: :class:`X448PrivateKey`

        .. doctest::

            >>> from cryptography.hazmat.primitives import serialization
            >>> from cryptography.hazmat.primitives.asymmetric import x448
            >>> private_key = x448.X448PrivateKey.generate()
            >>> private_bytes = private_key.private_bytes(
            ...     encoding=serialization.Encoding.Raw,
            ...     format=serialization.PrivateFormat.Raw,
            ...     encryption_algorithm=serialization.NoEncryption()
            ... )
            >>> loaded_private_key = x448.X448PrivateKey.from_private_bytes(private_bytes)

    .. method:: public_key()

        :returns: :class:`X448PublicKey`

    .. method:: exchange(peer_public_key)

        :param X448PublicKey peer_public_key: The public key for the
            peer.

        :returns bytes: A shared key.

    .. method:: private_bytes(encoding, format, encryption_algorithm)

        Allows serialization of the key to bytes. Encoding (
        :attr:`~cryptography.hazmat.primitives.serialization.Encoding.PEM`,
        :attr:`~cryptography.hazmat.primitives.serialization.Encoding.DER`, or
        :attr:`~cryptography.hazmat.primitives.serialization.Encoding.Raw`) and
        format (
        :attr:`~cryptography.hazmat.primitives.serialization.PrivateFormat.PKCS8`
        or
        :attr:`~cryptography.hazmat.primitives.serialization.PrivateFormat.Raw`
        ) are chosen to define the exact serialization.

        :param encoding: A value from the
            :class:`~cryptography.hazmat.primitives.serialization.Encoding` enum.

        :param format: A value from the
            :class:`~cryptography.hazmat.primitives.serialization.PrivateFormat`
            enum. If the ``encoding`` is
            :attr:`~cryptography.hazmat.primitives.serialization.Encoding.Raw`
            then ``format`` must be
            :attr:`~cryptography.hazmat.primitives.serialization.PrivateFormat.Raw`
            , otherwise it must be
            :attr:`~cryptography.hazmat.primitives.serialization.PrivateFormat.PKCS8`.

        :param encryption_algorithm: An instance of an object conforming to the
            :class:`~cryptography.hazmat.primitives.serialization.KeySerializationEncryption`
            interface.

        :return bytes: Serialized key.

.. class:: X448PublicKey

    .. versionadded:: 2.5

    .. classmethod:: from_public_bytes(data)

        :param bytes data: 56 byte public key.

        :returns: :class:`X448PublicKey`

        .. doctest::

            >>> from cryptography.hazmat.primitives import serialization
            >>> from cryptography.hazmat.primitives.asymmetric import x448
            >>> private_key = x448.X448PrivateKey.generate()
            >>> public_key = private_key.public_key()
            >>> public_bytes = public_key.public_bytes(
            ...     encoding=serialization.Encoding.Raw,
            ...     format=serialization.PublicFormat.Raw
            ... )
            >>> loaded_public_key = x448.X448PublicKey.from_public_bytes(public_bytes)

    .. method:: public_bytes(encoding, format)

        Allows serialization of the key to bytes. Encoding (
        :attr:`~cryptography.hazmat.primitives.serialization.Encoding.PEM`,
        :attr:`~cryptography.hazmat.primitives.serialization.Encoding.DER`, or
        :attr:`~cryptography.hazmat.primitives.serialization.Encoding.Raw`) and
        format (
        :attr:`~cryptography.hazmat.primitives.serialization.PublicFormat.SubjectPublicKeyInfo`
        or
        :attr:`~cryptography.hazmat.primitives.serialization.PublicFormat.Raw`
        ) are chosen to define the exact serialization.

        :param encoding: A value from the
            :class:`~cryptography.hazmat.primitives.serialization.Encoding` enum.

        :param format: A value from the
            :class:`~cryptography.hazmat.primitives.serialization.PublicFormat`
            enum. If the ``encoding`` is
            :attr:`~cryptography.hazmat.primitives.serialization.Encoding.Raw`
            then ``format`` must be
            :attr:`~cryptography.hazmat.primitives.serialization.PublicFormat.Raw`
            , otherwise it must be
            :attr:`~cryptography.hazmat.primitives.serialization.PublicFormat.SubjectPublicKeyInfo`.

        :returns bytes: The public key bytes.


.. _`Diffie-Hellman key exchange`: https://en.wikipedia.org/wiki/Diffie%E2%80%93Hellman_key_exchange
.. _`Curve448`: https://en.wikipedia.org/wiki/Curve448
.. hazmat::

Constant time functions
=======================

.. currentmodule:: cryptography.hazmat.primitives.constant_time

This module contains functions for operating with secret data in a way that
does not leak information about that data through how long it takes to perform
the operation. These functions should be used whenever operating on secret data
along with data that is user supplied.

An example would be comparing a HMAC signature received from a client to the
one generated by the server code for authentication purposes.

For more information about this sort of issue, see `Coda Hale's blog post`_
about the timing attacks on KeyCzar and Java's ``MessageDigest.isEqual()``.


.. function:: bytes_eq(a, b)

    Compares ``a`` and ``b`` with one another. If ``a`` and ``b`` have
    different lengths, this returns ``False`` immediately. Otherwise it
    compares them in a way that takes the same amount of time, regardless of
    how many characters are the same between the two.

    .. doctest::

        >>> from cryptography.hazmat.primitives import constant_time
        >>> constant_time.bytes_eq(b"foo", b"foo")
        True
        >>> constant_time.bytes_eq(b"foo", b"bar")
        False

    :param bytes a: The left-hand side.
    :param bytes b: The right-hand side.
    :returns bool: ``True`` if ``a`` has the same bytes as ``b``, otherwise
                   ``False``.
    :raises TypeError: This exception is raised if ``a`` or ``b`` is not
                       ``bytes``.


.. _`Coda Hale's blog post`: https://codahale.com/a-lesson-in-timing-attacks/
.. hazmat::

Message digests (Hashing)
=========================

.. module:: cryptography.hazmat.primitives.hashes

.. class:: Hash(algorithm, backend=None)

    A cryptographic hash function takes an arbitrary block of data and
    calculates a fixed-size bit string (a digest), such that different data
    results (with a high probability) in different digests.

    This is an implementation of
    :class:`~cryptography.hazmat.primitives.hashes.HashContext` meant to
    be used with
    :class:`~cryptography.hazmat.primitives.hashes.HashAlgorithm`
    implementations to provide an incremental interface to calculating
    various message digests.

    .. doctest::

        >>> from cryptography.hazmat.primitives import hashes
        >>> digest = hashes.Hash(hashes.SHA256())
        >>> digest.update(b"abc")
        >>> digest.update(b"123")
        >>> digest.finalize()
        b'l\xa1=R\xcap\xc8\x83\xe0\xf0\xbb\x10\x1eBZ\x89\xe8bM\xe5\x1d\xb2\xd29%\x93\xafj\x84\x11\x80\x90'

    If the backend doesn't support the requested ``algorithm`` an
    :class:`~cryptography.exceptions.UnsupportedAlgorithm` exception will be
    raised.

    Keep in mind that attacks against cryptographic hashes only get stronger
    with time, and that often algorithms that were once thought to be strong,
    become broken. Because of this it's important to include a plan for
    upgrading the hash algorithm you use over time. For more information, see
    `Lifetimes of cryptographic hash functions`_.

    :param algorithm: A
        :class:`~cryptography.hazmat.primitives.hashes.HashAlgorithm`
        instance such as those described in
        :ref:`below <cryptographic-hash-algorithms>`.
    :param backend: An optional
        :class:`~cryptography.hazmat.backends.interfaces.HashBackend`
        instance.

    :raises cryptography.exceptions.UnsupportedAlgorithm: This is raised if the
        provided ``backend`` does not implement
        :class:`~cryptography.hazmat.backends.interfaces.HashBackend`

    .. method:: update(data)

        :param bytes data: The bytes to be hashed.
        :raises cryptography.exceptions.AlreadyFinalized: See :meth:`finalize`.
        :raises TypeError: This exception is raised if ``data`` is not ``bytes``.

    .. method:: copy()

        Copy this :class:`Hash` instance, usually so that you may call
        :meth:`finalize` to get an intermediate digest value while we continue
        to call :meth:`update` on the original instance.

        :return: A new instance of :class:`Hash` that can be updated
             and finalized independently of the original instance.
        :raises cryptography.exceptions.AlreadyFinalized: See :meth:`finalize`.

    .. method:: finalize()

        Finalize the current context and return the message digest as bytes.

        After ``finalize`` has been called this object can no longer be used
        and :meth:`update`, :meth:`copy`, and :meth:`finalize` will raise an
        :class:`~cryptography.exceptions.AlreadyFinalized` exception.

        :return bytes: The message digest as bytes.


.. _cryptographic-hash-algorithms:

SHA-2 family
~~~~~~~~~~~~

.. class:: SHA224()

    SHA-224 is a cryptographic hash function from the SHA-2 family and is
    standardized by NIST. It produces a 224-bit message digest.

.. class:: SHA256()

    SHA-256 is a cryptographic hash function from the SHA-2 family and is
    standardized by NIST. It produces a 256-bit message digest.

.. class:: SHA384()

    SHA-384 is a cryptographic hash function from the SHA-2 family and is
    standardized by NIST. It produces a 384-bit message digest.

.. class:: SHA512()

    SHA-512 is a cryptographic hash function from the SHA-2 family and is
    standardized by NIST. It produces a 512-bit message digest.

.. class:: SHA512_224()

    .. versionadded:: 2.5

    SHA-512/224 is a cryptographic hash function from the SHA-2 family and is
    standardized by NIST. It produces a 224-bit message digest.

.. class:: SHA512_256()

    .. versionadded:: 2.5

    SHA-512/256 is a cryptographic hash function from the SHA-2 family and is
    standardized by NIST. It produces a 256-bit message digest.

BLAKE2
~~~~~~

`BLAKE2`_ is a cryptographic hash function specified in :rfc:`7693`. BLAKE2's
design makes it immune to `length-extension attacks`_, an advantage over the
SHA-family of hashes.

.. note::

    While the RFC specifies keying, personalization, and salting features,
    these are not supported at this time due to limitations in OpenSSL 1.1.0.

.. class:: BLAKE2b(digest_size)

    BLAKE2b is optimized for 64-bit platforms and produces an 1 to 64-byte
    message digest.

    :param int digest_size: The desired size of the hash output in bytes. Only
        ``64`` is supported at this time.

    :raises ValueError: If the ``digest_size`` is invalid.

.. class:: BLAKE2s(digest_size)

    BLAKE2s is optimized for 8 to 32-bit platforms and produces a
    1 to 32-byte message digest.

    :param int digest_size: The desired size of the hash output in bytes. Only
        ``32`` is supported at this time.

    :raises ValueError: If the ``digest_size`` is invalid.

SHA-3 family
~~~~~~~~~~~~

SHA-3 is the most recent NIST secure hash algorithm standard. Despite the
larger number SHA-3 is not considered to be better than SHA-2. Instead, it uses
a significantly different internal structure so that **if** an attack appears
against SHA-2 it is unlikely to apply to SHA-3. SHA-3 is significantly slower
than SHA-2 so at this time most users should choose SHA-2.

.. class:: SHA3_224()

    .. versionadded:: 2.5

    SHA3/224 is a cryptographic hash function from the SHA-3 family and is
    standardized by NIST. It produces a 224-bit message digest.

.. class:: SHA3_256()

    .. versionadded:: 2.5

    SHA3/256 is a cryptographic hash function from the SHA-3 family and is
    standardized by NIST. It produces a 256-bit message digest.

.. class:: SHA3_384()

    .. versionadded:: 2.5

    SHA3/384 is a cryptographic hash function from the SHA-3 family and is
    standardized by NIST. It produces a 384-bit message digest.

.. class:: SHA3_512()

    .. versionadded:: 2.5

    SHA3/512 is a cryptographic hash function from the SHA-3 family and is
    standardized by NIST. It produces a 512-bit message digest.

.. class:: SHAKE128(digest_size)

    .. versionadded:: 2.5

    SHAKE128 is an extendable output function (XOF) based on the same core
    permutations as SHA3. It allows the caller to obtain an arbitrarily long
    digest length. Longer lengths, however, do not increase security or
    collision resistance and lengths shorter than 128 bit (16 bytes) will
    decrease it.

    :param int digest_size: The length of output desired. Must be greater than
        zero.

    :raises ValueError: If the ``digest_size`` is invalid.

.. class:: SHAKE256(digest_size)

    .. versionadded:: 2.5

    SHAKE256 is an extendable output function (XOF) based on the same core
    permutations as SHA3. It allows the caller to obtain an arbitrarily long
    digest length. Longer lengths, however, do not increase security or
    collision resistance and lengths shorter than 256 bit (32 bytes) will
    decrease it.

    :param int digest_size: The length of output desired. Must be greater than
        zero.

    :raises ValueError: If the ``digest_size`` is invalid.

SHA-1
~~~~~

.. warning::

    SHA-1 is a deprecated hash algorithm that has practical known collision
    attacks. You are strongly discouraged from using it. Existing applications
    should strongly consider moving away.

.. class:: SHA1()

    SHA-1 is a cryptographic hash function standardized by NIST. It produces an
    160-bit message digest. Cryptanalysis of SHA-1 has demonstrated that it is
    vulnerable to practical collision attacks, and collisions have been
    demonstrated.

MD5
~~~

.. warning::

    MD5 is a deprecated hash algorithm that has practical known collision
    attacks. You are strongly discouraged from using it. Existing applications
    should strongly consider moving away.

.. class:: MD5()

    MD5 is a deprecated cryptographic hash function. It produces a 128-bit
    message digest and has practical known collision attacks.


Interfaces
~~~~~~~~~~

.. class:: HashAlgorithm

    .. attribute:: name

        :type: str

        The standard name for the hash algorithm, for example: ``"sha256"`` or
        ``"blake2b"``.

    .. attribute:: digest_size

        :type: int

        The size of the resulting digest in bytes.


.. class:: HashContext

    .. attribute:: algorithm

        A :class:`HashAlgorithm` that will be used by this context.

    .. method:: update(data)

        :param bytes data: The data you want to hash.

    .. method:: finalize()

        :return: The final digest as bytes.

    .. method:: copy()

        :return: A :class:`HashContext` that is a copy of the current context.


.. _`Lifetimes of cryptographic hash functions`: https://valerieaurora.org/hash.html
.. _`BLAKE2`: https://blake2.net
.. _`length-extension attacks`: https://en.wikipedia.org/wiki/Length_extension_attack
.. hazmat::

Primitives
==========

.. toctree::
    :maxdepth: 1

    aead
    asymmetric/index
    constant-time
    key-derivation-functions
    keywrap
    mac/index
    cryptographic-hashes
    symmetric-encryption
    padding
    twofactor
.. hazmat::

Key derivation functions
========================

.. module:: cryptography.hazmat.primitives.kdf

Key derivation functions derive bytes suitable for cryptographic operations
from passwords or other data sources using a pseudo-random function (PRF).
Different KDFs are suitable for different tasks such as:

* Cryptographic key derivation

    Deriving a key suitable for use as input to an encryption algorithm.
    Typically this means taking a password and running it through an algorithm
    such as :class:`~cryptography.hazmat.primitives.kdf.pbkdf2.PBKDF2HMAC` or
    :class:`~cryptography.hazmat.primitives.kdf.hkdf.HKDF`.
    This process is typically known as `key stretching`_.

* Password storage

    When storing passwords you want to use an algorithm that is computationally
    intensive. Legitimate users will only need to compute it once (for example,
    taking the user's password, running it through the KDF, then comparing it
    to the stored value), while attackers will need to do it billions of times.
    Ideal password storage KDFs will be demanding on both computational and
    memory resources.


Variable cost algorithms
~~~~~~~~~~~~~~~~~~~~~~~~


PBKDF2
------

.. currentmodule:: cryptography.hazmat.primitives.kdf.pbkdf2

.. class:: PBKDF2HMAC(algorithm, length, salt, iterations, backend=None)

    .. versionadded:: 0.2

    `PBKDF2`_ (Password Based Key Derivation Function 2) is typically used for
    deriving a cryptographic key from a password. It may also be used for
    key storage, but an alternate key storage KDF such as
    :class:`~cryptography.hazmat.primitives.kdf.scrypt.Scrypt` is generally
    considered a better solution.

    This class conforms to the
    :class:`~cryptography.hazmat.primitives.kdf.KeyDerivationFunction`
    interface.

    .. doctest::

        >>> import os
        >>> from cryptography.hazmat.primitives import hashes
        >>> from cryptography.hazmat.primitives.kdf.pbkdf2 import PBKDF2HMAC
        >>> # Salts should be randomly generated
        >>> salt = os.urandom(16)
        >>> # derive
        >>> kdf = PBKDF2HMAC(
        ...     algorithm=hashes.SHA256(),
        ...     length=32,
        ...     salt=salt,
        ...     iterations=100000,
        ... )
        >>> key = kdf.derive(b"my great password")
        >>> # verify
        >>> kdf = PBKDF2HMAC(
        ...     algorithm=hashes.SHA256(),
        ...     length=32,
        ...     salt=salt,
        ...     iterations=100000,
        ... )
        >>> kdf.verify(b"my great password", key)

    :param algorithm: An instance of
        :class:`~cryptography.hazmat.primitives.hashes.HashAlgorithm`.
    :param int length: The desired length of the derived key in bytes. Maximum
        is (2\ :sup:`32` - 1) * ``algorithm.digest_size``.
    :param bytes salt: A salt. Secure values [#nist]_ are 128-bits (16 bytes)
        or longer and randomly generated.
    :param int iterations: The number of iterations to perform of the hash
        function. This can be used to control the length of time the operation
        takes. Higher numbers help mitigate brute force attacks against derived
        keys. A `more detailed description`_ can be consulted for additional
        information.
    :param backend: An optional instance of
        :class:`~cryptography.hazmat.backends.interfaces.PBKDF2HMACBackend`.

    :raises cryptography.exceptions.UnsupportedAlgorithm: This is raised if the
        provided ``backend`` does not implement
        :class:`~cryptography.hazmat.backends.interfaces.PBKDF2HMACBackend`

    :raises TypeError: This exception is raised if ``salt`` is not ``bytes``.

    .. method:: derive(key_material)

        :param key_material: The input key material. For PBKDF2 this
            should be a password.
        :type key_material: :term:`bytes-like`
        :return bytes: the derived key.
        :raises cryptography.exceptions.AlreadyFinalized: This is raised when
                                                          :meth:`derive` or
                                                          :meth:`verify` is
                                                          called more than
                                                          once.

        :raises TypeError: This exception is raised if ``key_material`` is not
                           ``bytes``.

        This generates and returns a new key from the supplied password.

    .. method:: verify(key_material, expected_key)

        :param bytes key_material: The input key material. This is the same as
                                   ``key_material`` in :meth:`derive`.
        :param bytes expected_key: The expected result of deriving a new key,
                                   this is the same as the return value of
                                   :meth:`derive`.
        :raises cryptography.exceptions.InvalidKey: This is raised when the
                                                    derived key does not match
                                                    the expected key.
        :raises cryptography.exceptions.AlreadyFinalized: This is raised when
                                                          :meth:`derive` or
                                                          :meth:`verify` is
                                                          called more than
                                                          once.

        This checks whether deriving a new key from the supplied
        ``key_material`` generates the same key as the ``expected_key``, and
        raises an exception if they do not match. This can be used for
        checking whether the password a user provides matches the stored derived
        key.


Scrypt
------

.. currentmodule:: cryptography.hazmat.primitives.kdf.scrypt

.. class:: Scrypt(salt, length, n, r, p, backend=None)

    .. versionadded:: 1.6

    Scrypt is a KDF designed for password storage by Colin Percival to be
    resistant against hardware-assisted attackers by having a tunable memory
    cost. It is described in :rfc:`7914`.

    This class conforms to the
    :class:`~cryptography.hazmat.primitives.kdf.KeyDerivationFunction`
    interface.

    .. doctest::

        >>> import os
        >>> from cryptography.hazmat.primitives.kdf.scrypt import Scrypt
        >>> salt = os.urandom(16)
        >>> # derive
        >>> kdf = Scrypt(
        ...     salt=salt,
        ...     length=32,
        ...     n=2**14,
        ...     r=8,
        ...     p=1,
        ... )
        >>> key = kdf.derive(b"my great password")
        >>> # verify
        >>> kdf = Scrypt(
        ...     salt=salt,
        ...     length=32,
        ...     n=2**14,
        ...     r=8,
        ...     p=1,
        ... )
        >>> kdf.verify(b"my great password", key)

    :param bytes salt: A salt.
    :param int length: The desired length of the derived key in bytes.
    :param int n: CPU/Memory cost parameter. It must be larger than 1 and be a
        power of 2.
    :param int r: Block size parameter.
    :param int p: Parallelization parameter.
    :param backend: An optional instance of
        :class:`~cryptography.hazmat.backends.interfaces.ScryptBackend`.

    The computational and memory cost of Scrypt can be adjusted by manipulating
    the 3 parameters: ``n``, ``r``, and ``p``. In general, the memory cost of
    Scrypt is affected by the values of both ``n`` and ``r``, while ``n`` also
    determines the number of iterations performed. ``p`` increases the
    computational cost without affecting memory usage. A more in-depth
    explanation of the 3 parameters can be found `here`_.

    :rfc:`7914` `recommends`_ values of ``r=8`` and ``p=1`` while scaling ``n``
    to a number appropriate for your system. `The scrypt paper`_ suggests a
    minimum value of ``n=2**14`` for interactive logins (t < 100ms), or
    ``n=2**20`` for more sensitive files (t < 5s).

    :raises cryptography.exceptions.UnsupportedAlgorithm: This is raised if the
        provided ``backend`` does not implement
        :class:`~cryptography.hazmat.backends.interfaces.ScryptBackend`

    :raises TypeError: This exception is raised if ``salt`` is not ``bytes``.
    :raises ValueError: This exception is raised if ``n`` is less than 2, if
        ``n`` is not a power of 2, if ``r`` is less than 1 or if ``p`` is less
        than 1.

    .. method:: derive(key_material)

        :param key_material: The input key material.
        :type key_material: :term:`bytes-like`
        :return bytes: the derived key.
        :raises TypeError: This exception is raised if ``key_material`` is not
                           ``bytes``.
        :raises cryptography.exceptions.AlreadyFinalized: This is raised when
                                                          :meth:`derive` or
                                                          :meth:`verify` is
                                                          called more than
                                                          once.

        This generates and returns a new key from the supplied password.

    .. method:: verify(key_material, expected_key)

        :param bytes key_material: The input key material. This is the same as
                                   ``key_material`` in :meth:`derive`.
        :param bytes expected_key: The expected result of deriving a new key,
                                   this is the same as the return value of
                                   :meth:`derive`.
        :raises cryptography.exceptions.InvalidKey: This is raised when the
                                                    derived key does not match
                                                    the expected key.
        :raises cryptography.exceptions.AlreadyFinalized: This is raised when
                                                          :meth:`derive` or
                                                          :meth:`verify` is
                                                          called more than
                                                          once.

        This checks whether deriving a new key from the supplied
        ``key_material`` generates the same key as the ``expected_key``, and
        raises an exception if they do not match. This can be used for
        checking whether the password a user provides matches the stored derived
        key.

Fixed cost algorithms
~~~~~~~~~~~~~~~~~~~~~


ConcatKDF
---------

.. currentmodule:: cryptography.hazmat.primitives.kdf.concatkdf

.. class:: ConcatKDFHash(algorithm, length, otherinfo, backend=None)

    .. versionadded:: 1.0

    ConcatKDFHash (Concatenation Key Derivation Function) is defined by the
    NIST Special Publication `NIST SP 800-56Ar2`_ document, to be used to
    derive keys for use after a Key Exchange negotiation operation.

    .. warning::

        ConcatKDFHash should not be used for password storage.

    .. doctest::

        >>> import os
        >>> from cryptography.hazmat.primitives import hashes
        >>> from cryptography.hazmat.primitives.kdf.concatkdf import ConcatKDFHash
        >>> otherinfo = b"concatkdf-example"
        >>> ckdf = ConcatKDFHash(
        ...     algorithm=hashes.SHA256(),
        ...     length=32,
        ...     otherinfo=otherinfo,
        ... )
        >>> key = ckdf.derive(b"input key")
        >>> ckdf = ConcatKDFHash(
        ...     algorithm=hashes.SHA256(),
        ...     length=32,
        ...     otherinfo=otherinfo,
        ... )
        >>> ckdf.verify(b"input key", key)

    :param algorithm: An instance of
        :class:`~cryptography.hazmat.primitives.hashes.HashAlgorithm`.

    :param int length: The desired length of the derived key in bytes.
        Maximum is ``hashlen * (2^32 -1)``.

    :param bytes otherinfo: Application specific context information.
        If ``None`` is explicitly passed an empty byte string will be used.

    :param backend: An optional instance of
        :class:`~cryptography.hazmat.backends.interfaces.HashBackend`.

    :raises cryptography.exceptions.UnsupportedAlgorithm: This is raised
        if the provided ``backend`` does not implement
        :class:`~cryptography.hazmat.backends.interfaces.HashBackend`

    :raises TypeError: This exception is raised if ``otherinfo`` is not
        ``bytes``.

    .. method:: derive(key_material)

        :param key_material: The input key material.
        :type key_material: :term:`bytes-like`
        :return bytes: The derived key.
        :raises TypeError: This exception is raised if ``key_material`` is
                            not ``bytes``.
        :raises cryptography.exceptions.AlreadyFinalized: This is raised when
                                                          :meth:`derive` or
                                                          :meth:`verify` is
                                                          called more than
                                                          once.

        Derives a new key from the input key material.

    .. method:: verify(key_material, expected_key)

        :param bytes key_material: The input key material. This is the same as
                                   ``key_material`` in :meth:`derive`.
        :param bytes expected_key: The expected result of deriving a new key,
                                   this is the same as the return value of
                                   :meth:`derive`.
        :raises cryptography.exceptions.InvalidKey: This is raised when the
                                                    derived key does not match
                                                    the expected key.
        :raises cryptography.exceptions.AlreadyFinalized: This is raised when
                                                          :meth:`derive` or
                                                          :meth:`verify` is
                                                          called more than
                                                          once.

        This checks whether deriving a new key from the supplied
        ``key_material`` generates the same key as the ``expected_key``, and
        raises an exception if they do not match.


.. class:: ConcatKDFHMAC(algorithm, length, salt, otherinfo, backend=None)

    .. versionadded:: 1.0

    Similar to ConcatKFDHash but uses an HMAC function instead.

    .. warning::

        ConcatKDFHMAC should not be used for password storage.

    .. doctest::

        >>> import os
        >>> from cryptography.hazmat.primitives import hashes
        >>> from cryptography.hazmat.primitives.kdf.concatkdf import ConcatKDFHMAC
        >>> salt = os.urandom(16)
        >>> otherinfo = b"concatkdf-example"
        >>> ckdf = ConcatKDFHMAC(
        ...     algorithm=hashes.SHA256(),
        ...     length=32,
        ...     salt=salt,
        ...     otherinfo=otherinfo,
        ... )
        >>> key = ckdf.derive(b"input key")
        >>> ckdf = ConcatKDFHMAC(
        ...     algorithm=hashes.SHA256(),
        ...     length=32,
        ...     salt=salt,
        ...     otherinfo=otherinfo,
        ... )
        >>> ckdf.verify(b"input key", key)

    :param algorithm: An instance of
        :class:`~cryptography.hazmat.primitives.hashes.HashAlgorithm`.

    :param int length: The desired length of the derived key in bytes. Maximum
        is ``hashlen * (2^32 -1)``.

    :param bytes salt: A salt. Randomizes the KDF's output. Optional, but
        highly recommended. Ideally as many bits of entropy as the security
        level of the hash: often that means cryptographically random and as
        long as the hash output. Does not have to be secret, but may cause
        stronger security guarantees if secret; If ``None`` is explicitly
        passed a default salt of ``algorithm.block_size`` null bytes will be
        used.

    :param bytes otherinfo: Application specific context information.
        If ``None`` is explicitly passed an empty byte string will be used.

    :param backend: An optional instance of
        :class:`~cryptography.hazmat.backends.interfaces.HMACBackend`.

    :raises cryptography.exceptions.UnsupportedAlgorithm: This is raised if the
        provided ``backend`` does not implement
        :class:`~cryptography.hazmat.backends.interfaces.HMACBackend`

    :raises TypeError: This exception is raised if ``salt`` or ``otherinfo``
        is not ``bytes``.

    .. method:: derive(key_material)

        :param bytes key_material: The input key material.
        :return bytes: The derived key.
        :raises TypeError: This exception is raised if ``key_material`` is not
                           ``bytes``.
        :raises cryptography.exceptions.AlreadyFinalized: This is raised when
                                                          :meth:`derive` or
                                                          :meth:`verify` is
                                                          called more than
                                                          once.

        Derives a new key from the input key material.

    .. method:: verify(key_material, expected_key)

        :param bytes key_material: The input key material. This is the same as
                                   ``key_material`` in :meth:`derive`.
        :param bytes expected_key: The expected result of deriving a new key,
                                   this is the same as the return value of
                                   :meth:`derive`.
        :raises cryptography.exceptions.InvalidKey: This is raised when the
                                                    derived key does not match
                                                    the expected key.
        :raises cryptography.exceptions.AlreadyFinalized: This is raised when
                                                          :meth:`derive` or
                                                          :meth:`verify` is
                                                          called more than
                                                          once.

        This checks whether deriving a new key from the supplied
        ``key_material`` generates the same key as the ``expected_key``, and
        raises an exception if they do not match.


HKDF
----

.. currentmodule:: cryptography.hazmat.primitives.kdf.hkdf

.. class:: HKDF(algorithm, length, salt, info, backend=None)

    .. versionadded:: 0.2

    `HKDF`_ (HMAC-based Extract-and-Expand Key Derivation Function) is suitable
    for deriving keys of a fixed size used for other cryptographic operations.

    .. warning::

        HKDF should not be used for password storage.

    .. doctest::

        >>> import os
        >>> from cryptography.hazmat.primitives import hashes
        >>> from cryptography.hazmat.primitives.kdf.hkdf import HKDF
        >>> salt = os.urandom(16)
        >>> info = b"hkdf-example"
        >>> hkdf = HKDF(
        ...     algorithm=hashes.SHA256(),
        ...     length=32,
        ...     salt=salt,
        ...     info=info,
        ... )
        >>> key = hkdf.derive(b"input key")
        >>> hkdf = HKDF(
        ...     algorithm=hashes.SHA256(),
        ...     length=32,
        ...     salt=salt,
        ...     info=info,
        ... )
        >>> hkdf.verify(b"input key", key)

    :param algorithm: An instance of
        :class:`~cryptography.hazmat.primitives.hashes.HashAlgorithm`.

    :param int length: The desired length of the derived key in bytes. Maximum
        is ``255 * (algorithm.digest_size // 8)``.

    :param bytes salt: A salt. Randomizes the KDF's output. Optional, but
        highly recommended. Ideally as many bits of entropy as the security
        level of the hash: often that means cryptographically random and as
        long as the hash output. Worse (shorter, less entropy) salt values can
        still meaningfully contribute to security. May be reused. Does not have
        to be secret, but may cause stronger security guarantees if secret; see
        :rfc:`5869` and the `HKDF paper`_ for more details. If ``None`` is
        explicitly passed a default salt of ``algorithm.digest_size // 8`` null
        bytes will be used.

    :param bytes info: Application specific context information.  If ``None``
        is explicitly passed an empty byte string will be used.

    :param backend: An optional instance of
        :class:`~cryptography.hazmat.backends.interfaces.HMACBackend`.

    :raises cryptography.exceptions.UnsupportedAlgorithm: This is raised if the
        provided ``backend`` does not implement
        :class:`~cryptography.hazmat.backends.interfaces.HMACBackend`

    :raises TypeError: This exception is raised if ``salt`` or ``info`` is not
                       ``bytes``.

    .. method:: derive(key_material)

        :param key_material: The input key material.
        :type key_material: :term:`bytes-like`
        :return bytes: The derived key.
        :raises TypeError: This exception is raised if ``key_material`` is not
                           ``bytes``.
        :raises cryptography.exceptions.AlreadyFinalized: This is raised when
                                                          :meth:`derive` or
                                                          :meth:`verify` is
                                                          called more than
                                                          once.

        Derives a new key from the input key material by performing both the
        extract and expand operations.

    .. method:: verify(key_material, expected_key)

        :param bytes key_material: The input key material. This is the same as
                                   ``key_material`` in :meth:`derive`.
        :param bytes expected_key: The expected result of deriving a new key,
                                   this is the same as the return value of
                                   :meth:`derive`.
        :raises cryptography.exceptions.InvalidKey: This is raised when the
                                                    derived key does not match
                                                    the expected key.
        :raises cryptography.exceptions.AlreadyFinalized: This is raised when
                                                          :meth:`derive` or
                                                          :meth:`verify` is
                                                          called more than
                                                          once.

        This checks whether deriving a new key from the supplied
        ``key_material`` generates the same key as the ``expected_key``, and
        raises an exception if they do not match.


.. class:: HKDFExpand(algorithm, length, info, backend=None)

    .. versionadded:: 0.5

    HKDF consists of two stages, extract and expand. This class exposes an
    expand only version of HKDF that is suitable when the key material is
    already cryptographically strong.

    .. warning::

        HKDFExpand should only be used if the key material is
        cryptographically strong. You should use
        :class:`~cryptography.hazmat.primitives.kdf.hkdf.HKDF` if
        you are unsure.

    .. doctest::

        >>> import os
        >>> from cryptography.hazmat.primitives import hashes
        >>> from cryptography.hazmat.primitives.kdf.hkdf import HKDFExpand
        >>> info = b"hkdf-example"
        >>> key_material = os.urandom(16)
        >>> hkdf = HKDFExpand(
        ...     algorithm=hashes.SHA256(),
        ...     length=32,
        ...     info=info,
        ... )
        >>> key = hkdf.derive(key_material)
        >>> hkdf = HKDFExpand(
        ...     algorithm=hashes.SHA256(),
        ...     length=32,
        ...     info=info,
        ... )
        >>> hkdf.verify(key_material, key)

    :param algorithm: An instance of
        :class:`~cryptography.hazmat.primitives.hashes.HashAlgorithm`.

    :param int length: The desired length of the derived key in bytes. Maximum
        is ``255 * (algorithm.digest_size // 8)``.

    :param bytes info: Application specific context information.  If ``None``
        is explicitly passed an empty byte string will be used.

    :param backend: An optional instance of
        :class:`~cryptography.hazmat.backends.interfaces.HMACBackend`.

    :raises cryptography.exceptions.UnsupportedAlgorithm: This is raised if the
        provided ``backend`` does not implement
        :class:`~cryptography.hazmat.backends.interfaces.HMACBackend`
    :raises TypeError: This exception is raised if ``info`` is not ``bytes``.

    .. method:: derive(key_material)

        :param bytes key_material: The input key material.
        :return bytes: The derived key.

        :raises TypeError: This exception is raised if ``key_material`` is not
                           ``bytes``.
        :raises cryptography.exceptions.AlreadyFinalized: This is raised when
                                                          :meth:`derive` or
                                                          :meth:`verify` is
                                                          called more than
                                                          once.

        Derives a new key from the input key material by performing both the
        extract and expand operations.

    .. method:: verify(key_material, expected_key)

        :param bytes key_material: The input key material. This is the same as
                                   ``key_material`` in :meth:`derive`.
        :param bytes expected_key: The expected result of deriving a new key,
                                   this is the same as the return value of
                                   :meth:`derive`.
        :raises cryptography.exceptions.InvalidKey: This is raised when the
                                                    derived key does not match
                                                    the expected key.
        :raises cryptography.exceptions.AlreadyFinalized: This is raised when
                                                          :meth:`derive` or
                                                          :meth:`verify` is
                                                          called more than
                                                          once.
        :raises TypeError: This is raised if the provided ``key_material`` is
            a ``unicode`` object

        This checks whether deriving a new key from the supplied
        ``key_material`` generates the same key as the ``expected_key``, and
        raises an exception if they do not match.


KBKDF
-----

.. currentmodule:: cryptography.hazmat.primitives.kdf.kbkdf

.. class:: KBKDFHMAC(algorithm, mode, length, rlen, llen, location,\
           label, context, fixed, backend=None)

    .. versionadded:: 1.4

    KBKDF (Key Based Key Derivation Function) is defined by the
    `NIST SP 800-108`_ document, to be used to derive additional
    keys from a key that has been established through an automated
    key-establishment scheme.

    .. warning::

        KBKDFHMAC should not be used for password storage.

    .. doctest::

        >>> import os
        >>> from cryptography.hazmat.primitives import hashes
        >>> from cryptography.hazmat.primitives.kdf.kbkdf import (
        ...    CounterLocation, KBKDFHMAC, Mode
        ... )
        >>> label = b"KBKDF HMAC Label"
        >>> context = b"KBKDF HMAC Context"
        >>> kdf = KBKDFHMAC(
        ...     algorithm=hashes.SHA256(),
        ...     mode=Mode.CounterMode,
        ...     length=32,
        ...     rlen=4,
        ...     llen=4,
        ...     location=CounterLocation.BeforeFixed,
        ...     label=label,
        ...     context=context,
        ...     fixed=None,
        ... )
        >>> key = kdf.derive(b"input key")
        >>> kdf = KBKDFHMAC(
        ...     algorithm=hashes.SHA256(),
        ...     mode=Mode.CounterMode,
        ...     length=32,
        ...     rlen=4,
        ...     llen=4,
        ...     location=CounterLocation.BeforeFixed,
        ...     label=label,
        ...     context=context,
        ...     fixed=None,
        ... )
        >>> kdf.verify(b"input key", key)

    :param algorithm: An instance of
        :class:`~cryptography.hazmat.primitives.hashes.HashAlgorithm`.

    :param mode: The desired mode of the PRF. A value from the
      :class:`~cryptography.hazmat.primitives.kdf.kbkdf.Mode` enum.

    :param int length: The desired length of the derived key in bytes.

    :param int rlen: An integer that indicates the length of the binary
        representation of the counter in bytes.

    :param int llen: An integer that indicates the binary
        representation of the ``length`` in bytes.

    :param location: The desired location of the counter. A value from the
      :class:`~cryptography.hazmat.primitives.kdf.kbkdf.CounterLocation` enum.

    :param bytes label: Application specific label information. If ``None``
        is explicitly passed an empty byte string will be used.

    :param bytes context: Application specific context information. If ``None``
        is explicitly passed an empty byte string will be used.

    :param bytes fixed: Instead of specifying ``label`` and ``context`` you
        may supply your own fixed data. If ``fixed`` is specified, ``label``
        and ``context`` is ignored.

    :param backend: An optional instance of
        :class:`~cryptography.hazmat.backends.interfaces.HMACBackend`.

    :raises cryptography.exceptions.UnsupportedAlgorithm: This is raised
        if the provided ``backend`` does not implement
        :class:`~cryptography.hazmat.backends.interfaces.HMACBackend`

    :raises TypeError: This exception is raised if ``label`` or ``context``
        is not ``bytes``. Also raised if ``rlen`` or ``llen`` is not ``int``.

    :raises ValueError: This exception is raised if ``rlen`` or ``llen``
        is greater than 4 or less than 1. This exception is also raised if
        you specify a ``label`` or ``context`` and ``fixed``.

    .. method:: derive(key_material)

        :param key_material: The input key material.
        :type key_material: :term:`bytes-like`
        :return bytes: The derived key.
        :raises TypeError: This exception is raised if ``key_material`` is
                            not ``bytes``.
        :raises cryptography.exceptions.AlreadyFinalized: This is raised when
                                                          :meth:`derive` or
                                                          :meth:`verify` is
                                                          called more than
                                                          once.

        Derives a new key from the input key material.

    .. method:: verify(key_material, expected_key)

        :param bytes key_material: The input key material. This is the same as
                                   ``key_material`` in :meth:`derive`.
        :param bytes expected_key: The expected result of deriving a new key,
                                   this is the same as the return value of
                                   :meth:`derive`.
        :raises cryptography.exceptions.InvalidKey: This is raised when the
                                                    derived key does not match
                                                    the expected key.
        :raises cryptography.exceptions.AlreadyFinalized: This is raised when
                                                          :meth:`derive` or
                                                          :meth:`verify` is
                                                          called more than
                                                          once.

        This checks whether deriving a new key from the supplied
        ``key_material`` generates the same key as the ``expected_key``, and
        raises an exception if they do not match.

.. class:: Mode

    An enumeration for the key based key derivative modes.

    .. attribute:: CounterMode

        The output of the PRF is computed with a counter
        as the iteration variable.

.. class:: CounterLocation

    An enumeration for the key based key derivative counter location.

    .. attribute:: BeforeFixed

        The counter iteration variable will be concatenated before
        the fixed input data.

    .. attribute:: AfterFixed

        The counter iteration variable will be concatenated after
        the fixed input data.


X963KDF
-------

.. currentmodule:: cryptography.hazmat.primitives.kdf.x963kdf

.. class:: X963KDF(algorithm, length, otherinfo, backend=None)

    .. versionadded:: 1.1

    X963KDF (ANSI X9.63 Key Derivation Function) is defined by ANSI
    in the `ANSI X9.63:2001`_ document, to be used to derive keys for use
    after a Key Exchange negotiation operation.

    SECG in `SEC 1 v2.0`_ recommends that
    :class:`~cryptography.hazmat.primitives.kdf.concatkdf.ConcatKDFHash` be
    used for new projects. This KDF should only be used for backwards
    compatibility with pre-existing protocols.


    .. warning::

        X963KDF should not be used for password storage.

    .. doctest::

        >>> import os
        >>> from cryptography.hazmat.primitives import hashes
        >>> from cryptography.hazmat.primitives.kdf.x963kdf import X963KDF
        >>> sharedinfo = b"ANSI X9.63 Example"
        >>> xkdf = X963KDF(
        ...     algorithm=hashes.SHA256(),
        ...     length=32,
        ...     sharedinfo=sharedinfo,
        ... )
        >>> key = xkdf.derive(b"input key")
        >>> xkdf = X963KDF(
        ...     algorithm=hashes.SHA256(),
        ...     length=32,
        ...     sharedinfo=sharedinfo,
        ... )
        >>> xkdf.verify(b"input key", key)

    :param algorithm: An instance of
        :class:`~cryptography.hazmat.primitives.hashes.HashAlgorithm`.

    :param int length: The desired length of the derived key in bytes.
        Maximum is ``hashlen * (2^32 -1)``.

    :param bytes sharedinfo: Application specific context information.
        If ``None`` is explicitly passed an empty byte string will be used.

    :param backend: An optional instance of
        :class:`~cryptography.hazmat.backends.interfaces.HashBackend`.

    :raises cryptography.exceptions.UnsupportedAlgorithm: This is raised
        if the provided ``backend`` does not implement
        :class:`~cryptography.hazmat.backends.interfaces.HashBackend`

    :raises TypeError: This exception is raised if ``sharedinfo`` is not
        ``bytes``.

    .. method:: derive(key_material)

        :param key_material: The input key material.
        :type key_material: :term:`bytes-like`
        :return bytes: The derived key.
        :raises TypeError: This exception is raised if ``key_material`` is
                            not ``bytes``.
        :raises cryptography.exceptions.AlreadyFinalized: This is raised when
                                                          :meth:`derive` or
                                                          :meth:`verify` is
                                                          called more than
                                                          once.

        Derives a new key from the input key material.

    .. method:: verify(key_material, expected_key)

        :param bytes key_material: The input key material. This is the same as
                                   ``key_material`` in :meth:`derive`.
        :param bytes expected_key: The expected result of deriving a new key,
                                   this is the same as the return value of
                                   :meth:`derive`.
        :raises cryptography.exceptions.InvalidKey: This is raised when the
                                                    derived key does not match
                                                    the expected key.
        :raises cryptography.exceptions.AlreadyFinalized: This is raised when
                                                          :meth:`derive` or
                                                          :meth:`verify` is
                                                          called more than
                                                          once.

        This checks whether deriving a new key from the supplied
        ``key_material`` generates the same key as the ``expected_key``, and
        raises an exception if they do not match.


Interface
~~~~~~~~~

.. currentmodule:: cryptography.hazmat.primitives.kdf

.. class:: KeyDerivationFunction

    .. versionadded:: 0.2

    .. method:: derive(key_material)

        :param bytes key_material: The input key material. Depending on what
                                   key derivation function you are using this
                                   could be either random bytes, or a user
                                   supplied password.
        :return: The new key.
        :raises cryptography.exceptions.AlreadyFinalized: This is raised when
                                                          :meth:`derive` or
                                                          :meth:`verify` is
                                                          called more than
                                                          once.

        This generates and returns a new key from the supplied key material.

    .. method:: verify(key_material, expected_key)

        :param bytes key_material: The input key material. This is the same as
                                   ``key_material`` in :meth:`derive`.
        :param bytes expected_key: The expected result of deriving a new key,
                                   this is the same as the return value of
                                   :meth:`derive`.
        :raises cryptography.exceptions.InvalidKey: This is raised when the
                                                    derived key does not match
                                                    the expected key.
        :raises cryptography.exceptions.AlreadyFinalized: This is raised when
                                                          :meth:`derive` or
                                                          :meth:`verify` is
                                                          called more than
                                                          once.

        This checks whether deriving a new key from the supplied
        ``key_material`` generates the same key as the ``expected_key``, and
        raises an exception if they do not match. This can be used for
        something like checking whether a user's password attempt matches the
        stored derived key.


.. [#nist] See `NIST SP 800-132`_.

.. _`NIST SP 800-132`: https://csrc.nist.gov/publications/detail/sp/800-132/final
.. _`NIST SP 800-108`: https://csrc.nist.gov/publications/detail/sp/800-108/final
.. _`NIST SP 800-56Ar2`: https://csrc.nist.gov/publications/detail/sp/800-56a/rev-2/final
.. _`ANSI X9.63:2001`: https://webstore.ansi.org
.. _`SEC 1 v2.0`: https://www.secg.org/sec1-v2.pdf
.. _`more detailed description`: https://security.stackexchange.com/a/3993/43116
.. _`PBKDF2`: https://en.wikipedia.org/wiki/PBKDF2
.. _`key stretching`: https://en.wikipedia.org/wiki/Key_stretching
.. _`HKDF`: https://en.wikipedia.org/wiki/HKDF
.. _`HKDF paper`: https://eprint.iacr.org/2010/264
.. _`here`: https://stackoverflow.com/a/30308723/1170681
.. _`recommends`: https://tools.ietf.org/html/rfc7914#section-2
.. _`The scrypt paper`: https://www.tarsnap.com/scrypt/scrypt.pdf
.. hazmat::

.. module:: cryptography.hazmat.primitives.keywrap

Key wrapping
============

Key wrapping is a cryptographic construct that uses symmetric encryption to
encapsulate key material. Key wrapping algorithms are occasionally utilized
to protect keys at rest or transmit them over insecure networks. Many of the
protections offered by key wrapping are also offered by using authenticated
:doc:`symmetric encryption </hazmat/primitives/symmetric-encryption>`.

.. function:: aes_key_wrap(wrapping_key, key_to_wrap, backend=None)

    .. versionadded:: 1.1

    This function performs AES key wrap (without padding) as specified in
    :rfc:`3394`.

    :param bytes wrapping_key: The wrapping key.

    :param bytes key_to_wrap: The key to wrap.

    :param backend: An optional
        :class:`~cryptography.hazmat.backends.interfaces.CipherBackend`
        instance that supports
        :class:`~cryptography.hazmat.primitives.ciphers.algorithms.AES`.

    :return bytes: The wrapped key as bytes.

.. function:: aes_key_unwrap(wrapping_key, wrapped_key, backend=None)

    .. versionadded:: 1.1

    This function performs AES key unwrap (without padding) as specified in
    :rfc:`3394`.

    :param bytes wrapping_key: The wrapping key.

    :param bytes wrapped_key: The wrapped key.

    :param backend: An optional
        :class:`~cryptography.hazmat.backends.interfaces.CipherBackend`
        instance that supports
        :class:`~cryptography.hazmat.primitives.ciphers.algorithms.AES`.

    :return bytes: The unwrapped key as bytes.

    :raises cryptography.hazmat.primitives.keywrap.InvalidUnwrap: This is
        raised if the key is not successfully unwrapped.

.. function:: aes_key_wrap_with_padding(wrapping_key, key_to_wrap, backend=None)

    .. versionadded:: 2.2

    This function performs AES key wrap with padding as specified in
    :rfc:`5649`.

    :param bytes wrapping_key: The wrapping key.

    :param bytes key_to_wrap: The key to wrap.

    :param backend: An optional
        :class:`~cryptography.hazmat.backends.interfaces.CipherBackend`
        instance that supports
        :class:`~cryptography.hazmat.primitives.ciphers.algorithms.AES`.

    :return bytes: The wrapped key as bytes.

.. function:: aes_key_unwrap_with_padding(wrapping_key, wrapped_key, backend=None)

    .. versionadded:: 2.2

    This function performs AES key unwrap with padding as specified in
    :rfc:`5649`.

    :param bytes wrapping_key: The wrapping key.

    :param bytes wrapped_key: The wrapped key.

    :param backend: An optional
        :class:`~cryptography.hazmat.backends.interfaces.CipherBackend`
        instance that supports
        :class:`~cryptography.hazmat.primitives.ciphers.algorithms.AES`.

    :return bytes: The unwrapped key as bytes.

    :raises cryptography.hazmat.primitives.keywrap.InvalidUnwrap: This is
        raised if the key is not successfully unwrapped.

Exceptions
~~~~~~~~~~

.. class:: InvalidUnwrap

    This is raised when a wrapped key fails to unwrap. It can be caused by a
    corrupted or invalid wrapped key or an invalid wrapping key.
.. hazmat::

Cipher-based message authentication code (CMAC)
===============================================

.. currentmodule:: cryptography.hazmat.primitives.cmac

.. testsetup::

    import binascii
    key = binascii.unhexlify(b"0" * 32)

`Cipher-based message authentication codes`_ (or CMACs) are a tool for
calculating message authentication codes using a block cipher coupled with a
secret key. You can use an CMAC to verify both the integrity and authenticity
of a message.

A subset of CMAC with the AES-128 algorithm is described in :rfc:`4493`.

.. class:: CMAC(algorithm, backend=None)

    .. versionadded:: 0.4

    CMAC objects take a
    :class:`~cryptography.hazmat.primitives.ciphers.BlockCipherAlgorithm` instance.

    .. doctest::

        >>> from cryptography.hazmat.primitives import cmac
        >>> from cryptography.hazmat.primitives.ciphers import algorithms
        >>> c = cmac.CMAC(algorithms.AES(key))
        >>> c.update(b"message to authenticate")
        >>> c.finalize()
        b'CT\x1d\xc8\x0e\x15\xbe4e\xdb\xb6\x84\xca\xd9Xk'

    If the backend doesn't support the requested ``algorithm`` an
    :class:`~cryptography.exceptions.UnsupportedAlgorithm` exception will be
    raised.

    If ``algorithm`` isn't a
    :class:`~cryptography.hazmat.primitives.ciphers.BlockCipherAlgorithm`
    instance then ``TypeError`` will be raised.

    To check that a given signature is correct use the :meth:`verify` method.
    You will receive an exception if the signature is wrong:

    .. doctest::

        >>> c = cmac.CMAC(algorithms.AES(key))
        >>> c.update(b"message to authenticate")
        >>> c.verify(b"an incorrect signature")
        Traceback (most recent call last):
        ...
        cryptography.exceptions.InvalidSignature: Signature did not match digest.

    :param algorithm: An instance of
        :class:`~cryptography.hazmat.primitives.ciphers.BlockCipherAlgorithm`.
    :param backend: An optional instance of
        :class:`~cryptography.hazmat.backends.interfaces.CMACBackend`.
    :raises TypeError: This is raised if the provided ``algorithm`` is not an instance of
        :class:`~cryptography.hazmat.primitives.ciphers.BlockCipherAlgorithm`
    :raises cryptography.exceptions.UnsupportedAlgorithm: This is raised if the
        provided ``backend`` does not implement
        :class:`~cryptography.hazmat.backends.interfaces.CMACBackend`

    .. method:: update(data)

        :param bytes data: The bytes to hash and authenticate.
        :raises cryptography.exceptions.AlreadyFinalized: See :meth:`finalize`
        :raises TypeError: This exception is raised if ``data`` is not ``bytes``.

    .. method:: copy()

        Copy this :class:`CMAC` instance, usually so that we may call
        :meth:`finalize` to get an intermediate value while we continue
        to call :meth:`update` on the original instance.

        :return: A new instance of :class:`CMAC` that can be updated
            and finalized independently of the original instance.
        :raises cryptography.exceptions.AlreadyFinalized: See :meth:`finalize`

    .. method:: verify(signature)

        Finalize the current context and securely compare the MAC to
        ``signature``.

        :param bytes signature: The bytes to compare the current CMAC
                against.
        :raises cryptography.exceptions.AlreadyFinalized: See :meth:`finalize`
        :raises cryptography.exceptions.InvalidSignature: If signature does not
                                                                  match digest
        :raises TypeError: This exception is raised if ``signature`` is not
                           ``bytes``.

        .. method:: finalize()

        Finalize the current context and return the message authentication code
        as bytes.

        After ``finalize`` has been called this object can no longer be used
        and :meth:`update`, :meth:`copy`, :meth:`verify` and :meth:`finalize`
        will raise an :class:`~cryptography.exceptions.AlreadyFinalized`
        exception.

        :return bytes: The message authentication code as bytes.
        :raises cryptography.exceptions.AlreadyFinalized:


.. _`Cipher-based message authentication codes`: https://en.wikipedia.org/wiki/CMAC
.. hazmat::

Hash-based message authentication codes (HMAC)
==============================================

.. currentmodule:: cryptography.hazmat.primitives.hmac

.. testsetup::

    import binascii
    key = binascii.unhexlify(b"0" * 32)

Hash-based message authentication codes (or HMACs) are a tool for calculating
message authentication codes using a cryptographic hash function coupled with a
secret key. You can use an HMAC to verify both the integrity and authenticity
of a message.

.. class:: HMAC(key, algorithm, backend=None)

    HMAC objects take a ``key`` and a
    :class:`~cryptography.hazmat.primitives.hashes.HashAlgorithm` instance.
    The ``key`` should be :doc:`randomly generated bytes </random-numbers>` and
    is recommended to be equal in length to the ``digest_size`` of the hash
    function chosen. You must keep the ``key`` secret.

    This is an implementation of :rfc:`2104`.

    .. doctest::

        >>> from cryptography.hazmat.primitives import hashes, hmac
        >>> h = hmac.HMAC(key, hashes.SHA256())
        >>> h.update(b"message to hash")
        >>> h.finalize()
        b'#F\xdaI\x8b"e\xc4\xf1\xbb\x9a\x8fc\xff\xf5\xdex.\xbc\xcd/+\x8a\x86\x1d\x84\'\xc3\xa6\x1d\xd8J'

    If the backend doesn't support the requested ``algorithm`` an
    :class:`~cryptography.exceptions.UnsupportedAlgorithm` exception will be
    raised.

    If ``algorithm`` isn't a
    :class:`~cryptography.hazmat.primitives.hashes.HashAlgorithm` instance
    then ``TypeError`` will be raised.

    To check that a given signature is correct use the :meth:`verify` method.
    You will receive an exception if the signature is wrong:

    .. doctest::

        >>> h = hmac.HMAC(key, hashes.SHA256())
        >>> h.update(b"message to hash")
        >>> h.verify(b"an incorrect signature")
        Traceback (most recent call last):
        ...
        cryptography.exceptions.InvalidSignature: Signature did not match digest.

    :param key: Secret key as ``bytes``.
    :type key: :term:`bytes-like`
    :param algorithm: An
        :class:`~cryptography.hazmat.primitives.hashes.HashAlgorithm`
        instance such as those described in
        :ref:`Cryptographic Hashes <cryptographic-hash-algorithms>`.
    :param backend: An optional
        :class:`~cryptography.hazmat.backends.interfaces.HMACBackend`
        instance.

    :raises cryptography.exceptions.UnsupportedAlgorithm: This is raised if the
        provided ``backend`` does not implement
        :class:`~cryptography.hazmat.backends.interfaces.HMACBackend`

    .. method:: update(msg)

        :param msg: The bytes to hash and authenticate.
        :type msg: :term:`bytes-like`
        :raises cryptography.exceptions.AlreadyFinalized: See :meth:`finalize`
        :raises TypeError: This exception is raised if ``msg`` is not ``bytes``.

    .. method:: copy()

        Copy this :class:`HMAC` instance, usually so that we may call
        :meth:`finalize` to get an intermediate digest value while we continue
        to call :meth:`update` on the original instance.

        :return: A new instance of :class:`HMAC` that can be updated
            and finalized independently of the original instance.
        :raises cryptography.exceptions.AlreadyFinalized: See :meth:`finalize`

    .. method:: verify(signature)

        Finalize the current context and securely compare digest to
        ``signature``.

        :param bytes signature: The bytes to compare the current digest
                                against.
        :raises cryptography.exceptions.AlreadyFinalized: See :meth:`finalize`
        :raises cryptography.exceptions.InvalidSignature: If signature does not
                                                          match digest
        :raises TypeError: This exception is raised if ``signature`` is not
                           ``bytes``.

    .. method:: finalize()

        Finalize the current context and return the message digest as bytes.

        After ``finalize`` has been called this object can no longer be used
        and :meth:`update`, :meth:`copy`, :meth:`verify` and :meth:`finalize`
        will raise an :class:`~cryptography.exceptions.AlreadyFinalized`
        exception.

        :return bytes: The message digest as bytes.
        :raises cryptography.exceptions.AlreadyFinalized:
.. hazmat::

Message authentication codes
============================

While cryptography supports multiple MAC algorithms, we strongly
recommend that HMAC should be used unless you have a very specific need.

For more information on why HMAC is preferred, see `Use cases for CMAC vs.
HMAC?`_

.. toctree::
    :maxdepth: 1

    cmac
    hmac
    poly1305

.. _`Use cases for CMAC vs. HMAC?`: https://crypto.stackexchange.com/questions/15721/use-cases-for-cmac-vs-hmac
.. hazmat::

Poly1305
========

.. currentmodule:: cryptography.hazmat.primitives.poly1305

.. testsetup::

    key = b"\x01" * 32

Poly1305 is an authenticator that takes a 32-byte key and a message and
produces a 16-byte tag. This tag is used to authenticate the message. Each key
**must** only be used once. Using the same key to generate tags for multiple
messages allows an attacker to forge tags. Poly1305 is described in
:rfc:`7539`.

.. class:: Poly1305(key)

    .. versionadded:: 2.7

    .. warning::

        Using the same key to generate tags for multiple messages allows an
        attacker to forge tags. Always generate a new key per message you want
        to authenticate. If you are using this as a MAC for
        symmetric encryption please use
        :class:`~cryptography.hazmat.primitives.ciphers.aead.ChaCha20Poly1305`
        instead.

    .. doctest::

        >>> from cryptography.hazmat.primitives import poly1305
        >>> p = poly1305.Poly1305(key)
        >>> p.update(b"message to authenticate")
        >>> p.finalize()
        b'T\xae\xff3\xbdW\xef\xd5r\x01\xe2n=\xb7\xd2h'

    To check that a given tag is correct use the :meth:`verify` method.
    You will receive an exception if the tag is wrong:

    .. doctest::

        >>> p = poly1305.Poly1305(key)
        >>> p.update(b"message to authenticate")
        >>> p.verify(b"an incorrect tag")
        Traceback (most recent call last):
        ...
        cryptography.exceptions.InvalidSignature: Value did not match computed tag.

    :param key: Secret key as ``bytes``.
    :type key: :term:`bytes-like`
    :raises cryptography.exceptions.UnsupportedAlgorithm: This is raised if
        the version of OpenSSL ``cryptography`` is compiled against does not
        support this algorithm.

    .. method:: update(data)

        :param data: The bytes to hash and authenticate.
        :type data: :term:`bytes-like`
        :raises cryptography.exceptions.AlreadyFinalized: See :meth:`finalize`
        :raises TypeError: This exception is raised if ``data`` is not ``bytes``.

    .. method:: verify(tag)

        Finalize the current context and securely compare the MAC to
        ``tag``.

        :param bytes tag: The bytes to compare against.
        :raises cryptography.exceptions.AlreadyFinalized: See :meth:`finalize`
        :raises cryptography.exceptions.InvalidSignature: If tag does not
                                                          match.
        :raises TypeError: This exception is raised if ``tag`` is not
                           ``bytes``.

        .. method:: finalize()

        Finalize the current context and return the message authentication code
        as bytes.

        After ``finalize`` has been called this object can no longer be used
        and :meth:`update`, :meth:`verify`, and :meth:`finalize`
        will raise an :class:`~cryptography.exceptions.AlreadyFinalized`
        exception.

        :return bytes: The message authentication code as bytes.
        :raises cryptography.exceptions.AlreadyFinalized:

    .. classmethod:: generate_tag(key, data)

        A single step alternative to do sign operations. Returns the message
        authentication code as ``bytes`` for the given ``key`` and ``data``.

        :param key: Secret key as ``bytes``.
        :type key: :term:`bytes-like`
        :param data: The bytes to hash and authenticate.
        :type data: :term:`bytes-like`
        :return bytes: The message authentication code as bytes.
        :raises cryptography.exceptions.UnsupportedAlgorithm: This is raised if
            the version of OpenSSL ``cryptography`` is compiled against does not
            support this algorithm.
        :raises TypeError: This exception is raised if ``key`` or ``data`` are
            not ``bytes``.

        .. doctest::

            >>> poly1305.Poly1305.generate_tag(key, b"message to authenticate")
            b'T\xae\xff3\xbdW\xef\xd5r\x01\xe2n=\xb7\xd2h'

    .. classmethod:: verify_tag(key, data, tag)

        A single step alternative to do verify operations. Securely compares the
        MAC to ``tag``, using the given ``key`` and ``data``.

        :param key: Secret key as ``bytes``.
        :type key: :term:`bytes-like`
        :param data: The bytes to hash and authenticate.
        :type data: :term:`bytes-like`
        :param bytes tag: The bytes to compare against.
        :raises cryptography.exceptions.UnsupportedAlgorithm: This is raised if
            the version of OpenSSL ``cryptography`` is compiled against does not
            support this algorithm.
        :raises TypeError: This exception is raised if ``key``, ``data`` or
            ``tag`` are not ``bytes``.
        :raises cryptography.exceptions.InvalidSignature: If tag does not match.

        .. doctest::

            >>> poly1305.Poly1305.verify_tag(key, b"message to authenticate", b"an incorrect tag")
            Traceback (most recent call last):
            ...
            cryptography.exceptions.InvalidSignature: Value did not match computed tag.
.. hazmat::

Symmetric Padding
=================

.. module:: cryptography.hazmat.primitives.padding

Padding is a way to take data that may or may not be a multiple of the block
size for a cipher and extend it out so that it is. This is required for many
block cipher modes as they require the data to be encrypted to be an exact
multiple of the block size.


.. class:: PKCS7(block_size)

    PKCS7 padding is a generalization of PKCS5 padding (also known as standard
    padding). PKCS7 padding works by appending ``N`` bytes with the value of
    ``chr(N)``, where ``N`` is the number of bytes required to make the final
    block of data the same size as the block size. A simple example of padding
    is:

    .. doctest::

        >>> from cryptography.hazmat.primitives import padding
        >>> padder = padding.PKCS7(128).padder()
        >>> padded_data = padder.update(b"11111111111111112222222222")
        >>> padded_data
        b'1111111111111111'
        >>> padded_data += padder.finalize()
        >>> padded_data
        b'11111111111111112222222222\x06\x06\x06\x06\x06\x06'
        >>> unpadder = padding.PKCS7(128).unpadder()
        >>> data = unpadder.update(padded_data)
        >>> data
        b'1111111111111111'
        >>> data + unpadder.finalize()
        b'11111111111111112222222222'

    :param block_size: The size of the block in :term:`bits` that the data is
        being padded to.
    :raises ValueError: Raised if block size is not a multiple of 8 or is not
        between 0 and 2040 inclusive.

    .. method:: padder()

        :returns: A padding
            :class:`~cryptography.hazmat.primitives.padding.PaddingContext`
            instance.

    .. method:: unpadder()

        :returns: An unpadding
            :class:`~cryptography.hazmat.primitives.padding.PaddingContext`
            instance.


.. class:: ANSIX923(block_size)

    .. versionadded:: 1.3

    `ANSI X.923`_ padding works by appending ``N-1`` bytes with the value of
    ``0`` and a last byte with the value of ``chr(N)``, where ``N`` is the
    number of bytes required to make the final block of data the same size as
    the block size. A simple example of padding is:

    .. doctest::

        >>> padder = padding.ANSIX923(128).padder()
        >>> padded_data = padder.update(b"11111111111111112222222222")
        >>> padded_data
        b'1111111111111111'
        >>> padded_data += padder.finalize()
        >>> padded_data
        b'11111111111111112222222222\x00\x00\x00\x00\x00\x06'
        >>> unpadder = padding.ANSIX923(128).unpadder()
        >>> data = unpadder.update(padded_data)
        >>> data
        b'1111111111111111'
        >>> data + unpadder.finalize()
        b'11111111111111112222222222'

    :param block_size: The size of the block in :term:`bits` that the data is
        being padded to.
    :raises ValueError: Raised if block size is not a multiple of 8 or is not
        between 0 and 2040 inclusive.

    .. method:: padder()

        :returns: A padding
            :class:`~cryptography.hazmat.primitives.padding.PaddingContext`
            instance.

    .. method:: unpadder()

        :returns: An unpadding
            :class:`~cryptography.hazmat.primitives.padding.PaddingContext`
            instance.


.. class:: PaddingContext

    When calling ``padder()`` or ``unpadder()`` the result will conform to the
    ``PaddingContext`` interface. You can then call ``update(data)`` with data
    until you have fed everything into the context. Once that is done call
    ``finalize()`` to finish the operation and obtain the remainder of the
    data.

    .. method:: update(data)

        :param data: The data you wish to pass into the context.
        :type data: :term:`bytes-like`
        :return bytes: Returns the data that was padded or unpadded.
        :raises TypeError: Raised if data is not bytes.
        :raises cryptography.exceptions.AlreadyFinalized: See :meth:`finalize`.
        :raises TypeError: This exception is raised if ``data`` is not ``bytes``.

    .. method:: finalize()

        Finalize the current context and return the rest of the data.

        After ``finalize`` has been called this object can no longer be used;
        :meth:`update` and :meth:`finalize` will raise an
        :class:`~cryptography.exceptions.AlreadyFinalized` exception.

        :return bytes: Returns the remainder of the data.
        :raises TypeError: Raised if data is not bytes.
        :raises ValueError: When trying to remove padding from incorrectly
                            padded data.

.. _`ANSI X.923`: https://en.wikipedia.org/wiki/Padding_%28cryptography%29#ANSI_X9.23
.. hazmat:: /fernet


Symmetric encryption
====================

.. module:: cryptography.hazmat.primitives.ciphers

Symmetric encryption is a way to `encrypt`_ or hide the contents of material
where the sender and receiver both use the same secret key. Note that symmetric
encryption is **not** sufficient for most applications because it only
provides secrecy but not authenticity. That means an attacker can't see the
message but an attacker can create bogus messages and force the application to
decrypt them. In many contexts, a lack of authentication on encrypted messages
can result in a loss of secrecy as well.

For this reason it is **strongly** recommended to combine encryption with a
message authentication code, such as :doc:`HMAC </hazmat/primitives/mac/hmac>`,
in an "encrypt-then-MAC" formulation as `described by Colin Percival`_.
``cryptography`` includes a recipe named :doc:`/fernet` that does this for you.
**To minimize the risk of security issues you should evaluate Fernet to see if
it fits your needs before implementing anything using this module.**

.. class:: Cipher(algorithm, mode, backend=None)

    Cipher objects combine an algorithm such as
    :class:`~cryptography.hazmat.primitives.ciphers.algorithms.AES` with a
    mode like
    :class:`~cryptography.hazmat.primitives.ciphers.modes.CBC` or
    :class:`~cryptography.hazmat.primitives.ciphers.modes.CTR`. A simple
    example of encrypting and then decrypting content with AES is:

    .. doctest::

        >>> import os
        >>> from cryptography.hazmat.primitives.ciphers import Cipher, algorithms, modes
        >>> key = os.urandom(32)
        >>> iv = os.urandom(16)
        >>> cipher = Cipher(algorithms.AES(key), modes.CBC(iv))
        >>> encryptor = cipher.encryptor()
        >>> ct = encryptor.update(b"a secret message") + encryptor.finalize()
        >>> decryptor = cipher.decryptor()
        >>> decryptor.update(ct) + decryptor.finalize()
        b'a secret message'

    :param algorithm: A
        :class:`~cryptography.hazmat.primitives.ciphers.CipherAlgorithm`
        instance such as those described
        :ref:`below <symmetric-encryption-algorithms>`.
    :param mode: A :class:`~cryptography.hazmat.primitives.ciphers.modes.Mode`
        instance such as those described
        :ref:`below <symmetric-encryption-modes>`.
    :param backend: An optional
        :class:`~cryptography.hazmat.backends.interfaces.CipherBackend`
        instance.

    :raises cryptography.exceptions.UnsupportedAlgorithm: This is raised if the
        provided ``backend`` does not implement
        :class:`~cryptography.hazmat.backends.interfaces.CipherBackend`

    .. method:: encryptor()

        :return: An encrypting
            :class:`~cryptography.hazmat.primitives.ciphers.CipherContext`
            instance.

        If the backend doesn't support the requested combination of ``cipher``
        and ``mode`` an :class:`~cryptography.exceptions.UnsupportedAlgorithm`
        exception will be raised.

    .. method:: decryptor()

        :return: A decrypting
            :class:`~cryptography.hazmat.primitives.ciphers.CipherContext`
            instance.

        If the backend doesn't support the requested combination of ``cipher``
        and ``mode`` an :class:`~cryptography.exceptions.UnsupportedAlgorithm`
        exception will be raised.

.. _symmetric-encryption-algorithms:

Algorithms
~~~~~~~~~~

.. currentmodule:: cryptography.hazmat.primitives.ciphers.algorithms

.. class:: AES(key)

    AES (Advanced Encryption Standard) is a block cipher standardized by NIST.
    AES is both fast, and cryptographically strong. It is a good default
    choice for encryption.

    :param key: The secret key. This must be kept secret. Either ``128``,
        ``192``, or ``256`` :term:`bits` long.
    :type key: :term:`bytes-like`

.. class:: Camellia(key)

    Camellia is a block cipher approved for use by `CRYPTREC`_ and ISO/IEC.
    It is considered to have comparable security and performance to AES but
    is not as widely studied or deployed.

    :param key: The secret key. This must be kept secret. Either ``128``,
        ``192``, or ``256`` :term:`bits` long.
    :type key: :term:`bytes-like`

.. class:: ChaCha20(key)

    .. versionadded:: 2.1

    .. note::

        In most cases users should use
        :class:`~cryptography.hazmat.primitives.ciphers.aead.ChaCha20Poly1305`
        instead of this class. `ChaCha20` alone does not provide integrity
        so it must be combined with a MAC to be secure.
        :class:`~cryptography.hazmat.primitives.ciphers.aead.ChaCha20Poly1305`
        does this for you.

    ChaCha20 is a stream cipher used in several IETF protocols. It is
    standardized in :rfc:`7539`.

    :param key: The secret key. This must be kept secret. ``256``
        :term:`bits` (32 bytes) in length.
    :type key: :term:`bytes-like`

    :param nonce: Should be unique, a :term:`nonce`. It is
        critical to never reuse a ``nonce`` with a given key.  Any reuse of a
        nonce with the same key compromises the security of every message
        encrypted with that key. The nonce does not need to be kept secret
        and may be included with the ciphertext. This must be ``128``
        :term:`bits` in length.
    :type nonce: :term:`bytes-like`

        .. note::

            In :rfc:`7539` the nonce is defined as a 96-bit value that is later
            concatenated with a block counter (encoded as a 32-bit
            little-endian). If you have a separate nonce and block counter
            you will need to concatenate it yourself before passing it. For
            example, if you have an initial block counter of 2 and a 96-bit
            nonce the concatenated nonce would be
            ``struct.pack("<i", 2) + nonce``.

    .. doctest::

        >>> from cryptography.hazmat.primitives.ciphers import Cipher, algorithms, modes
        >>> nonce = os.urandom(16)
        >>> algorithm = algorithms.ChaCha20(key, nonce)
        >>> cipher = Cipher(algorithm, mode=None)
        >>> encryptor = cipher.encryptor()
        >>> ct = encryptor.update(b"a secret message")
        >>> decryptor = cipher.decryptor()
        >>> decryptor.update(ct)
        b'a secret message'

.. class:: TripleDES(key)

    Triple DES (Data Encryption Standard), sometimes referred to as 3DES, is a
    block cipher standardized by NIST. Triple DES has known crypto-analytic
    flaws, however none of them currently enable a practical attack.
    Nonetheless, Triple DES is not recommended for new applications because it
    is incredibly slow; old applications should consider moving away from it.

    :param key: The secret key. This must be kept secret. Either ``64``,
        ``128``, or ``192`` :term:`bits` long. DES only uses ``56``, ``112``,
        or ``168`` bits of the key as there is a parity byte in each component
        of the key.  Some writing refers to there being up to three separate
        keys that are each ``56`` bits long, they can simply be concatenated
        to produce the full key.
    :type key: :term:`bytes-like`

.. class:: CAST5(key)

    .. versionadded:: 0.2

    CAST5 (also known as CAST-128) is a block cipher approved for use in the
    Canadian government by the `Communications Security Establishment`_. It is
    a variable key length cipher and supports keys from 40-128 :term:`bits` in
    length.

    :param key: The secret key, This must be kept secret. 40 to 128
        :term:`bits` in length in increments of 8 bits.
    :type key: :term:`bytes-like`

.. class:: SEED(key)

    .. versionadded:: 0.4

    SEED is a block cipher developed by the Korea Information Security Agency
    (KISA). It is defined in :rfc:`4269` and is used broadly throughout South
    Korean industry, but rarely found elsewhere.

    :param key: The secret key. This must be kept secret. ``128``
        :term:`bits` in length.
    :type key: :term:`bytes-like`

Weak ciphers
------------

.. warning::

    These ciphers are considered weak for a variety of reasons. New
    applications should avoid their use and existing applications should
    strongly consider migrating away.

.. class:: Blowfish(key)

    Blowfish is a block cipher developed by Bruce Schneier. It is known to be
    susceptible to attacks when using weak keys. The author has recommended
    that users of Blowfish move to newer algorithms such as :class:`AES`.

    :param key: The secret key. This must be kept secret. 32 to 448
        :term:`bits` in length in increments of 8 bits.
    :type key: :term:`bytes-like`

.. class:: ARC4(key)

    ARC4 (Alleged RC4) is a stream cipher with serious weaknesses in its
    initial stream output. Its use is strongly discouraged. ARC4 does not use
    mode constructions.

    :param key: The secret key. This must be kept secret. Either ``40``,
        ``56``, ``64``, ``80``, ``128``, ``192``, or ``256`` :term:`bits` in
        length.
    :type key: :term:`bytes-like`

    .. doctest::

        >>> from cryptography.hazmat.primitives.ciphers import Cipher, algorithms, modes
        >>> algorithm = algorithms.ARC4(key)
        >>> cipher = Cipher(algorithm, mode=None)
        >>> encryptor = cipher.encryptor()
        >>> ct = encryptor.update(b"a secret message")
        >>> decryptor = cipher.decryptor()
        >>> decryptor.update(ct)
        b'a secret message'

.. class:: IDEA(key)

    IDEA (`International Data Encryption Algorithm`_) is a block cipher created
    in 1991. It is an optional component of the `OpenPGP`_ standard. This cipher
    is susceptible to attacks when using weak keys. It is recommended that you
    do not use this cipher for new applications.

    :param key: The secret key. This must be kept secret. ``128``
        :term:`bits` in length.
    :type key: :term:`bytes-like`


.. _symmetric-encryption-modes:

Modes
~~~~~

.. module:: cryptography.hazmat.primitives.ciphers.modes

.. class:: CBC(initialization_vector)

    CBC (Cipher Block Chaining) is a mode of operation for block ciphers. It is
    considered cryptographically strong.

    **Padding is required when using this mode.**

    :param initialization_vector: Must be :doc:`random bytes
        </random-numbers>`. They do not need to be kept secret and they can be
        included in a transmitted message. Must be the same number of bytes as
        the ``block_size`` of the cipher. Each time something is encrypted a
        new ``initialization_vector`` should be generated. Do not reuse an
        ``initialization_vector`` with a given ``key``, and particularly do not
        use a constant ``initialization_vector``.
    :type initialization_vector: :term:`bytes-like`

    A good construction looks like:

    .. doctest::

        >>> import os
        >>> from cryptography.hazmat.primitives.ciphers.modes import CBC
        >>> iv = os.urandom(16)
        >>> mode = CBC(iv)

    While the following is bad and will leak information:

    .. doctest::

        >>> from cryptography.hazmat.primitives.ciphers.modes import CBC
        >>> iv = b"a" * 16
        >>> mode = CBC(iv)


.. class:: CTR(nonce)

    .. warning::

        Counter mode is not recommended for use with block ciphers that have a
        block size of less than 128-:term:`bits`.

    CTR (Counter) is a mode of operation for block ciphers. It is considered
    cryptographically strong. It transforms a block cipher into a stream
    cipher.

    **This mode does not require padding.**

    :param nonce: Should be unique, a :term:`nonce`. It is
        critical to never reuse a ``nonce`` with a given key.  Any reuse of a
        nonce with the same key compromises the security of every message
        encrypted with that key. Must be the same number of bytes as the
        ``block_size`` of the cipher with a given key. The nonce does not need
        to be kept secret and may be included with the ciphertext.
    :type nonce: :term:`bytes-like`

.. class:: OFB(initialization_vector)

    OFB (Output Feedback) is a mode of operation for block ciphers. It
    transforms a block cipher into a stream cipher.

    **This mode does not require padding.**

    :param initialization_vector: Must be :doc:`random bytes
        </random-numbers>`. They do not need to be kept secret and they can be
        included in a transmitted message. Must be the same number of bytes as
        the ``block_size`` of the cipher. Do not reuse an
        ``initialization_vector`` with a given ``key``.
    :type initialization_vector: :term:`bytes-like`

.. class:: CFB(initialization_vector)

    CFB (Cipher Feedback) is a mode of operation for block ciphers. It
    transforms a block cipher into a stream cipher.

    **This mode does not require padding.**

    :param initialization_vector: Must be :doc:`random bytes
        </random-numbers>`. They do not need to be kept secret and they can be
        included in a transmitted message. Must be the same number of bytes as
        the ``block_size`` of the cipher. Do not reuse an
        ``initialization_vector`` with a given ``key``.
    :type initialization_vector: :term:`bytes-like`

.. class:: CFB8(initialization_vector)

    CFB (Cipher Feedback) is a mode of operation for block ciphers. It
    transforms a block cipher into a stream cipher. The CFB8 variant uses an
    8-bit shift register.

    **This mode does not require padding.**

    :param initialization_vector: Must be :doc:`random bytes
        </random-numbers>`. They do not need to be kept secret and they can be
        included in a transmitted message. Must be the same number of bytes as
        the ``block_size`` of the cipher. Do not reuse an
        ``initialization_vector`` with a given ``key``.
    :type initialization_vector: :term:`bytes-like`

.. class:: GCM(initialization_vector, tag=None, min_tag_length=16)

    .. danger::

        If you are encrypting data that can fit into memory you should strongly
        consider using
        :class:`~cryptography.hazmat.primitives.ciphers.aead.AESGCM` instead
        of this.

        When using this mode you **must** not use the decrypted data until
        the appropriate finalization method
        (:meth:`~cryptography.hazmat.primitives.ciphers.CipherContext.finalize`
        or
        :meth:`~cryptography.hazmat.primitives.ciphers.AEADDecryptionContext.finalize_with_tag`)
        has been called. GCM provides **no** guarantees of ciphertext integrity
        until decryption is complete.

    GCM (Galois Counter Mode) is a mode of operation for block ciphers. An
    AEAD (authenticated encryption with additional data) mode is a type of
    block cipher mode that simultaneously encrypts the message as well as
    authenticating it. Additional unencrypted data may also be authenticated.
    Additional means of verifying integrity such as
    :doc:`HMAC </hazmat/primitives/mac/hmac>` are not necessary.

    **This mode does not require padding.**

    :param initialization_vector: Must be unique, a :term:`nonce`.
        They do not need to be kept secret and they can be included in a
        transmitted message. NIST `recommends a 96-bit IV length`_ for
        performance critical situations but it can be up to 2\ :sup:`64` - 1
        :term:`bits`. Do not reuse an ``initialization_vector`` with a given
        ``key``.
    :type initialization_vector: :term:`bytes-like`

    .. note::

        Cryptography will generate a 128-bit tag when finalizing encryption.
        You can shorten a tag by truncating it to the desired length but this
        is **not recommended** as it makes it easier to forge messages, and
        also potentially leaks the key (`NIST SP-800-38D`_ recommends
        96-:term:`bits` or greater).  Applications wishing to allow truncation
        can pass the ``min_tag_length`` parameter.

        .. versionchanged:: 0.5

            The ``min_tag_length`` parameter was added in ``0.5``, previously
            truncation down to ``4`` bytes was always allowed.

    :param bytes tag: The tag bytes to verify during decryption. When
        encrypting this must be ``None``. When decrypting, it may be ``None``
        if the tag is supplied on finalization using
        :meth:`~cryptography.hazmat.primitives.ciphers.AEADDecryptionContext.finalize_with_tag`.
        Otherwise, the tag is mandatory.

    :param int min_tag_length: The minimum length ``tag`` must be. By default
        this is ``16``, meaning tag truncation is not allowed. Allowing tag
        truncation is strongly discouraged for most applications.

    :raises ValueError: This is raised if ``len(tag) < min_tag_length`` or the
        ``initialization_vector`` is too short.

    An example of securely encrypting and decrypting data with ``AES`` in the
    ``GCM`` mode looks like:

    .. testcode::

        import os

        from cryptography.hazmat.primitives.ciphers import (
            Cipher, algorithms, modes
        )

        def encrypt(key, plaintext, associated_data):
            # Generate a random 96-bit IV.
            iv = os.urandom(12)

            # Construct an AES-GCM Cipher object with the given key and a
            # randomly generated IV.
            encryptor = Cipher(
                algorithms.AES(key),
                modes.GCM(iv),
            ).encryptor()

            # associated_data will be authenticated but not encrypted,
            # it must also be passed in on decryption.
            encryptor.authenticate_additional_data(associated_data)

            # Encrypt the plaintext and get the associated ciphertext.
            # GCM does not require padding.
            ciphertext = encryptor.update(plaintext) + encryptor.finalize()

            return (iv, ciphertext, encryptor.tag)

        def decrypt(key, associated_data, iv, ciphertext, tag):
            # Construct a Cipher object, with the key, iv, and additionally the
            # GCM tag used for authenticating the message.
            decryptor = Cipher(
                algorithms.AES(key),
                modes.GCM(iv, tag),
            ).decryptor()

            # We put associated_data back in or the tag will fail to verify
            # when we finalize the decryptor.
            decryptor.authenticate_additional_data(associated_data)

            # Decryption gets us the authenticated plaintext.
            # If the tag does not match an InvalidTag exception will be raised.
            return decryptor.update(ciphertext) + decryptor.finalize()

        iv, ciphertext, tag = encrypt(
            key,
            b"a secret message!",
            b"authenticated but not encrypted payload"
        )

        print(decrypt(
            key,
            b"authenticated but not encrypted payload",
            iv,
            ciphertext,
            tag
        ))

    .. testoutput::

        b'a secret message!'

.. class:: XTS(tweak)

    .. versionadded:: 2.1

    .. warning::

        XTS mode is meant for disk encryption and should not be used in other
        contexts. ``cryptography`` only supports XTS mode with
        :class:`~cryptography.hazmat.primitives.ciphers.algorithms.AES`.

    .. note::

        AES XTS keys are double length. This means that to do AES-128
        encryption in XTS mode you need a 256-bit key. Similarly, AES-256
        requires passing a 512-bit key. AES 192 is not supported in XTS mode.

    XTS (XEX-based tweaked-codebook mode with ciphertext stealing) is a mode
    of operation for the AES block cipher that is used for `disk encryption`_.

    **This mode does not require padding.**

    :param tweak: The tweak is a 16 byte value typically derived from
        something like the disk sector number. A given ``(tweak, key)`` pair
        should not be reused, although doing so is less catastrophic than
        in CTR mode.
    :type tweak: :term:`bytes-like`

Insecure modes
--------------

.. warning::

    These modes are insecure. New applications should never make use of them,
    and existing applications should strongly consider migrating away.


.. class:: ECB()

    ECB (Electronic Code Book) is the simplest mode of operation for block
    ciphers. Each block of data is encrypted in the same way. This means
    identical plaintext blocks will always result in identical ciphertext
    blocks, which can leave `significant patterns in the output`_.

    **Padding is required when using this mode.**

Interfaces
~~~~~~~~~~

.. currentmodule:: cryptography.hazmat.primitives.ciphers

.. class:: CipherContext

    When calling ``encryptor()`` or ``decryptor()`` on a ``Cipher`` object
    the result will conform to the ``CipherContext`` interface. You can then
    call ``update(data)`` with data until you have fed everything into the
    context. Once that is done call ``finalize()`` to finish the operation and
    obtain the remainder of the data.

    Block ciphers require that the plaintext or ciphertext always be a multiple
    of their block size. Because of that **padding** is sometimes required to
    make a message the correct size. ``CipherContext`` will not automatically
    apply any padding; you'll need to add your own. For block ciphers the
    recommended padding is
    :class:`~cryptography.hazmat.primitives.padding.PKCS7`. If you are using a
    stream cipher mode (such as
    :class:`~cryptography.hazmat.primitives.ciphers.modes.CTR`) you don't have
    to worry about this.

    .. method:: update(data)

        :param data: The data you wish to pass into the context.
        :type data: :term:`bytes-like`
        :return bytes: Returns the data that was encrypted or decrypted.
        :raises cryptography.exceptions.AlreadyFinalized: See :meth:`finalize`

        When the ``Cipher`` was constructed in a mode that turns it into a
        stream cipher (e.g.
        :class:`~cryptography.hazmat.primitives.ciphers.modes.CTR`), this will
        return bytes immediately, however in other modes it will return chunks
        whose size is determined by the cipher's block size.

    .. method:: update_into(data, buf)

        .. versionadded:: 1.8

        .. warning::

            This method allows you to avoid a memory copy by passing a writable
            buffer and reading the resulting data. You are responsible for
            correctly sizing the buffer and properly handling the data. This
            method should only be used when extremely high performance is a
            requirement and you will be making many small calls to
            ``update_into``.

        :param data: The data you wish to pass into the context.
        :type data: :term:`bytes-like`
        :param buf: A writable Python buffer that the data will be written
            into. This buffer should be ``len(data) + n - 1`` bytes where ``n``
            is the block size (in bytes) of the cipher and mode combination.
        :return int: Number of bytes written.
        :raises NotImplementedError: This is raised if the version of ``cffi``
            used is too old (this can happen on older PyPy releases).
        :raises ValueError: This is raised if the supplied buffer is too small.

        .. doctest::

            >>> import os
            >>> from cryptography.hazmat.primitives.ciphers import Cipher, algorithms, modes
            >>> key = os.urandom(32)
            >>> iv = os.urandom(16)
            >>> cipher = Cipher(algorithms.AES(key), modes.CBC(iv))
            >>> encryptor = cipher.encryptor()
            >>> # the buffer needs to be at least len(data) + n - 1 where n is cipher/mode block size in bytes
            >>> buf = bytearray(31)
            >>> len_encrypted = encryptor.update_into(b"a secret message", buf)
            >>> # get the ciphertext from the buffer reading only the bytes written to it (len_encrypted)
            >>> ct = bytes(buf[:len_encrypted]) + encryptor.finalize()
            >>> decryptor = cipher.decryptor()
            >>> len_decrypted = decryptor.update_into(ct, buf)
            >>> # get the plaintext from the buffer reading only the bytes written (len_decrypted)
            >>> bytes(buf[:len_decrypted]) + decryptor.finalize()
            b'a secret message'

    .. method:: finalize()

        :return bytes: Returns the remainder of the data.
        :raises ValueError: This is raised when the data provided isn't
            a multiple of the algorithm's block size.

        Once ``finalize`` is called this object can no longer be used and
        :meth:`update` and :meth:`finalize` will raise an
        :class:`~cryptography.exceptions.AlreadyFinalized` exception.

.. class:: AEADCipherContext

    When calling ``encryptor`` or ``decryptor`` on a ``Cipher`` object
    with an AEAD mode (e.g.
    :class:`~cryptography.hazmat.primitives.ciphers.modes.GCM`) the result will
    conform to the ``AEADCipherContext`` and ``CipherContext`` interfaces. If
    it is an encryption or decryption context it will additionally be an
    ``AEADEncryptionContext`` or ``AEADDecryptionContext`` instance,
    respectively. ``AEADCipherContext`` contains an additional method
    :meth:`authenticate_additional_data` for adding additional authenticated
    but unencrypted data (see note below). You should call this before calls to
    ``update``. When you are done call ``finalize`` to finish the operation.

    .. note::

        In AEAD modes all data passed to ``update()`` will be both encrypted
        and authenticated. Do not pass encrypted data to the
        ``authenticate_additional_data()`` method. It is meant solely for
        additional data you may want to authenticate but leave unencrypted.

    .. method:: authenticate_additional_data(data)

        :param data: Any data you wish to authenticate but not encrypt.
        :type data: :term:`bytes-like`
        :raises: :class:`~cryptography.exceptions.AlreadyFinalized`

.. class:: AEADEncryptionContext

    When creating an encryption context using ``encryptor`` on a ``Cipher``
    object with an AEAD mode such as
    :class:`~cryptography.hazmat.primitives.ciphers.modes.GCM` an object
    conforming to both the ``AEADEncryptionContext`` and ``AEADCipherContext``
    interfaces will be returned.  This interface provides one
    additional attribute ``tag``. ``tag`` can only be obtained after
    ``finalize`` has been called.

    .. attribute:: tag

        :return bytes: Returns the tag value as bytes.
        :raises: :class:`~cryptography.exceptions.NotYetFinalized` if called
            before the context is finalized.

.. class:: AEADDecryptionContext

    .. versionadded:: 1.9

    When creating an encryption context using ``decryptor`` on a ``Cipher``
    object with an AEAD mode such as
    :class:`~cryptography.hazmat.primitives.ciphers.modes.GCM` an object
    conforming to both the ``AEADDecryptionContext`` and ``AEADCipherContext``
    interfaces will be returned.  This interface provides one additional method
    :meth:`finalize_with_tag` that allows passing the authentication tag for
    validation after the ciphertext has been decrypted.

    .. method:: finalize_with_tag(tag)

        :param bytes tag: The tag bytes to verify after decryption.
        :return bytes: Returns the remainder of the data.
        :raises ValueError: This is raised when the data provided isn't
            a multiple of the algorithm's block size, if ``min_tag_length`` is
            less than 4, or if ``len(tag) < min_tag_length``.
            ``min_tag_length`` is an argument to the ``GCM`` constructor.

        If the authentication tag was not already supplied to the constructor
        of the :class:`~cryptography.hazmat.primitives.ciphers.modes.GCM` mode
        object, this method must be used instead of
        :meth:`~cryptography.hazmat.primitives.ciphers.CipherContext.finalize`.

.. class:: CipherAlgorithm

    A named symmetric encryption algorithm.

    .. attribute:: name

        :type: str

        The standard name for the mode, for example, "AES", "Camellia", or
        "Blowfish".

    .. attribute:: key_size

        :type: int

        The number of :term:`bits` in the key being used.


.. class:: BlockCipherAlgorithm

    A block cipher algorithm.

    .. attribute:: block_size

        :type: int

        The number of :term:`bits` in a block.

Interfaces used by the symmetric cipher modes described in
:ref:`Symmetric Encryption Modes <symmetric-encryption-modes>`.

.. currentmodule:: cryptography.hazmat.primitives.ciphers.modes

.. class:: Mode

    A named cipher mode.

    .. attribute:: name

        :type: str

        This should be the standard shorthand name for the mode, for example
        Cipher-Block Chaining mode is "CBC".

        The name may be used by a backend to influence the operation of a
        cipher in conjunction with the algorithm's name.

    .. method:: validate_for_algorithm(algorithm)

        :param cryptography.hazmat.primitives.ciphers.CipherAlgorithm algorithm:

        Checks that the combination of this mode with the provided algorithm
        meets any necessary invariants. This should raise an exception if they
        are not met.

        For example, the
        :class:`~cryptography.hazmat.primitives.ciphers.modes.CBC` mode uses
        this method to check that the provided initialization vector's length
        matches the block size of the algorithm.


.. class:: ModeWithInitializationVector

    A cipher mode with an initialization vector.

    .. attribute:: initialization_vector

        :type: :term:`bytes-like`

        Exact requirements of the initialization are described by the
        documentation of individual modes.


.. class:: ModeWithNonce

    A cipher mode with a nonce.

    .. attribute:: nonce

        :type: :term:`bytes-like`

        Exact requirements of the nonce are described by the documentation of
        individual modes.


.. class:: ModeWithAuthenticationTag

    A cipher mode with an authentication tag.

    .. attribute:: tag

        :type: :term:`bytes-like`

        Exact requirements of the tag are described by the documentation of
        individual modes.


.. class:: ModeWithTweak

    .. versionadded:: 2.1

    A cipher mode with a tweak.

    .. attribute:: tweak

        :type: :term:`bytes-like`

        Exact requirements of the tweak are described by the documentation of
        individual modes.

Exceptions
~~~~~~~~~~

.. currentmodule:: cryptography.exceptions


.. class:: InvalidTag

    This is raised if an authenticated encryption tag fails to verify during
    decryption.



.. _`described by Colin Percival`: https://www.daemonology.net/blog/2009-06-11-cryptographic-right-answers.html
.. _`recommends a 96-bit IV length`: https://csrc.nist.gov/publications/detail/sp/800-38d/final
.. _`NIST SP-800-38D`: https://csrc.nist.gov/publications/detail/sp/800-38d/final
.. _`Communications Security Establishment`: https://www.cse-cst.gc.ca
.. _`encrypt`: https://ssd.eff.org/en/module/what-should-i-know-about-encryption
.. _`CRYPTREC`: https://www.cryptrec.go.jp/english/
.. _`significant patterns in the output`: https://en.wikipedia.org/wiki/Block_cipher_mode_of_operation#Electronic_codebook_(ECB)
.. _`International Data Encryption Algorithm`: https://en.wikipedia.org/wiki/International_Data_Encryption_Algorithm
.. _`OpenPGP`: https://www.openpgp.org/
.. _`disk encryption`: https://en.wikipedia.org/wiki/Disk_encryption_theory#XTS
.. hazmat::

Two-factor authentication
=========================

.. currentmodule:: cryptography.hazmat.primitives.twofactor

This module contains algorithms related to two-factor authentication.

Currently, it contains an algorithm for generating and verifying
one time password values based on Hash-based message authentication
codes (HMAC).

.. class:: InvalidToken

    This is raised when the verify method of a one time password function's
    computed token does not match the expected token.

.. currentmodule:: cryptography.hazmat.primitives.twofactor.hotp

.. class:: HOTP(key, length, algorithm, backend=None, enforce_key_length=True)

    .. versionadded:: 0.3

    HOTP objects take a ``key``, ``length`` and ``algorithm`` parameter. The
    ``key`` should be :doc:`randomly generated bytes </random-numbers>` and is
    recommended to be 160 :term:`bits` in length. The ``length`` parameter
    controls the length of the generated one time password and must be >= 6
    and <= 8.

    This is an implementation of :rfc:`4226`.

    .. doctest::

        >>> import os
        >>> from cryptography.hazmat.primitives.twofactor.hotp import HOTP
        >>> from cryptography.hazmat.primitives.hashes import SHA1
        >>> key = os.urandom(20)
        >>> hotp = HOTP(key, 6, SHA1())
        >>> hotp_value = hotp.generate(0)
        >>> hotp.verify(hotp_value, 0)

    :param key: Per-user secret key. This value must be kept secret
                and be at least 128 :term:`bits`. It is recommended that
                the key be 160 bits.
    :type key: :term:`bytes-like`
    :param int length: Length of generated one time password as ``int``.
    :param cryptography.hazmat.primitives.hashes.HashAlgorithm algorithm: A
        :class:`~cryptography.hazmat.primitives.hashes`
        instance.
    :param backend: An optional
        :class:`~cryptography.hazmat.backends.interfaces.HMACBackend`
        instance.
    :param enforce_key_length: A boolean flag defaulting to True that toggles
        whether a minimum key length of 128 :term:`bits` is enforced. This
        exists to work around the fact that as documented in `Issue #2915`_,
        the Google Authenticator PAM module by default generates 80 bit keys.
        If this flag is set to False, the application developer should
        implement additional checks of the key length before passing it into
        :class:`~cryptography.hazmat.primitives.twofactor.hotp.HOTP`.

        .. versionadded:: 1.5

    :raises ValueError: This is raised if the provided ``key`` is shorter than
        128 :term:`bits` or if the ``length`` parameter is not 6, 7 or 8.
    :raises TypeError: This is raised if the provided ``algorithm`` is not
        :class:`~cryptography.hazmat.primitives.hashes.SHA1()`,
        :class:`~cryptography.hazmat.primitives.hashes.SHA256()` or
        :class:`~cryptography.hazmat.primitives.hashes.SHA512()` or if the
        ``length`` parameter is not an integer.
    :raises cryptography.exceptions.UnsupportedAlgorithm: This is raised if the
        provided ``backend`` does not implement
        :class:`~cryptography.hazmat.backends.interfaces.HMACBackend`

    .. method:: generate(counter)

        :param int counter: The counter value used to generate the one time
            password.
        :return bytes: A one time password value.

    .. method:: verify(hotp, counter)

        :param bytes hotp: The one time password value to validate.
        :param int counter: The counter value to validate against.
        :raises cryptography.hazmat.primitives.twofactor.InvalidToken: This
             is raised when the supplied HOTP does not match the expected HOTP.

    .. method:: get_provisioning_uri(account_name, counter, issuer)

        .. versionadded:: 1.0

        :param account_name: The display name of account, such as
            ``'Alice Smith'`` or ``'alice@example.com'``.
        :type account_name: :term:`text`
        :param issuer: The optional display name of issuer. This is typically
            the provider or service the user wants to access using the OTP
            token.
        :type issuer: :term:`text` or `None`
        :param int counter: The current value of counter.
        :return: A URI string.

Throttling
~~~~~~~~~~

Due to the fact that the HOTP algorithm generates rather short tokens that are
6 - 8 digits long, brute force attacks are possible. It is highly recommended
that the server that validates the token implement a throttling scheme that
locks out the account for a period of time after a number of failed attempts.
The number of allowed attempts should be as low as possible while still
ensuring that usability is not significantly impacted.

Re-synchronization of the counter
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

The server's counter value should only be incremented on a successful HOTP
authentication. However, the counter on the client is incremented every time a
new HOTP value is requested. This can lead to the counter value being out of
synchronization between the client and server.

Due to this, it is highly recommended that the server sets a look-ahead window
that allows the server to calculate the next ``x`` HOTP values and check them
against the supplied HOTP value. This can be accomplished with something
similar to the following code.

.. code-block:: python

    def verify(hotp, counter, look_ahead):
        assert look_ahead >= 0
        correct_counter = None

        otp = HOTP(key, 6)
        for count in range(counter, counter + look_ahead):
            try:
                otp.verify(hotp, count)
                correct_counter = count
            except InvalidToken:
                pass

        return correct_counter

.. currentmodule:: cryptography.hazmat.primitives.twofactor.totp

.. class:: TOTP(key, length, algorithm, time_step, backend=None, enforce_key_length=True)

    TOTP objects take a ``key``, ``length``, ``algorithm`` and ``time_step``
    parameter. The ``key`` should be :doc:`randomly generated bytes
    </random-numbers>` and is recommended to be as long as your hash function's
    output (e.g 256-bit for SHA256). The ``length`` parameter controls the
    length of the generated one time password and must be >= 6 and <= 8.

    This is an implementation of :rfc:`6238`.

    .. doctest::

        >>> import os
        >>> import time
        >>> from cryptography.hazmat.primitives.twofactor.totp import TOTP
        >>> from cryptography.hazmat.primitives.hashes import SHA1
        >>> key = os.urandom(20)
        >>> totp = TOTP(key, 8, SHA1(), 30)
        >>> time_value = time.time()
        >>> totp_value = totp.generate(time_value)
        >>> totp.verify(totp_value, time_value)

    :param key: Per-user secret key. This value must be kept secret
                and be at least 128 :term:`bits`. It is recommended that the
                key be 160 bits.
    :type key: :term:`bytes-like`
    :param int length: Length of generated one time password as ``int``.
    :param cryptography.hazmat.primitives.hashes.HashAlgorithm algorithm: A
        :class:`~cryptography.hazmat.primitives.hashes`
        instance.
    :param int time_step: The time step size. The recommended size is 30.
    :param backend: An optional
        :class:`~cryptography.hazmat.backends.interfaces.HMACBackend`
        instance.
    :param enforce_key_length: A boolean flag defaulting to True that toggles
        whether a minimum key length of 128 :term:`bits` is enforced. This exists to
        work around the fact that as documented in `Issue #2915`_, the
        Google Authenticator PAM module by default generates 80 bit keys. If
        this flag is set to False, the application develop should implement
        additional checks of the key length before passing it into
        :class:`~cryptography.hazmat.primitives.twofactor.totp.TOTP`.

        .. versionadded:: 1.5
    :raises ValueError: This is raised if the provided ``key`` is shorter than
        128 :term:`bits` or if the ``length`` parameter is not 6, 7 or 8.
    :raises TypeError: This is raised if the provided ``algorithm`` is not
        :class:`~cryptography.hazmat.primitives.hashes.SHA1()`,
        :class:`~cryptography.hazmat.primitives.hashes.SHA256()` or
        :class:`~cryptography.hazmat.primitives.hashes.SHA512()` or if the
        ``length`` parameter is not an integer.
    :raises cryptography.exceptions.UnsupportedAlgorithm: This is raised if the
        provided ``backend`` does not implement
        :class:`~cryptography.hazmat.backends.interfaces.HMACBackend`

    .. method:: generate(time)

        :param int time: The time value used to generate the one time password.
        :return bytes: A one time password value.

    .. method:: verify(totp, time)

        :param bytes totp: The one time password value to validate.
        :param int time: The time value to validate against.
        :raises cryptography.hazmat.primitives.twofactor.InvalidToken: This
             is raised when the supplied TOTP does not match the expected TOTP.

    .. method:: get_provisioning_uri(account_name, issuer)

        .. versionadded:: 1.0

        :param account_name: The display name of account, such as
            ``'Alice Smith'`` or ``'alice@example.com'``.
        :type account_name: :term:`text`
        :param issuer: The optional display name of issuer. This is typically
            the provider or service the user wants to access using the OTP
            token.
        :type issuer: :term:`text` or `None`
        :return: A URI string.

Provisioning URI
~~~~~~~~~~~~~~~~

The provisioning URI of HOTP and TOTP is a `feature of Google Authenticator`_
and not actually part of the HOTP or TOTP RFCs. However, it is widely supported
by web sites and mobile applications which are using Two-Factor authentication.

For generating a provisioning URI you can use the ``get_provisioning_uri``
method of HOTP/TOTP instances.

.. code-block:: python

    counter = 5
    account_name = 'alice@example.com'
    issuer_name = 'Example Inc'

    hotp_uri = hotp.get_provisioning_uri(account_name, counter, issuer_name)
    totp_uri = totp.get_provisioning_uri(account_name, issuer_name)

A common usage is encoding the provisioning URI into QR code and guiding users
to scan it with Two-Factor authentication applications in their mobile devices.

.. _`feature of Google Authenticator`: https://github.com/google/google-authenticator/wiki/Key-Uri-Format
.. _`Issue #2915`: https://github.com/pyca/cryptography/issues/2915
Welcome to ``pyca/cryptography``
================================

``cryptography`` includes both high level recipes and low level interfaces to
common cryptographic algorithms such as symmetric ciphers, message digests, and
key derivation functions. For example, to encrypt something with
``cryptography``'s high level symmetric encryption recipe:

.. code-block:: pycon

    >>> from cryptography.fernet import Fernet
    >>> # Put this somewhere safe!
    >>> key = Fernet.generate_key()
    >>> f = Fernet(key)
    >>> token = f.encrypt(b"A really secret message. Not for prying eyes.")
    >>> token
    '...'
    >>> f.decrypt(token)
    'A really secret message. Not for prying eyes.'

If you are interested in learning more about the field of cryptography, we
recommend `Crypto 101, by Laurens Van Houtven`_ and `The Cryptopals Crypto
Challenges`_.

Installation
------------
You can install ``cryptography`` with ``pip``:

.. code-block:: console

    $ pip install cryptography

See :doc:`Installation <installation>` for more information.

.. _cryptography-layout:


Layout
------

``cryptography`` is broadly divided into two levels. One with safe
cryptographic recipes that require little to no configuration choices. These
are safe and easy to use and don't require developers to make many decisions.

The other level is low-level cryptographic primitives. These are often
dangerous and can be used incorrectly. They require making decisions and having
an in-depth knowledge of the cryptographic concepts at work. Because of the
potential danger in working at this level, this is referred to as the
"hazardous materials" or "hazmat" layer. These live in the
``cryptography.hazmat`` package, and their documentation will always contain an
admonition at the top.

We recommend using the recipes layer whenever possible, and falling back to the
hazmat layer only when necessary.

.. toctree::
    :maxdepth: 2
    :caption: The recipes layer

    fernet
    x509/index

.. toctree::
    :maxdepth: 2
    :caption: The hazardous materials layer

    hazmat/primitives/index
    exceptions
    random-numbers
    hazmat/backends/index

.. toctree::
    :maxdepth: 2
    :caption: The cryptography open source project

    installation
    changelog
    faq
    development/index
    security
    limitations
    api-stability
    doing-a-release
    community
    glossary


.. note::

    ``cryptography`` has not been subjected to an external audit of its code or
    documentation. If you're interested in discussing an audit please
    :doc:`get in touch </community>`.

.. _`Crypto 101, by Laurens Van Houtven`: https://www.crypto101.io/
.. _`The Cryptopals Crypto Challenges`: https://cryptopals.com/
Installation
============

You can install ``cryptography`` with ``pip``:

.. code-block:: console

    $ pip install cryptography

Supported platforms
-------------------

Currently we test ``cryptography`` on Python 2.7, 3.6+,
PyPy 7.3.1, and PyPy3 7.3.1 on these operating systems.

* x86-64 CentOS 7.x
* x86-64 & AArch64 CentOS 8.x
* x86-64 Fedora (latest)
* x86-64 macOS 10.15 Catalina
* x86-64 & AArch64 Ubuntu 18.04, 20.04
* x86-64 Ubuntu rolling
* x86-64 Debian Stretch (9.x), Buster (10.x), Bullseye (11.x), and Sid
  (unstable)
* x86-64 Alpine (latest)
* 32-bit and 64-bit Python on 64-bit Windows Server 2019

We test compiling with ``clang`` as well as ``gcc`` and use the following
OpenSSL releases:

* ``OpenSSL 1.1.0-latest``
* ``OpenSSL 1.1.1-latest``


Building cryptography on Windows
--------------------------------

The wheel package on Windows is a statically linked build (as of 0.5) so all
dependencies are included. To install ``cryptography``, you will typically
just run

.. code-block:: console

    $ pip install cryptography

If you prefer to compile it yourself you'll need to have OpenSSL installed.
You can compile OpenSSL yourself as well or use `a binary distribution`_.
Be sure to download the proper version for your architecture and Python
(VC2010 works for Python 2.7 while VC2015 is required for 3.6 and above).
Wherever you place your copy of OpenSSL you'll need to set the ``LIB`` and ``INCLUDE``
environment variables to include the proper locations. For example:

.. code-block:: console

    C:\> \path\to\vcvarsall.bat x86_amd64
    C:\> set LIB=C:\OpenSSL-win64\lib;%LIB%
    C:\> set INCLUDE=C:\OpenSSL-win64\include;%INCLUDE%
    C:\> pip install cryptography

As of OpenSSL 1.1.0 the library names have changed from ``libeay32`` and
``ssleay32`` to ``libcrypto`` and ``libssl`` (matching their names on all other
platforms). ``cryptography`` links against the new 1.1.0 names by default. If
you need to compile ``cryptography`` against an older version then you **must**
set ``CRYPTOGRAPHY_WINDOWS_LINK_LEGACY_OPENSSL`` or else installation will fail.

If you need to rebuild ``cryptography`` for any reason be sure to clear the
local `wheel cache`_.

.. _build-on-linux:

Building cryptography on Linux
------------------------------

``cryptography`` ships ``manylinux`` wheels (as of 2.0) so all dependencies
are included. For users on pip 8.1 or above running on a ``manylinux1`` or
``manylinux2010`` compatible distribution (almost everything except Alpine)
all you should need to do is:

.. code-block:: console

    $ pip install cryptography

If you are on Alpine or just want to compile it yourself then
``cryptography`` requires a compiler, headers for Python (if you're not
using ``pypy``), and headers for the OpenSSL and ``libffi`` libraries
available on your system.

Alpine
~~~~~~

Replace ``python3-dev`` with ``python-dev`` if you're using Python 2.

.. code-block:: console

    $ sudo apk add gcc musl-dev python3-dev libffi-dev openssl-dev

If you get an error with ``openssl-dev`` you may have to use ``libressl-dev``.

Debian/Ubuntu
~~~~~~~~~~~~~

Replace ``python3-dev`` with ``python-dev`` if you're using Python 2.

.. code-block:: console

    $ sudo apt-get install build-essential libssl-dev libffi-dev python3-dev

RHEL/CentOS
~~~~~~~~~~~

.. code-block:: console

    $ sudo yum install redhat-rpm-config gcc libffi-devel python-devel \
        openssl-devel


Building
~~~~~~~~

You should now be able to build and install cryptography. To avoid getting
the pre-built wheel on ``manylinux`` compatible distributions you'll need to
use ``--no-binary``.

.. code-block:: console

    $ pip install cryptography --no-binary cryptography


Using your own OpenSSL on Linux
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

Python links to OpenSSL for its own purposes and this can sometimes cause
problems when you wish to use a different version of OpenSSL with cryptography.
If you want to use cryptography with your own build of OpenSSL you will need to
make sure that the build is configured correctly so that your version of
OpenSSL doesn't conflict with Python's.

The options you need to add allow the linker to identify every symbol correctly
even when multiple versions of the library are linked into the same program. If
you are using your distribution's source packages these will probably be
patched in for you already, otherwise you'll need to use options something like
this when configuring OpenSSL:

.. code-block:: console

    $ ./config -Wl,--version-script=openssl.ld -Wl,-Bsymbolic-functions -fPIC shared

You'll also need to generate your own ``openssl.ld`` file. For example::

    OPENSSL_1.1.0E_CUSTOM {
        global:
            *;
    };

You should replace the version string on the first line as appropriate for your
build.

Static Wheels
~~~~~~~~~~~~~

Cryptography ships statically-linked wheels for macOS, Windows, and Linux (via
``manylinux``). This allows compatible environments to use the most recent
OpenSSL, regardless of what is shipped by default on those platforms. Some
Linux distributions (most notably Alpine) are not ``manylinux`` compatible so
we cannot distribute wheels for them.

However, you can build your own statically-linked wheels that will work on your
own systems. This will allow you to continue to use relatively old Linux
distributions (such as LTS releases), while making sure you have the most
recent OpenSSL available to your Python programs.

To do so, you should find yourself a machine that is as similar as possible to
your target environment (e.g. your production environment): for example, spin
up a new cloud server running your target Linux distribution. On this machine,
install the Cryptography dependencies as mentioned in :ref:`build-on-linux`.
Please also make sure you have `virtualenv`_ installed: this should be
available from your system package manager.

Then, paste the following into a shell script. You'll need to populate the
``OPENSSL_VERSION`` variable. To do that, visit `openssl.org`_ and find the
latest non-FIPS release version number, then set the string appropriately. For
example, for OpenSSL 1.0.2k, use ``OPENSSL_VERSION="1.0.2k"``.

When this shell script is complete, you'll find a collection of wheel files in
a directory called ``wheelhouse``. These wheels can be installed by a
sufficiently-recent version of ``pip``. The Cryptography wheel in this
directory contains a statically-linked OpenSSL binding, which ensures that you
have access to the most-recent OpenSSL releases without corrupting your system
dependencies.

.. code-block:: console

    set -e

    OPENSSL_VERSION="VERSIONGOESHERE"
    CWD=$(pwd)

    virtualenv env
    . env/bin/activate
    pip install -U setuptools
    pip install -U wheel pip
    curl -O https://www.openssl.org/source/openssl-${OPENSSL_VERSION}.tar.gz
    tar xvf openssl-${OPENSSL_VERSION}.tar.gz
    cd openssl-${OPENSSL_VERSION}
    ./config no-shared no-ssl2 no-ssl3 -fPIC --prefix=${CWD}/openssl
    make && make install
    cd ..
    CFLAGS="-I${CWD}/openssl/include" LDFLAGS="-L${CWD}/openssl/lib" pip wheel --no-binary :all: cryptography

Building cryptography on macOS
------------------------------

.. note::

    If installation gives a ``fatal error: 'openssl/aes.h' file not found``
    see the :doc:`FAQ </faq>` for information about how to fix this issue.

The wheel package on macOS is a statically linked build (as of 1.0.1) so for
users with pip 8 or above you only need one step:

.. code-block:: console

    $ pip install cryptography

If you want to build cryptography yourself or are on an older macOS version,
cryptography requires the presence of a C compiler, development headers, and
the proper libraries. On macOS much of this is provided by Apple's Xcode
development tools.  To install the Xcode command line tools (on macOS 10.10+)
open a terminal window and run:

.. code-block:: console

    $ xcode-select --install

This will install a compiler (clang) along with (most of) the required
development headers.

You'll also need OpenSSL, which you can obtain from `Homebrew`_ or `MacPorts`_.
Cryptography does **not** support Apple's deprecated OpenSSL distribution.

To build cryptography and dynamically link it:

`Homebrew`_

.. code-block:: console

    $ brew install openssl@1.1
    $ env LDFLAGS="-L$(brew --prefix openssl@1.1)/lib" CFLAGS="-I$(brew --prefix openssl@1.1)/include" pip install cryptography

`MacPorts`_:

.. code-block:: console

    $ sudo port install openssl
    $ env LDFLAGS="-L/opt/local/lib" CFLAGS="-I/opt/local/include" pip install cryptography

You can also build cryptography statically:

`Homebrew`_

.. code-block:: console

    $ brew install openssl@1.1
    $ env CRYPTOGRAPHY_SUPPRESS_LINK_FLAGS=1 LDFLAGS="$(brew --prefix openssl@1.1)/lib/libssl.a $(brew --prefix openssl@1.1)/lib/libcrypto.a" CFLAGS="-I$(brew --prefix openssl@1.1)/include" pip install cryptography

`MacPorts`_:

.. code-block:: console

    $ sudo port install openssl
    $ env CRYPTOGRAPHY_SUPPRESS_LINK_FLAGS=1 LDFLAGS="/opt/local/lib/libssl.a /opt/local/lib/libcrypto.a" CFLAGS="-I/opt/local/include" pip install cryptography

If you need to rebuild ``cryptography`` for any reason be sure to clear the
local `wheel cache`_.


.. _`Homebrew`: https://brew.sh
.. _`MacPorts`: https://www.macports.org
.. _`a binary distribution`: https://wiki.openssl.org/index.php/Binaries
.. _virtualenv: https://virtualenv.pypa.io/en/latest/
.. _openssl.org: https://www.openssl.org/source/
.. _`wheel cache`: https://pip.pypa.io/en/stable/reference/pip_install/#caching
Known security limitations
==========================

Secure memory wiping
--------------------

`Memory wiping`_ is used to protect secret data or key material from attackers
with access to deallocated memory. This is a defense-in-depth measure against
vulnerabilities that leak application memory.

Many ``cryptography`` APIs which accept ``bytes`` also accept types which
implement the buffer interface. Thus, users wishing to do so can pass
``memoryview`` or another mutable type to ``cryptography`` APIs, and overwrite
the contents once the data is no longer needed.

However, ``cryptography`` does not clear memory by default, as there is no way
to clear immutable structures such as ``bytes``. As a result, ``cryptography``,
like almost all software in Python is potentially vulnerable to this attack. The
`CERT secure coding guidelines`_ assesses this issue as "Severity: medium,
Likelihood: unlikely, Remediation Cost: expensive to repair" and we do not
consider this a high risk for most users.

RSA PKCS1 v1.5 constant time decryption
---------------------------------------

RSA decryption has several different modes, one of which is PKCS1 v1.5. When
used in online contexts, a secure protocol implementation requires that peers
not be able to tell whether RSA PKCS1 v1.5 decryption failed or succeeded,
even by timing variability.

``cryptography`` does not provide an API that makes this possible, due to the
fact that RSA decryption raises an exception on failure, which takes a
different amount of time than returning a value in the success case.

For this reason, at present, we recommend not implementing online protocols
that use RSA PKCS1 v1.5 decryption with ``cryptography`` -- independent of this
limitation, such protocols generally have poor security properties due to their
lack of forward security.

If a constant time RSA PKCS1 v1.5 decryption API is truly required, you should
contribute one to ``cryptography``.

.. _`Memory wiping`:  https://devblogs.microsoft.com/oldnewthing/?p=4223
.. _`CERT secure coding guidelines`: https://wiki.sei.cmu.edu/confluence/display/c/MEM03-C.+Clear+sensitive+information+stored+in+reusable+resources
Random number generation
========================

When generating random data for use in cryptographic operations, such as an
initialization vector for encryption in
:class:`~cryptography.hazmat.primitives.ciphers.modes.CBC` mode, you do not
want to use the standard :mod:`random` module APIs. This is because they do not
provide a cryptographically secure random number generator, which can result in
major security issues depending on the algorithms in use.

Therefore, it is our recommendation to `always use your operating system's
provided random number generator`_, which is available as :func:`os.urandom`.
For example, if you need 16 bytes of random data for an initialization vector,
you can obtain them with:

.. doctest::

    >>> import os
    >>> iv = os.urandom(16)

This will use ``/dev/urandom`` on UNIX platforms, and ``CryptGenRandom`` on
Windows.

If you need your random number as an integer (for example, for
:meth:`~cryptography.x509.CertificateBuilder.serial_number`), you can use
``int.from_bytes`` to convert the result of ``os.urandom``:

.. code-block:: pycon

    >>> serial = int.from_bytes(os.urandom(20), byteorder="big")

Starting with Python 3.6 the `standard library includes`_ the ``secrets``
module, which can be used for generating cryptographically secure random
numbers, with specific helpers for text-based formats.

.. _`always use your operating system's provided random number generator`: https://sockpuppet.org/blog/2014/02/25/safely-generate-random-numbers/
.. _`standard library includes`: https://docs.python.org/3/library/secrets.html
Security
========

We take the security of ``cryptography`` seriously. The following are a set of
policies we have adopted to ensure that security issues are addressed in a
timely fashion.

Infrastructure
--------------

In addition to ``cryptography``'s code, we're also concerned with the security
of the infrastructure we run (primarily ``cryptography.io``).  If you discover
a security vulnerability in our infrastructure, we ask you to report it using
the same procedure.

What is a security issue?
-------------------------

Anytime it's possible to write code using ``cryptography``'s public API which
does not provide the guarantees that a reasonable developer would expect it to
based on our documentation.

That's a bit academic, but basically it means the scope of what we consider a
vulnerability is broad, and we do not require a proof of concept or even a
specific exploit, merely a reasonable threat model under which ``cryptography``
could be attacked.

To give a few examples of things we would consider security issues:

* If a recipe, such as Fernet, made it easy for a user to bypass
  confidentiality or integrity with the public API (e.g. if the API let a user
  reuse nonces).
* If, under any circumstances, we used a CSPRNG which wasn't fork-safe.
* If ``cryptography`` used an API in an underlying C library and failed to
  handle error conditions safely.

Examples of things we wouldn't consider security issues:

* Offering ECB mode for symmetric encryption in the *Hazmat* layer. Though ECB
  is critically weak, it is documented as being weak in our documentation.
* Using a variable time comparison somewhere, if it's not possible to
  articulate any particular program in which this would result in problematic
  information disclosure.

In general, if you're unsure, we request that you to default to treating things
as security issues and handling them sensitively, the worst thing that can
happen is that we'll ask you to file a public issue.

Reporting a security issue
--------------------------

We ask that you do not report security issues to our normal GitHub issue
tracker.

If you believe you've identified a security issue with ``cryptography``, please
report it to ``alex.gaynor@gmail.com`` and/or ``paul.l.kehrer@gmail.com``. You
should verify that your MTA uses TLS to ensure the confidentiality of your
message.

Once you've submitted an issue via email, you should receive an acknowledgment
within 48 hours, and depending on the action to be taken, you may receive
further follow-up emails.

Supported Versions
------------------

At any given time, we will provide security support for the `master`_ branch
as well as the most recent release.

New releases for OpenSSL updates
--------------------------------

As of versions 0.5, 1.0.1, and 2.0.0, ``cryptography`` statically links OpenSSL
in binary distributions for Windows, macOS, and Linux respectively, to ease
installation. Due to this, ``cryptography`` will release a new version whenever
OpenSSL has a security or bug fix release to avoid shipping insecure software.

Like all our other releases, this will be announced on the mailing list and we
strongly recommend that you upgrade as soon as possible.

Disclosure Process
------------------

When we become aware of a security bug in ``cryptography``, we will endeavor to
fix it and issue a release as quickly as possible. We will generally issue a new
release for any security issue.

The steps for issuing a security release are described in our
:doc:`/doing-a-release` documentation.


.. _`master`: https://github.com/pyca/cryptography
Certificate Transparency
========================

.. currentmodule:: cryptography.x509.certificate_transparency

`Certificate Transparency`_ is a set of protocols specified in :rfc:`6962`
which allow X.509 certificates to be sent to append-only logs and have small
cryptographic proofs that a certificate has been publicly logged. This allows
for external auditing of the certificates that a certificate authority has
issued.

.. class:: SignedCertificateTimestamp

    .. versionadded:: 2.0

    SignedCertificateTimestamps (SCTs) are small cryptographically signed
    assertions that the specified certificate has been submitted to a
    Certificate Transparency Log, and that it will be part of the public log
    within some time period, this is called the "maximum merge delay" (MMD) and
    each log specifies its own.

    .. attribute:: version

        :type: :class:`~cryptography.x509.certificate_transparency.Version`

        The SCT version as an enumeration. Currently only one version has been
        specified.

    .. attribute:: log_id

        :type: bytes

        An opaque identifier, indicating which log this SCT is from. This is
        the SHA256 hash of the log's public key.

    .. attribute:: timestamp

        :type: :class:`datetime.datetime`

        A naïve datetime representing the time in UTC at which the log asserts
        the certificate had been submitted to it.

    .. attribute:: entry_type

        :type:
            :class:`~cryptography.x509.certificate_transparency.LogEntryType`

        The type of submission to the log that this SCT is for. Log submissions
        can either be certificates themselves or "pre-certificates" which
        indicate a binding-intent to issue a certificate for the same data,
        with SCTs embedded in it.


.. class:: Version

    .. versionadded:: 2.0

    An enumeration for SignedCertificateTimestamp versions.

    .. attribute:: v1

        For version 1 SignedCertificateTimestamps.

.. class:: LogEntryType

    .. versionadded:: 2.0

    An enumeration for SignedCertificateTimestamp log entry types.

    .. attribute:: X509_CERTIFICATE

        For SCTs corresponding to X.509 certificates.

    .. attribute:: PRE_CERTIFICATE

        For SCTs corresponding to pre-certificates.


.. _`Certificate Transparency`: https://www.certificate-transparency.org/
X.509
=====

X.509 is an ITU-T standard for a `public key infrastructure`_. X.509v3 is
defined in :rfc:`5280` (which obsoletes :rfc:`2459` and :rfc:`3280`). X.509
certificates are commonly used in protocols like `TLS`_.

.. toctree::
    :maxdepth: 2

    tutorial
    certificate-transparency
    ocsp
    reference

.. _`public key infrastructure`: https://en.wikipedia.org/wiki/Public_key_infrastructure
.. _`TLS`: https://en.wikipedia.org/wiki/Transport_Layer_Security
OCSP
====

.. currentmodule:: cryptography.x509.ocsp

.. testsetup::

    import base64
    pem_cert = b"""
    -----BEGIN CERTIFICATE-----
    MIIFvTCCBKWgAwIBAgICPyAwDQYJKoZIhvcNAQELBQAwRzELMAkGA1UEBhMCVVMx
    FjAUBgNVBAoTDUdlb1RydXN0IEluYy4xIDAeBgNVBAMTF1JhcGlkU1NMIFNIQTI1
    NiBDQSAtIEczMB4XDTE0MTAxNTEyMDkzMloXDTE4MTExNjAxMTUwM1owgZcxEzAR
    BgNVBAsTCkdUNDg3NDI5NjUxMTAvBgNVBAsTKFNlZSB3d3cucmFwaWRzc2wuY29t
    L3Jlc291cmNlcy9jcHMgKGMpMTQxLzAtBgNVBAsTJkRvbWFpbiBDb250cm9sIFZh
    bGlkYXRlZCAtIFJhcGlkU1NMKFIpMRwwGgYDVQQDExN3d3cuY3J5cHRvZ3JhcGh5
    LmlvMIICIjANBgkqhkiG9w0BAQEFAAOCAg8AMIICCgKCAgEAom/FebKJIot7Sp3s
    itG1sicpe3thCssjI+g1JDAS7I3GLVNmbms1DOdIIqwf01gZkzzXBN2+9sOnyRaR
    PPfCe1jTr3dk2y6rPE559vPa1nZQkhlzlhMhlPyjaT+S7g4Tio4qV2sCBZU01DZJ
    CaksfohN+5BNVWoJzTbOcrHOEJ+M8B484KlBCiSxqf9cyNQKru4W3bHaCVNVJ8eu
    6i6KyhzLa0L7yK3LXwwXVs583C0/vwFhccGWsFODqD/9xHUzsBIshE8HKjdjDi7Y
    3BFQzVUQFjBB50NSZfAA/jcdt1blxJouc7z9T8Oklh+V5DDBowgAsrT4b6Z2Fq6/
    r7D1GqivLK/ypUQmxq2WXWAUBb/Q6xHgxASxI4Br+CByIUQJsm8L2jzc7k+mF4hW
    ltAIUkbo8fGiVnat0505YJgxWEDKOLc4Gda6d/7GVd5AvKrz242bUqeaWo6e4MTx
    diku2Ma3rhdcr044Qvfh9hGyjqNjvhWY/I+VRWgihU7JrYvgwFdJqsQ5eiKT4OHi
    gsejvWwkZzDtiQ+aQTrzM1FsY2swJBJsLSX4ofohlVRlIJCn/ME+XErj553431Lu
    YQ5SzMd3nXzN78Vj6qzTfMUUY72UoT1/AcFiUMobgIqrrmwuNxfrkbVE2b6Bga74
    FsJX63prvrJ41kuHK/16RQBM7fcCAwEAAaOCAWAwggFcMB8GA1UdIwQYMBaAFMOc
    8/zTRgg0u85Gf6B8W/PiCMtZMFcGCCsGAQUFBwEBBEswSTAfBggrBgEFBQcwAYYT
    aHR0cDovL2d2LnN5bWNkLmNvbTAmBggrBgEFBQcwAoYaaHR0cDovL2d2LnN5bWNi
    LmNvbS9ndi5jcnQwDgYDVR0PAQH/BAQDAgWgMB0GA1UdJQQWMBQGCCsGAQUFBwMB
    BggrBgEFBQcDAjAvBgNVHREEKDAmghN3d3cuY3J5cHRvZ3JhcGh5Lmlvgg9jcnlw
    dG9ncmFwaHkuaW8wKwYDVR0fBCQwIjAgoB6gHIYaaHR0cDovL2d2LnN5bWNiLmNv
    bS9ndi5jcmwwDAYDVR0TAQH/BAIwADBFBgNVHSAEPjA8MDoGCmCGSAGG+EUBBzYw
    LDAqBggrBgEFBQcCARYeaHR0cHM6Ly93d3cucmFwaWRzc2wuY29tL2xlZ2FsMA0G
    CSqGSIb3DQEBCwUAA4IBAQAzIYO2jx7h17FBT74tJ2zbV9OKqGb7QF8y3wUtP4xc
    dH80vprI/Cfji8s86kr77aAvAqjDjaVjHn7UzebhSUivvRPmfzRgyWBacomnXTSt
    Xlt2dp2nDQuwGyK2vB7dMfKnQAkxwq1sYUXznB8i0IhhCAoXp01QGPKq51YoIlnF
    7DRMk6iEaL1SJbkIrLsCQyZFDf0xtfW9DqXugMMLoxeCsBhZJQzNyS2ryirrv9LH
    aK3+6IZjrcyy9bkpz/gzJucyhU+75c4My/mnRCrtItRbCQuiI5pd5poDowm+HH9i
    GVI9+0lAFwxOUnOnwsoI40iOoxjLMGB+CgFLKCGUcWxP
    -----END CERTIFICATE-----
    """
    pem_issuer = b"""
    -----BEGIN CERTIFICATE-----
    MIIEJTCCAw2gAwIBAgIDAjp3MA0GCSqGSIb3DQEBCwUAMEIxCzAJBgNVBAYTAlVT
    MRYwFAYDVQQKEw1HZW9UcnVzdCBJbmMuMRswGQYDVQQDExJHZW9UcnVzdCBHbG9i
    YWwgQ0EwHhcNMTQwODI5MjEzOTMyWhcNMjIwNTIwMjEzOTMyWjBHMQswCQYDVQQG
    EwJVUzEWMBQGA1UEChMNR2VvVHJ1c3QgSW5jLjEgMB4GA1UEAxMXUmFwaWRTU0wg
    U0hBMjU2IENBIC0gRzMwggEiMA0GCSqGSIb3DQEBAQUAA4IBDwAwggEKAoIBAQCv
    VJvZWF0eLFbG1eh/9H0WA//Qi1rkjqfdVC7UBMBdmJyNkA+8EGVf2prWRHzAn7Xp
    SowLBkMEu/SW4ib2YQGRZjEiwzQ0Xz8/kS9EX9zHFLYDn4ZLDqP/oIACg8PTH2lS
    1p1kD8mD5xvEcKyU58Okaiy9uJ5p2L4KjxZjWmhxgHsw3hUEv8zTvz5IBVV6s9cQ
    DAP8m/0Ip4yM26eO8R5j3LMBL3+vV8M8SKeDaCGnL+enP/C1DPz1hNFTvA5yT2AM
    QriYrRmIV9cE7Ie/fodOoyH5U/02mEiN1vi7SPIpyGTRzFRIU4uvt2UevykzKdkp
    YEj4/5G8V1jlNS67abZZAgMBAAGjggEdMIIBGTAfBgNVHSMEGDAWgBTAephojYn7
    qwVkDBF9qn1luMrMTjAdBgNVHQ4EFgQUw5zz/NNGCDS7zkZ/oHxb8+IIy1kwEgYD
    VR0TAQH/BAgwBgEB/wIBADAOBgNVHQ8BAf8EBAMCAQYwNQYDVR0fBC4wLDAqoCig
    JoYkaHR0cDovL2cuc3ltY2IuY29tL2NybHMvZ3RnbG9iYWwuY3JsMC4GCCsGAQUF
    BwEBBCIwIDAeBggrBgEFBQcwAYYSaHR0cDovL2cuc3ltY2QuY29tMEwGA1UdIARF
    MEMwQQYKYIZIAYb4RQEHNjAzMDEGCCsGAQUFBwIBFiVodHRwOi8vd3d3Lmdlb3Ry
    dXN0LmNvbS9yZXNvdXJjZXMvY3BzMA0GCSqGSIb3DQEBCwUAA4IBAQCjWB7GQzKs
    rC+TeLfqrlRARy1+eI1Q9vhmrNZPc9ZE768LzFvB9E+aj0l+YK/CJ8cW8fuTgZCp
    fO9vfm5FlBaEvexJ8cQO9K8EWYOHDyw7l8NaEpt7BDV7o5UzCHuTcSJCs6nZb0+B
    kvwHtnm8hEqddwnxxYny8LScVKoSew26T++TGezvfU5ho452nFnPjJSxhJf3GrkH
    uLLGTxN5279PURt/aQ1RKsHWFf83UTRlUfQevjhq7A6rvz17OQV79PP7GqHQyH5O
    ZI3NjGFVkP46yl0lD/gdo0p0Vk8aVUBwdSWmMy66S6VdU5oNMOGNX2Esr8zvsJmh
    gP8L8mJMcCaY
    -----END CERTIFICATE-----
    """
    pem_responder_cert = b"""
    -----BEGIN CERTIFICATE-----
    MIIBPjCB5KADAgECAgQHW80VMAoGCCqGSM49BAMCMCcxCzAJBgNVBAYTAlVTMRgw
    FgYDVQQDDA9DcnlwdG9ncmFwaHkgQ0EwHhcNMTgxMDA3MTIzNTEwWhcNMjgxMDA0
    MTIzNTEwWjAnMQswCQYDVQQGEwJVUzEYMBYGA1UEAwwPQ3J5cHRvZ3JhcGh5IENB
    MFkwEwYHKoZIzj0CAQYIKoZIzj0DAQcDQgAEbQ2E0N/E3R0zEG+qa+yAFXBY6Fte
    QzyvFdq7EZHDktlyUllaVJBrbX1ItV0MlayFwwQPhZmuLPpQBzuVKyrUfTAKBggq
    hkjOPQQDAgNJADBGAiEAo0NQRmfPvhWQpSvJzV+2Ag441Zeckk+bib7swduQIjIC
    IQCqYD9pArB2SWfmhQCSZkNEATlsPIML8lvlSkbNcrmrqQ==
    -----END CERTIFICATE-----
    """
    pem_responder_key = b"""
    -----BEGIN PRIVATE KEY-----
    MIGHAgEAMBMGByqGSM49AgEGCCqGSM49AwEHBG0wawIBAQQgO+vsRu8xDIVZE+xh
    s8ESqJqcpJlwmj8CtF8HPHxrDSGhRANCAARtDYTQ38TdHTMQb6pr7IAVcFjoW15D
    PK8V2rsRkcOS2XJSWVpUkGttfUi1XQyVrIXDBA+Fma4s+lAHO5UrKtR9
    -----END PRIVATE KEY-----
    """
    der_ocsp_req = (
        b"0V0T0R0P0N0\t\x06\x05+\x0e\x03\x02\x1a\x05\x00\x04\x148\xcaF\x8c"
        b"\x07D\x8d\xf4\x81\x96\xc7mmLpQ\x9e`\xa7\xbd\x04\x14yu\xbb\x84:\xcb"
        b",\xdez\t\xbe1\x1bC\xbc\x1c*MSX\x02\x15\x00\x98\xd9\xe5\xc0\xb4\xc3"
        b"sU-\xf7|]\x0f\x1e\xb5\x12\x8eIE\xf9"
    )
    der_ocsp_resp_unauth = b"0\x03\n\x01\x06"

OCSP (Online Certificate Status Protocol) is a method of checking the
revocation status of certificates. It is specified in :rfc:`6960`, as well
as other obsoleted RFCs.


Loading Requests
~~~~~~~~~~~~~~~~

.. function:: load_der_ocsp_request(data)

    .. versionadded:: 2.4

    Deserialize an OCSP request from DER encoded data.

    :param bytes data: The DER encoded OCSP request data.

    :returns: An instance of :class:`~cryptography.x509.ocsp.OCSPRequest`.

    .. doctest::

        >>> from cryptography.x509 import ocsp
        >>> ocsp_req = ocsp.load_der_ocsp_request(der_ocsp_req)
        >>> print(ocsp_req.serial_number)
        872625873161273451176241581705670534707360122361


Creating Requests
~~~~~~~~~~~~~~~~~

.. class:: OCSPRequestBuilder

    .. versionadded:: 2.4

    This class is used to create :class:`~cryptography.x509.ocsp.OCSPRequest`
    objects.


    .. method:: add_certificate(cert, issuer, algorithm)

        Adds a request using a certificate, issuer certificate, and hash
        algorithm. This can only be called once.

        :param cert: The :class:`~cryptography.x509.Certificate` whose validity
            is being checked.

        :param issuer: The issuer :class:`~cryptography.x509.Certificate` of
            the certificate that is being checked.

        :param algorithm: A
            :class:`~cryptography.hazmat.primitives.hashes.HashAlgorithm`
            instance. For OCSP only
            :class:`~cryptography.hazmat.primitives.hashes.SHA1`,
            :class:`~cryptography.hazmat.primitives.hashes.SHA224`,
            :class:`~cryptography.hazmat.primitives.hashes.SHA256`,
            :class:`~cryptography.hazmat.primitives.hashes.SHA384`, and
            :class:`~cryptography.hazmat.primitives.hashes.SHA512` are allowed.

    .. method:: add_extension(extension, critical)

        Adds an extension to the request.

        :param extension: An extension conforming to the
            :class:`~cryptography.x509.ExtensionType` interface.

        :param critical: Set to ``True`` if the extension must be understood and
             handled.

    .. method:: build()

        :returns: A new :class:`~cryptography.x509.ocsp.OCSPRequest`.

    .. doctest::

        >>> from cryptography.hazmat.primitives import serialization
        >>> from cryptography.hazmat.primitives.hashes import SHA1
        >>> from cryptography.x509 import load_pem_x509_certificate, ocsp
        >>> cert = load_pem_x509_certificate(pem_cert)
        >>> issuer = load_pem_x509_certificate(pem_issuer)
        >>> builder = ocsp.OCSPRequestBuilder()
        >>> # SHA1 is in this example because RFC 5019 mandates its use.
        >>> builder = builder.add_certificate(cert, issuer, SHA1())
        >>> req = builder.build()
        >>> base64.b64encode(req.public_bytes(serialization.Encoding.DER))
        b'MEMwQTA/MD0wOzAJBgUrDgMCGgUABBRAC0Z68eay0wmDug1gfn5ZN0gkxAQUw5zz/NNGCDS7zkZ/oHxb8+IIy1kCAj8g'

Loading Responses
~~~~~~~~~~~~~~~~~

.. function:: load_der_ocsp_response(data)

    .. versionadded:: 2.4

    Deserialize an OCSP response from DER encoded data.

    :param bytes data: The DER encoded OCSP response data.

    :returns: An instance of :class:`~cryptography.x509.ocsp.OCSPResponse`.

    .. doctest::

        >>> from cryptography.x509 import ocsp
        >>> ocsp_resp = ocsp.load_der_ocsp_response(der_ocsp_resp_unauth)
        >>> print(ocsp_resp.response_status)
        OCSPResponseStatus.UNAUTHORIZED


Creating Responses
~~~~~~~~~~~~~~~~~~

.. class:: OCSPResponseBuilder

    .. versionadded:: 2.4

    This class is used to create :class:`~cryptography.x509.ocsp.OCSPResponse`
    objects. You cannot set ``produced_at`` on OCSP responses at this time.
    Instead the field is set to current UTC time when calling ``sign``. For
    unsuccessful statuses call the class method
    :meth:`~cryptography.x509.ocsp.OCSPResponseBuilder.build_unsuccessful`.

    .. method:: add_response(cert, issuer, algorithm, cert_status, this_update, next_update, revocation_time, revocation_reason)

        This method adds status information about the certificate that was
        requested to the response.

        :param cert: The :class:`~cryptography.x509.Certificate` whose validity
            is being checked.

        :param issuer: The issuer :class:`~cryptography.x509.Certificate` of
            the certificate that is being checked.

        :param algorithm: A
            :class:`~cryptography.hazmat.primitives.hashes.HashAlgorithm`
            instance. For OCSP only
            :class:`~cryptography.hazmat.primitives.hashes.SHA1`,
            :class:`~cryptography.hazmat.primitives.hashes.SHA224`,
            :class:`~cryptography.hazmat.primitives.hashes.SHA256`,
            :class:`~cryptography.hazmat.primitives.hashes.SHA384`, and
            :class:`~cryptography.hazmat.primitives.hashes.SHA512` are allowed.

        :param cert_status: An item from the
            :class:`~cryptography.x509.ocsp.OCSPCertStatus` enumeration.

        :param this_update: A naïve :class:`datetime.datetime` object
            representing the most recent time in UTC at which the status being
            indicated is known by the responder to be correct.

        :param next_update: A naïve :class:`datetime.datetime` object or
            ``None``. The time in UTC at or before which newer information will
            be available about the status of the certificate.

        :param revocation_time: A naïve :class:`datetime.datetime` object or
            ``None`` if the ``cert`` is not revoked. The time in UTC at which
            the certificate was revoked.

        :param revocation_reason: An item from the
            :class:`~cryptography.x509.ReasonFlags` enumeration or ``None`` if
            the ``cert`` is not revoked.

    .. method:: certificates(certs)

        Add additional certificates that should be used to verify the
        signature on the response. This is typically used when the responder
        utilizes an OCSP delegate.

        :param list certs: A list of :class:`~cryptography.x509.Certificate`
            objects.

    .. method:: responder_id(encoding, responder_cert)

        Set the ``responderID`` on the OCSP response. This is the data a
        client will use to determine what certificate signed the response.

        :param responder_cert: The :class:`~cryptography.x509.Certificate`
            object for the certificate whose private key will sign the
            OCSP response. If the certificate and key do not match an
            error will be raised when calling ``sign``.
        :param encoding: Either
            :attr:`~cryptography.x509.ocsp.OCSPResponderEncoding.HASH` or
            :attr:`~cryptography.x509.ocsp.OCSPResponderEncoding.NAME`.

    .. method:: add_extension(extension, critical)

        Adds an extension to the response.

        :param extension: An extension conforming to the
            :class:`~cryptography.x509.ExtensionType` interface.

        :param critical: Set to ``True`` if the extension must be understood and
             handled.

    .. method:: sign(private_key, algorithm)

        Creates the OCSP response that can then be serialized and sent to
        clients. This method will create a
        :attr:`~cryptography.x509.ocsp.OCSPResponseStatus.SUCCESSFUL` response.

        :param private_key: The
            :class:`~cryptography.hazmat.primitives.asymmetric.rsa.RSAPrivateKey`,
            :class:`~cryptography.hazmat.primitives.asymmetric.dsa.DSAPrivateKey`,
            :class:`~cryptography.hazmat.primitives.asymmetric.ec.EllipticCurvePrivateKey`,
            :class:`~cryptography.hazmat.primitives.asymmetric.ed25519.Ed25519PrivateKey` or
            :class:`~cryptography.hazmat.primitives.asymmetric.ed448.Ed448PrivateKey`
            that will be used to sign the certificate.

        :param algorithm: The
            :class:`~cryptography.hazmat.primitives.hashes.HashAlgorithm` that
            will be used to generate the signature.  This must be ``None`` if
            the ``private_key`` is an
            :class:`~cryptography.hazmat.primitives.asymmetric.ed25519.Ed25519PrivateKey`
            or an
            :class:`~cryptography.hazmat.primitives.asymmetric.ed448.Ed448PrivateKey`
            and an instance of a
            :class:`~cryptography.hazmat.primitives.hashes.HashAlgorithm`
            otherwise.

        :returns: A new :class:`~cryptography.x509.ocsp.OCSPResponse`.

    .. doctest::

        >>> import datetime
        >>> from cryptography.hazmat.primitives import hashes, serialization
        >>> from cryptography.x509 import load_pem_x509_certificate, ocsp
        >>> cert = load_pem_x509_certificate(pem_cert)
        >>> issuer = load_pem_x509_certificate(pem_issuer)
        >>> responder_cert = load_pem_x509_certificate(pem_responder_cert)
        >>> responder_key = serialization.load_pem_private_key(pem_responder_key, None)
        >>> builder = ocsp.OCSPResponseBuilder()
        >>> # SHA1 is in this example because RFC 5019 mandates its use.
        >>> builder = builder.add_response(
        ...     cert=cert, issuer=issuer, algorithm=hashes.SHA1(),
        ...     cert_status=ocsp.OCSPCertStatus.GOOD,
        ...     this_update=datetime.datetime.now(),
        ...     next_update=datetime.datetime.now(),
        ...     revocation_time=None, revocation_reason=None
        ... ).responder_id(
        ...     ocsp.OCSPResponderEncoding.HASH, responder_cert
        ... )
        >>> response = builder.sign(responder_key, hashes.SHA256())
        >>> response.certificate_status
        <OCSPCertStatus.GOOD: 0>

    .. classmethod:: build_unsuccessful(response_status)

        Creates an unsigned OCSP response which can then be serialized and
        sent to clients. ``build_unsuccessful`` may only be called with a
        :class:`~cryptography.x509.ocsp.OCSPResponseStatus` that is not
        ``SUCCESSFUL``. Since this is a class method note that no other
        methods can or should be called as unsuccessful statuses do not
        encode additional data.

        :returns: A new :class:`~cryptography.x509.ocsp.OCSPResponse`.

    .. doctest::

        >>> from cryptography.hazmat.primitives import hashes, serialization
        >>> from cryptography.x509 import load_pem_x509_certificate, ocsp
        >>> response = ocsp.OCSPResponseBuilder.build_unsuccessful(
        ...     ocsp.OCSPResponseStatus.UNAUTHORIZED
        ... )
        >>> response.response_status
        <OCSPResponseStatus.UNAUTHORIZED: 6>


Interfaces
~~~~~~~~~~

.. class:: OCSPRequest

    .. versionadded:: 2.4

    An ``OCSPRequest`` is an object containing information about a certificate
    whose status is being checked.

    .. attribute:: issuer_key_hash

        :type: bytes

        The hash of the certificate issuer's key. The hash algorithm used
        is defined by the ``hash_algorithm`` property.

    .. attribute:: issuer_name_hash

        :type: bytes

        The hash of the certificate issuer's name. The hash algorithm used
        is defined by the ``hash_algorithm`` property.

    .. attribute:: hash_algorithm

        :type: :class:`~cryptography.hazmat.primitives.hashes.HashAlgorithm`

        The algorithm used to generate the ``issuer_key_hash`` and
        ``issuer_name_hash``.

    .. attribute:: serial_number

        :type: int

        The serial number of the certificate to check.

    .. attribute:: extensions

        :type: :class:`~cryptography.x509.Extensions`

        The extensions encoded in the request.

    .. method:: public_bytes(encoding)

        :param encoding: The encoding to use. Only
            :attr:`~cryptography.hazmat.primitives.serialization.Encoding.DER`
            is supported.

        :return bytes: The serialized OCSP request.

.. class:: OCSPResponse

    .. versionadded:: 2.4

    An ``OCSPResponse`` is the data provided by an OCSP responder in response
    to an ``OCSPRequest``.

    .. attribute:: response_status

        :type: :class:`~cryptography.x509.ocsp.OCSPResponseStatus`

        The status of the response.

    .. attribute:: signature_algorithm_oid

        :type: :class:`~cryptography.x509.ObjectIdentifier`

        Returns the object identifier of the signature algorithm used
        to sign the response. This will be one of the OIDs from
        :class:`~cryptography.x509.oid.SignatureAlgorithmOID`.

        :raises ValueError: If ``response_status`` is not
            :class:`~cryptography.x509.ocsp.OCSPResponseStatus.SUCCESSFUL`.

    .. attribute:: signature_hash_algorithm

        .. versionadded:: 2.5

        :type: :class:`~cryptography.hazmat.primitives.hashes.HashAlgorithm`

        Returns the
        :class:`~cryptography.hazmat.primitives.hashes.HashAlgorithm` which
        was used in signing this response.  Can be ``None`` if signature
        did not use separate hash
        (:attr:`~cryptography.x509.oid.SignatureAlgorithmOID.ED25519`,
        :attr:`~cryptography.x509.oid.SignatureAlgorithmOID.ED448`).

    .. attribute:: signature

        :type: bytes

        The signature bytes.

        :raises ValueError: If ``response_status`` is not
            :class:`~cryptography.x509.ocsp.OCSPResponseStatus.SUCCESSFUL`.

    .. attribute:: tbs_response_bytes

        :type: bytes

        The DER encoded bytes payload that is hashed and then signed. This
        data may be used to validate the signature on the OCSP response.

        :raises ValueError: If ``response_status`` is not
            :class:`~cryptography.x509.ocsp.OCSPResponseStatus.SUCCESSFUL`.

    .. attribute:: certificates

        :type: list

        A list of zero or more :class:`~cryptography.x509.Certificate` objects
        used to help build a chain to verify the OCSP response. This situation
        occurs when the OCSP responder uses a delegate certificate.

        :raises ValueError: If ``response_status`` is not
            :class:`~cryptography.x509.ocsp.OCSPResponseStatus.SUCCESSFUL`.

    .. attribute:: responder_key_hash

        :type: bytes or None

        The responder's key hash or ``None`` if the response has a
        ``responder_name``.

        :raises ValueError: If ``response_status`` is not
            :class:`~cryptography.x509.ocsp.OCSPResponseStatus.SUCCESSFUL`.

    .. attribute:: responder_name

        :type: :class:`~cryptography.x509.Name` or None

        The responder's ``Name`` or ``None`` if the response has a
        ``responder_key_hash``.

        :raises ValueError: If ``response_status`` is not
            :class:`~cryptography.x509.ocsp.OCSPResponseStatus.SUCCESSFUL`.

    .. attribute:: produced_at

        :type: :class:`datetime.datetime`

        A naïve datetime representing the time when the response was produced.

        :raises ValueError: If ``response_status`` is not
            :class:`~cryptography.x509.ocsp.OCSPResponseStatus.SUCCESSFUL`.

    .. attribute:: certificate_status

        :type: :class:`~cryptography.x509.ocsp.OCSPCertStatus`

        The status of the certificate being checked.

        :raises ValueError: If ``response_status`` is not
            :class:`~cryptography.x509.ocsp.OCSPResponseStatus.SUCCESSFUL`.

    .. attribute:: revocation_time

        :type: :class:`datetime.datetime` or None

        A naïve datetime representing the time when the certificate was revoked
        or ``None`` if the certificate has not been revoked.

        :raises ValueError: If ``response_status`` is not
            :class:`~cryptography.x509.ocsp.OCSPResponseStatus.SUCCESSFUL`.

    .. attribute:: revocation_reason

        :type: :class:`~cryptography.x509.ReasonFlags` or None

        The reason the certificate was revoked or ``None`` if not specified or
        not revoked.

        :raises ValueError: If ``response_status`` is not
            :class:`~cryptography.x509.ocsp.OCSPResponseStatus.SUCCESSFUL`.

    .. attribute:: this_update

        :type: :class:`datetime.datetime`

        A naïve datetime representing the most recent time at which the status
        being indicated is known by the responder to have been correct.

        :raises ValueError: If ``response_status`` is not
            :class:`~cryptography.x509.ocsp.OCSPResponseStatus.SUCCESSFUL`.

    .. attribute:: next_update

        :type: :class:`datetime.datetime`

        A naïve datetime representing the time when newer information will
        be available.

        :raises ValueError: If ``response_status`` is not
            :class:`~cryptography.x509.ocsp.OCSPResponseStatus.SUCCESSFUL`.

    .. attribute:: issuer_key_hash

        :type: bytes

        The hash of the certificate issuer's key. The hash algorithm used
        is defined by the ``hash_algorithm`` property.

        :raises ValueError: If ``response_status`` is not
            :class:`~cryptography.x509.ocsp.OCSPResponseStatus.SUCCESSFUL`.

    .. attribute:: issuer_name_hash

        :type: bytes

        The hash of the certificate issuer's name. The hash algorithm used
        is defined by the ``hash_algorithm`` property.

        :raises ValueError: If ``response_status`` is not
            :class:`~cryptography.x509.ocsp.OCSPResponseStatus.SUCCESSFUL`.

    .. attribute:: hash_algorithm

        :type: :class:`~cryptography.hazmat.primitives.hashes.HashAlgorithm`

        The algorithm used to generate the ``issuer_key_hash`` and
        ``issuer_name_hash``.

        :raises ValueError: If ``response_status`` is not
            :class:`~cryptography.x509.ocsp.OCSPResponseStatus.SUCCESSFUL`.

    .. attribute:: serial_number

        :type: int

        The serial number of the certificate that was checked.

        :raises ValueError: If ``response_status`` is not
            :class:`~cryptography.x509.ocsp.OCSPResponseStatus.SUCCESSFUL`.

    .. attribute:: extensions

        :type: :class:`~cryptography.x509.Extensions`

        The extensions encoded in the response.

    .. attribute:: single_extensions

        .. versionadded:: 2.9

        :type: :class:`~cryptography.x509.Extensions`

        The single extensions encoded in the response.

    .. method:: public_bytes(encoding)

        :param encoding: The encoding to use. Only
            :attr:`~cryptography.hazmat.primitives.serialization.Encoding.DER`
            is supported.

        :return bytes: The serialized OCSP response.

.. class:: OCSPResponseStatus

    .. versionadded:: 2.4

    An enumeration of response statuses.

    .. attribute:: SUCCESSFUL

        Represents a successful OCSP response.

    .. attribute:: MALFORMED_REQUEST

        May be returned by an OCSP responder that is unable to parse a
        given request.

    .. attribute:: INTERNAL_ERROR

        May be returned by an OCSP responder that is currently experiencing
        operational problems.

    .. attribute:: TRY_LATER

        May be returned by an OCSP responder that is overloaded.

    .. attribute:: SIG_REQUIRED

        May be returned by an OCSP responder that requires signed OCSP
        requests.

    .. attribute:: UNAUTHORIZED

        May be returned by an OCSP responder when queried for a certificate for
        which the responder is unaware or an issuer for which the responder is
        not authoritative.


.. class:: OCSPCertStatus

    .. versionadded:: 2.4

    An enumeration of certificate statuses in an OCSP response.

    .. attribute:: GOOD

        The value for a certificate that is not revoked.

    .. attribute:: REVOKED

        The certificate being checked is revoked.

    .. attribute:: UNKNOWN

        The certificate being checked is not known to the OCSP responder.

.. class:: OCSPResponderEncoding

    .. versionadded:: 2.4

    An enumeration of ``responderID`` encodings that can be passed to
    :meth:`~cryptography.x509.ocsp.OCSPResponseBuilder.responder_id`.

    .. attribute:: HASH

        Encode the hash of the public key whose corresponding private key
        signed the response.

    .. attribute:: NAME

        Encode the X.509 ``Name`` of the certificate whose private key signed
        the response.
X.509 Reference
===============

.. currentmodule:: cryptography.x509

.. testsetup::

    pem_crl_data = b"""
    -----BEGIN X509 CRL-----
    MIIBtDCBnQIBAjANBgkqhkiG9w0BAQsFADAnMQswCQYDVQQGEwJVUzEYMBYGA1UE
    AwwPY3J5cHRvZ3JhcGh5LmlvGA8yMDE1MDEwMTAwMDAwMFoYDzIwMTYwMTAxMDAw
    MDAwWjA+MDwCAQAYDzIwMTUwMTAxMDAwMDAwWjAmMBgGA1UdGAQRGA8yMDE1MDEw
    MTAwMDAwMFowCgYDVR0VBAMKAQEwDQYJKoZIhvcNAQELBQADggEBABRA4ww50Lz5
    zk1j2+aluC4HPHqb7o06h4pTDcCGeXUKXIGeP5ntGGmIoxa26sNoLeOr8+5b43Gf
    yWraHertllOwaOpNFEe+YZFaE9femtoDbf+GLMvRx/0wDfd3KxPoXnXKMXb2d1w4
    RCLgmkYx6JyvS+5ciuLQVIKC+l7jwIUeZFLJMUJ8msM4pFYoGameeZmtjMbd/TNg
    cVBfmZxNMHuLladJxvSo2esARo0TYPhYsgrREKoHwhpzSxdynjn4bOVkILfguwsN
    qtEEMZFEv5Kb0GqRp2+Iagv2S6dg9JGvxVdsoGjaB6EbYSZ3Psx4aODasIn11uwo
    X4B9vUQNXqc=
    -----END X509 CRL-----
    """.strip()

    pem_req_data = b"""
    -----BEGIN CERTIFICATE REQUEST-----
    MIICcDCCAVgCAQAwDTELMAkGA1UEBhMCVVMwggEiMA0GCSqGSIb3DQEBAQUAA4IB
    DwAwggEKAoIBAQCb+ec0zYAYLzk/MDdDJYvzdvEO2ZUrBYM6z1r8NedwpJfxUWqC
    hvK1cpc9EbQeCwS1eooTIGoNveeCrwL+pWdmf1sh6gz7SsxdN/07nyhSM8M6Xkec
    +tGrjyi1H/N1afwWXox3WcvBNbxu3Df5RKLDb0yt9aqhmJylbl/tbvgJesXymwmp
    Rc1vXL0fOedUtuAJ3xQ15M0pgLF8qDn4lySJz25x76pMYPeN5/a7x+SR/jj81kep
    VaVpuh/2hePV5uwUX3uWoj5sAkrBCifi4NPge0Npd6KeKVvXytLOymH/4+WvV719
    wCO+MyrkhpdHSakJDTIaQIxsqVeVVKdPLAPJAgMBAAGgHjAcBgkqhkiG9w0BCQcx
    DwwNY2hhbGxlbmdlIG1lITANBgkqhkiG9w0BAQsFAAOCAQEAMmgeSa8szbjPFD/4
    vcPBr/vBEROFGgL8mX3o5pF9gpr7nRjhLKBkgJvlRm6Ma3Xvdfc/r5Hp2ZBTA7sZ
    ZYhyeezGfCQN/Qhda1v+sCwG58IjvGfCSS7Y5tGlEBQ4MDf0Q7PYPSxaNUEBH7vo
    +M7U+nFuNSmyWlt6SFBSkohZkWoVSGx3KsAO+SAHYZ7JtqsAS/dm7Dflp8KxeDg7
    wzGBDQRpGF4CpI1VQjGSJQXSEdD+J7mtvBEOD34abRfV6zOUGzOOo3NWE6wNpYgt
    0A7gVlzSYpdwqjBdvACfXR2r/mu+4KkAvYh8WwCiTcYgGjl2pT1bO4hEmcJ0RSWy
    /fGD8Q==
    -----END CERTIFICATE REQUEST-----
    """.strip()

    pem_data = b"""
    -----BEGIN CERTIFICATE-----
    MIIDfDCCAmSgAwIBAgIBAjANBgkqhkiG9w0BAQsFADBFMQswCQYDVQQGEwJVUzEf
    MB0GA1UEChMWVGVzdCBDZXJ0aWZpY2F0ZXMgMjAxMTEVMBMGA1UEAxMMVHJ1c3Qg
    QW5jaG9yMB4XDTEwMDEwMTA4MzAwMFoXDTMwMTIzMTA4MzAwMFowQDELMAkGA1UE
    BhMCVVMxHzAdBgNVBAoTFlRlc3QgQ2VydGlmaWNhdGVzIDIwMTExEDAOBgNVBAMT
    B0dvb2QgQ0EwggEiMA0GCSqGSIb3DQEBAQUAA4IBDwAwggEKAoIBAQCQWJpHYo37
    Xfb7oJSPe+WvfTlzIG21WQ7MyMbGtK/m8mejCzR6c+f/pJhEH/OcDSMsXq8h5kXa
    BGqWK+vSwD/Pzp5OYGptXmGPcthDtAwlrafkGOS4GqIJ8+k9XGKs+vQUXJKsOk47
    RuzD6PZupq4s16xaLVqYbUC26UcY08GpnoLNHJZS/EmXw1ZZ3d4YZjNlpIpWFNHn
    UGmdiGKXUPX/9H0fVjIAaQwjnGAbpgyCumWgzIwPpX+ElFOUr3z7BoVnFKhIXze+
    VmQGSWxZxvWDUN90Ul0tLEpLgk3OVxUB4VUGuf15OJOpgo1xibINPmWt14Vda2N9
    yrNKloJGZNqLAgMBAAGjfDB6MB8GA1UdIwQYMBaAFOR9X9FclYYILAWuvnW2ZafZ
    XahmMB0GA1UdDgQWBBRYAYQkG7wrUpRKPaUQchRR9a86yTAOBgNVHQ8BAf8EBAMC
    AQYwFwYDVR0gBBAwDjAMBgpghkgBZQMCATABMA8GA1UdEwEB/wQFMAMBAf8wDQYJ
    KoZIhvcNAQELBQADggEBADWHlxbmdTXNwBL/llwhQqwnazK7CC2WsXBBqgNPWj7m
    tvQ+aLG8/50Qc2Sun7o2VnwF9D18UUe8Gj3uPUYH+oSI1vDdyKcjmMbKRU4rk0eo
    3UHNDXwqIVc9CQS9smyV+x1HCwL4TTrq+LXLKx/qVij0Yqk+UJfAtrg2jnYKXsCu
    FMBQQnWCGrwa1g1TphRp/RmYHnMynYFmZrXtzFz+U9XEA7C+gPq4kqDI/iVfIT1s
    6lBtdB50lrDVwl2oYfAvW/6sC2se2QleZidUmrziVNP4oEeXINokU6T6p//HM1FG
    QYw2jOvpKcKtWCSAnegEbgsGYzATKjmPJPJ0npHFqzM=
    -----END CERTIFICATE-----
    """.strip()

    cryptography_cert_pem = b"""
    -----BEGIN CERTIFICATE-----
    MIIFvTCCBKWgAwIBAgICPyAwDQYJKoZIhvcNAQELBQAwRzELMAkGA1UEBhMCVVMx
    FjAUBgNVBAoTDUdlb1RydXN0IEluYy4xIDAeBgNVBAMTF1JhcGlkU1NMIFNIQTI1
    NiBDQSAtIEczMB4XDTE0MTAxNTEyMDkzMloXDTE4MTExNjAxMTUwM1owgZcxEzAR
    BgNVBAsTCkdUNDg3NDI5NjUxMTAvBgNVBAsTKFNlZSB3d3cucmFwaWRzc2wuY29t
    L3Jlc291cmNlcy9jcHMgKGMpMTQxLzAtBgNVBAsTJkRvbWFpbiBDb250cm9sIFZh
    bGlkYXRlZCAtIFJhcGlkU1NMKFIpMRwwGgYDVQQDExN3d3cuY3J5cHRvZ3JhcGh5
    LmlvMIICIjANBgkqhkiG9w0BAQEFAAOCAg8AMIICCgKCAgEAom/FebKJIot7Sp3s
    itG1sicpe3thCssjI+g1JDAS7I3GLVNmbms1DOdIIqwf01gZkzzXBN2+9sOnyRaR
    PPfCe1jTr3dk2y6rPE559vPa1nZQkhlzlhMhlPyjaT+S7g4Tio4qV2sCBZU01DZJ
    CaksfohN+5BNVWoJzTbOcrHOEJ+M8B484KlBCiSxqf9cyNQKru4W3bHaCVNVJ8eu
    6i6KyhzLa0L7yK3LXwwXVs583C0/vwFhccGWsFODqD/9xHUzsBIshE8HKjdjDi7Y
    3BFQzVUQFjBB50NSZfAA/jcdt1blxJouc7z9T8Oklh+V5DDBowgAsrT4b6Z2Fq6/
    r7D1GqivLK/ypUQmxq2WXWAUBb/Q6xHgxASxI4Br+CByIUQJsm8L2jzc7k+mF4hW
    ltAIUkbo8fGiVnat0505YJgxWEDKOLc4Gda6d/7GVd5AvKrz242bUqeaWo6e4MTx
    diku2Ma3rhdcr044Qvfh9hGyjqNjvhWY/I+VRWgihU7JrYvgwFdJqsQ5eiKT4OHi
    gsejvWwkZzDtiQ+aQTrzM1FsY2swJBJsLSX4ofohlVRlIJCn/ME+XErj553431Lu
    YQ5SzMd3nXzN78Vj6qzTfMUUY72UoT1/AcFiUMobgIqrrmwuNxfrkbVE2b6Bga74
    FsJX63prvrJ41kuHK/16RQBM7fcCAwEAAaOCAWAwggFcMB8GA1UdIwQYMBaAFMOc
    8/zTRgg0u85Gf6B8W/PiCMtZMFcGCCsGAQUFBwEBBEswSTAfBggrBgEFBQcwAYYT
    aHR0cDovL2d2LnN5bWNkLmNvbTAmBggrBgEFBQcwAoYaaHR0cDovL2d2LnN5bWNi
    LmNvbS9ndi5jcnQwDgYDVR0PAQH/BAQDAgWgMB0GA1UdJQQWMBQGCCsGAQUFBwMB
    BggrBgEFBQcDAjAvBgNVHREEKDAmghN3d3cuY3J5cHRvZ3JhcGh5Lmlvgg9jcnlw
    dG9ncmFwaHkuaW8wKwYDVR0fBCQwIjAgoB6gHIYaaHR0cDovL2d2LnN5bWNiLmNv
    bS9ndi5jcmwwDAYDVR0TAQH/BAIwADBFBgNVHSAEPjA8MDoGCmCGSAGG+EUBBzYw
    LDAqBggrBgEFBQcCARYeaHR0cHM6Ly93d3cucmFwaWRzc2wuY29tL2xlZ2FsMA0G
    CSqGSIb3DQEBCwUAA4IBAQAzIYO2jx7h17FBT74tJ2zbV9OKqGb7QF8y3wUtP4xc
    dH80vprI/Cfji8s86kr77aAvAqjDjaVjHn7UzebhSUivvRPmfzRgyWBacomnXTSt
    Xlt2dp2nDQuwGyK2vB7dMfKnQAkxwq1sYUXznB8i0IhhCAoXp01QGPKq51YoIlnF
    7DRMk6iEaL1SJbkIrLsCQyZFDf0xtfW9DqXugMMLoxeCsBhZJQzNyS2ryirrv9LH
    aK3+6IZjrcyy9bkpz/gzJucyhU+75c4My/mnRCrtItRbCQuiI5pd5poDowm+HH9i
    GVI9+0lAFwxOUnOnwsoI40iOoxjLMGB+CgFLKCGUcWxP
    -----END CERTIFICATE-----
    """.strip()

    pem_issuer_public_key = b"""
    -----BEGIN RSA PUBLIC KEY-----
    MIICCgKCAgEAyYcqyuT6oQxpvg/VSn2Zc68wZ823D0VAJ2woramFx+2KPWB7B7Ot
    tVSNRfm0OxJOU3TFAoep54Z2wgOoz0zRmeW6/7gvIuBKp2TW0qZAt3l9sgpE29iw
    CsoZQlMrLKiDPzCC6Fptk+YPSST9sqwhWDKK1QvOg68DKRxTpEek1hBpC0XRsnuX
    fvJJQqP39vxzpA0PsicI/wrvWX3vO8z+j9+botPerbeamoeHCsc0xgTLyIygWysB
    rNskxlzC2U4Kw6mQhGghlLReo1rFsO2/hLTnvLs+Y1lQhnFeOKCx1WVXhzBIyO9B
    dVVH5Cinb5wBNKvxbevRf4icdWcwtknmgKf69xj7yvFjt/vft74BB1Y5ltLYFmEb
    0JBxm5MAJfW4YnMQr0AxdjOhjHq4MN7X4ZzwEpJaYJdRmvMsMGN88cyjYPxsaOG+
    dZ/E9MmTjh0gnTjyD4gmsvR/gtTR/XFJ2wkbnnL1RyxNi6j2UW8C7tpNv0TIuArx
    3SHGPZN0WsaKTxZPb0L/ob1WBT0mhiq1GzB431cXgbxyh8EdKk+xSptA3V+ca2V2
    NuXlJIJaOoPMj/qjDW4I/peKGnk9tLknJ0hpRzz11j77pJsV0dGoGKVHIR2oZqT5
    0ZJJb5DXNbiTnspKLNmBt0YlNiXtlCIPxVUkhL141FuCLc8h6FjD6E0CAwEAAQ==
    -----END RSA PUBLIC KEY-----
    """.strip()

    pem_data_to_check = b"""
    -----BEGIN CERTIFICATE-----
    MIIErjCCApagAwIBAgIUUrUZsZrrBmRD2hvRuspp+lPsZXcwDQYJKoZIhvcNAQEN
    BQAwETEPMA0GA1UEAwwGSXNzdWVyMB4XDTE4MTAwODEzNDg1NFoXDTE4MTAxODEz
    NDg1NFowETEPMA0GA1UEAwwGSXNzdWVyMIICIjANBgkqhkiG9w0BAQEFAAOCAg8A
    MIICCgKCAgEAyYcqyuT6oQxpvg/VSn2Zc68wZ823D0VAJ2woramFx+2KPWB7B7Ot
    tVSNRfm0OxJOU3TFAoep54Z2wgOoz0zRmeW6/7gvIuBKp2TW0qZAt3l9sgpE29iw
    CsoZQlMrLKiDPzCC6Fptk+YPSST9sqwhWDKK1QvOg68DKRxTpEek1hBpC0XRsnuX
    fvJJQqP39vxzpA0PsicI/wrvWX3vO8z+j9+botPerbeamoeHCsc0xgTLyIygWysB
    rNskxlzC2U4Kw6mQhGghlLReo1rFsO2/hLTnvLs+Y1lQhnFeOKCx1WVXhzBIyO9B
    dVVH5Cinb5wBNKvxbevRf4icdWcwtknmgKf69xj7yvFjt/vft74BB1Y5ltLYFmEb
    0JBxm5MAJfW4YnMQr0AxdjOhjHq4MN7X4ZzwEpJaYJdRmvMsMGN88cyjYPxsaOG+
    dZ/E9MmTjh0gnTjyD4gmsvR/gtTR/XFJ2wkbnnL1RyxNi6j2UW8C7tpNv0TIuArx
    3SHGPZN0WsaKTxZPb0L/ob1WBT0mhiq1GzB431cXgbxyh8EdKk+xSptA3V+ca2V2
    NuXlJIJaOoPMj/qjDW4I/peKGnk9tLknJ0hpRzz11j77pJsV0dGoGKVHIR2oZqT5
    0ZJJb5DXNbiTnspKLNmBt0YlNiXtlCIPxVUkhL141FuCLc8h6FjD6E0CAwEAATAN
    BgkqhkiG9w0BAQ0FAAOCAgEAVFzNKhEpkH8V8l0NEBAZHNi1e+lcg35fZZ9plqcw
    Pvk+6M7LW0KD0QWYQWm/dJme4DFsM7lh5u4/m+H4yS7/RP9pads9YwBudchvGR1c
    S4CCrRAmO8/A0vpQJcEwdS7fdYShBsqMrZ2TvzceVn2dvQbxB6pLkK7KIbDPVJA2
    HXFFXe2npHmdc80iTz2ShbdVSvyPvk6vc6NFFCg6lSQFuif3vV0+aYqi6DXv4h92
    9qAdES8ZLDfDulxyajyPbtF35f2Of99CumP5UzG4RQbvtI8gShuK1YFYe2sWJFE0
    MgSsqGCbl5mcrWxm9YxysRKMZ+Hc4tnkvfmG6GsKtp8u/5pG11XgxXaQl4fZ7JNa
    QFuD5gEXkEC1mCnhWlnguJgjQlpKadMOORmVTqG9dNQ6GEsha+XWpinm5L9fEZuA
    F88nNyubKLwEl68N7WWWKQlIl4q8Pe5FEp1pd9rLjOW4gzgYBccIfBK3oMC7uFJg
    a/9GeOKPiq90UMrCI+CAsIbzuPOaAp3g69JonuDwcs4cu8ui1udxs9q7ox3qSWGZ
    G1U/hmwvZH9kfIv5BKIzNLy4oxXPDJ7MZIBsxVxaNv8KUQ/JLtpVJa3oYqEx18+V
    JNr8Pr3y61X8pLmJnaCu+ixshiy2gjxXxDFBVEEt1G9JHrSs3R+yvcHxCrM3+ian
    Nh4=
    -----END CERTIFICATE-----
    """.strip()

Loading Certificates
~~~~~~~~~~~~~~~~~~~~

.. function:: load_pem_x509_certificate(data, backend=None)

    .. versionadded:: 0.7

    Deserialize a certificate from PEM encoded data. PEM certificates are
    base64 decoded and have delimiters that look like
    ``-----BEGIN CERTIFICATE-----``.

    :param bytes data: The PEM encoded certificate data.

    :param backend: An optional backend supporting the
        :class:`~cryptography.hazmat.backends.interfaces.X509Backend`
        interface.

    :returns: An instance of :class:`~cryptography.x509.Certificate`.

    .. doctest::

        >>> from cryptography import x509
        >>> cert = x509.load_pem_x509_certificate(pem_data)
        >>> cert.serial_number
        2

.. function:: load_der_x509_certificate(data, backend=None)

    .. versionadded:: 0.7

    Deserialize a certificate from DER encoded data. DER is a binary format
    and is commonly found in files with the ``.cer`` extension (although file
    extensions are not a guarantee of encoding type).

    :param bytes data: The DER encoded certificate data.

    :param backend: An optional backend supporting the
        :class:`~cryptography.hazmat.backends.interfaces.X509Backend`
        interface.

    :returns: An instance of :class:`~cryptography.x509.Certificate`.

Loading Certificate Revocation Lists
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

.. function:: load_pem_x509_crl(data, backend=None)

    .. versionadded:: 1.1

    Deserialize a certificate revocation list (CRL) from PEM encoded data. PEM
    requests are base64 decoded and have delimiters that look like
    ``-----BEGIN X509 CRL-----``.

    :param bytes data: The PEM encoded request data.

    :param backend: An optional backend supporting the
        :class:`~cryptography.hazmat.backends.interfaces.X509Backend`
        interface.

    :returns: An instance of
        :class:`~cryptography.x509.CertificateRevocationList`.

    .. doctest::

        >>> from cryptography import x509
        >>> from cryptography.hazmat.primitives import hashes
        >>> crl = x509.load_pem_x509_crl(pem_crl_data)
        >>> isinstance(crl.signature_hash_algorithm, hashes.SHA256)
        True

.. function:: load_der_x509_crl(data, backend=None)

    .. versionadded:: 1.1

    Deserialize a certificate revocation list (CRL) from DER encoded data. DER
    is a binary format.

    :param bytes data: The DER encoded request data.

    :param backend: An optional backend supporting the
        :class:`~cryptography.hazmat.backends.interfaces.X509Backend`
        interface.

    :returns: An instance of
        :class:`~cryptography.x509.CertificateRevocationList`.

Loading Certificate Signing Requests
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

.. function:: load_pem_x509_csr(data, backend=None)

    .. versionadded:: 0.9

    Deserialize a certificate signing request (CSR) from PEM encoded data. PEM
    requests are base64 decoded and have delimiters that look like
    ``-----BEGIN CERTIFICATE REQUEST-----``. This format is also known as
    PKCS#10.

    :param bytes data: The PEM encoded request data.

    :param backend: An optional backend supporting the
        :class:`~cryptography.hazmat.backends.interfaces.X509Backend`
        interface.

    :returns: An instance of
        :class:`~cryptography.x509.CertificateSigningRequest`.

    .. doctest::

        >>> from cryptography import x509
        >>> from cryptography.hazmat.primitives import hashes
        >>> csr = x509.load_pem_x509_csr(pem_req_data)
        >>> isinstance(csr.signature_hash_algorithm, hashes.SHA256)
        True

.. function:: load_der_x509_csr(data, backend=None)

    .. versionadded:: 0.9

    Deserialize a certificate signing request (CSR) from DER encoded data. DER
    is a binary format and is not commonly used with CSRs.

    :param bytes data: The DER encoded request data.

    :param backend: An optional backend supporting the
        :class:`~cryptography.hazmat.backends.interfaces.X509Backend`
        interface.

    :returns: An instance of
        :class:`~cryptography.x509.CertificateSigningRequest`.

X.509 Certificate Object
~~~~~~~~~~~~~~~~~~~~~~~~

.. class:: Certificate

    .. versionadded:: 0.7

    .. attribute:: version

        :type: :class:`~cryptography.x509.Version`

        The certificate version as an enumeration. Version 3 certificates are
        the latest version and also the only type you should see in practice.

        :raises cryptography.x509.InvalidVersion: If the version in the
            certificate is not a known
            :class:`X.509 version <cryptography.x509.Version>`.

        .. doctest::

            >>> cert.version
            <Version.v3: 2>

    .. method:: fingerprint(algorithm)

        :param algorithm: The
            :class:`~cryptography.hazmat.primitives.hashes.HashAlgorithm`
            that will be used to generate the fingerprint.

        :return bytes: The fingerprint using the supplied hash algorithm, as
            bytes.

        .. doctest::

            >>> from cryptography.hazmat.primitives import hashes
            >>> cert.fingerprint(hashes.SHA256())
            b'\x86\xd2\x187Gc\xfc\xe7}[+E9\x8d\xb4\x8f\x10\xe5S\xda\x18u\xbe}a\x03\x08[\xac\xa04?'

    .. attribute:: serial_number

        :type: int

        The serial as a Python integer.

        .. doctest::

            >>> cert.serial_number
            2

    .. method:: public_key()

        The public key associated with the certificate.

        :returns: One of
            :class:`~cryptography.hazmat.primitives.asymmetric.rsa.RSAPublicKey`,
            :class:`~cryptography.hazmat.primitives.asymmetric.dsa.DSAPublicKey`,
            :class:`~cryptography.hazmat.primitives.asymmetric.ec.EllipticCurvePublicKey`,
            :class:`~cryptography.hazmat.primitives.asymmetric.ed25519.Ed25519PublicKey` or
            :class:`~cryptography.hazmat.primitives.asymmetric.ed448.Ed448PublicKey`

        .. doctest::

            >>> from cryptography.hazmat.primitives.asymmetric import rsa
            >>> public_key = cert.public_key()
            >>> isinstance(public_key, rsa.RSAPublicKey)
            True

    .. attribute:: not_valid_before

        :type: :class:`datetime.datetime`

        A naïve datetime representing the beginning of the validity period for
        the certificate in UTC. This value is inclusive.

        .. doctest::

            >>> cert.not_valid_before
            datetime.datetime(2010, 1, 1, 8, 30)

    .. attribute:: not_valid_after

        :type: :class:`datetime.datetime`

        A naïve datetime representing the end of the validity period for the
        certificate in UTC. This value is inclusive.

        .. doctest::

            >>> cert.not_valid_after
            datetime.datetime(2030, 12, 31, 8, 30)

    .. attribute:: issuer

        .. versionadded:: 0.8

        :type: :class:`Name`

        The :class:`Name` of the issuer.

    .. attribute:: subject

        .. versionadded:: 0.8

        :type: :class:`Name`

        The :class:`Name` of the subject.

    .. attribute:: signature_hash_algorithm

        :type: :class:`~cryptography.hazmat.primitives.hashes.HashAlgorithm`

        Returns the
        :class:`~cryptography.hazmat.primitives.hashes.HashAlgorithm` which
        was used in signing this certificate.  Can be ``None`` if signature
        did not use separate hash
        (:attr:`~cryptography.x509.oid.SignatureAlgorithmOID.ED25519`,
        :attr:`~cryptography.x509.oid.SignatureAlgorithmOID.ED448`).

        .. doctest::

            >>> from cryptography.hazmat.primitives import hashes
            >>> isinstance(cert.signature_hash_algorithm, hashes.SHA256)
            True

    .. attribute:: signature_algorithm_oid

        .. versionadded:: 1.6

        :type: :class:`ObjectIdentifier`

        Returns the :class:`ObjectIdentifier` of the signature algorithm used
        to sign the certificate. This will be one of the OIDs from
        :class:`~cryptography.x509.oid.SignatureAlgorithmOID`.


        .. doctest::

            >>> cert.signature_algorithm_oid
            <ObjectIdentifier(oid=1.2.840.113549.1.1.11, name=sha256WithRSAEncryption)>

    .. attribute:: extensions

        :type: :class:`Extensions`

        The extensions encoded in the certificate.

        :raises cryptography.x509.DuplicateExtension: If more than one
            extension of the same type is found within the certificate.

        :raises cryptography.x509.UnsupportedGeneralNameType: If an extension
            contains a general name that is not supported.

        .. doctest::

            >>> for ext in cert.extensions:
            ...     print(ext)
            <Extension(oid=<ObjectIdentifier(oid=2.5.29.35, name=authorityKeyIdentifier)>, critical=False, value=<AuthorityKeyIdentifier(key_identifier=b'\xe4}_\xd1\\\x95\x86\x08,\x05\xae\xbeu\xb6e\xa7\xd9]\xa8f', authority_cert_issuer=None, authority_cert_serial_number=None)>)>
            <Extension(oid=<ObjectIdentifier(oid=2.5.29.14, name=subjectKeyIdentifier)>, critical=False, value=<SubjectKeyIdentifier(digest=b'X\x01\x84$\x1b\xbc+R\x94J=\xa5\x10r\x14Q\xf5\xaf:\xc9')>)>
            <Extension(oid=<ObjectIdentifier(oid=2.5.29.15, name=keyUsage)>, critical=True, value=<KeyUsage(digital_signature=False, content_commitment=False, key_encipherment=False, data_encipherment=False, key_agreement=False, key_cert_sign=True, crl_sign=True, encipher_only=False, decipher_only=False)>)>
            <Extension(oid=<ObjectIdentifier(oid=2.5.29.32, name=certificatePolicies)>, critical=False, value=<CertificatePolicies([<PolicyInformation(policy_identifier=<ObjectIdentifier(oid=2.16.840.1.101.3.2.1.48.1, name=Unknown OID)>, policy_qualifiers=None)>])>)>
            <Extension(oid=<ObjectIdentifier(oid=2.5.29.19, name=basicConstraints)>, critical=True, value=<BasicConstraints(ca=True, path_length=None)>)>

    .. attribute:: signature

        .. versionadded:: 1.2

        :type: bytes

        The bytes of the certificate's signature.

    .. attribute:: tbs_certificate_bytes

        .. versionadded:: 1.2

        :type: bytes

        The DER encoded bytes payload (as defined by :rfc:`5280`) that is hashed
        and then signed by the private key of the certificate's issuer. This
        data may be used to validate a signature, but use extreme caution as
        certificate validation is a complex problem that involves much more
        than just signature checks.

        To validate the signature on a certificate you can do the following.
        Note: This only verifies that the certificate was signed with the
        private key associated with the public key provided and does not
        perform any of the other checks needed for secure certificate
        validation. Additionally, this example will only work for RSA public
        keys with ``PKCS1v15`` signatures, and so it can't be used for general
        purpose signature verification.

        .. doctest::

           >>> from cryptography.hazmat.primitives.serialization import load_pem_public_key
           >>> from cryptography.hazmat.primitives.asymmetric import padding
           >>> issuer_public_key = load_pem_public_key(pem_issuer_public_key)
           >>> cert_to_check = x509.load_pem_x509_certificate(pem_data_to_check)
           >>> issuer_public_key.verify(
           ...     cert_to_check.signature,
           ...     cert_to_check.tbs_certificate_bytes,
           ...     # Depends on the algorithm used to create the certificate
           ...     padding.PKCS1v15(),
           ...     cert_to_check.signature_hash_algorithm,
           ... )

           An
           :class:`~cryptography.exceptions.InvalidSignature`
           exception will be raised if the signature fails to verify.

    .. method:: public_bytes(encoding)

        .. versionadded:: 1.0

        :param encoding: The
            :class:`~cryptography.hazmat.primitives.serialization.Encoding`
            that will be used to serialize the certificate.

        :return bytes: The data that can be written to a file or sent
            over the network to be verified by clients.

X.509 CRL (Certificate Revocation List) Object
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

.. class:: CertificateRevocationList

    .. versionadded:: 1.0

    A CertificateRevocationList is an object representing a list of revoked
    certificates. The object is iterable and will yield the RevokedCertificate
    objects stored in this CRL.

    .. doctest::

            >>> len(crl)
            1
            >>> revoked_certificate = crl[0]
            >>> type(revoked_certificate)
            <class 'cryptography.hazmat.backends.openssl.x509._RevokedCertificate'>
            >>> for r in crl:
            ...     print(r.serial_number)
            0

    .. method:: fingerprint(algorithm)

        :param algorithm: The
            :class:`~cryptography.hazmat.primitives.hashes.HashAlgorithm`
            that will be used to generate the fingerprint.

        :return bytes: The fingerprint using the supplied hash algorithm, as
            bytes.

        .. doctest::

            >>> from cryptography.hazmat.primitives import hashes
            >>> crl.fingerprint(hashes.SHA256())
            b'e\xcf.\xc4:\x83?1\xdc\xf3\xfc\x95\xd7\xb3\x87\xb3\x8e\xf8\xb93!\x87\x07\x9d\x1b\xb4!\xb9\xe4W\xf4\x1f'

    .. method:: get_revoked_certificate_by_serial_number(serial_number)

        .. versionadded:: 2.3

        :param serial_number: The serial as a Python integer.
        :returns: :class:`~cryptography.x509.RevokedCertificate` if the
            ``serial_number`` is present in the CRL or ``None`` if it
            is not.

    .. attribute:: signature_hash_algorithm

        :type: :class:`~cryptography.hazmat.primitives.hashes.HashAlgorithm`

        Returns the
        :class:`~cryptography.hazmat.primitives.hashes.HashAlgorithm` which
        was used in signing this CRL.  Can be ``None`` if signature
        did not use separate hash
        (:attr:`~cryptography.x509.oid.SignatureAlgorithmOID.ED25519`,
        :attr:`~cryptography.x509.oid.SignatureAlgorithmOID.ED448`).

        .. doctest::

            >>> from cryptography.hazmat.primitives import hashes
            >>> isinstance(crl.signature_hash_algorithm, hashes.SHA256)
            True

    .. attribute:: signature_algorithm_oid

        .. versionadded:: 1.6

        :type: :class:`ObjectIdentifier`

        Returns the :class:`ObjectIdentifier` of the signature algorithm used
        to sign the CRL. This will be one of the OIDs from
        :class:`~cryptography.x509.oid.SignatureAlgorithmOID`.

        .. doctest::

            >>> crl.signature_algorithm_oid
            <ObjectIdentifier(oid=1.2.840.113549.1.1.11, name=sha256WithRSAEncryption)>

    .. attribute:: issuer

        :type: :class:`Name`

        The :class:`Name` of the issuer.

        .. doctest::

            >>> crl.issuer
            <Name(C=US,CN=cryptography.io)>

    .. attribute:: next_update

        :type: :class:`datetime.datetime`

        A naïve datetime representing when the next update to this CRL is
        expected.

        .. doctest::

            >>> crl.next_update
            datetime.datetime(2016, 1, 1, 0, 0)

    .. attribute:: last_update

        :type: :class:`datetime.datetime`

        A naïve datetime representing when this CRL was last updated.

        .. doctest::

            >>> crl.last_update
            datetime.datetime(2015, 1, 1, 0, 0)

    .. attribute:: extensions

        :type: :class:`Extensions`

        The extensions encoded in the CRL.

    .. attribute:: signature

        .. versionadded:: 1.2

        :type: bytes

        The bytes of the CRL's signature.

    .. attribute:: tbs_certlist_bytes

        .. versionadded:: 1.2

        :type: bytes

        The DER encoded bytes payload (as defined by :rfc:`5280`) that is hashed
        and then signed by the private key of the CRL's issuer. This data may be
        used to validate a signature, but use extreme caution as CRL validation
        is a complex problem that involves much more than just signature checks.

    .. method:: public_bytes(encoding)

        .. versionadded:: 1.2

        :param encoding: The
            :class:`~cryptography.hazmat.primitives.serialization.Encoding`
            that will be used to serialize the certificate revocation list.

        :return bytes: The data that can be written to a file or sent
            over the network and used as part of a certificate verification
            process.

    .. method:: is_signature_valid(public_key)

        .. versionadded:: 2.1

        .. warning::

            Checking the validity of the signature on the CRL is insufficient
            to know if the CRL should be trusted. More details are available
            in :rfc:`5280`.

        Returns True if the CRL signature is correct for given public key,
        False otherwise.

X.509 Certificate Builder
~~~~~~~~~~~~~~~~~~~~~~~~~

.. class:: CertificateBuilder

    .. versionadded:: 1.0

    .. doctest::

        >>> from cryptography import x509
        >>> from cryptography.hazmat.primitives import hashes
        >>> from cryptography.hazmat.primitives.asymmetric import rsa
        >>> from cryptography.x509.oid import NameOID
        >>> import datetime
        >>> one_day = datetime.timedelta(1, 0, 0)
        >>> private_key = rsa.generate_private_key(
        ...     public_exponent=65537,
        ...     key_size=2048,
        ... )
        >>> public_key = private_key.public_key()
        >>> builder = x509.CertificateBuilder()
        >>> builder = builder.subject_name(x509.Name([
        ...     x509.NameAttribute(NameOID.COMMON_NAME, u'cryptography.io'),
        ... ]))
        >>> builder = builder.issuer_name(x509.Name([
        ...     x509.NameAttribute(NameOID.COMMON_NAME, u'cryptography.io'),
        ... ]))
        >>> builder = builder.not_valid_before(datetime.datetime.today() - one_day)
        >>> builder = builder.not_valid_after(datetime.datetime.today() + (one_day * 30))
        >>> builder = builder.serial_number(x509.random_serial_number())
        >>> builder = builder.public_key(public_key)
        >>> builder = builder.add_extension(
        ...     x509.SubjectAlternativeName(
        ...         [x509.DNSName(u'cryptography.io')]
        ...     ),
        ...     critical=False
        ... )
        >>> builder = builder.add_extension(
        ...     x509.BasicConstraints(ca=False, path_length=None), critical=True,
        ... )
        >>> certificate = builder.sign(
        ...     private_key=private_key, algorithm=hashes.SHA256(),
        ... )
        >>> isinstance(certificate, x509.Certificate)
        True

    .. method:: issuer_name(name)

        Sets the issuer's distinguished name.

        :param name: The :class:`~cryptography.x509.Name` that describes the
            issuer (CA).

    .. method:: subject_name(name)

        Sets the subject's distinguished name.

        :param name: The :class:`~cryptography.x509.Name` that describes the
            subject.

    .. method:: public_key(public_key)

        Sets the subject's public key.

        :param public_key: The subject's public key. This can be one of
            :class:`~cryptography.hazmat.primitives.asymmetric.rsa.RSAPublicKey`,
            :class:`~cryptography.hazmat.primitives.asymmetric.dsa.DSAPublicKey`,
            :class:`~cryptography.hazmat.primitives.asymmetric.ec.EllipticCurvePublicKey`,
            :class:`~cryptography.hazmat.primitives.asymmetric.ed25519.Ed25519PublicKey` or
            :class:`~cryptography.hazmat.primitives.asymmetric.ed448.Ed448PublicKey`.

    .. method:: serial_number(serial_number)

        Sets the certificate's serial number (an integer).  The CA's policy
        determines how it attributes serial numbers to certificates. This
        number must uniquely identify the certificate given the issuer.
        `CABForum Guidelines`_ require entropy in the serial number
        to provide protection against hash collision attacks. For more
        information on secure random number generation, see
        :doc:`/random-numbers`.

        :param serial_number: Integer number that will be used by the CA to
            identify this certificate (most notably during certificate
            revocation checking). Users should consider using
            :func:`~cryptography.x509.random_serial_number` when possible.

    .. method:: not_valid_before(time)

        Sets the certificate's activation time.  This is the time from which
        clients can start trusting the certificate.  It may be different from
        the time at which the certificate was created.

        :param time: The :class:`datetime.datetime` object (in UTC) that marks the
            activation time for the certificate.  The certificate may not be
            trusted clients if it is used before this time.

    .. method:: not_valid_after(time)

        Sets the certificate's expiration time.  This is the time from which
        clients should no longer trust the certificate.  The CA's policy will
        determine how long the certificate should remain in use.

        :param time: The :class:`datetime.datetime` object (in UTC) that marks the
            expiration time for the certificate.  The certificate may not be
            trusted clients if it is used after this time.

    .. method:: add_extension(extension, critical)

        Adds an X.509 extension to the certificate.

        :param extension: An extension conforming to the
            :class:`~cryptography.x509.ExtensionType` interface.

        :param critical: Set to ``True`` if the extension must be understood and
             handled by whoever reads the certificate.

    .. method:: sign(private_key, algorithm, backend=None)

        Sign the certificate using the CA's private key.

        :param private_key: The
            :class:`~cryptography.hazmat.primitives.asymmetric.rsa.RSAPrivateKey`,
            :class:`~cryptography.hazmat.primitives.asymmetric.dsa.DSAPrivateKey`,
            :class:`~cryptography.hazmat.primitives.asymmetric.ec.EllipticCurvePrivateKey`,
            :class:`~cryptography.hazmat.primitives.asymmetric.ed25519.Ed25519PrivateKey` or
            :class:`~cryptography.hazmat.primitives.asymmetric.ed448.Ed448PrivateKey`
            that will be used to sign the certificate.

        :param algorithm: The
            :class:`~cryptography.hazmat.primitives.hashes.HashAlgorithm` that
            will be used to generate the signature. This must be ``None`` if
            the ``private_key`` is an
            :class:`~cryptography.hazmat.primitives.asymmetric.ed25519.Ed25519PrivateKey`
            or an
            :class:`~cryptography.hazmat.primitives.asymmetric.ed448.Ed448PrivateKey`
            and an instance of a
            :class:`~cryptography.hazmat.primitives.hashes.HashAlgorithm`
            otherwise.

        :param backend: An optional backend used to build the certificate.
            Must support the
            :class:`~cryptography.hazmat.backends.interfaces.X509Backend`
            interface.

        :returns: :class:`~cryptography.x509.Certificate`


X.509 CSR (Certificate Signing Request) Object
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

.. class:: CertificateSigningRequest

    .. versionadded:: 0.9

    .. method:: public_key()

        The public key associated with the request.

        :returns: One of
            :class:`~cryptography.hazmat.primitives.asymmetric.rsa.RSAPublicKey`,
            :class:`~cryptography.hazmat.primitives.asymmetric.dsa.DSAPublicKey`,
            :class:`~cryptography.hazmat.primitives.asymmetric.ec.EllipticCurvePublicKey`,
            :class:`~cryptography.hazmat.primitives.asymmetric.ed25519.Ed25519PublicKey` or
            :class:`~cryptography.hazmat.primitives.asymmetric.ed448.Ed448PublicKey`.

        .. doctest::

            >>> from cryptography.hazmat.primitives.asymmetric import rsa
            >>> public_key = csr.public_key()
            >>> isinstance(public_key, rsa.RSAPublicKey)
            True

    .. attribute:: subject

        :type: :class:`Name`

        The :class:`Name` of the subject.

    .. attribute:: signature_hash_algorithm

        :type: :class:`~cryptography.hazmat.primitives.hashes.HashAlgorithm`

        Returns the
        :class:`~cryptography.hazmat.primitives.hashes.HashAlgorithm` which
        was used in signing this request.  Can be ``None`` if signature
        did not use separate hash
        (:attr:`~cryptography.x509.oid.SignatureAlgorithmOID.ED25519`,
        :attr:`~cryptography.x509.oid.SignatureAlgorithmOID.ED448`).

        .. doctest::

            >>> from cryptography.hazmat.primitives import hashes
            >>> isinstance(csr.signature_hash_algorithm, hashes.SHA256)
            True

    .. attribute:: signature_algorithm_oid

        .. versionadded:: 1.6

        :type: :class:`ObjectIdentifier`

        Returns the :class:`ObjectIdentifier` of the signature algorithm used
        to sign the request. This will be one of the OIDs from
        :class:`~cryptography.x509.oid.SignatureAlgorithmOID`.

        .. doctest::

            >>> csr.signature_algorithm_oid
            <ObjectIdentifier(oid=1.2.840.113549.1.1.11, name=sha256WithRSAEncryption)>

    .. attribute:: extensions

        :type: :class:`Extensions`

        The extensions encoded in the certificate signing request.

        :raises cryptography.x509.DuplicateExtension: If more than one
            extension of the same type is found within the certificate signing request.

        :raises cryptography.x509.UnsupportedGeneralNameType: If an extension
            contains a general name that is not supported.

    .. method:: get_attribute_for_oid(oid)

        .. versionadded:: 3.0

        :param oid: An :class:`ObjectIdentifier` instance.

        :returns: The bytes value of the attribute or an exception if not
            found.

        :raises cryptography.x509.AttributeNotFound: If the request does
            not have the attribute requested.

    .. method:: public_bytes(encoding)

        .. versionadded:: 1.0

        :param encoding: The
            :class:`~cryptography.hazmat.primitives.serialization.Encoding`
            that will be used to serialize the certificate request.

        :return bytes: The data that can be written to a file or sent
            over the network to be signed by the certificate
            authority.

    .. attribute:: signature

        .. versionadded:: 1.2

        :type: bytes

        The bytes of the certificate signing request's signature.

    .. attribute:: tbs_certrequest_bytes

        .. versionadded:: 1.2

        :type: bytes

        The DER encoded bytes payload (as defined by :rfc:`2986`) that is
        hashed and then signed by the private key (corresponding to the public
        key embedded in the CSR). This data may be used to validate the CSR
        signature.

    .. attribute:: is_signature_valid

        .. versionadded:: 1.3

        Returns True if the CSR signature is correct, False otherwise.

X.509 Certificate Revocation List Builder
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

.. class:: CertificateRevocationListBuilder

    .. versionadded:: 1.2

    .. doctest::

        >>> from cryptography import x509
        >>> from cryptography.hazmat.primitives import hashes
        >>> from cryptography.hazmat.primitives.asymmetric import rsa
        >>> from cryptography.x509.oid import NameOID
        >>> import datetime
        >>> one_day = datetime.timedelta(1, 0, 0)
        >>> private_key = rsa.generate_private_key(
        ...     public_exponent=65537,
        ...     key_size=2048,
        ... )
        >>> builder = x509.CertificateRevocationListBuilder()
        >>> builder = builder.issuer_name(x509.Name([
        ...     x509.NameAttribute(NameOID.COMMON_NAME, u'cryptography.io CA'),
        ... ]))
        >>> builder = builder.last_update(datetime.datetime.today())
        >>> builder = builder.next_update(datetime.datetime.today() + one_day)
        >>> revoked_cert = x509.RevokedCertificateBuilder().serial_number(
        ...     333
        ... ).revocation_date(
        ...     datetime.datetime.today()
        ... ).build()
        >>> builder = builder.add_revoked_certificate(revoked_cert)
        >>> crl = builder.sign(
        ...     private_key=private_key, algorithm=hashes.SHA256(),
        ... )
        >>> len(crl)
        1

    .. method:: issuer_name(name)

        Sets the issuer's distinguished name.

        :param name: The :class:`~cryptography.x509.Name` that describes the
            issuer (CA).

    .. method:: last_update(time)

        Sets this CRL's activation time.  This is the time from which
        clients can start trusting this CRL.  It may be different from
        the time at which this CRL was created. This is also known as the
        ``thisUpdate`` time.

        :param time: The :class:`datetime.datetime` object (in UTC) that marks
            the activation time for this CRL.  The CRL may not be trusted if it
            is used before this time.

    .. method:: next_update(time)

        Sets this CRL's next update time. This is the time by which
        a new CRL will be issued. The CA is allowed to issue a new CRL before
        this date, however clients are not required to check for it.

        :param time: The :class:`datetime.datetime` object (in UTC) that marks
            the next update time for this CRL.

    .. method:: add_extension(extension, critical)

        Adds an X.509 extension to this CRL.

        :param extension: An extension with the
            :class:`~cryptography.x509.ExtensionType` interface.

        :param critical: Set to ``True`` if the extension must be understood and
             handled by whoever reads the CRL.

    .. method:: add_revoked_certificate(revoked_certificate)

        Adds a revoked certificate to this CRL.

        :param revoked_certificate: An instance of
            :class:`~cryptography.x509.RevokedCertificate`. These can be
            obtained from an existing CRL or created with
            :class:`~cryptography.x509.RevokedCertificateBuilder`.

    .. method:: sign(private_key, algorithm, backend=None)

        Sign this CRL using the CA's private key.

        :param private_key: The
            :class:`~cryptography.hazmat.primitives.asymmetric.rsa.RSAPrivateKey`,
            :class:`~cryptography.hazmat.primitives.asymmetric.dsa.DSAPrivateKey`,
            :class:`~cryptography.hazmat.primitives.asymmetric.ec.EllipticCurvePrivateKey`,
            :class:`~cryptography.hazmat.primitives.asymmetric.ed25519.Ed25519PrivateKey` or
            :class:`~cryptography.hazmat.primitives.asymmetric.ed448.Ed448PrivateKey`
            that will be used to sign the certificate.

        :param algorithm: The
            :class:`~cryptography.hazmat.primitives.hashes.HashAlgorithm` that
            will be used to generate the signature.
            This must be ``None`` if the ``private_key`` is an
            :class:`~cryptography.hazmat.primitives.asymmetric.ed25519.Ed25519PrivateKey`
            or an
            :class:`~cryptography.hazmat.primitives.asymmetric.ed448.Ed448PrivateKey`
            and an instance of a
            :class:`~cryptography.hazmat.primitives.hashes.HashAlgorithm`
            otherwise.

        :param backend: An optional backend used to build the CRL.
            Must support the
            :class:`~cryptography.hazmat.backends.interfaces.X509Backend`
            interface.

        :returns: :class:`~cryptography.x509.CertificateRevocationList`

X.509 Revoked Certificate Object
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

.. class:: RevokedCertificate

    .. versionadded:: 1.0

    .. attribute:: serial_number

        :type: :class:`int`

        An integer representing the serial number of the revoked certificate.

        .. doctest::

            >>> revoked_certificate.serial_number
            0

    .. attribute:: revocation_date

        :type: :class:`datetime.datetime`

        A naïve datetime representing the date this certificates was revoked.

        .. doctest::

            >>> revoked_certificate.revocation_date
            datetime.datetime(2015, 1, 1, 0, 0)

    .. attribute:: extensions

        :type: :class:`Extensions`

        The extensions encoded in the revoked certificate.

        .. doctest::

            >>> for ext in revoked_certificate.extensions:
            ...     print(ext)
            <Extension(oid=<ObjectIdentifier(oid=2.5.29.24, name=invalidityDate)>, critical=False, value=<InvalidityDate(invalidity_date=2015-01-01 00:00:00)>)>
            <Extension(oid=<ObjectIdentifier(oid=2.5.29.21, name=cRLReason)>, critical=False, value=<CRLReason(reason=ReasonFlags.key_compromise)>)>

X.509 Revoked Certificate Builder
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

.. class:: RevokedCertificateBuilder

    This class is used to create :class:`~cryptography.x509.RevokedCertificate`
    objects that can be used with the
    :class:`~cryptography.x509.CertificateRevocationListBuilder`.

    .. versionadded:: 1.2

    .. doctest::

        >>> from cryptography import x509
        >>> import datetime
        >>> builder = x509.RevokedCertificateBuilder()
        >>> builder = builder.revocation_date(datetime.datetime.today())
        >>> builder = builder.serial_number(3333)
        >>> revoked_certificate = builder.build()
        >>> isinstance(revoked_certificate, x509.RevokedCertificate)
        True

    .. method:: serial_number(serial_number)

        Sets the revoked certificate's serial number.

        :param serial_number: Integer number that is used to identify the
            revoked certificate.

    .. method:: revocation_date(time)

        Sets the certificate's revocation date.

        :param time: The :class:`datetime.datetime` object (in UTC) that marks the
            revocation time for the certificate.

    .. method:: add_extension(extension, critical)

        Adds an X.509 extension to this revoked certificate.

        :param extension: An instance of one of the
            :ref:`CRL entry extensions <crl_entry_extensions>`.

        :param critical: Set to ``True`` if the extension must be understood and
             handled.

    .. method:: build(backend=None)

        Create a revoked certificate object using the provided backend.

        :param backend: An optional backend used to build the revoked
            certificate.  Must support the
            :class:`~cryptography.hazmat.backends.interfaces.X509Backend`
            interface.

        :returns: :class:`~cryptography.x509.RevokedCertificate`

X.509 CSR (Certificate Signing Request) Builder Object
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

.. class:: CertificateSigningRequestBuilder

    .. versionadded:: 1.0

    .. doctest::

        >>> from cryptography import x509
        >>> from cryptography.hazmat.primitives import hashes
        >>> from cryptography.hazmat.primitives.asymmetric import rsa
        >>> from cryptography.x509.oid import AttributeOID, NameOID
        >>> private_key = rsa.generate_private_key(
        ...     public_exponent=65537,
        ...     key_size=2048,
        ... )
        >>> builder = x509.CertificateSigningRequestBuilder()
        >>> builder = builder.subject_name(x509.Name([
        ...     x509.NameAttribute(NameOID.COMMON_NAME, u'cryptography.io'),
        ... ]))
        >>> builder = builder.add_extension(
        ...     x509.BasicConstraints(ca=False, path_length=None), critical=True,
        ... )
        >>> builder = builder.add_attribute(
        ...     AttributeOID.CHALLENGE_PASSWORD, b"changeit"
        ... )
        >>> request = builder.sign(
        ...     private_key, hashes.SHA256()
        ... )
        >>> isinstance(request, x509.CertificateSigningRequest)
        True

    .. method:: subject_name(name)

        :param name: The :class:`~cryptography.x509.Name` of the certificate
            subject.
        :returns: A new
            :class:`~cryptography.x509.CertificateSigningRequestBuilder`.

    .. method:: add_extension(extension, critical)

        :param extension: An extension conforming to the
            :class:`~cryptography.x509.ExtensionType` interface.
        :param critical: Set to `True` if the extension must be understood and
             handled by whoever reads the certificate.
        :returns: A new
            :class:`~cryptography.x509.CertificateSigningRequestBuilder`.

    .. method:: add_attribute(oid, value)

        .. versionadded:: 3.0

        :param oid: An :class:`ObjectIdentifier` instance.
        :param value: The value of the attribute.
        :type value: bytes
        :returns: A new
            :class:`~cryptography.x509.CertificateSigningRequestBuilder`.

    .. method:: sign(private_key, algorithm, backend=None)

        :param backend: An optional backend used to sign the request.
            Must support the
            :class:`~cryptography.hazmat.backends.interfaces.X509Backend`
            interface.

        :param private_key: The
            :class:`~cryptography.hazmat.primitives.asymmetric.rsa.RSAPrivateKey`,
            :class:`~cryptography.hazmat.primitives.asymmetric.dsa.DSAPrivateKey`,
            :class:`~cryptography.hazmat.primitives.asymmetric.ec.EllipticCurvePrivateKey`,
            :class:`~cryptography.hazmat.primitives.asymmetric.ed25519.Ed25519PrivateKey` or
            :class:`~cryptography.hazmat.primitives.asymmetric.ed448.Ed448PrivateKey`
            that will be used to sign the request.  When the request is
            signed by a certificate authority, the private key's associated
            public key will be stored in the resulting certificate.

        :param algorithm: The
            :class:`~cryptography.hazmat.primitives.hashes.HashAlgorithm`
            that will be used to generate the request signature.
            This must be ``None`` if the ``private_key`` is an
            :class:`~cryptography.hazmat.primitives.asymmetric.ed25519.Ed25519PrivateKey`
            or an
            :class:`~cryptography.hazmat.primitives.asymmetric.ed448.Ed448PrivateKey`
            and an instance of a
            :class:`~cryptography.hazmat.primitives.hashes.HashAlgorithm`
            otherwise.

        :returns: A new
            :class:`~cryptography.x509.CertificateSigningRequest`.


.. class:: Name

    .. versionadded:: 0.8

    An X509 Name is an ordered list of attributes. The object is iterable to
    get every attribute or you can use :meth:`Name.get_attributes_for_oid` to
    obtain the specific type you want. Names are sometimes represented as a
    slash or comma delimited string (e.g. ``/CN=mydomain.com/O=My Org/C=US`` or
    ``CN=mydomain.com,O=My Org,C=US``).

    Technically, a Name is a list of *sets* of attributes, called *Relative
    Distinguished Names* or *RDNs*, although multi-valued RDNs are rarely
    encountered.  The iteration order of values within a multi-valued RDN is
    preserved.  If you need to handle multi-valued RDNs, the ``rdns`` property
    gives access to an ordered list of :class:`RelativeDistinguishedName`
    objects.

    A Name can be initialized with an iterable of :class:`NameAttribute` (the
    common case where each RDN has a single attribute) or an iterable of
    :class:`RelativeDistinguishedName` objects (in the rare case of
    multi-valued RDNs).

    .. doctest::

        >>> len(cert.subject)
        3
        >>> for attribute in cert.subject:
        ...     print(attribute)
        <NameAttribute(oid=<ObjectIdentifier(oid=2.5.4.6, name=countryName)>, value='US')>
        <NameAttribute(oid=<ObjectIdentifier(oid=2.5.4.10, name=organizationName)>, value='Test Certificates 2011')>
        <NameAttribute(oid=<ObjectIdentifier(oid=2.5.4.3, name=commonName)>, value='Good CA')>

    .. attribute:: rdns

        .. versionadded:: 1.6

        :type: list of :class:`RelativeDistinguishedName`

    .. method:: get_attributes_for_oid(oid)

        :param oid: An :class:`ObjectIdentifier` instance.

        :returns: A list of :class:`NameAttribute` instances that match the
            OID provided. If nothing matches an empty list will be returned.

        .. doctest::

            >>> cert.subject.get_attributes_for_oid(NameOID.COMMON_NAME)
            [<NameAttribute(oid=<ObjectIdentifier(oid=2.5.4.3, name=commonName)>, value='Good CA')>]

    .. method:: public_bytes(backend=None)

        .. versionadded:: 1.6

        :param backend: An optional backend supporting the
            :class:`~cryptography.hazmat.backends.interfaces.X509Backend`
            interface.

        :return bytes: The DER encoded name.

    .. method:: rfc4514_string()

        .. versionadded:: 2.5

        :return str: Format the given name as a :rfc:`4514` Distinguished Name
            string, for example ``CN=mydomain.com,O=My Org,C=US``.


.. class:: Version

    .. versionadded:: 0.7

    An enumeration for X.509 versions.

    .. attribute:: v1

        For version 1 X.509 certificates.

    .. attribute:: v3

        For version 3 X.509 certificates.

.. class:: NameAttribute

    .. versionadded:: 0.8

    An X.509 name consists of a list of :class:`RelativeDistinguishedName`
    instances, which consist of a set of :class:`NameAttribute` instances.

    .. attribute:: oid

        :type: :class:`ObjectIdentifier`

        The attribute OID.

    .. attribute:: value

        :type: :term:`text`

        The value of the attribute.

    .. method:: rfc4514_string()

        .. versionadded:: 2.5

        :return str: Format the given attribute as a :rfc:`4514` Distinguished
            Name string.


.. class:: RelativeDistinguishedName(attributes)

    .. versionadded:: 1.6

    A relative distinguished name is a non-empty set of name attributes.  The
    object is iterable to get every attribute, preserving the original order.
    Passing duplicate attributes to the constructor raises ``ValueError``.

    .. method:: get_attributes_for_oid(oid)

        :param oid: An :class:`ObjectIdentifier` instance.

        :returns: A list of :class:`NameAttribute` instances that match the OID
            provided.  The list should contain zero or one values.

    .. method:: rfc4514_string()

        .. versionadded:: 2.5

        :return str: Format the given RDN set as a :rfc:`4514` Distinguished
            Name string.


.. class:: ObjectIdentifier

    .. versionadded:: 0.8

    Object identifiers (frequently seen abbreviated as OID) identify the type
    of a value (see: :class:`NameAttribute`).

    .. attribute:: dotted_string

        :type: :class:`str`

        The dotted string value of the OID (e.g. ``"2.5.4.3"``)


.. _general_name_classes:

General Name Classes
~~~~~~~~~~~~~~~~~~~~

.. class:: GeneralName

    .. versionadded:: 0.9

    This is the generic interface that all the following classes are registered
    against.

.. class:: RFC822Name(value)

    .. versionadded:: 0.9

    .. versionchanged:: 3.1

        :term:`U-label` support has been removed. Encode them to
        :term:`A-label` before use.

    This corresponds to an email address. For example, ``user@example.com``.

    :param value: The email address. If the address contains an
        internationalized domain name then it must be encoded to an
        :term:`A-label` string before being passed.

    :raises ValueError: If the provided string is not an :term:`A-label`.

    .. attribute:: value

        :type: :term:`text`

.. class:: DNSName(value)

    .. versionadded:: 0.9

    .. versionchanged:: 3.1

        :term:`U-label` support has been removed. Encode them to
        :term:`A-label` before use.

    This corresponds to a domain name. For example, ``cryptography.io``.

    :param value: The domain name. If it is an internationalized domain
        name then it must be encoded to an :term:`A-label` string before being
        passed.

    :raises ValueError: If the provided string is not an :term:`A-label`.

        :type: :term:`text`

    .. attribute:: value

        :type: :term:`text`

.. class:: DirectoryName(value)

    .. versionadded:: 0.9

    This corresponds to a directory name.

    .. attribute:: value

        :type: :class:`Name`

.. class:: UniformResourceIdentifier(value)

    .. versionadded:: 0.9

    .. versionchanged:: 3.1

        :term:`U-label` support has been removed. Encode them to
        :term:`A-label` before use.

    This corresponds to a uniform resource identifier.  For example,
    ``https://cryptography.io``.

    :param value: The URI. If it contains an internationalized domain
        name then it must be encoded to an :term:`A-label` string before
        being passed.

    :raises ValueError: If the provided string is not an :term:`A-label`.

    .. attribute:: value

        :type: :term:`text`

.. class:: IPAddress(value)

    .. versionadded:: 0.9

    This corresponds to an IP address.

    .. attribute:: value

        :type: :class:`~ipaddress.IPv4Address`,
            :class:`~ipaddress.IPv6Address`,  :class:`~ipaddress.IPv4Network`,
            or :class:`~ipaddress.IPv6Network`.

.. class:: RegisteredID(value)

    .. versionadded:: 0.9

    This corresponds to a registered ID.

    .. attribute:: value

        :type: :class:`ObjectIdentifier`

.. class:: OtherName(type_id, value)

    .. versionadded:: 1.0

    This corresponds to an ``otherName.``  An ``otherName`` has a type identifier and a value represented in binary DER format.

    .. attribute:: type_id

        :type: :class:`ObjectIdentifier`

    .. attribute:: value

        :type: bytes

X.509 Extensions
~~~~~~~~~~~~~~~~

.. class:: Extensions

    .. versionadded:: 0.9

    An X.509 Extensions instance is an ordered list of extensions.  The object
    is iterable to get every extension.

    .. method:: get_extension_for_oid(oid)

        :param oid: An :class:`ObjectIdentifier` instance.

        :returns: An instance of the extension class.

        :raises cryptography.x509.ExtensionNotFound: If the certificate does
            not have the extension requested.

        .. doctest::

            >>> from cryptography.x509.oid import ExtensionOID
            >>> cert.extensions.get_extension_for_oid(ExtensionOID.BASIC_CONSTRAINTS)
            <Extension(oid=<ObjectIdentifier(oid=2.5.29.19, name=basicConstraints)>, critical=True, value=<BasicConstraints(ca=True, path_length=None)>)>

    .. method:: get_extension_for_class(extclass)

        .. versionadded:: 1.1

        :param extclass: An extension class.

        :returns: An instance of the extension class.

        :raises cryptography.x509.ExtensionNotFound: If the certificate does
            not have the extension requested.

        .. doctest::

            >>> from cryptography import x509
            >>> cert.extensions.get_extension_for_class(x509.BasicConstraints)
            <Extension(oid=<ObjectIdentifier(oid=2.5.29.19, name=basicConstraints)>, critical=True, value=<BasicConstraints(ca=True, path_length=None)>)>

.. class:: Extension

    .. versionadded:: 0.9

    .. attribute:: oid

        :type: :class:`ObjectIdentifier`

        One of the :class:`~cryptography.x509.oid.ExtensionOID` OIDs.

    .. attribute:: critical

        :type: bool

        Determines whether a given extension is critical or not. :rfc:`5280`
        requires that "A certificate-using system MUST reject the certificate
        if it encounters a critical extension it does not recognize or a
        critical extension that contains information that it cannot process".

    .. attribute:: value

        Returns an instance of the extension type corresponding to the OID.

.. class:: ExtensionType

    .. versionadded:: 1.0

    This is the interface against which all the following extension types are
    registered.

.. class:: KeyUsage(digital_signature, content_commitment, key_encipherment, data_encipherment, key_agreement, key_cert_sign, crl_sign, encipher_only, decipher_only)

    .. versionadded:: 0.9

    The key usage extension defines the purpose of the key contained in the
    certificate.  The usage restriction might be employed when a key that could
    be used for more than one operation is to be restricted.

    .. attribute:: oid

        .. versionadded:: 1.0

        :type: :class:`ObjectIdentifier`

        Returns :attr:`~cryptography.x509.oid.ExtensionOID.KEY_USAGE`.

    .. attribute:: digital_signature

        :type: bool

        This purpose is set to true when the subject public key is used for verifying
        digital signatures, other than signatures on certificates
        (``key_cert_sign``) and CRLs (``crl_sign``).

    .. attribute:: content_commitment

        :type: bool

        This purpose is set to true when the subject public key is used for verifying
        digital signatures, other than signatures on certificates
        (``key_cert_sign``) and CRLs (``crl_sign``). It is used to provide a
        non-repudiation service that protects against the signing entity
        falsely denying some action. In the case of later conflict, a
        reliable third party may determine the authenticity of the signed
        data. This was called ``non_repudiation`` in older revisions of the
        X.509 specification.

    .. attribute:: key_encipherment

        :type: bool

        This purpose is set to true when the subject public key is used for
        enciphering private or secret keys.

    .. attribute:: data_encipherment

        :type: bool

        This purpose is set to true when the subject public key is used for
        directly enciphering raw user data without the use of an intermediate
        symmetric cipher.

    .. attribute:: key_agreement

        :type: bool

        This purpose is set to true when the subject public key is used for key
        agreement.  For example, when a Diffie-Hellman key is to be used for
        key management, then this purpose is set to true.

    .. attribute:: key_cert_sign

        :type: bool

        This purpose is set to true when the subject public key is used for
        verifying signatures on public key certificates. If this purpose is set
        to true then ``ca`` must be true in the :class:`BasicConstraints`
        extension.

    .. attribute:: crl_sign

        :type: bool

        This purpose is set to true when the subject public key is used for
        verifying signatures on certificate revocation lists.

    .. attribute:: encipher_only

        :type: bool

        When this purposes is set to true and the ``key_agreement`` purpose is
        also set, the subject public key may be used only for enciphering data
        while performing key agreement.

        :raises ValueError: This is raised if accessed when ``key_agreement``
            is false.

    .. attribute:: decipher_only

        :type: bool

        When this purposes is set to true and the ``key_agreement`` purpose is
        also set, the subject public key may be used only for deciphering data
        while performing key agreement.

        :raises ValueError: This is raised if accessed when ``key_agreement``
            is false.


.. class:: BasicConstraints(ca, path_length)

    .. versionadded:: 0.9

    Basic constraints is an X.509 extension type that defines whether a given
    certificate is allowed to sign additional certificates and what path
    length restrictions may exist.

    .. attribute:: oid

        .. versionadded:: 1.0

        :type: :class:`ObjectIdentifier`

        Returns :attr:`~cryptography.x509.oid.ExtensionOID.BASIC_CONSTRAINTS`.

    .. attribute:: ca

        :type: bool

        Whether the certificate can sign certificates.

    .. attribute:: path_length

        :type: int or None

        The maximum path length for certificates subordinate to this
        certificate. This attribute only has meaning if ``ca`` is true.
        If ``ca`` is true then a path length of None means there's no
        restriction on the number of subordinate CAs in the certificate chain.
        If it is zero or greater then it defines the maximum length for a
        subordinate CA's certificate chain. For example, a ``path_length`` of 1
        means the certificate can sign a subordinate CA, but the subordinate CA
        is not allowed to create subordinates with ``ca`` set to true.

.. class:: ExtendedKeyUsage(usages)

    .. versionadded:: 0.9

    This extension indicates one or more purposes for which the certified
    public key may be used, in addition to or in place of the basic
    purposes indicated in the key usage extension. The object is
    iterable to obtain the list of
    :class:`~cryptography.x509.oid.ExtendedKeyUsageOID` OIDs present.

    :param list usages: A list of
        :class:`~cryptography.x509.oid.ExtendedKeyUsageOID` OIDs.

    .. attribute:: oid

        .. versionadded:: 1.0

        :type: :class:`ObjectIdentifier`

        Returns :attr:`~cryptography.x509.oid.ExtensionOID.EXTENDED_KEY_USAGE`.


.. class:: OCSPNoCheck()

    .. versionadded:: 1.0

    This presence of this extension indicates that an OCSP client can trust a
    responder for the lifetime of the responder's certificate. CAs issuing
    such a certificate should realize that a compromise of the responder's key
    is as serious as the compromise of a CA key used to sign CRLs, at least for
    the validity period of this certificate. CA's may choose to issue this type
    of certificate with a very short lifetime and renew it frequently. This
    extension is only relevant when the certificate is an authorized OCSP
    responder.

    .. attribute:: oid

        .. versionadded:: 1.0

        :type: :class:`ObjectIdentifier`

        Returns :attr:`~cryptography.x509.oid.ExtensionOID.OCSP_NO_CHECK`.


.. class:: TLSFeature(features)

    .. versionadded:: 2.1

    The TLS Feature extension is defined in :rfc:`7633` and is used in
    certificates for OCSP Must-Staple. The object is iterable to get every
    element.

    :param list features: A list of features to enable from the
        :class:`~cryptography.x509.TLSFeatureType` enum. At this time only
        ``status_request`` or ``status_request_v2`` are allowed.

    .. attribute:: oid

        :type: :class:`ObjectIdentifier`

        Returns :attr:`~cryptography.x509.oid.ExtensionOID.TLS_FEATURE`.

.. class:: TLSFeatureType

    .. versionadded:: 2.1

    An enumeration of TLS Feature types.

    .. attribute:: status_request

        This feature type is defined in :rfc:`6066` and, when embedded in
        an X.509 certificate, signals to the client that it should require
        a stapled OCSP response in the TLS handshake. Commonly known as OCSP
        Must-Staple in certificates.

    .. attribute:: status_request_v2

        This feature type is defined in :rfc:`6961`. This value is not
        commonly used and if you want to enable OCSP Must-Staple you should
        use ``status_request``.


.. class:: NameConstraints(permitted_subtrees, excluded_subtrees)

    .. versionadded:: 1.0

    The name constraints extension, which only has meaning in a CA certificate,
    defines a name space within which all subject names in certificates issued
    beneath the CA certificate must (or must not) be in. For specific details
    on the way this extension should be processed see :rfc:`5280`.

    .. attribute:: oid

        .. versionadded:: 1.0

        :type: :class:`ObjectIdentifier`

        Returns :attr:`~cryptography.x509.oid.ExtensionOID.NAME_CONSTRAINTS`.

    .. attribute:: permitted_subtrees

        :type: list of :class:`GeneralName` objects or None

        The set of permitted name patterns. If a name matches this and an
        element in ``excluded_subtrees`` it is invalid. At least one of
        ``permitted_subtrees`` and ``excluded_subtrees`` will be non-None.

    .. attribute:: excluded_subtrees

        :type: list of :class:`GeneralName` objects or None

        Any name matching a restriction in the ``excluded_subtrees`` field is
        invalid regardless of information appearing in the
        ``permitted_subtrees``. At least one of ``permitted_subtrees`` and
        ``excluded_subtrees`` will be non-None.

.. class:: AuthorityKeyIdentifier(key_identifier, authority_cert_issuer, authority_cert_serial_number)

    .. versionadded:: 0.9

    The authority key identifier extension provides a means of identifying the
    public key corresponding to the private key used to sign a certificate.
    This extension is typically used to assist in determining the appropriate
    certificate chain. For more information about generation and use of this
    extension see `RFC 5280 section 4.2.1.1`_.

    .. attribute:: oid

        .. versionadded:: 1.0

        :type: :class:`ObjectIdentifier`

        Returns
        :attr:`~cryptography.x509.oid.ExtensionOID.AUTHORITY_KEY_IDENTIFIER`.

    .. attribute:: key_identifier

        :type: bytes

        A value derived from the public key used to verify the certificate's
        signature.

    .. attribute:: authority_cert_issuer

        :type: A list of :class:`GeneralName` instances or None

        The :class:`GeneralName` (one or multiple) of the issuer's issuer.

    .. attribute:: authority_cert_serial_number

        :type: int or None

        The serial number of the issuer's issuer.

    .. classmethod:: from_issuer_public_key(public_key)

        .. versionadded:: 1.0

        .. note::

            This method should be used if the issuer certificate does not
            contain a :class:`~cryptography.x509.SubjectKeyIdentifier`.
            Otherwise, use
            :meth:`~cryptography.x509.AuthorityKeyIdentifier.from_issuer_subject_key_identifier`.

        Creates a new AuthorityKeyIdentifier instance using the public key
        provided to generate the appropriate digest. This should be the
        **issuer's public key**. The resulting object will contain
        :attr:`~cryptography.x509.AuthorityKeyIdentifier.key_identifier`, but
        :attr:`~cryptography.x509.AuthorityKeyIdentifier.authority_cert_issuer`
        and
        :attr:`~cryptography.x509.AuthorityKeyIdentifier.authority_cert_serial_number`
        will be None.
        The generated ``key_identifier`` is the SHA1 hash of the ``subjectPublicKey``
        ASN.1 bit string. This is the first recommendation in :rfc:`5280`
        section 4.2.1.2.

        :param public_key: One of
            :class:`~cryptography.hazmat.primitives.asymmetric.rsa.RSAPublicKey`,
            :class:`~cryptography.hazmat.primitives.asymmetric.dsa.DSAPublicKey`,
            :class:`~cryptography.hazmat.primitives.asymmetric.ec.EllipticCurvePublicKey`,
            :class:`~cryptography.hazmat.primitives.asymmetric.ed25519.Ed25519PublicKey` or
            :class:`~cryptography.hazmat.primitives.asymmetric.ed448.Ed448PublicKey`.

        .. doctest::

            >>> from cryptography import x509
            >>> issuer_cert = x509.load_pem_x509_certificate(pem_data)
            >>> x509.AuthorityKeyIdentifier.from_issuer_public_key(issuer_cert.public_key())
            <AuthorityKeyIdentifier(key_identifier=b'X\x01\x84$\x1b\xbc+R\x94J=\xa5\x10r\x14Q\xf5\xaf:\xc9', authority_cert_issuer=None, authority_cert_serial_number=None)>

    .. classmethod:: from_issuer_subject_key_identifier(ski)

        .. versionadded:: 1.3

        .. note::
            This method should be used if the issuer certificate contains a
            :class:`~cryptography.x509.SubjectKeyIdentifier`.  Otherwise, use
            :meth:`~cryptography.x509.AuthorityKeyIdentifier.from_issuer_public_key`.

        Creates a new AuthorityKeyIdentifier instance using the
        SubjectKeyIdentifier from the issuer certificate. The resulting object
        will contain
        :attr:`~cryptography.x509.AuthorityKeyIdentifier.key_identifier`, but
        :attr:`~cryptography.x509.AuthorityKeyIdentifier.authority_cert_issuer`
        and
        :attr:`~cryptography.x509.AuthorityKeyIdentifier.authority_cert_serial_number`
        will be None.

        :param ski: The
            :class:`~cryptography.x509.SubjectKeyIdentifier` from the issuer
            certificate.

        .. doctest::

            >>> from cryptography import x509
            >>> issuer_cert = x509.load_pem_x509_certificate(pem_data)
            >>> ski_ext = issuer_cert.extensions.get_extension_for_class(x509.SubjectKeyIdentifier)
            >>> x509.AuthorityKeyIdentifier.from_issuer_subject_key_identifier(ski_ext.value)
            <AuthorityKeyIdentifier(key_identifier=b'X\x01\x84$\x1b\xbc+R\x94J=\xa5\x10r\x14Q\xf5\xaf:\xc9', authority_cert_issuer=None, authority_cert_serial_number=None)>

.. class:: SubjectKeyIdentifier(digest)

    .. versionadded:: 0.9

    The subject key identifier extension provides a means of identifying
    certificates that contain a particular public key.

    .. attribute:: oid

        .. versionadded:: 1.0

        :type: :class:`ObjectIdentifier`

        Returns
        :attr:`~cryptography.x509.oid.ExtensionOID.SUBJECT_KEY_IDENTIFIER`.

    .. attribute:: digest

        :type: bytes

        The binary value of the identifier.

    .. classmethod:: from_public_key(public_key)

        .. versionadded:: 1.0

        Creates a new SubjectKeyIdentifier instance using the public key
        provided to generate the appropriate digest. This should be the public
        key that is in the certificate. The generated digest is the SHA1 hash
        of the ``subjectPublicKey`` ASN.1 bit string. This is the first
        recommendation in :rfc:`5280` section 4.2.1.2.

        :param public_key: One of
            :class:`~cryptography.hazmat.primitives.asymmetric.rsa.RSAPublicKey`,
            :class:`~cryptography.hazmat.primitives.asymmetric.dsa.DSAPublicKey`,
            :class:`~cryptography.hazmat.primitives.asymmetric.ec.EllipticCurvePublicKey`,
            :class:`~cryptography.hazmat.primitives.asymmetric.ed25519.Ed25519PublicKey` or
            :class:`~cryptography.hazmat.primitives.asymmetric.ed448.Ed448PublicKey`.

        .. doctest::

            >>> from cryptography import x509
            >>> csr = x509.load_pem_x509_csr(pem_req_data)
            >>> x509.SubjectKeyIdentifier.from_public_key(csr.public_key())
            <SubjectKeyIdentifier(digest=b'\x8c"\x98\xe2\xb5\xbf]\xe8*2\xf8\xd2\'?\x00\xd2\xc7#\xe4c')>

.. class:: SubjectAlternativeName(general_names)

    .. versionadded:: 0.9

    Subject alternative name is an X.509 extension that provides a list of
    :ref:`general name <general_name_classes>` instances that provide a set
    of identities for which the certificate is valid. The object is iterable to
    get every element.

    :param list general_names: A list of :class:`GeneralName` instances.

    .. attribute:: oid

        .. versionadded:: 1.0

        :type: :class:`ObjectIdentifier`

        Returns
        :attr:`~cryptography.x509.oid.ExtensionOID.SUBJECT_ALTERNATIVE_NAME`.

    .. method:: get_values_for_type(type)

        :param type: A :class:`GeneralName` instance. This is one of the
            :ref:`general name classes <general_name_classes>`.

        :returns: A list of values extracted from the matched general names.
            The type of the returned values depends on the :class:`GeneralName`.

        .. doctest::

            >>> from cryptography import x509
            >>> from cryptography.hazmat.primitives import hashes
            >>> cert = x509.load_pem_x509_certificate(cryptography_cert_pem)
            >>> # Get the subjectAltName extension from the certificate
            >>> ext = cert.extensions.get_extension_for_oid(ExtensionOID.SUBJECT_ALTERNATIVE_NAME)
            >>> # Get the dNSName entries from the SAN extension
            >>> ext.value.get_values_for_type(x509.DNSName)
            ['www.cryptography.io', 'cryptography.io']


.. class:: IssuerAlternativeName(general_names)

    .. versionadded:: 1.0

    Issuer alternative name is an X.509 extension that provides a list of
    :ref:`general name <general_name_classes>` instances that provide a set
    of identities for the certificate issuer. The object is iterable to
    get every element.

    :param list general_names: A list of :class:`GeneralName` instances.

    .. attribute:: oid

        .. versionadded:: 1.0

        :type: :class:`ObjectIdentifier`

        Returns
        :attr:`~cryptography.x509.oid.ExtensionOID.ISSUER_ALTERNATIVE_NAME`.

    .. method:: get_values_for_type(type)

        :param type: A :class:`GeneralName` instance. This is one of the
            :ref:`general name classes <general_name_classes>`.

        :returns: A list of values extracted from the matched general names.


.. class:: PrecertificateSignedCertificateTimestamps(scts)

    .. versionadded:: 2.0

    This extension contains
    :class:`~cryptography.x509.certificate_transparency.SignedCertificateTimestamp`
    instances which were issued for the pre-certificate corresponding to this
    certificate. These can be used to verify that the certificate is included
    in a public Certificate Transparency log.

    It is an iterable containing one or more
    :class:`~cryptography.x509.certificate_transparency.SignedCertificateTimestamp`
    objects.

    :param list scts: A ``list`` of
        :class:`~cryptography.x509.certificate_transparency.SignedCertificateTimestamp`
        objects.

    .. attribute:: oid

        :type: :class:`ObjectIdentifier`

        Returns
        :attr:`~cryptography.x509.oid.ExtensionOID.PRECERT_SIGNED_CERTIFICATE_TIMESTAMPS`.


.. class:: PrecertPoison()

    .. versionadded:: 2.4

    This extension indicates that the certificate should not be treated as a
    certificate for the purposes of validation, but is instead for submission
    to a certificate transparency log in order to obtain SCTs which will be
    embedded in a :class:`PrecertificateSignedCertificateTimestamps` extension
    on the final certificate.

    .. attribute:: oid

        :type: :class:`ObjectIdentifier`

        Returns :attr:`~cryptography.x509.oid.ExtensionOID.PRECERT_POISON`.


.. class:: SignedCertificateTimestamps(scts)

    .. versionadded:: 3.0

    This extension contains
    :class:`~cryptography.x509.certificate_transparency.SignedCertificateTimestamp`
    instances. These can be used to verify that the certificate is included
    in a public Certificate Transparency log. This extension is only found
    in OCSP responses. For SCTs in an X.509 certificate see
    :class:`~cryptography.x509.PrecertificateSignedCertificateTimestamps`.

    It is an iterable containing one or more
    :class:`~cryptography.x509.certificate_transparency.SignedCertificateTimestamp`
    objects.

    :param list scts: A ``list`` of
        :class:`~cryptography.x509.certificate_transparency.SignedCertificateTimestamp`
        objects.

    .. attribute:: oid

        :type: :class:`ObjectIdentifier`

        Returns
        :attr:`~cryptography.x509.oid.ExtensionOID.SIGNED_CERTIFICATE_TIMESTAMPS`.


.. class:: DeltaCRLIndicator(crl_number)

    .. versionadded:: 2.1

    The delta CRL indicator is a CRL extension that identifies a CRL as being
    a delta CRL. Delta CRLs contain updates to revocation information
    previously distributed, rather than all the information that would appear
    in a complete CRL.

    :param int crl_number: The CRL number of the complete CRL that the
        delta CRL is updating.

    .. attribute:: oid

        :type: :class:`ObjectIdentifier`

        Returns
        :attr:`~cryptography.x509.oid.ExtensionOID.DELTA_CRL_INDICATOR`.

    .. attribute:: crl_number

        :type: int


.. class:: AuthorityInformationAccess(descriptions)

    .. versionadded:: 0.9

    The authority information access extension indicates how to access
    information and services for the issuer of the certificate in which
    the extension appears. Information and services may include online
    validation services (such as OCSP) and issuer data. It is an iterable,
    containing one or more :class:`~cryptography.x509.AccessDescription`
    instances.

    :param list descriptions: A list of :class:`AccessDescription` objects.

    .. attribute:: oid

        .. versionadded:: 1.0

        :type: :class:`ObjectIdentifier`

        Returns
        :attr:`~cryptography.x509.oid.ExtensionOID.AUTHORITY_INFORMATION_ACCESS`.


.. class:: SubjectInformationAccess(descriptions)

    .. versionadded:: 3.0

    The subject information access extension indicates how to access
    information and services for the subject of the certificate in which
    the extension appears. When the subject is a CA, information and
    services may include certificate validation services and CA policy
    data. When the subject is an end entity, the information describes
    the type of services offered and how to access them. It is an iterable,
    containing one or more :class:`~cryptography.x509.AccessDescription`
    instances.

    :param list descriptions: A list of :class:`AccessDescription` objects.

    .. attribute:: oid

        :type: :class:`ObjectIdentifier`

        Returns
        :attr:`~cryptography.x509.oid.ExtensionOID.SUBJECT_INFORMATION_ACCESS`.


.. class:: AccessDescription(access_method, access_location)

    .. versionadded:: 0.9

    .. attribute:: access_method

        :type: :class:`ObjectIdentifier`

        The access method defines what the ``access_location`` means. It must
        be
        :attr:`~cryptography.x509.oid.AuthorityInformationAccessOID.OCSP` or
        :attr:`~cryptography.x509.oid.AuthorityInformationAccessOID.CA_ISSUERS`
        when used with :class:`~cryptography.x509.AuthorityInformationAccess`
        or
        :attr:`~cryptography.x509.oid.SubjectInformationAccessOID.CA_REPOSITORY`
        when used with :class:`~cryptography.x509.SubjectInformationAccess`.

        If it is
        :attr:`~cryptography.x509.oid.AuthorityInformationAccessOID.OCSP`
        the access location will be where to obtain OCSP
        information for the certificate. If it is
        :attr:`~cryptography.x509.oid.AuthorityInformationAccessOID.CA_ISSUERS`
        the access location will provide additional information about the
        issuing certificate. Finally, if it is
        :attr:`~cryptography.x509.oid.SubjectInformationAccessOID.CA_REPOSITORY`
        the access location will be the location of the CA's repository.

    .. attribute:: access_location

        :type: :class:`GeneralName`

        Where to access the information defined by the access method.

.. class:: FreshestCRL(distribution_points)

    .. versionadded:: 2.1

    The freshest CRL extension (also known as Delta CRL Distribution Point)
    identifies how delta CRL information is obtained. It is an iterable,
    containing one or more :class:`DistributionPoint` instances.

    :param list distribution_points: A list of :class:`DistributionPoint`
        instances.

    .. attribute:: oid

        :type: :class:`ObjectIdentifier`

        Returns
        :attr:`~cryptography.x509.oid.ExtensionOID.FRESHEST_CRL`.

.. class:: CRLDistributionPoints(distribution_points)

    .. versionadded:: 0.9

    The CRL distribution points extension identifies how CRL information is
    obtained. It is an iterable, containing one or more
    :class:`DistributionPoint` instances.

    :param list distribution_points: A list of :class:`DistributionPoint`
        instances.

    .. attribute:: oid

        .. versionadded:: 1.0

        :type: :class:`ObjectIdentifier`

        Returns
        :attr:`~cryptography.x509.oid.ExtensionOID.CRL_DISTRIBUTION_POINTS`.

.. class:: DistributionPoint(full_name, relative_name, reasons, crl_issuer)

    .. versionadded:: 0.9

    .. attribute:: full_name

        :type: list of :class:`GeneralName` instances or None

        This field describes methods to retrieve the CRL. At most one of
        ``full_name`` or ``relative_name`` will be non-None.

    .. attribute:: relative_name

        :type: :class:`RelativeDistinguishedName` or None

        This field describes methods to retrieve the CRL relative to the CRL
        issuer. At most one of ``full_name`` or ``relative_name`` will be
        non-None.

        .. versionchanged:: 1.6
            Changed from :class:`Name` to :class:`RelativeDistinguishedName`.

    .. attribute:: crl_issuer

        :type: list of :class:`GeneralName` instances or None

        Information about the issuer of the CRL.

    .. attribute:: reasons

        :type: frozenset of :class:`ReasonFlags` or None

        The reasons a given distribution point may be used for when performing
        revocation checks.

.. class:: ReasonFlags

    .. versionadded:: 0.9

    An enumeration for CRL reasons.

    .. attribute:: unspecified

        It is unspecified why the certificate was revoked. This reason cannot
        be used as a reason flag in a :class:`DistributionPoint`.

    .. attribute:: key_compromise

        This reason indicates that the private key was compromised.

    .. attribute:: ca_compromise

        This reason indicates that the CA issuing the certificate was
        compromised.

    .. attribute:: affiliation_changed

        This reason indicates that the subject's name or other information has
        changed.

    .. attribute:: superseded

        This reason indicates that a certificate has been superseded.

    .. attribute:: cessation_of_operation

        This reason indicates that the certificate is no longer required.

    .. attribute:: certificate_hold

        This reason indicates that the certificate is on hold.

    .. attribute:: privilege_withdrawn

        This reason indicates that the privilege granted by this certificate
        have been withdrawn.

    .. attribute:: aa_compromise

        When an attribute authority has been compromised.

    .. attribute:: remove_from_crl

        This reason indicates that the certificate was on hold and should be
        removed from the CRL. This reason cannot be used as a reason flag
        in a :class:`DistributionPoint`.

.. class:: InhibitAnyPolicy(skip_certs)

    .. versionadded:: 1.0

    The inhibit ``anyPolicy`` extension indicates that the special OID
    :attr:`~cryptography.x509.oid.CertificatePoliciesOID.ANY_POLICY`, is not
    considered an explicit match for other :class:`CertificatePolicies` except
    when it appears in an intermediate self-issued CA certificate.  The value
    indicates the number of additional non-self-issued certificates that may
    appear in the path before
    :attr:`~cryptography.x509.oid.CertificatePoliciesOID.ANY_POLICY` is no
    longer permitted.  For example, a value of one indicates that
    :attr:`~cryptography.x509.oid.CertificatePoliciesOID.ANY_POLICY` may be
    processed in certificates issued by the subject of this certificate, but
    not in additional certificates in the path.

    .. attribute:: oid

        .. versionadded:: 1.0

        :type: :class:`ObjectIdentifier`

        Returns
        :attr:`~cryptography.x509.oid.ExtensionOID.INHIBIT_ANY_POLICY`.

    .. attribute:: skip_certs

        :type: int

.. class:: PolicyConstraints

    .. versionadded:: 1.3

    The policy constraints extension is used to inhibit policy mapping or
    require that each certificate in a chain contain an acceptable policy
    identifier. For more information about the use of this extension see
    :rfc:`5280`.

    .. attribute:: oid

        :type: :class:`ObjectIdentifier`

        Returns :attr:`~cryptography.x509.oid.ExtensionOID.POLICY_CONSTRAINTS`.

    .. attribute:: require_explicit_policy

        :type: int or None

        If this field is not None, the value indicates the number of additional
        certificates that may appear in the chain before an explicit policy is
        required for the entire path. When an explicit policy is required, it
        is necessary for all certificates in the chain to contain an acceptable
        policy identifier in the certificate policies extension.  An
        acceptable policy identifier is the identifier of a policy required
        by the user of the certification path or the identifier of a policy
        that has been declared equivalent through policy mapping.

    .. attribute:: inhibit_policy_mapping

        :type: int or None

        If this field is not None, the value indicates the number of additional
        certificates that may appear in the chain before policy mapping is no
        longer permitted.  For example, a value of one indicates that policy
        mapping may be processed in certificates issued by the subject of this
        certificate, but not in additional certificates in the chain.

.. class:: CRLNumber(crl_number)

    .. versionadded:: 1.2

    The CRL number is a CRL extension that conveys a monotonically increasing
    sequence number for a given CRL scope and CRL issuer. This extension allows
    users to easily determine when a particular CRL supersedes another CRL.
    :rfc:`5280` requires that this extension be present in conforming CRLs.

    .. attribute:: oid

        :type: :class:`ObjectIdentifier`

        Returns
        :attr:`~cryptography.x509.oid.ExtensionOID.CRL_NUMBER`.

    .. attribute:: crl_number

        :type: int

.. class:: IssuingDistributionPoint(full_name, relative_name,\
           only_contains_user_certs, only_contains_ca_certs, only_some_reasons,\
           indirect_crl, only_contains_attribute_certs)

    .. versionadded:: 2.5

    Issuing distribution point is a CRL extension that identifies the CRL
    distribution point and scope for a particular CRL. It indicates whether
    the CRL covers revocation for end entity certificates only, CA certificates
    only, attribute certificates only, or a limited set of reason codes. For
    specific details on the way this extension should be processed see
    :rfc:`5280`.

    .. attribute:: oid

        :type: :class:`ObjectIdentifier`

        Returns
        :attr:`~cryptography.x509.oid.ExtensionOID.ISSUING_DISTRIBUTION_POINT`.

    .. attribute:: only_contains_user_certs

        :type: bool

        Set to ``True`` if the CRL this extension is embedded within only
        contains information about user certificates.

    .. attribute:: only_contains_ca_certs

        :type: bool

        Set to ``True`` if the CRL this extension is embedded within only
        contains information about CA certificates.

    .. attribute:: indirect_crl

        :type: bool

        Set to ``True`` if the CRL this extension is embedded within includes
        certificates issued by one or more authorities other than the CRL
        issuer.

    .. attribute:: only_contains_attribute_certs

        :type: bool

        Set to ``True`` if the CRL this extension is embedded within only
        contains information about attribute certificates.

    .. attribute:: only_some_reasons

        :type: frozenset of :class:`ReasonFlags` or None

        The reasons for which the issuing distribution point is valid. None
        indicates that it is valid for all reasons.

    .. attribute:: full_name

        :type: list of :class:`GeneralName` instances or None

        This field describes methods to retrieve the CRL. At most one of
        ``full_name`` or ``relative_name`` will be non-None.

    .. attribute:: relative_name

        :type: :class:`RelativeDistinguishedName` or None

        This field describes methods to retrieve the CRL relative to the CRL
        issuer. At most one of ``full_name`` or ``relative_name`` will be
        non-None.

.. class:: UnrecognizedExtension

    .. versionadded:: 1.2

    A generic extension class used to hold the raw value of extensions that
    ``cryptography`` does not know how to parse. This can also be used when
    creating new certificates, CRLs, or OCSP requests and responses to encode
    extensions that ``cryptography`` does not know how to generate.

    .. attribute:: oid

        :type: :class:`ObjectIdentifier`

        Returns the OID associated with this extension.

    .. attribute:: value

        :type: bytes

        Returns the DER encoded bytes payload of the extension.

.. class:: CertificatePolicies(policies)

    .. versionadded:: 0.9

    The certificate policies extension is an iterable, containing one or more
    :class:`PolicyInformation` instances.

    :param list policies: A list of :class:`PolicyInformation` instances.

    As an example of how ``CertificatePolicies`` might be used, if you wanted
    to check if a certificated contained the CAB Forum's "domain-validated"
    policy, you might write code like:

    .. code-block:: python

        def contains_domain_validated(policies):
            return any(
                policy.oid.dotted_string == "2.23.140.1.2.1"
                for policy in policies
            )

    .. attribute:: oid

        .. versionadded:: 1.0

        :type: :class:`ObjectIdentifier`

        Returns
        :attr:`~cryptography.x509.oid.ExtensionOID.CERTIFICATE_POLICIES`.

Certificate Policies Classes
~~~~~~~~~~~~~~~~~~~~~~~~~~~~

These classes may be present within a :class:`CertificatePolicies` instance.

.. class:: PolicyInformation(policy_identifier, policy_qualifiers)

    .. versionadded:: 0.9

    Contains a policy identifier and an optional list of qualifiers.

    .. attribute:: policy_identifier

        :type: :class:`ObjectIdentifier`

    .. attribute:: policy_qualifiers

        :type: list

        A list consisting of :term:`text` and/or :class:`UserNotice` objects.
        If the value is text it is a pointer to the practice statement
        published by the certificate authority. If it is a user notice it is
        meant for display to the relying party when the certificate is
        used.

.. class:: UserNotice(notice_reference, explicit_text)

    .. versionadded:: 0.9

    User notices are intended for display to a relying party when a certificate
    is used. In practice, few if any UIs expose this data and it is a rarely
    encoded component.

    .. attribute:: notice_reference

        :type: :class:`NoticeReference` or None

        The notice reference field names an organization and identifies,
        by number, a particular statement prepared by that organization.

    .. attribute:: explicit_text

        This field includes an arbitrary textual statement directly in the
        certificate.

        :type: :term:`text`

.. class:: NoticeReference(organization, notice_numbers)

    Notice reference can name an organization and provide information about
    notices related to the certificate. For example, it might identify the
    organization name and notice number 1. Application software could
    have a notice file containing the current set of notices for the named
    organization; the application would then extract the notice text from the
    file and display it. In practice this is rarely seen.

    .. versionadded:: 0.9

    .. attribute:: organization

        :type: :term:`text`

    .. attribute:: notice_numbers

        :type: list

        A list of integers.

.. _crl_entry_extensions:

CRL Entry Extensions
~~~~~~~~~~~~~~~~~~~~

These extensions are only valid within a :class:`RevokedCertificate` object.

.. class:: CertificateIssuer(general_names)

    .. versionadded:: 1.2

    The certificate issuer is an extension that is only valid inside
    :class:`~cryptography.x509.RevokedCertificate` objects.  If the
    ``indirectCRL`` property of the parent CRL's IssuingDistributionPoint
    extension is set, then this extension identifies the certificate issuer
    associated with the revoked certificate. The object is iterable to get
    every element.

    :param list general_names: A list of :class:`GeneralName` instances.

    .. attribute:: oid

        :type: :class:`ObjectIdentifier`

        Returns
        :attr:`~cryptography.x509.oid.CRLEntryExtensionOID.CERTIFICATE_ISSUER`.

    .. method:: get_values_for_type(type)

        :param type: A :class:`GeneralName` instance. This is one of the
            :ref:`general name classes <general_name_classes>`.

        :returns: A list of values extracted from the matched general names.
            The type of the returned values depends on the :class:`GeneralName`.

.. class:: CRLReason(reason)

    .. versionadded:: 1.2

    CRL reason (also known as ``reasonCode``) is an extension that is only
    valid inside :class:`~cryptography.x509.RevokedCertificate` objects. It
    identifies a reason for the certificate revocation.

    :param reason: An element from :class:`~cryptography.x509.ReasonFlags`.

    .. attribute:: oid

        :type: :class:`ObjectIdentifier`

        Returns
        :attr:`~cryptography.x509.oid.CRLEntryExtensionOID.CRL_REASON`.

    .. attribute:: reason

        :type: An element from :class:`~cryptography.x509.ReasonFlags`

.. class:: InvalidityDate(invalidity_date)

    .. versionadded:: 1.2

    Invalidity date is an extension that is only valid inside
    :class:`~cryptography.x509.RevokedCertificate` objects. It provides
    the date on which it is known or suspected that the private key was
    compromised or that the certificate otherwise became invalid.
    This date may be earlier than the revocation date in the CRL entry,
    which is the date at which the CA processed the revocation.

    :param invalidity_date: The :class:`datetime.datetime` when it is known
        or suspected that the private key was compromised.

    .. attribute:: oid

        :type: :class:`ObjectIdentifier`

        Returns
        :attr:`~cryptography.x509.oid.CRLEntryExtensionOID.INVALIDITY_DATE`.

    .. attribute:: invalidity_date

        :type: :class:`datetime.datetime`

OCSP Extensions
~~~~~~~~~~~~~~~

.. class:: OCSPNonce(nonce)

    .. versionadded:: 2.4

    OCSP nonce is an extension that is only valid inside
    :class:`~cryptography.x509.ocsp.OCSPRequest` and
    :class:`~cryptography.x509.ocsp.OCSPResponse` objects. The nonce
    cryptographically binds a request and a response to prevent replay attacks.
    In practice nonces are rarely used in OCSP due to the desire to precompute
    OCSP responses at large scale.

    .. attribute:: oid

        :type: :class:`ObjectIdentifier`

        Returns
        :attr:`~cryptography.x509.oid.OCSPExtensionOID.NONCE`.

    .. attribute:: nonce

        :type: bytes

Object Identifiers
~~~~~~~~~~~~~~~~~~

X.509 elements are frequently identified by :class:`ObjectIdentifier`
instances. The following common OIDs are available as constants.

.. currentmodule:: cryptography.x509.oid

.. class:: NameOID

    These OIDs are typically seen in X.509 names.

    .. versionadded:: 1.0

    .. attribute:: COMMON_NAME

        Corresponds to the dotted string ``"2.5.4.3"``. Historically the domain
        name would be encoded here for server certificates. :rfc:`2818`
        deprecates this practice and names of that type should now be located
        in a :class:`~cryptography.x509.SubjectAlternativeName` extension.

    .. attribute:: COUNTRY_NAME

        Corresponds to the dotted string ``"2.5.4.6"``.

    .. attribute:: LOCALITY_NAME

        Corresponds to the dotted string ``"2.5.4.7"``.

    .. attribute:: STATE_OR_PROVINCE_NAME

        Corresponds to the dotted string ``"2.5.4.8"``.

    .. attribute:: STREET_ADDRESS

        .. versionadded:: 1.6

        Corresponds to the dotted string ``"2.5.4.9"``.

    .. attribute:: ORGANIZATION_NAME

        Corresponds to the dotted string ``"2.5.4.10"``.

    .. attribute:: ORGANIZATIONAL_UNIT_NAME

        Corresponds to the dotted string ``"2.5.4.11"``.

    .. attribute:: SERIAL_NUMBER

        Corresponds to the dotted string ``"2.5.4.5"``. This is distinct from
        the serial number of the certificate itself (which can be obtained with
        :func:`~cryptography.x509.Certificate.serial_number`).

    .. attribute:: SURNAME

        Corresponds to the dotted string ``"2.5.4.4"``.

    .. attribute:: GIVEN_NAME

        Corresponds to the dotted string ``"2.5.4.42"``.

    .. attribute:: TITLE

        Corresponds to the dotted string ``"2.5.4.12"``.

    .. attribute:: GENERATION_QUALIFIER

        Corresponds to the dotted string ``"2.5.4.44"``.

    .. attribute:: X500_UNIQUE_IDENTIFIER

        .. versionadded:: 1.6

        Corresponds to the dotted string ``"2.5.4.45"``.

    .. attribute:: DN_QUALIFIER

        Corresponds to the dotted string ``"2.5.4.46"``. This specifies
        disambiguating information to add to the relative distinguished name of an
        entry. See :rfc:`2256`.

    .. attribute:: PSEUDONYM

        Corresponds to the dotted string ``"2.5.4.65"``.

    .. attribute:: USER_ID

        .. versionadded:: 1.6

        Corresponds to the dotted string ``"0.9.2342.19200300.100.1.1"``.

    .. attribute:: DOMAIN_COMPONENT

        Corresponds to the dotted string ``"0.9.2342.19200300.100.1.25"``. A string
        holding one component of a domain name. See :rfc:`4519`.

    .. attribute:: EMAIL_ADDRESS

        Corresponds to the dotted string ``"1.2.840.113549.1.9.1"``.

    .. attribute:: JURISDICTION_COUNTRY_NAME

        Corresponds to the dotted string ``"1.3.6.1.4.1.311.60.2.1.3"``.

    .. attribute:: JURISDICTION_LOCALITY_NAME

        Corresponds to the dotted string ``"1.3.6.1.4.1.311.60.2.1.1"``.

    .. attribute:: JURISDICTION_STATE_OR_PROVINCE_NAME

        Corresponds to the dotted string ``"1.3.6.1.4.1.311.60.2.1.2"``.

    .. attribute:: BUSINESS_CATEGORY

        Corresponds to the dotted string ``"2.5.4.15"``.

    .. attribute:: POSTAL_ADDRESS

        .. versionadded:: 1.6

        Corresponds to the dotted string ``"2.5.4.16"``.

    .. attribute:: POSTAL_CODE

        .. versionadded:: 1.6

        Corresponds to the dotted string ``"2.5.4.17"``.

    .. attribute:: UNSTRUCTURED_NAME

        .. versionadded:: 3.0

        Corresponds to the dotted string ``"1.2.840.113549.1.9.2"``.


.. class:: SignatureAlgorithmOID

    .. versionadded:: 1.0

    .. attribute:: RSA_WITH_MD5

        Corresponds to the dotted string ``"1.2.840.113549.1.1.4"``. This is
        an MD5 digest signed by an RSA key.

    .. attribute:: RSA_WITH_SHA1

        Corresponds to the dotted string ``"1.2.840.113549.1.1.5"``. This is
        a SHA1 digest signed by an RSA key.

    .. attribute:: RSA_WITH_SHA224

        Corresponds to the dotted string ``"1.2.840.113549.1.1.14"``. This is
        a SHA224 digest signed by an RSA key.

    .. attribute:: RSA_WITH_SHA256

        Corresponds to the dotted string ``"1.2.840.113549.1.1.11"``. This is
        a SHA256 digest signed by an RSA key.

    .. attribute:: RSA_WITH_SHA384

        Corresponds to the dotted string ``"1.2.840.113549.1.1.12"``. This is
        a SHA384 digest signed by an RSA key.

    .. attribute:: RSA_WITH_SHA512

        Corresponds to the dotted string ``"1.2.840.113549.1.1.13"``. This is
        a SHA512 digest signed by an RSA key.

    .. attribute:: RSASSA_PSS

        .. versionadded:: 2.3

        Corresponds to the dotted string ``"1.2.840.113549.1.1.10"``. This is
        signed by an RSA key using the Probabilistic Signature Scheme (PSS)
        padding from :rfc:`4055`. The hash function and padding are defined by
        signature algorithm parameters.

    .. attribute:: ECDSA_WITH_SHA1

        Corresponds to the dotted string ``"1.2.840.10045.4.1"``. This is a SHA1
        digest signed by an ECDSA key.

    .. attribute:: ECDSA_WITH_SHA224

        Corresponds to the dotted string ``"1.2.840.10045.4.3.1"``. This is
        a SHA224 digest signed by an ECDSA key.

    .. attribute:: ECDSA_WITH_SHA256

        Corresponds to the dotted string ``"1.2.840.10045.4.3.2"``. This is
        a SHA256 digest signed by an ECDSA key.

    .. attribute:: ECDSA_WITH_SHA384

        Corresponds to the dotted string ``"1.2.840.10045.4.3.3"``. This is
        a SHA384 digest signed by an ECDSA key.

    .. attribute:: ECDSA_WITH_SHA512

        Corresponds to the dotted string ``"1.2.840.10045.4.3.4"``. This is
        a SHA512 digest signed by an ECDSA key.

    .. attribute:: DSA_WITH_SHA1

        Corresponds to the dotted string ``"1.2.840.10040.4.3"``. This is
        a SHA1 digest signed by a DSA key.

    .. attribute:: DSA_WITH_SHA224

        Corresponds to the dotted string ``"2.16.840.1.101.3.4.3.1"``. This is
        a SHA224 digest signed by a DSA key.

    .. attribute:: DSA_WITH_SHA256

        Corresponds to the dotted string ``"2.16.840.1.101.3.4.3.2"``. This is
        a SHA256 digest signed by a DSA key.

    .. attribute:: ED25519

        .. versionadded:: 2.8

        Corresponds to the dotted string ``"1.3.101.112"``. This is a signature
        using an ed25519 key.

    .. attribute:: ED448

        .. versionadded:: 2.8

        Corresponds to the dotted string ``"1.3.101.113"``. This is a signature
        using an ed448 key.


.. class:: ExtendedKeyUsageOID

    .. versionadded:: 1.0

    .. attribute:: SERVER_AUTH

        Corresponds to the dotted string ``"1.3.6.1.5.5.7.3.1"``. This is used
        to denote that a certificate may be used for TLS web server
        authentication.

    .. attribute:: CLIENT_AUTH

        Corresponds to the dotted string ``"1.3.6.1.5.5.7.3.2"``. This is used
        to denote that a certificate may be used for TLS web client
        authentication.

    .. attribute:: CODE_SIGNING

        Corresponds to the dotted string ``"1.3.6.1.5.5.7.3.3"``. This is used
        to denote that a certificate may be used for code signing.

    .. attribute:: EMAIL_PROTECTION

        Corresponds to the dotted string ``"1.3.6.1.5.5.7.3.4"``. This is used
        to denote that a certificate may be used for email protection.

    .. attribute:: TIME_STAMPING

        Corresponds to the dotted string ``"1.3.6.1.5.5.7.3.8"``. This is used
        to denote that a certificate may be used for time stamping.

    .. attribute:: OCSP_SIGNING

        Corresponds to the dotted string ``"1.3.6.1.5.5.7.3.9"``. This is used
        to denote that a certificate may be used for signing OCSP responses.

    .. attribute:: ANY_EXTENDED_KEY_USAGE

        .. versionadded:: 2.0

        Corresponds to the dotted string ``"2.5.29.37.0"``. This is used to
        denote that a certificate may be used for _any_ purposes. However,
        :rfc:`5280` additionally notes that applications that require the
        presence of a particular purpose _MAY_ reject certificates that include
        the ``anyExtendedKeyUsage`` OID but not the particular OID expected for
        the application. Therefore, the presence of this OID does not mean a
        given application will accept the certificate for all purposes.


.. class:: AuthorityInformationAccessOID

    .. versionadded:: 1.0

    .. attribute:: OCSP

        Corresponds to the dotted string ``"1.3.6.1.5.5.7.48.1"``. Used as the
        identifier for OCSP data in
        :class:`~cryptography.x509.AccessDescription` objects.

    .. attribute:: CA_ISSUERS

        Corresponds to the dotted string ``"1.3.6.1.5.5.7.48.2"``. Used as the
        identifier for CA issuer data in
        :class:`~cryptography.x509.AccessDescription` objects.


.. class:: SubjectInformationAccessOID

    .. versionadded:: 3.0

    .. attribute:: CA_REPOSITORY

        Corresponds to the dotted string ``"1.3.6.1.5.5.7.48.5"``. Used as the
        identifier for CA repository data in
        :class:`~cryptography.x509.AccessDescription` objects.


.. class:: CertificatePoliciesOID

    .. versionadded:: 1.0

    .. attribute:: CPS_QUALIFIER

        Corresponds to the dotted string ``"1.3.6.1.5.5.7.2.1"``.

    .. attribute:: CPS_USER_NOTICE

        Corresponds to the dotted string ``"1.3.6.1.5.5.7.2.2"``.

    .. attribute:: ANY_POLICY

        Corresponds to the dotted string ``"2.5.29.32.0"``.


.. class:: ExtensionOID

    .. versionadded:: 1.0

    .. attribute:: BASIC_CONSTRAINTS

        Corresponds to the dotted string ``"2.5.29.19"``. The identifier for the
        :class:`~cryptography.x509.BasicConstraints` extension type.

    .. attribute:: KEY_USAGE

        Corresponds to the dotted string ``"2.5.29.15"``. The identifier for the
        :class:`~cryptography.x509.KeyUsage` extension type.

    .. attribute:: SUBJECT_ALTERNATIVE_NAME

        Corresponds to the dotted string ``"2.5.29.17"``. The identifier for the
        :class:`~cryptography.x509.SubjectAlternativeName` extension type.

    .. attribute:: ISSUER_ALTERNATIVE_NAME

        Corresponds to the dotted string ``"2.5.29.18"``. The identifier for the
        :class:`~cryptography.x509.IssuerAlternativeName` extension type.

    .. attribute:: SUBJECT_KEY_IDENTIFIER

        Corresponds to the dotted string ``"2.5.29.14"``. The identifier for the
        :class:`~cryptography.x509.SubjectKeyIdentifier` extension type.

    .. attribute:: NAME_CONSTRAINTS

        Corresponds to the dotted string ``"2.5.29.30"``. The identifier for the
        :class:`~cryptography.x509.NameConstraints` extension type.

    .. attribute:: CRL_DISTRIBUTION_POINTS

        Corresponds to the dotted string ``"2.5.29.31"``. The identifier for the
        :class:`~cryptography.x509.CRLDistributionPoints` extension type.

    .. attribute:: CERTIFICATE_POLICIES

        Corresponds to the dotted string ``"2.5.29.32"``. The identifier for the
        :class:`~cryptography.x509.CertificatePolicies` extension type.

    .. attribute:: AUTHORITY_KEY_IDENTIFIER

        Corresponds to the dotted string ``"2.5.29.35"``. The identifier for the
        :class:`~cryptography.x509.AuthorityKeyIdentifier` extension type.

    .. attribute:: EXTENDED_KEY_USAGE

        Corresponds to the dotted string ``"2.5.29.37"``. The identifier for the
        :class:`~cryptography.x509.ExtendedKeyUsage` extension type.

    .. attribute:: AUTHORITY_INFORMATION_ACCESS

        Corresponds to the dotted string ``"1.3.6.1.5.5.7.1.1"``. The identifier
        for the :class:`~cryptography.x509.AuthorityInformationAccess` extension
        type.

    .. attribute:: SUBJECT_INFORMATION_ACCESS

        .. versionadded:: 3.0

        Corresponds to the dotted string ``"1.3.6.1.5.5.7.1.11"``. The
        identifier for the :class:`~cryptography.x509.SubjectInformationAccess`
        extension type.

    .. attribute:: INHIBIT_ANY_POLICY

        Corresponds to the dotted string ``"2.5.29.54"``. The identifier
        for the :class:`~cryptography.x509.InhibitAnyPolicy` extension type.

    .. attribute:: OCSP_NO_CHECK

        Corresponds to the dotted string ``"1.3.6.1.5.5.7.48.1.5"``. The
        identifier for the :class:`~cryptography.x509.OCSPNoCheck` extension
        type.

    .. attribute:: TLS_FEATURE

        Corresponds to the dotted string ``"1.3.6.1.5.5.7.1.24"``. The
        identifier for the :class:`~cryptography.x509.TLSFeature` extension
        type.

    .. attribute:: CRL_NUMBER

        Corresponds to the dotted string ``"2.5.29.20"``. The identifier for
        the ``CRLNumber`` extension type. This extension only has meaning
        for certificate revocation lists.

    .. attribute:: DELTA_CRL_INDICATOR

        .. versionadded:: 2.1

        Corresponds to the dotted string ``"2.5.29.27"``. The identifier for
        the ``DeltaCRLIndicator`` extension type. This extension only has
        meaning for certificate revocation lists.

    .. attribute:: PRECERT_SIGNED_CERTIFICATE_TIMESTAMPS

        .. versionadded:: 1.9

        Corresponds to the dotted string ``"1.3.6.1.4.1.11129.2.4.2"``.

    .. attribute:: PRECERT_POISON

        .. versionadded:: 2.4

        Corresponds to the dotted string ``"1.3.6.1.4.1.11129.2.4.3"``.

    .. attribute:: SIGNED_CERTIFICATE_TIMESTAMPS

        .. versionadded:: 3.0

        Corresponds to the dotted string ``"1.3.6.1.4.1.11129.2.4.5"``.

    .. attribute:: POLICY_CONSTRAINTS

        Corresponds to the dotted string ``"2.5.29.36"``. The identifier for the
        :class:`~cryptography.x509.PolicyConstraints` extension type.

    .. attribute:: FRESHEST_CRL

        Corresponds to the dotted string ``"2.5.29.46"``. The identifier for the
        :class:`~cryptography.x509.FreshestCRL` extension type.

    .. attribute:: ISSUING_DISTRIBUTION_POINT

        .. versionadded:: 2.4

        Corresponds to the dotted string ``"2.5.29.28"``.


.. class:: CRLEntryExtensionOID

    .. versionadded:: 1.2

    .. attribute:: CERTIFICATE_ISSUER

        Corresponds to the dotted string ``"2.5.29.29"``.

    .. attribute:: CRL_REASON

        Corresponds to the dotted string ``"2.5.29.21"``.

    .. attribute:: INVALIDITY_DATE

        Corresponds to the dotted string ``"2.5.29.24"``.


.. class:: OCSPExtensionOID

    .. versionadded:: 2.4

    .. attribute:: NONCE

        Corresponds to the dotted string ``"1.3.6.1.5.5.7.48.1.2"``.


.. class:: AttributeOID

    .. versionadded:: 3.0

    .. attribute:: CHALLENGE_PASSWORD

        Corresponds to the dotted string ``"1.2.840.113549.1.9.7"``.

    .. attribute:: UNSTRUCTURED_NAME

        Corresponds to the dotted string ``"1.2.840.113549.1.9.2"``.

Helper Functions
~~~~~~~~~~~~~~~~
.. currentmodule:: cryptography.x509

.. function:: random_serial_number()

    .. versionadded:: 1.6

    Generates a random serial number suitable for use when constructing
    certificates.

Exceptions
~~~~~~~~~~
.. currentmodule:: cryptography.x509

.. class:: InvalidVersion

    This is raised when an X.509 certificate has an invalid version number.

    .. attribute:: parsed_version

        :type: int

        Returns the raw version that was parsed from the certificate.

.. class:: DuplicateExtension

    This is raised when more than one X.509 extension of the same type is
    found within a certificate.

    .. attribute:: oid

        :type: :class:`ObjectIdentifier`

        Returns the OID.

.. class:: ExtensionNotFound

    This is raised when calling :meth:`Extensions.get_extension_for_oid` with
    an extension OID that is not present in the certificate.

    .. attribute:: oid

        :type: :class:`ObjectIdentifier`

        Returns the OID.

.. class:: AttributeNotFound

    This is raised when calling
    :meth:`CertificateSigningRequest.get_attribute_for_oid` with
    an attribute OID that is not present in the request.

    .. attribute:: oid

        :type: :class:`ObjectIdentifier`

        Returns the OID.

.. class:: UnsupportedGeneralNameType

    This is raised when a certificate contains an unsupported general name
    type in an extension.

    .. attribute:: type

        :type: int

        The integer value of the unsupported type. The complete list of
        types can be found in `RFC 5280 section 4.2.1.6`_.


.. _`RFC 5280 section 4.2.1.1`: https://tools.ietf.org/html/rfc5280#section-4.2.1.1
.. _`RFC 5280 section 4.2.1.6`: https://tools.ietf.org/html/rfc5280#section-4.2.1.6
.. _`CABForum Guidelines`: https://cabforum.org/baseline-requirements-documents/
Tutorial
========

X.509 certificates are used to authenticate clients and servers. The most
common use case is for web servers using HTTPS.

Creating a Certificate Signing Request (CSR)
--------------------------------------------

When obtaining a certificate from a certificate authority (CA), the usual
flow is:

1. You generate a private/public key pair.
2. You create a request for a certificate, which is signed by your key (to
   prove that you own that key).
3. You give your CSR to a CA (but *not* the private key).
4. The CA validates that you own the resource (e.g. domain) you want a
   certificate for.
5. The CA gives you a certificate, signed by them, which identifies your public
   key, and the resource you are authenticated for.
6. You configure your server to use that certificate, combined with your
   private key, to server traffic.

If you want to obtain a certificate from a typical commercial CA, here's how.
First, you'll need to generate a private key, we'll generate an RSA key (these
are the most common types of keys on the web right now):

.. code-block:: pycon

    >>> from cryptography.hazmat.primitives import serialization
    >>> from cryptography.hazmat.primitives.asymmetric import rsa
    >>> # Generate our key
    >>> key = rsa.generate_private_key(
    ...     public_exponent=65537,
    ...     key_size=2048,
    ... )
    >>> # Write our key to disk for safe keeping
    >>> with open("path/to/store/key.pem", "wb") as f:
    ...     f.write(key.private_bytes(
    ...         encoding=serialization.Encoding.PEM,
    ...         format=serialization.PrivateFormat.TraditionalOpenSSL,
    ...         encryption_algorithm=serialization.BestAvailableEncryption(b"passphrase"),
    ...     ))

If you've already generated a key you can load it with
:func:`~cryptography.hazmat.primitives.serialization.load_pem_private_key`.

Next we need to generate a certificate signing request. A typical CSR contains
a few details:

* Information about our public key (including a signature of the entire body).
* Information about who *we* are.
* Information about what domains this certificate is for.

.. code-block:: pycon

    >>> from cryptography import x509
    >>> from cryptography.x509.oid import NameOID
    >>> from cryptography.hazmat.primitives import hashes
    >>> # Generate a CSR
    >>> csr = x509.CertificateSigningRequestBuilder().subject_name(x509.Name([
    ...     # Provide various details about who we are.
    ...     x509.NameAttribute(NameOID.COUNTRY_NAME, u"US"),
    ...     x509.NameAttribute(NameOID.STATE_OR_PROVINCE_NAME, u"California"),
    ...     x509.NameAttribute(NameOID.LOCALITY_NAME, u"San Francisco"),
    ...     x509.NameAttribute(NameOID.ORGANIZATION_NAME, u"My Company"),
    ...     x509.NameAttribute(NameOID.COMMON_NAME, u"mysite.com"),
    ... ])).add_extension(
    ...     x509.SubjectAlternativeName([
    ...         # Describe what sites we want this certificate for.
    ...         x509.DNSName(u"mysite.com"),
    ...         x509.DNSName(u"www.mysite.com"),
    ...         x509.DNSName(u"subdomain.mysite.com"),
    ...     ]),
    ...     critical=False,
    ... # Sign the CSR with our private key.
    ... ).sign(key, hashes.SHA256())
    >>> # Write our CSR out to disk.
    >>> with open("path/to/csr.pem", "wb") as f:
    ...     f.write(csr.public_bytes(serialization.Encoding.PEM))

Now we can give our CSR to a CA, who will give a certificate to us in return.

Creating a self-signed certificate
----------------------------------

While most of the time you want a certificate that has been *signed* by someone
else (i.e. a certificate authority), so that trust is established, sometimes
you want to create a self-signed certificate. Self-signed certificates are not
issued by a certificate authority, but instead they are signed by the private
key corresponding to the public key they embed.

This means that other people don't trust these certificates, but it also means
they can be issued very easily. In general the only use case for a self-signed
certificate is local testing, where you don't need anyone else to trust your
certificate.

Like generating a CSR, we start with creating a new private key:

.. code-block:: pycon

    >>> # Generate our key
    >>> key = rsa.generate_private_key(
    ...     public_exponent=65537,
    ...     key_size=2048,
    ... )
    >>> # Write our key to disk for safe keeping
    >>> with open("path/to/store/key.pem", "wb") as f:
    ...     f.write(key.private_bytes(
    ...         encoding=serialization.Encoding.PEM,
    ...         format=serialization.PrivateFormat.TraditionalOpenSSL,
    ...         encryption_algorithm=serialization.BestAvailableEncryption(b"passphrase"),
    ...     ))

Then we generate the certificate itself:

.. code-block:: pycon

    >>> # Various details about who we are. For a self-signed certificate the
    >>> # subject and issuer are always the same.
    >>> subject = issuer = x509.Name([
    ...     x509.NameAttribute(NameOID.COUNTRY_NAME, u"US"),
    ...     x509.NameAttribute(NameOID.STATE_OR_PROVINCE_NAME, u"California"),
    ...     x509.NameAttribute(NameOID.LOCALITY_NAME, u"San Francisco"),
    ...     x509.NameAttribute(NameOID.ORGANIZATION_NAME, u"My Company"),
    ...     x509.NameAttribute(NameOID.COMMON_NAME, u"mysite.com"),
    ... ])
    >>> cert = x509.CertificateBuilder().subject_name(
    ...     subject
    ... ).issuer_name(
    ...     issuer
    ... ).public_key(
    ...     key.public_key()
    ... ).serial_number(
    ...     x509.random_serial_number()
    ... ).not_valid_before(
    ...     datetime.datetime.utcnow()
    ... ).not_valid_after(
    ...     # Our certificate will be valid for 10 days
    ...     datetime.datetime.utcnow() + datetime.timedelta(days=10)
    ... ).add_extension(
    ...     x509.SubjectAlternativeName([x509.DNSName(u"localhost")]),
    ...     critical=False,
    ... # Sign our certificate with our private key
    ... ).sign(key, hashes.SHA256())
    >>> # Write our certificate out to disk.
    >>> with open("path/to/certificate.pem", "wb") as f:
    ...     f.write(cert.public_bytes(serialization.Encoding.PEM))

And now we have a private key and certificate that can be used for local
testing.

Determining Certificate or Certificate Signing Request Key Type
---------------------------------------------------------------

Certificates and certificate signing requests can be issued with multiple
key types. You can determine what the key type is by using ``isinstance``
checks:

.. code-block:: pycon

    >>> public_key = cert.public_key()
    >>> if isinstance(public_key, rsa.RSAPublicKey):
    ...     # Do something RSA specific
    ... elif isinstance(public_key, ec.EllipticCurvePublicKey):
    ...     # Do something EC specific
    ... else:
    ...     # Remember to handle this case
netifaces 0.10.6
================

.. image:: https://drone.io/bitbucket.org/al45tair/netifaces/status.png
   :target: https://drone.io/bitbucket.org/al45tair/netifaces/latest
   :alt: Build Status

1. What is this?
----------------

It's been annoying me for some time that there's no easy way to get the
address(es) of the machine's network interfaces from Python.  There is
a good reason for this difficulty, which is that it is virtually impossible
to do so in a portable manner.  However, it seems to me that there should
be a package you can easy_install that will take care of working out the
details of doing so on the machine you're using, then you can get on with
writing Python code without concerning yourself with the nitty gritty of
system-dependent low-level networking APIs.

This package attempts to solve that problem.

2. How do I use it?
-------------------

First you need to install it, which you can do by typing::

  tar xvzf netifaces-0.10.6.tar.gz
  cd netifaces-0.10.6
  python setup.py install

**Note that you will need the relevant developer tools for your platform**,
as netifaces is written in C and installing this way will compile the extension.

Once that's done, you'll need to start Python and do something like the
following::

>>> import netifaces

Then if you enter

>>> netifaces.interfaces()
['lo0', 'gif0', 'stf0', 'en0', 'en1', 'fw0']

you'll see the list of interface identifiers for your machine.

You can ask for the addresses of a particular interface by doing

>>> netifaces.ifaddresses('lo0')
{18: [{'addr': ''}], 2: [{'peer': '127.0.0.1', 'netmask': '255.0.0.0', 'addr': '127.0.0.1'}], 30: [{'peer': '::1', 'netmask': 'ffff:ffff:ffff:ffff:ffff:ffff:ffff:ffff', 'addr': '::1'}, {'peer': '', 'netmask': 'ffff:ffff:ffff:ffff::', 'addr': 'fe80::1%lo0'}]}

Hmmmm.  That result looks a bit cryptic; let's break it apart and explain
what each piece means.  It returned a dictionary, so let's look there first::

  { 18: [...], 2: [...], 30: [...] }

Each of the numbers refers to a particular address family.  In this case, we
have three address families listed; on my system, 18 is ``AF_LINK`` (which means
the link layer interface, e.g. Ethernet), 2 is ``AF_INET`` (normal Internet
addresses), and 30 is ``AF_INET6`` (IPv6).

But wait!  Don't use these numbers in your code.  The numeric values here are
system dependent; fortunately, I thought of that when writing netifaces, so
the module declares a range of values that you might need.  e.g.

>>> netifaces.AF_LINK
18

Again, on your system, the number may be different.

So, what we've established is that the dictionary that's returned has one
entry for each address family for which this interface has an address.  Let's
take a look at the ``AF_INET`` addresses now:

>>> addrs = netifaces.ifaddresses('lo0')
>>> addrs[netifaces.AF_INET]
[{'peer': '127.0.0.1', 'netmask': '255.0.0.0', 'addr': '127.0.0.1'}]

You might be wondering why this value is a list.  The reason is that it's
possible for an interface to have more than one address, even within the
same family.  I'll say that again: *you can have more than one address of
the same type associated with each interface*.

*Asking for "the" address of a particular interface doesn't make sense.*

Right, so, we can see that this particular interface only has one address,
and, because it's a loopback interface, it's point-to-point and therefore
has a *peer* address rather than a broadcast address.

Let's look at a more interesting interface.

>>> addrs = netifaces.ifaddresses('en0')
>>> addrs[netifaces.AF_INET]
[{'broadcast': '10.15.255.255', 'netmask': '255.240.0.0', 'addr': '10.0.1.4'}, {'broadcast': '192.168.0.255', 'addr': '192.168.0.47'}]

This interface has two addresses (see, I told you...)  Both of them are
regular IPv4 addresses, although in one case the netmask has been changed
from its default.  The netmask *may not* appear on your system if it's set
to the default for the address range.

Because this interface isn't point-to-point, it also has broadcast addresses.

Now, say we want, instead of the IP addresses, to get the MAC address; that
is, the hardware address of the Ethernet adapter running this interface.  We
can do

>>> addrs[netifaces.AF_LINK]
[{'addr': '00:12:34:56:78:9a'}]

Note that this may not be available on platforms without getifaddrs(), unless
they happen to implement ``SIOCGIFHWADDR``.  Note also that you just get the
address; it's unlikely that you'll see anything else with an ``AF_LINK`` address.
Oh, and don't assume that all ``AF_LINK`` addresses are Ethernet; you might, for
instance, be on a Mac, in which case:

>>> addrs = netifaces.ifaddresses('fw0')
>>> addrs[netifaces.AF_LINK]
[{'addr': '00:12:34:56:78:9a:bc:de'}]

No, that isn't an exceptionally long Ethernet MAC address---it's a FireWire
address.

As of version 0.10.0, you can also obtain a list of gateways on your
machine:

>>> netifaces.gateways()
{2: [('10.0.1.1', 'en0', True), ('10.2.1.1', 'en1', False)], 30: [('fe80::1', 'en0', True)], 'default': { 2: ('10.0.1.1', 'en0'), 30: ('fe80::1', 'en0') }}

This dictionary is keyed on address family---in this case, ``AF_INET``---and
each entry is a list of gateways as ``(address, interface, is_default)`` tuples.
Notice that here we have two separate gateways for IPv4 (``AF_INET``); some
operating systems support configurations like this and can either route packets
based on their source, or based on administratively configured routing tables.

For convenience, we also allow you to index the dictionary with the special
value ``'default'``, which returns a dictionary mapping address families to the
default gateway in each case.  Thus you can get the default IPv4 gateway with

>>> gws = netifaces.gateways()
>>> gws['default'][netifaces.AF_INET]
('10.0.1.1', 'en0')

Do note that there may be no default gateway for any given address family;
this is currently very common for IPv6 and much less common for IPv4 but it
can happen even for ``AF_INET``.

BTW, if you're trying to configure your machine to have multiple gateways for
the same address family, it's a very good idea to check the documentation for
your operating system *very* carefully, as some systems become extremely
confused or route packets in a non-obvious manner.

I'm very interested in hearing from anyone (on any platform) for whom the
``gateways()`` method doesn't produce the expected results.  It's quite
complicated extracting this information from the operating system (whichever
operating system we're talking about), and so I expect there's at least one
system out there where this just won't work.

3. This is great!  What platforms does it work on?
--------------------------------------------------

It gets regular testing on OS X, Linux and Windows.  It has also been used
successfully on Solaris, and it's expected to work properly on other UNIX-like
systems as well.  If you are running something that is not supported, and
wish to contribute a patch, please use BitBucket to send a pull request.

4. What license is this under?
------------------------------

It's an MIT-style license.  Here goes:

Copyright (c) 2007-2017 Alastair Houghton

Permission is hereby granted, free of charge, to any person obtaining a copy
of this software and associated documentation files (the "Software"), to deal
in the Software without restriction, including without limitation the rights
to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
copies of the Software, and to permit persons to whom the Software is
furnished to do so, subject to the following conditions:

The above copyright notice and this permission notice shall be included in all
copies or substantial portions of the Software.

THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
SOFTWARE.

5. Why the jump to 0.10.0?
--------------------------

Because someone released a fork of netifaces with the version 0.9.0.
Hopefully skipping the version number should remove any confusion.  In 
addition starting with 0.10.0 Python 3 is now supported and other 
features/bugfixes have been included as well.  See the CHANGELOG for a
more complete list of changes.
Decorators for Humans
=====================

The goal of the decorator module is to make it easy to define
signature-preserving function decorators and decorator factories.
It also includes an implementation of multiple dispatch and other niceties
(please check the docs). It is released under a two-clauses
BSD license, i.e. basically you can do whatever you want with it but I am not
responsible.

Installation
-------------

If you are lazy, just perform

 ``$ pip install decorator``

which will install just the module on your system.

If you prefer to install the full distribution from source, including
the documentation, clone the `GitHub repo`_ or download the tarball_, unpack it and run

 ``$ pip install .``

in the main directory, possibly as superuser.

.. _tarball: https://pypi.org/project/decorator/#files
.. _GitHub repo: https://github.com/micheles/decorator

Testing
--------

If you have the source code installation you can run the tests with

 `$ python src/tests/test.py -v`

or (if you have setuptools installed)

 `$ python setup.py test`

Notice that you may run into trouble if in your system there
is an older version of the decorator module; in such a case remove the
old version. It is safe even to copy the module `decorator.py` over
an existing one, since we kept backward-compatibility for a long time.

Repository
---------------

The project is hosted on GitHub. You can look at the source here:

 https://github.com/micheles/decorator

Documentation
---------------

The documentation has been moved to https://github.com/micheles/decorator/blob/master/docs/documentation.md

From there you can get a PDF version by simply using the print
functionality of your browser.

Here is the documentation for previous versions of the module:

https://github.com/micheles/decorator/blob/4.3.2/docs/tests.documentation.rst
https://github.com/micheles/decorator/blob/4.2.1/docs/tests.documentation.rst
https://github.com/micheles/decorator/blob/4.1.2/docs/tests.documentation.rst
https://github.com/micheles/decorator/blob/4.0.0/documentation.rst
https://github.com/micheles/decorator/blob/3.4.2/documentation.rst

For the impatient
-----------------

Here is an example of how to define a family of decorators tracing slow
operations:

.. code-block:: python

   from decorator import decorator

   @decorator
   def warn_slow(func, timelimit=60, *args, **kw):
       t0 = time.time()
       result = func(*args, **kw)
       dt = time.time() - t0
       if dt > timelimit:
           logging.warn('%s took %d seconds', func.__name__, dt)
       else:
           logging.info('%s took %d seconds', func.__name__, dt)
       return result

   @warn_slow  # warn if it takes more than 1 minute
   def preprocess_input_files(inputdir, tempdir):
       ...

   @warn_slow(timelimit=600)  # warn if it takes more than 10 minutes
   def run_calculation(tempdir, outdir):
       ...

Enjoy!
History
=======

1.2.2: 2020-08-07
-----------------

- Use USER_NAME instead of HOSTBASED_SERVICE for user principals
- Remove unused imports in example code
- Fix typo in explicit mech example

1.2.1: 2020-03-31
-----------------

- Include tests in sdist tarball
- Don't limit contexts to a single server name

1.2.0: 2020-02-18
-----------------

- Add support for specifing an explicit GSSAPI mech

1.1.1: 2020-02-18
-----------------

- Fix DOS bug around Negotiate regular expressoin
- Update README to include section on setup

1.1.0: 2019-05-21
-----------------

- Disable mutual authentication by default
- Add more documentation on MutualAuthenticationError

1.0.1: 2019-04-10
-----------------

- Fix example in README
- Fix license detection for PyPI
- Fix a problem with regex escaping
- Add COPR Makefile target

1.0.0: 2017-12-14
-----------------

- Fork project to requests-gssapi
- Replace pykerberos with python-gssapi
- Add HTTPSPNEGOAuth interface.  HTTPKerberosAuth is retained as a shim, but
  bump the major version anyway for clarity.

0.11.0: 2016-11-02
------------------

- Switch dependency on Windows from kerberos-sspi/pywin32 to WinKerberos.
  This brings Custom Principal support to Windows users.

0.10.0: 2016-05-18
------------------

- Make it possible to receive errors without having their contents and headers
  stripped.
- Resolve a bug caused by passing the ``principal`` keyword argument to
  kerberos-sspi on Windows.

0.9.0: 2016-05-06
-----------------

- Support for principal, hostname, and realm override.

- Added support for mutual auth.

0.8.0: 2016-01-07
-----------------

- Support for Kerberos delegation.

- Fixed problems declaring kerberos-sspi on Windows installs.

0.7.0: 2015-05-04
-----------------

- Added Windows native authentication support by adding kerberos-sspi as an
  alternative backend.

- Prevent infinite recursion when a server returns 401 to an authorization
  attempt.

- Reduce the logging during successful responses.

0.6.1: 2014-11-14
-----------------

- Fix HTTPKerberosAuth not to treat non-file as a file

- Prevent infinite recursion when GSSErrors occurs

0.6: 2014-11-04
---------------

- Handle mutual authentication (see pull request 36_)

  All users should upgrade immediately. This has been reported to
  oss-security_ and we are awaiting a proper CVE identifier.

  **Update**: We were issued CVE-2014-8650

- Distribute as a wheel.

.. _36: https://github.com/requests/requests-kerberos/pull/36
.. _oss-security: http://www.openwall.com/lists/oss-security/

0.5: 2014-05-14
---------------

- Allow non-HTTP service principals with HTTPKerberosAuth using a new optional
  argument ``service``.

- Fix bug in ``setup.py`` on distributions where the ``compiler`` module is
  not available.

- Add test dependencies to ``setup.py`` so ``python setup.py test`` will work.

0.4: 2013-10-26
---------------

- Minor updates in the README
- Change requirements to depend on requests above 1.1.0

0.3: 2013-06-02
---------------

- Work with servers operating on non-standard ports

0.2: 2013-03-26
---------------

- Not documented

0.1: Never released
-------------------

- Initial Release
requests GSSAPI authentication library
===============================================

Requests is an HTTP library, written in Python, for human beings. This library
adds optional GSSAPI authentication support and supports mutual
authentication.

It provides a fully backward-compatible shim for the old
python-requests-kerberos library: simply replace ``import requests_kerberos``
with ``import requests_gssapi``.  A more powerful interface is provided by the
HTTPSPNEGOAuth component, but this is of course not guaranteed to be
compatible.  Documentation below is written toward the new interface.

Basic GET usage:


.. code-block:: python

    >>> import requests
    >>> from requests_gssapi import HTTPSPNEGOAuth
    >>> r = requests.get("http://example.org", auth=HTTPSPNEGOAuth())
    ...

The entire ``requests.api`` should be supported.

Setup
-----

In order to use this library, there must already be a Kerberos Ticket-Granting
Ticket (TGT) in a credential cache (ccache).  Whether a TGT is available can
be easily determined by running the ``klist`` command.  If no TGT is
available, then it first must be obtained (for instance, by running the
``kinit`` command, or pointing the $KRB5CCNAME to a credential cache with a
valid TGT).

In short, the library will handle the "negotiations" of Kerberos
authentication, but ensuring that a credentials are available and valid is the
responsibility of the user.

Authentication Failures
-----------------------

Client authentication failures will be communicated to the caller by returning
a 401 response.  A 401 response may also be the result of expired credentials
(including the TGT).

Mutual Authentication
---------------------

Mutual authentication is a poorly-named feature of the GSSAPI which doesn't
provide any additional security benefit to most possible uses of
requests_gssapi.  Practically speaking, in most mechanism implementations
(including krb5), it requires another round-trip between the client and server
during the authentication handshake.  Many clients and servers do not properly
handle the authentication handshake taking more than one round-trip.  If you
encounter a MutualAuthenticationError, this is probably why.

So long as you're running over a TLS link whose security guarantees you trust,
there's no benefit to mutual authentication.  If you don't trust the link at
all, mutual authentication won't help (since it's not tamper-proof, and GSSAPI
isn't being used post-authentication.  There's some middle ground between the
two where it helps a small amount (e.g., passive adversary over
encrypted-but-unverified channel), but for Negotiate (what we're doing here),
it's not generally helpful.

For a more technical explanation of what mutual authentication actually
guarantees, I refer you to rfc2743 (GSSAPIv2), rfc4120 (krb5 in GSSAPI),
rfc4178 (SPNEGO), and rfc4559 (HTTP Negotiate).


DISABLED
^^^^^^^^

By default, there's no need to explicitly disable mutual authentication.
However, for compatability with older versions of request_gssapi or
requests_kerberos, you can explicitly request it not be attempted:

.. code-block:: python

    >>> import requests
    >>> from requests_gssapi import HTTPSPNEGOAuth, DISABLED
    >>> gssapi_auth = HTTPSPNEGOAuth(mutual_authentication=DISABLED)
    >>> r = requests.get("https://example.org", auth=gssapi_auth)
    ...

REQUIRED
^^^^^^^^

This was historically the default, but no longer is.  If requested,
``HTTPSPNEGOAuth`` will require mutual authentication from the server, and if
a server emits a non-error response which cannot be authenticated, a
``requests_gssapi.errors.MutualAuthenticationError`` will be raised.  (See
above for what this means.)  If a server emits an error which cannot be
authenticated, it will be returned to the user but with its contents and
headers stripped.  If the response content is more important than the need for
mutual auth on errors, (eg, for certain WinRM calls) the stripping behavior
can be suppressed by setting ``sanitize_mutual_error_response=False``:

.. code-block:: python

    >>> import requests
    >>> from requests_gssapi import HTTPSPNEGOAuth, REQUIRED
    >>> gssapi_auth = HTTPSPNEGOAuth(mutual_authentication=REQUIRED, sanitize_mutual_error_response=False)
    >>> r = requests.get("https://windows.example.org/wsman", auth=gssapi_auth)
    ...

OPTIONAL
^^^^^^^^

This will cause ``requests_gssapi`` to attempt mutual authentication if the
server advertises that it supports it, and cause a failure if authentication
fails, but not if the server does not support it at all.  This is probably not
what you want: link tampering will either cause hard failures, or silently
cause it to not happen at all.  It is retained for compatability.

.. code-block:: python

    >>> import requests
    >>> from requests_gssapi import HTTPSPNEGOAuth, OPTIONAL
    >>> gssapi_auth = HTTPSPNEGOAuth(mutual_authentication=OPTIONAL)
    >>> r = requests.get("https://example.org", auth=gssapi_auth)
    ...

Opportunistic Authentication
----------------------------

``HTTPSPNEGOAuth`` can be forced to preemptively initiate the GSSAPI
exchange and present a token on the initial request (and all
subsequent). By default, authentication only occurs after a
``401 Unauthorized`` response containing a Negotiate challenge
is received from the origin server. This can cause mutual authentication
failures for hosts that use a persistent connection (eg, Windows/WinRM), as
no GSSAPI challenges are sent after the initial auth handshake. This
behavior can be altered by setting  ``opportunistic_auth=True``:

.. code-block:: python

    >>> import requests
    >>> from requests_gssapi import HTTPSPNEGOAuth
    >>> gssapi_auth = HTTPSPNEGOAuth(opportunistic_auth=True)
    >>> r = requests.get("https://windows.example.org/wsman", auth=gssapi_auth)
    ...

Hostname Override
-----------------

If communicating with a host whose DNS name doesn't match its
hostname (eg, behind a content switch or load balancer),
the hostname used for the GSSAPI exchange can be overridden by
passing in a custom name (string or ``gssapi.Name``):

.. code-block:: python

    >>> import requests
    >>> from requests_gssapi import HTTPSPNEGOAuth
    >>> gssapi_auth = HTTPSPNEGOAuth(target_name="internalhost.local")
    >>> r = requests.get("https://externalhost.example.org/", auth=gssapi_auth)
    ...

Explicit Principal
------------------

``HTTPSPNEGOAuth`` normally uses the default principal (ie, the user for whom
you last ran ``kinit`` or ``kswitch``, or an SSO credential if
applicable). However, an explicit credential can be in instead, if desired.

.. code-block:: python

    >>> import gssapi
    >>> import requests
    >>> from requests_gssapi import HTTPSPNEGOAuth
    >>> name = gssapi.Name("user@REALM", gssapi.NameType.hostbased_service)
    >>> creds = gssapi.Credentials(name=name, usage="initiate")
    >>> gssapi_auth = HTTPSPNEGOAuth(creds=creds)
    >>> r = requests.get("http://example.org", auth=gssapi_auth)
    ...

Explicit Mechanism
------------------

``HTTPSPNEGOAuth`` normally lets the underlying ``gssapi`` library decide which
negotiation mechanism to use. However, an explicit mechanism can be used instead
if desired. The ``mech`` parameter will be passed straight through to ``gssapi``
without interference. It is expected to be an instance of ``gssapi.mechs.Mechanism``.

.. code-block:: python

    >>> import gssapi
    >>> import requests
    >>> from requests_gssapi import HTTPSPNEGOAuth
    >>> try:
    ...   spnego = gssapi.mechs.Mechanism.from_sasl_name("SPNEGO")
    ... except AttributeError:
    ...   spnego = gssapi.OID.from_int_seq("1.3.6.1.5.5.2")
    >>> gssapi_auth = HTTPSPNEGOAuth(mech=spnego)
    >>> r = requests.get("http://example.org", auth=gssapi_auth)
    ...

Delegation
----------

``requests_gssapi`` supports credential delegation (``GSS_C_DELEG_FLAG``).
To enable delegation of credentials to a server that requests delegation, pass
``delegate=True`` to ``HTTPSPNEGOAuth``:

.. code-block:: python

    >>> import requests
    >>> from requests_gssapi import HTTPSPNEGOAuth
    >>> r = requests.get("http://example.org", auth=HTTPSPNEGOAuth(delegate=True))
    ...

Be careful to only allow delegation to servers you trust as they will be able
to impersonate you using the delegated credentials.

Logging
-------

This library makes extensive use of Python's logging facilities.

Log messages are logged to the ``requests_gssapi`` and
``requests_gssapi.gssapi`` named loggers.

If you are having difficulty we suggest you configure logging. Issues with the
underlying GSSAPI libraries will be made apparent. Additionally, copious debug
information is made available which may assist in troubleshooting if you
increase your log level all the way up to debug.
================
CVE-2017-1002153
================

Koji 1.13.0 does not properly validate SCM paths.


Summary
-------

Koji 1.13.0 does not properly validate SCM paths, allowing an attacker to work around blacklisted paths for build submission.


Bug fix
-------

Koji versions 1.14.0 and forward contain the fix.

This bug was tracked as `issue#563 <https://pagure.io/koji/issue/563>`_

Links
-----

Fixed versions can be found at our releases page:

    `https://pagure.io/koji/releases <https://pagure.io/koji/releases>`_
========================
FAQ for CVE-2018-1002150
========================

Following are answers to some questions regarding CVE-2018-1002150
for Koji. If you haven’t already, you should read the
:doc:`announcement <CVE-2018-1002150>`.

If you have questions not covered here or in the announcement, please
ask them on the koji-devel mailing list.

    https://lists.fedorahosted.org/archives/list/koji-devel@lists.fedorahosted.org/

Q: Does this issue affect Koji clients or builders?

    The issue only affects the Koji hub.

Q: How can I tell if I’ve been attacked?

    We don’t know of any exploits in the wild. However, to be
    safe, we will release an intrusion detection document in a few
    days.

Q: Where are the fixed versions?

    | Koji versions before 1.12.0 are unaffected
    | For Koji 1.12, 1.12.1 and higher includes the fix
    | For Koji 1.13, 1.13.1 and higher includes the fix
    | For Koji 1.14, 1.14.1 and higher includes the fix
    | For Koji 1.15, 1.15.1 and higher includes the fix
    | Koji 1.16.0 and higher will include the fix

    You can find all of these versions on our releases page:

    https://pagure.io/koji/releases

Q: What about versions before 1.12.0?

    Koji versions before 1.12.0 are unaffected (they don't have the dist-repo
    feature). However, it would be wise to update your system to the current
    version.

Q: What can be done with this exploit?

    The attacker can trick Koji into moving files around. These can be
    almost any file that the httpd user can write. The attacker could
    use this to corrupt Koji’s file store or to reveal any secret files
    that the httpd user can read.

Q: Can the attacker execute arbitrary code?

    Not that we know of.

Q: Where can I get more help?

    You can ask questions on the koji-devel mailing list
    (`koji-devel@fedorahosted.org <mailto:koji-devel@fedorahosted.org>`_).

    For real time communication, we have the #koji IRC channel on
    `Freenode <https://freenode.net/>`_.
    The best time to ask would be during the Koji devel team
    “office hours”, which are held each Tuesday and Thursday from
    10-11am eastern time.
================
CVE-2018-1002150
================

Dist repo call missing authorization check allowing filesystem manipulation


.. toctree::
    :hidden:

    CVE-2018-1002150-FAQ

Summary
-------

This is a critical security bug.

From versions 1.12.0 to 1.15.0, the Koji hub did not perform proper
access checks for the hub.distRepoMove call. By passing carefully
constructed arguments to the call, an unauthenticated user can trick
Koji into moving content around that it should not. This could result in
corrupting any files that the httpd process can write to, or revealing
any files that the httpd process can read. If the user can authenticate
(at any privilege level), then they can use this mechanism to replace a
file with one that they have uploaded.

Workaround
----------

*We strongly recommend that all Koji admins implement this workaround
immediately.* This workaround will effectively disable dist-repo
functionality.

Because use of the hub.distRepoMove call requires a valid dist repo that
exists on disk, exploitation can be blocked by ensuring that there are
none. There are many ways this might be done. We recommend the
following:

1. Move the repos-dist directory to another location (if it exists)
2. Replace it with a plain text file warning of the situation. Do not
   skip this step.

For example::

    $ cd /mnt/koji
    $ mv repos-dist repos-dist.old
    $ echo "DO NOT REMOVE. CVE-2018-1002150" > repos-dist
    $ ls -l /mnt/koji/repos-dist
    -rw-r--r--. 1 root root 32 Mar 19 14:35 /mnt/koji/repos-dist

When applying this workaround, make sure to take both steps. If you do
not, then the system will recreate the directory if anyone creates
a new dist repo.


Bug fix
-------

*Note: because code fixes can take time to deploy, we strongly recommend
that all admins apply the above workaround first. The workaround can be
easily undone once the fix is in place.*

We are releasing updates for each affected version of Koji to fix this
bug. The following `releases <https://pagure.io/koji/releases>`_ all
contain the fix:

-  1.15.1
-  1.14.1
-  1.13.1
-  1.12.1

Versions prior to 1.12.0 are not vulnerable because they do not have the
dist-repo feature. Also, the legacy-py24 branch is unaffected since it
is client-only (no hub).

For users who have customized their Koji code, we recommend rebasing
your work onto the appropriate update release. If this is not feasible,
the patch should be very easy to apply. Please see `issue
#850 <https://pagure.io/koji/issue/850>`_ for the code details.

As with all changes to hub code, you must restart httpd for the changes
to take effect.

Links
-----

Fixed versions can be found at our releases page:

    https://pagure.io/koji/releases

Questions and answers about this issue

    :doc:`CVE-2018-1002150-FAQ`
========================
FAQ for CVE-2018-1002161
========================

Following are answers to some questions regarding CVE-2018-1002161
for Koji. If you haven’t already, you should read the
:doc:`announcement <CVE-2018-1002161>`.

If you have questions not covered here or in the announcement, please
ask them on the koji-devel mailing list.

    https://lists.fedorahosted.org/archives/list/koji-devel@lists.fedorahosted.org/

Q: Does this issue affect Koji clients or builders?

    The issue only affects the Koji hub.

Q: Which versions of Koji are affected?

    All previous versions of Koji are affected, except for the legacy-py24
    branch because it contains no hub code.

Q: Where are the fixed versions?

    | For Koji 1.11, 1.11.1 and higher include the fix
    | For Koji 1.12, 1.12.2 and higher include the fix
    | For Koji 1.13, 1.13.2 and higher include the fix
    | For Koji 1.14, 1.14.2 and higher include the fix
    | For Koji 1.15, 1.15.2 and higher include the fix
    | For Koji 1.16.2 and higher include the fix

    You can find all of these versions on our releases page:

    https://pagure.io/koji/releases

Q: What about older versions?

    We have only backported the fix to Koji versions released in the past few
    years. If you are still using a very old version of Koji, we strongly
    recommend that you shut it down and migrate to a newer version.

Q: What can be done with this exploit?

    The attacker can directly manipulate the database as they see fit. This
    would, among other things, allow them to gain the admin permission within
    Koji. They could destroy or corrupt the database, add new builds, replace
    existing builds, or any number of other things.

Q: Can the attacker execute arbitrary code?

    On the hub, not that we know of.

    However, they could create arbitrary tasks, which would be run by the build
    hosts.

Q: Where can I get more help?

    You can ask questions on the koji-devel mailing list
    (`koji-devel@fedorahosted.org <mailto:koji-devel@fedorahosted.org>`_).

    For real time communication, we have the #koji IRC channel on
    `Freenode <https://freenode.net/>`_.
    The best time to ask would be during the Koji devel team
    “office hours”, which are held each Tuesday and Thursday from
    10-11am eastern time.
================
CVE-2018-1002161
================

SQL injection in multiple remote calls

.. toctree::
    :hidden:

    CVE-2018-1002161-FAQ


Summary
-------

This is a critical security bug.

Multiple xmlrpc call handlers in Koji’s hub code contain SQL injection bugs. By
passing carefully constructed arguments to these calls, an unauthenticated user
can issue arbitrary SQL commands to Koji’s database. This gives the attacker
broad ability to manipulate or destroy data.

There is no known workaround. All Koji admins are encouraged to update to a
fixed version as soon as possible.



Bug fix
-------

Note: because code fixes can take time to deploy, we recommend
that all admins shut down their Koji hub instances until the fix
can be applied.

We are releasing updates for several recent versions of Koji to fix this
bug. The following `releases <https://pagure.io/koji/releases>`_ all
contain the fix:

-  1.16.2
-  1.15.2
-  1.14.2
-  1.13.2
-  1.12.2
-  1.11.1

Note: the legacy-py24 branch is unaffected since it
is client-only (no hub).

For users who have customized their Koji code, we recommend rebasing
your work onto the appropriate update release. If this is not feasible,
the patch should be very easy to apply. Please see `issue
#1183 <https://pagure.io/koji/issue/1183>`_ for the code details.

As with all changes to hub code, you must restart httpd for the changes
to take effect.

Links
-----

Fixed versions can be found at our releases page:

    https://pagure.io/koji/releases

Questions and answers about this issue

    :doc:`CVE-2018-1002161-FAQ`
==============
CVE-2019-17109
==============

Koji hub allows arbitrary upload destinations


Summary
-------

The way that the hub code validates upload paths allows for an attacker to
choose an arbitrary destination for the uploaded file.

Uploading still requires login. However, an attacker with credentials could
damage the integrity of the Koji system.

There is no known workaround. All Koji admins are encouraged to update to a
fixed version as soon as possible.



Bug fix
-------

We are releasing updates for affected versions of Koji from within the
past two years.
The following releases all contain the fix:

- 1.18.1
- 1.17.1
- 1.16.3
- 1.15.3
- 1.14.3

Note: the legacy-py24 branch is unaffected since it is client-only (no hub).

Anyone using a Koji version older than two years should update to a more
current version as soon as possible.

For users who have customized their Koji code, we recommend rebasing your work
onto the appropriate update release. Please see Koji
`issue #1634 <https://pagure.io/koji/issue/1634>`_ for the code details.

As with all changes to hub code, you must restart httpd for the changes to
take effect.


Links
-----

Fixed versions can be found at our releases page:

    https://pagure.io/koji/releases
==============
CVE-2020-15856
==============

XSS attack on kojiweb

Summary
-------

Web interface can be abused by XSS attack. Attackers can supply subversive HTTP
links containing malicious javascript code. Such links were not controlled
properly, so attackers can potentially force users to submit actions which were
not intended. Some actions which can be done via web UI can be destructive, so
updating to this version is highly recommended.

Bug fix
-------

We are releasing updates for affected versions of Koji from within the
past year.
The following releases all contain the fix:

- 1.23.1
- 1.22.2
- 1.21.2

Anyone using a Koji version older than a year should update to a more
current version as soon as possible.

For users who have customized their Koji code, we recommend rebasing your work
onto the appropriate update release. Please see Koji
`issue #2645 <https://pagure.io/koji/issue/2645>`_ for the code details.

As with all changes to web code, you must restart httpd for the changes to
take effect.

Links
-----

Fixed versions can be found at our releases page:

    https://pagure.io/koji/releases
=========
Koji CVEs
=========

.. toctree::
    :titlesonly:

    CVE-2020-15856
    CVE-2019-17109
    CVE-2018-1002161
    CVE-2018-1002150
    CVE-2017-1002153
==========
Koji HOWTO
==========

Introduction
============

Koji is a system for building and tracking RPMs. It was designed with
the following features in mind:

**Security**

-  New buildroot for each build
-  nfs is used (mostly) read-only

**Leverage other software**

-  Uses Yum and Mock open-source components
-  XML-RPC APIs for easy integration with other tools

**Flexibility**

-  rich data model
-  active code base

**Usability**

-  Web interface with Kerberos authentication
-  Thin, portable client
-  Users can create local buildroots

**Reproducibility**

-  Buildroot contents are tracked in the database
-  Versioned data

This HOWTO document covers the basic tasks that a developer needs to be
able to accomplish with Koji.

Getting started
===============

The web interface
-----------------

The primary interface for viewing Koji data is a web application. Most
of the interface is read-only, but if you are logged in (see below) and
have sufficient privileges there are some actions that can be performed
though the web. For example:

-  Cancel a build
-  Resubmit a failed task

Those with admin privileges will find additional actions, such as:

-  Create/Edit/Delete a tag
-  Create/Edit/Delete a target
-  Enable/Disable a build host

The web site utilizes Kerberos authentication. In order to log in you
will need a valid Kerberos ticket and your web browser will need to be
configured to send the Kerberos information to the server.

In Firefox, you will need to use the about:config page to set
a Kerberos parameter. Use the search term 'negotiate' to filter the list.
Change network.negotiate-auth.trusted-uris to the domain you want to
authenticate against, e.g .example.com. You can leave
network.negotiate-auth.delegation-uris blank, as it enables Kerberos
ticket passing, which is not required.

In order to obtain a Kerberos ticket, use the kinit command.

Installing the Koji cli
-----------------------

There is a single point of entry for most operations. The command is
called 'koji' and is included in the main koji package.

The koji tool authenticates to the central server using Kerberos, so you
will need to have a valid Kerberos ticket to use many features. However,
many of the read-only commands will work without authentication.

Building a package
------------------

Builds are initiated with the command line tool. To build a package, the
syntax is:

::

    $ koji build <build target> <git URL>

For example:

::

    $ koji build f25 git://pkgs.fedoraproject.org/rpms/eclipse-jgit?#00ca55985303b1ce19c632922ebcca283ab6e296

The ``koji build`` command creates a build task in Koji. By default the
tool will wait and print status updates until the build completes. You
can override this with the ``--nowait`` option. To view other options to
the build command use the ``--help`` option.

::

    $ koji build --help

Build Options
-------------

There are a few options to the build command. Here are some more
detailed explanations of them:

``--skip-tag``
    Normally the package is tagged after the build completes. This
    option causes the tagging step to be skipped. The package will be in
    the system, but untagged (you can later tag it with the tag-build
    command)
``--scratch``
    This makes the build into a scratch build. The build will not be
    imported into the db, it will just be built. The rpms will land
    under <topdir>/scratch. Scratch builds are not tracked and can never
    be tagged, but can be convenient for testing. Scratch builds are
    typically removed from the filesystem after one week.
``--nowait``
    As stated above, this prevents the cli from waiting on the build
    task.
``--arch-override``
    This option allows you to override the base set of arches to build
    for. This option is really only for testing during the beta period,
    but it may be retained for scratch builds in the future.

Build Failures
--------------

If your package fails to build, you will see something like this.

::

          420066 buildArch (kernel-2.6.18-1.2739.10.9.el5.jjf.215394.2.src.rpm,
          ia64): open (build-1.example.com) -> FAILED: BuildrootError:
          error building package (arch ia64), mock exited with status 10

You can figure out why the build failed by looking at the log files. If
there is a build.log, start there. Otherwise, look at init.log

::

          $ ls -1 <topdir>/work/tasks/420066/*
          <topdir>/work/tasks/420066/build.log
          <topdir>/work/tasks/420066/init.log
          <topdir>/work/tasks/420066/mockconfig.log
          <topdir>/work/tasks/420066/root.log


Koji Architecture
=================

Terminology
-----------

In Koji, it is sometimes necessary to distinguish between the a package
in general, a specific build of a package, and the various rpm files
created by a build. When precision is needed, these terms should be
interpreted as follows:

Package
    The name of a source rpm. This refers to the package in general and
    not any particular build or subpackage. For example: kernel, glibc,
    etc.
Build
    A particular build of a package. This refers to the entire build:
    all arches and subpackages. For example: kernel-2.6.9-34.EL,
    glibc-2.3.4-2.19.
RPM
    A particular rpm. A specific arch and subpackage of a build. For
    example: kernel-2.6.9-34.EL.x86\_64, kernel-devel-2.6.9-34.EL.s390,
    glibc-2.3.4-2.19.i686, glibc-common-2.3.4-2.19.ia64

Koji Components
---------------

Koji is comprised of several components:

-  **koji-hub** is the center of all Koji operations. It is an XML-RPC
   server running under mod\_wsgi in Apache. koji-hub is passive in
   that it only receives XML-RPC calls and relies upon the build daemons
   and other components to initiate communication. koji-hub is the only
   component that has direct access to the database and is one of the
   two components that have write access to the file system.
-  **kojid** is the build daemon that runs on each of the build machines.
   Its primary responsibility is polling for incoming build requests and
   handling them accordingly. Koji also has support for tasks other than
   building. Creating install images is one example. kojid is
   responsible for handling these tasks as well.

   kojid uses mock for building. It also creates a fresh buildroot for
   every build. kojid is written in Python and communicates with
   koji-hub via XML-RPC.

-  **koji-web** is a set of scripts that run in mod\_wsgi and use the
   Cheetah templating engine to provide an web interface to Koji.
   koji-web exposes a lot of information and also provides a means for
   certain operations, such as cancelling builds.
-  **koji** is a CLI written in Python that provides many hooks into Koji.
   It allows the user to query much of the data as well as perform
   actions such as build initiation.
-  **kojira** is a daemon that keeps the build root repodata updated.

Package Organization
--------------------

**Tags and Targets**

Koji organizes packages using tags. In Koji a tag is roughly analogous
to a beehive collection instance, but differ in a number of ways:

-  Tags are tracked in the database but not on disk
-  Tags support multiple inheritance
-  Each tag has its own list of valid packages (inheritable)
-  Package ownership can be set per-tag (inheritable)
-  Tag inheritance is more configurable
-  When you build you specify a *target* rather than a tag

A build target specifies where a package should be built and how it
should be tagged afterwards. This allows target names to remain fixed as
tags change through releases. You can get a full list of build targets
with the following command:

::

    $ koji list-targets

You can see just a single target with the ``--name`` option:

::

    $ koji list-targets --name dist-fc7
    Name                           Buildroot                      Destination
    ---------------------------------------------------------------------------------------------
    dist-fc7                       dist-fc7-build                 dist-fc7

This tells you a build for target dist-fc7 will use a buildroot with
packages from the tag dist-fc7-build and tag the resulting packages as
dist-fc7.

You can get a list of tags with the following command:

::

    $ koji list-tags

*Package lists*

As mentioned above, each tag has its own list of packages that may be
placed in the tag. To see that list for a tag, use the ``list-pkgs``
command:

::

    $ koji list-pkgs --tag dist-fc7
    Package                 Tag                     Extra Arches     Owner
    ----------------------- ----------------------- ---------------- ----------------
    ElectricFence           dist-fc6                                 pmachata
    GConf2                  dist-fc6                                 rstrode
    lucene                  dist-fc6                                 dbhole
    lvm2                    dist-fc6                                 lvm-team
    ImageMagick             dist-fc6                                 nmurray
    m17n-db                 dist-fc6                                 majain
    m17n-lib                dist-fc6                                 majain
    MAKEDEV                 dist-fc6                                 clumens
    ...

The first column is the name of the package, the second tells you which
tag the package entry has been inherited from, and the third tells you
the owner of the package.

**Latest Builds**

To see the latest builds for a tag, use the ``latest-build`` command:

::

    $ koji latest-build --all dist-fc7
    Build                                     Tag                   Built by
    ----------------------------------------  --------------------  ----------------
    ConsoleKit-0.1.0-5.fc7                    dist-fc7              davidz
    ElectricFence-2.2.2-20.2.2                dist-fc6              jkeating
    GConf2-2.16.0-6.fc7                       dist-fc7              mclasen
    ImageMagick-6.2.8.0-3.fc6.1               dist-fc6-updates      nmurray
    MAKEDEV-3.23-1.2                          dist-fc6              nalin
    MySQL-python-1.2.1_p2-2                   dist-fc7              katzj
    NetworkManager-0.6.5-0.3.cvs20061025.fc7  dist-fc7              caillon
    ORBit2-2.14.6-1.fc7                       dist-fc7              mclasen

The output gives you not only the latest builds, but which tag they have
been inherited from and who built them (note: for builds imported from
beehive the "built by" field may be misleading)

Exploring Koji
--------------

We've tried to make Koji self-documenting wherever possible. The command
line tool will print a list of valid commands and each command supports
``--help``. For example:

::

    $ koji help
    Koji commands are:
            build                Build a package from source
            cancel-task          Cancel a task
            help                 List available commands
            latest-build         Print the latest builds for a tag
    ...
    $ koji build --help
    usage: koji build [options] tag URL
    (Specify the --help global option for a list of other help options)

    options:
      -h, --help            show this help message and exit
      --skip-tag            Do not attempt to tag package
      --scratch             Perform a scratch build
      --nowait              Don't wait on build
    ...
===============
Access Controls
===============

Koji is a complex system, so there are many places where some kind of access
control is used. Here is the documentation hub for all the mechanisms in place.

User/Builder Authentication
===========================

Users (and builders) are authenticated via one of the following mechanisms. Most
preferred is GSSAPI/Kerberos authentication. Second best is authentication via
SSL certificates. Mostly for testing environments we also support authenticating via
username/password but it has its limitations which you should be aware of.

Details can be found at :ref:`auth-config`

.. _allowed-scms:

Allowed SCMs
============

The ``allowed_scms`` option in builder's config controls which SCMs (Source Control Management
systems) are allowed for building.
We recommend that every production environment choose a limited set of trusted sources.

Details of the ``allowed_scms`` option are covered under :ref:`scm-config`

We also provides ``build_from_scm`` hub policy for the same purpose, you can choose either/both
of the two approaches by the switch options in ``/etc/kojid.conf`` per build:

    * ``allowed_scms_use_config``, default: ``true``
    * ``allowed_scms_use_policy``, default: ``false``

For more details of the ``build_from_scm``, please read :doc:`defining_hub_policies`.

Hub Policies
============

Hub policies are a powerful way for administrators to control Koji's behavior.
Koji's hub allows several different policies to be configured, some of which are
access control policies.

An access control policy is consulted by the hub to determine if an action should be allowed.
Such policies return results of ``deny`` or ``allow``.

Examples of access control polices are:

* tag: control which tag operations are allowed
* package_list: control which package list updates are allowed
* cg_import: control which content generator imports are allowed
* vm: control which windows build tasks are allowed
* dist_repo: control which distRepo tasks are allowed
* build_rpm: control whether builds are allowed, this is superceding older ``build_from_srpm``
             to handle all task types. ``build_from_srpm`` and ``build_from_repo_id`` are now
             deprecated and will be removed in koji 1.33. Default policy allows everything.
* build_from_srpm [deprecated]: control whether builds from srpm are allowed
* build_from_scm: control whether builds from the SCM are allowed and the behavior of the SCM
* build_from_repo_id [deprecated]: control whether builds from user-specified repos ids are allowed

Note that not all policies are access control policies.
The ``channel`` and ``volume`` policies are used to control which channels tasks go to
and which volumes build are stored on.

For more details see :doc:`defining_hub_policies`.

User Permissions
================

Every user can have a set of permissions which allow them to perform some actions directly.
These permissions may be checked directly by the hub, or they may be referenced in policies.

See :doc:`permissions` for details.
Unit tests in Fedora's Jenkins
==============================

We're using CentOS's `Jenkins
<https://jenkins-fedora-infra.apps.ocp.ci.centos.org/job/koji>`_ infrastructure
for automatically running unit tests for new commits in master branch.

Pagure triggers tests for any new pull request. It can be also triggered
manually by pressing "Rerun CI" button in PR.

Currently we run tests on Fedora 33/34/rawhide and CentOS 7/8 platforms.

Usage
-----

If you need any change in jenkins setup, please file a pagure issue. As part
of solving issue, this documentation must be updated to reflect current state
in jenkins.

Configuration
-------------

- Public access is for everyone with no need to log in to jenkins.
- Admin access is via same interface and currently tkopecek have
   access there. If you need access for yourself (you're probably part of
   brew/koji team) create ticket in https://pagure.io/centos-infra requesting this.
   Prerequisite for this is CentOS account.

- Setup - Following items are set-up via
   https://jenkins-fedora-infra.apps.ocp.ci.centos.org/job/koji/configure

- *Job notifications* - when jenkins job finishes, it will send some info to
   pagure. Such call will add a comment to PR. For this REST hook needs to
   be configured:

   * Format: JSON
   * Protocol: HTTP
   * Event: Job Finalized
   * URL: <url taken from koji pagure settings which will look similar to https://pagure.io/api/0/ci/jenkins//koji/<hash>/build-finished
   * Timeout: 30000
   * Log: 0

- *This build is parametrized* (checkbox set to true) - it allows jenkins
   to use other branches than master (especially for PRs). Three parameters
   are defined there:

   * REPO - The repository for the pull request.
   * BRANCH - The branch for the pull request.
   * BRANCH_TO - A branch into which the pull request should be merged.

- *Discard Old Builds*

   * Strategy: Rotation
   * Days to keep build: 20
   * Max # of builds to keep: empty

- *Disable build* - it could be used if a lot of failing build happens with
    no vision of early recovery - temporarily suspend jenkins jobs
- *Restrict where this project can be run* - F33 is the only used agent now.
    Basically we need an agent which supports podman as all the tests are run in
    docker images managed by podman.

- *Source Code Management*

  * Git
  * Repositories

    * Repository URL: https://pagure.io/koji.git
    * Credentials: none
    * Branches to build: origin/master

- *Build triggers*

  * *Trigger builds remotely*: true

    * Authentication tokens: <token from koji pagure settings>

  * *Poll SCM*:

    * Schedule: H/5 * * * *

- *Build* - most important part - script which runs tests itself. Basically we
  run tests in containers, last one also run flake8 and is used for coverage
  report.


.. code-block:: shell

    # merge PR into main repository  
    if [ -n "$REPO" -a -n "$BRANCH" ]; then  
        git config --global user.email "test@example.com"  
        git config --global user.name "Tester"
        git remote rm proposed || true  
        git remote add proposed "$REPO"  
        git fetch proposed   
        git checkout "origin/${BRANCH_TO:-master}"  
        git merge --no-ff "proposed/$BRANCH" -m "Merge PR"  
    fi  

    # centos:7
    rm -rf .tox
    podman run --rm --pull=always -v $PWD:/koji --security-opt label=disable --name koji-centos-test-7 quay.io/tkopecek/koji-centos-test:7 bash -c "cd /koji && tox -e py2"
    podman rmi quay.io/tkopecek/koji-centos-test:7

    # centos:8
    rm -rf .tox
    podman run --rm --pull=always -v $PWD:/koji --security-opt label=disable --name koji-centos-test-8 quay.io/tkopecek/koji-centos-test:8 bash -c "cd /koji && tox -e py3"
    podman rmi quay.io/tkopecek/koji-centos-test:8

    # fedora:32
    rm -rf .tox
    podman run --rm --pull=always -v $PWD:/koji --security-opt label=disable --name koji-fedora-test-32 quay.io/tkopecek/koji-fedora-test:32 bash -c "cd /koji && tox -e py3"
    podman rmi quay.io/tkopecek/koji-fedora-test:32

    # fedora:33
    rm -rf .tox
    podman run --rm --pull=always -v $PWD:/koji --security-opt label=disable --name koji-fedora-test-33 quay.io/tkopecek/koji-fedora-test:33 bash -c "cd /koji && tox -e py3"
    podman rmi quay.io/tkopecek/koji-fedora-test:33

    # fedora:rawhide
    rm -rf .tox
    podman run --rm --pull=always -v $PWD:/koji --security-opt label=disable --name koji-fedora-test-rawhide quay.io/tkopecek/koji-fedora-test:rawhide bash -c "cd /koji && tox -e flake8,py3"
    podman rmi quay.io/tkopecek/koji-fedora-test:rawhide

- *Post-build actions*

  * *Publish Cobertura Coverage report*: coverage.xml - this will create coverage report accessible via jenkins web ui
  * *Report Violations* - *pep8*: flake8_report.txt
  * *E-mail notification*:

    * Recipients: tkopecek@redhat.com exd-sp-rhel-build-alerts@redhat.com
    * Send separate e-mails to individuals who broke the build

- *Send messages to fedmsg*
===============================
Koji Content Generator Metadata
===============================

This document describes the Koji Content Generator Metadata
Format (version 0). This is the metadata that should be provided by a
Content Generator in order for the content to be imported and managed by
Koji. If you have further questions about :doc:`Content
Generators <content_generators>`, please email
koji-devel@lists.fedorahosted.org.

Format
======

Content Generator Metadata for a single build is provided as a JSON map.
The map has four top-level entries:

-  metadata\_version: The version of the metadata format used. Currently
   must be 0.
-  build: A map containing information about the build.
-  buildroots: A list of maps, one for each environment in which build
   output was generated, containing information about that environment.
-  output: A list of maps, one map for each file that will be imported
   and managed by Koji.

metadata\_version
-----------------

This is an integer which indicates the version of the metadata format
contained in this file. It will start at 0 and be incremented as the
metadata format evolves.

build
-----

The build map contains the following entries:

-  name: The name of the build.
-  version: The version of the build.
-  release: The release of the build.
-  source: The SCM URL of the sources used in the build.
-  start\_time: The time the build started, in seconds since the epoch.
-  end\_time: The time the build was completed, in seconds since the
   epoch.
-  owner: The owner of the build task in username format. This field
   is optional.
-  build_id: Reserved build ID. This field is optional.
-  extra: A map of extra metadata associated with the build, which
   must include at least one of:

   - typeinfo: A map whose entries are the names of the build types used for
     this build, which are free form maps containing type-specific information
     for this build.
   - maven, win, or image: Legacy build type names which appear at this level
     instead of inside typeinfo.

buildroots
----------

Each map in the buildroots list contains the following entries:

-  id: An id for this buildroot entry. Only needs to be consistent
   within this file (it will be referenced by the output). Can be
   synthetic/generated/random.
-  host: Map containing information about the host where the build was
   run.

   -  os: The operating system that was running on the host.
   -  arch: The processor architecture of the host.

-  content\_generator: Map containing information about the Content
   Generator which ran the build.

   -  name: The short name of the Content Generator.
   -  version: The version of the Content Generator.

-  container: Map containing information about the container in which
   the build was run.

   -  type: The type of container that was used, eg. none, directory,
      chroot, mock-chroot, kvm, docker
   -  arch: The architecture of the container. May be different than the
      architecture of the host, eg. i686 container on x86\_64 host.

-  tools: List of maps containing information about the tools used to
   run the build. Each map contains:

   -  name: Name of the tool used.
   -  version: Version of the tool used.

-  components: List of maps containing information about content
   installed in the build environment (if any). Each map is guaranteed
   to contain a **type** field, which determines what other fields are
   present in the map. For maps where **type = rpm**, the following
   fields will be present:

   -  name: The rpm name.
   -  version: The rpm version.
   -  release: The rpm release.
   -  epoch: The rpm epoch.
   -  arch: The rpm arch.
   -  sigmd5: The SIGMD5 tag from the rpm header.
   -  signature: The signature used to sign the rpm (if any).

-  For maps where **type = file**, the following fields will be present:

   -  filename: The name of the file.
   -  filesize: The size of the file.
   -  checksum: The checksum of the file.
   -  checksum\_type: The checksum type used.

.. _metadata-kojifile:

-  For maps where **type = kojifile**, the following fields will be present:

   -  filename: The name of the file.
   -  filesize: The size of the file.
   -  checksum: The checksum of the file.
   -  checksum\_type: The checksum type used.
   -  nvr: Build nvr from which this file origins.
   -  archive\_id: ID of archive from specified build.

-  The format may be extended with other types in the future.
-  extra: A map containing information specific to the Content Generator
   that produced the files to import. For OSBS, the extra map should
   contain a osbs entry, which is a map with the following fields:

   -  build\_id: The ID of the build in OSBS.
   -  builder\_image\_id: The ID of the image in OSBS that was used to
      run the build.

output
------

Each map in the output list contains the following entries:

-  buildroot\_id: The id of the buildroot used to create this file. Must
   match an entry in the buildroots list.
-  filename: The name of the file.
-  filesize: The size of the file.
-  arch: The architecture of the file (if applicable).
-  checksum: The checksum of the file.
-  checksum\_type: The checksum type used.
-  type: The type of the file. Log files should use "log".
-  components: If the output file is composed from other units, those
   should be listed here. The format is the same as the **components**
   field of a buildroot map.
-  extra: Free-form, but should contain IDs that allow tracking the
   output back to the system in which it was generated (if that system
   retains a record of output). For docker, the extra map should contain
   a docker entry, which is a map with the following fields:

   -  id: The ID of the docker image produced in the repo used by the
      build tool
   -  parent\_id: The parent ID of the docker image produced (if
      applicable).
   -  repositories: A list of repository locations where the image is
      available.
   -  digests: A map of media type (such as
      "application/vnd.docker.distribution.manifest.v2+json") to
      manifest digest (a string usually starting "sha256:"), for each
      available media type.

Example Metadata JSON
=====================

The below JSON is based loosely on the output of a docker image build.

::

    {"metadata_version": 0,
     "build": {"name": "rhel-server-docker",
               "version": "7.1",
               "release": "4",
               "source": "git://git.engineering.redhat.com/users/vpavlin/tdl_templates.git#a14f145244",
               "extra": {},
               "start_time": 1423148398,
               "end_time": 1423148828,
               "owner": "jdoe"},
     "buildroots": [{"id": 1,
                     "host": {"os": "rhel-7",
                              "arch": "x86_64"},
                     "content_generator": {"name": "osbs",
                                           "version": "0.2"},
                     "container": {"type": "docker",
                                   "arch": "x86_64"},
                     "tools": [{"name": "docker",
                                "version": "1.5.0"}],
                     "components": [{"type": "rpm",
                                     "name": "glibc",
                                     "version": "2.17",
                                     "release": "75.el7",
                                     "epoch": null,
                                     "arch": "x86_64",
                                     "sigmd5": "a1b2c3...",
                                     "signature": "fd431d51"},
                                    {"type": "rpm",
                                     "name": "openssl",
                                     "version": "1.0.1e",
                                     "release": "42.el7",
                                     "epoch": null,
                                     "arch": "x86_64",
                                     "sigmd5": "d4e5f6...",
                                     "signature": "fd431d51"},
                                    {"type": "rpm",
                                     "name": "bind-libs",
                                     "version": "9.9.4",
                                     "release": "18.el7",
                                     "epoch": 32,
                                     "arch": "x86_64",
                                     "sigmd5": "987abc...",
                                     "signature": null},
                                    {"type": "rpm",
                                     "name": "python-urllib3",
                                     "version": "1.5",
                                     "release": "8.el7",
                                     "epoch": null,
                                     "arch": "noarch",
                                     "sigmd5": "123hgf...",
                                     "signature": null},
                                    {"type": "file",
                                     "filename": "jboss-eap-6.3.3-full-build.zip",
                                     "filesize": 12345678,
                                     "checksum": "5ec2f29c4e1c2e2aa6552836e236a158",
                                     "checksum_type": "md5"}],
                     "extra": {"osbs": {"build_id": 12345,
                                        "builder_image_id": 67890}}
                     }],
     "output": [{"buildroot_id": 1,
                "filename": "rhel-server-docker-7.1-4.x86_64.tar.xz",
                "filesize": 34440656,
                "arch": "x86_64",
                "checksum_type": "md5",
                "checksum": "275ae42a45cfedbdb0c0a1acc0b55a1b",
                "type": "docker-image",
                "components": "",
                "extra": {"docker": {"id": "987654...",
                                     "parent_id": "a1b2c3...",
                                     "repositories": ["repository.example.com/username/imagename:7.1-4",
                                                      "repository.example.com/username/imagename@sha256:100000...",
                                                      "repository.example.com/username/imagename@sha256:200000..."],
                                     "digests": {"application/vnd.docker.distribution.manifest.v1+json": "sha256:100000...",
                                                 "application/vnd.docker.distribution.manifest.v2+json": "sha256:200000..."}
                                     }}},
               {"buildroot_id": 1,
                "filename": "checkout.log",
                "filesize": 85724,
                "arch": "noarch",
                "checksum_type": "md5",
                "checksum": "a1b2c3...",
                "type": "log"},
               {"buildroot_id": 1,
                "filename": "os-indirection.log",
                "filesize": 27189,
                "arch": "noarch",
                "checksum_type": "md5",
                "checksum": "d4f5g6...",
                "type": "log"}
               ]
    }
=======================
Koji Content Generators
=======================

A Koji Content Generator is an external service that generates content
(jars, zips, tarballs, .npm, .wheel, .gem, etc) which is then passed to
Koji for management and delivery to other processes in the release
workflow. Content Generators can evolve independently of the Koji
codebase, enabling the build process to be more agile and flexible to
changing requirements and new technologies, while allowing Koji to
provide stable APIs and interfaces to other processes.

Along with the content to be managed by Koji, a Content Generator will
provide enough metadata to enable a reasonable level of auditing and
reproducibility. The exact data provided and the format used is being
discussed, but will include information like the upstream source URL,
build tools used, build environment contents, and any
container/virtualization technologies used.

The intention is that a team dedicated to managing a specific content
type will design and maintain their own Content Generator, in
coordination with the Koji developers. Once the Content Generator is
ready for production use it will be given permission to import content
and metadata it produces into Koji. Policies on the Koji hub will
validate imported content and metadata and ensure that it is complete
and consistent.

Requirements for writing a Content Generator
============================================

From an implementation perspective, content generators have wide
latitude in how they perform builds. To ensure sanity in the build
process, we strongly recommend that administrators of Koji systems set
policies about what content generators are allowed to do, and make sure
that those policies are followed before the content generator is granted
authorization in their Koji system.

Below are some examples of the sorts of policies that one might require.
Content Generators should be designed and implemented with these
requirements in mind. Please note that the list below is not complete.

Avoid Using the Host's Software
-------------------------------

During the building process, the code should avoid using the host's
installed software. The more reliance on installed software, the more
risk in the future that changes (such as upgrading a builder) will break
the build processes. Use mock chroots, VM guests, or containers wherever
possible to insulate against changes. Isolating the build environment
from the host environment makes reproducing work much easier and
predictable.

Source of build environment content
-----------------------------------

The build environment must come from somewhere. In a standard Koji
build, it comes from content already in Koji, or from configured
external repositories.

CG authors will likely want to pull content from sources outside of
Koji. Koji administrators should set a clear policy about which sources
are acceptable. The use of arbitrary sources can make it difficult or
impossible to reproduce build environments.

Binaries (or other compiled content) from Upstream May Not become included in output
------------------------------------------------------------------------------------

If tools or other content downloaded from external sources are used in
the build, they may not be included in CG build output, and may not be
imported into Koji. In other words, output must be built from sources in
the CG or Koji, not retrieved from the internet. Tools necessary to
build product content can be downloaded and cached in the CG.

Log all Transformations of Content
----------------------------------

When the content is building, as much should be logged as possible. In
addition to compilation, if the content goes through other
transformations, perhaps changing formats, that should be logged as
well. There can be no black-box transformations of the output. Imagine
having to figure out how a piece of content was built 5 years into the
future to understand the motivation behind this requirement. Details of
the build environment and tools used in the environment should be
recorded too.

Preserve All Inputs
-------------------

All inputs to a build task should be preserved either as logs, a
database, or as output of the build itself.

Preserve All Outputs
--------------------

Naturally the outputs of a build should be preserved too. Transient
artifacts are not strictly required, but if they're not onerous to
maintain, they should be included. It must not be necessary to further
transform the content to make it usable.

Do Not Use Caching Mechanisms
-----------------------------

Content Generators must build without caching mechanisms (in compilers
or DNF\ \|\ YUM) wherever possible. Caches make
reproducing results in the future more difficult, and also introduce
layers of indirection that can make debugging a build more difficult.
Consider the risk of re-shipping a security flaw that is compiled in
because an outdated library was cached in the Content Generator, this is
why we have this requirement.

Metadata
========

Metadata will be provided by the Content Generator as a JSON file. There
is a proposal of the :doc:`Content Generator
Metadata <content_generator_metadata>` format available for review.

.. _cg_api:

API
===

Relevant API calls for Content Generator are:

- ``CGImport(metadata, directory, token=None)``: This is basic integration point
  of Content Generator with koji. It is supplied with metadata as json encoded
  string or dict or filename of metadata described in previous chapter and
  directory with all uploaded content referenced from metadata. These files
  needs to be uploaded before ``CGImport`` is called.

  Optionally, ``token`` can be specified in case, that build ID reservation was
  done before.

- ``CGInitBuild(cg, data)``: It can be helpful in many cases to reserve NVR for
  future build before Content Generator evend starts building.  Especially, if
  there is some CI or other workflow competing for same NVRs.  This call creates
  special ``token`` which can be used to claim specific build (ID + NVR). Such
  claimed build will be displayed as BUILDING and can be used by ``CGImport``
  call later.

  As an input are here Content Generator name and `data` which is basically
  dictionary with name/version/release/epoch keys. Call will return a dict
  containing ``token`` and ``build_id``. ``token`` would be used in subsequent
  call of ``CGImport`` while ``build_id`` needs to be part of metadata (as item
  in ``build`` key).

- ``CGRefundBuild(cg, build_id, token, state=koji.BUILD_STATES['FAILED'])`` -
  content generator can drop the reservation created by ``CGInitBuild`` in case
  that build failed or was cancelled. Caller must know ``build_id`` and ``token``.

Current Public Implementations
==============================

Following koji plugins uses CGs as part of their workflow.

- `OpenShift Build Service <https://github.com/containerbuildsystem/>`__
- `OSBuild <https://github.com/osbuild/>`__
- `MBS <https://pagure.io/fm-orchestrator/blob/master/f/module_build_service/builder/KojiContentGenerator.py>`__

Standalone CG utilites are here:
- `CoreOS <https://github.com/coreos/coreos-assembler/blob/master/src/cmd-koji-upload>`__
- `misoctl - Debian package CG <https://github.com/red-hat-storage/misoctl>`__
- `Project NewCastle - Java package CG component <https://github.com/project-ncl/causeway>`__
Database Howto
==============

For small to middle-sized deployments you should be ok with standard
distribution settings. Anyway, for larger one, it can start to be
problematic to deal with specific indices, disk space allocation, etc.
This section contains some useful practices to deal with such
problems.

Partitions
----------

Some tables - especially ``buildroot_listings`` and ``tasks`` can grow
in time and start to be problematic during backups, etc. One of the
solutions is to use partitioning feature of postgres.

It simply says, that one big table can be split to smaller ones (even
ending in different storages) while it is still transparent to
application. What could be tricky, is by which ranges tables should be
split. It is relatively easy for ``buildroot_listings``, where we
almost always query by ``buildroot_id``.

It has three steps - first is to backup your db and turn hub offline.

.. note::
    SQL syntax used here (PARTITION BY) is in Postgres from version 10.  Same
    behaviour can be made to work even with older releases but needs a bit of
    additional code to replace it.

Second is creating trigger, which will be used when new buildroot is
created and will ensure that potential new partition is created:

.. code-block:: plpgsql

  -- create_partition_and_insert trigger will be called anytime
  -- new buildroot is inserted to buildroot table. In such case,
  -- it is checked if it falls to existing partition or if new one needs to be created
  CREATE OR REPLACE FUNCTION create_partition_and_insert() RETURNS trigger AS
  $$
  DECLARE
    partition_start INTEGER;
    partition_end INTEGER;
    partition_size INTEGER;
    partition TEXT;
  BEGIN
    -- you can set it to any reasonable size, but it must be same
    -- number as later in buildroot_listing_partition
    partition_size = 1000000;
    partition_start := DIV(NEW.id, partition_size) * partition_size;
    partition_end := partition_start + partition_size;
    partition := 'buildroot_listing_' || partition_start || '_' || partition_end - 1;
    IF NOT EXISTS(SELECT relname FROM pg_class WHERE relname=partition) THEN
      EXECUTE 'CREATE TABLE ' || partition || ' PARTITION OF buildroot_listing_partition FOR VALUES FROM (' || partition_start ||') TO (' || partition_end || ')';
      EXECUTE 'CREATE UNIQUE INDEX ' || partition || '_broot_rpm ON ' || partition || '(buildroot_id, rpm_id)';
      RAISE NOTICE 'A partition % has been created', partition;
    END IF;
    RETURN NULL;
  END;
  $$
  LANGUAGE plpgsql VOLATILE
  COST 100;

  CREATE TRIGGER testing_partition_insert_trigger
  BEFORE INSERT ON buildroot
  FOR EACH ROW EXECUTE PROCEDURE create_partition_and_insert();


The third one is one-time code, which will be used for converting
existing tables.

.. code-block:: plpgsql

  -- temporary table for partitioning, will be populated and in the end renamed to buildroot_listing
  CREATE TABLE buildroot_listing_partition (
      buildroot_id INTEGER NOT NULL,
      rpm_id INTEGER NOT NULL,
      is_update BOOLEAN NOT NULL DEFAULT FALSE
  ) PARTITION BY RANGE (buildroot_id);


  CREATE OR REPLACE FUNCTION partition_buildroot_listing() RETURNS integer AS
  $$
  DECLARE
    partition TEXT;
    partition_start INTEGER;
    partition_end INTEGER;
    partition_count INTEGER;
    partition_size INTEGER;
  BEGIN
    -- same number as in create_partition_and_insert
    partition_size = 1000000;
    SELECT DIV(MAX(id), partition_size)  FROM buildroot INTO partition_count;
    RAISE NOTICE 'Will create % partitions', partition_count;

    -- create partitions
    FOR i IN 0..partition_count LOOP
      partition_start = i * partition_size;
      partition_end = partition_start + partition_size;
      partition := 'buildroot_listing_' || partition_start || '_' || partition_end - 1;
      EXECUTE 'CREATE TABLE ' || partition || ' PARTITION OF buildroot_listing_partition FOR VALUES FROM (' || partition_start ||') TO (' || partition_end || ')';
      RAISE NOTICE 'A partition % has been created', partition;
    END LOOP;

    -- copy data
    INSERT INTO buildroot_listing_partition SELECT * FROM buildroot_listing;
    RAISE NOTICE 'Data were copied from buildroot_listing to buildroot_listing_partition';

    DROP TABLE buildroot_listing;
    RAISE NOTICE 'Original buildroot_listing dropped';

    ALTER TABLE buildroot_listing_partition RENAME TO buildroot_listing;
    RAISE NOTICE 'buildroot_listing_partition renamed back to buildroot_listing';

    -- create indices after copy
    FOR i IN 0..partition_count LOOP
      partition_start = i * partition_size;
      partition_end = partition_start + partition_size;
      partition := 'buildroot_listing_' || partition_start || '_' || partition_end - 1;
      EXECUTE 'CREATE UNIQUE INDEX ' || partition || '_broot_rpm ON ' || partition || '(buildroot_id, rpm_id)';
      RAISE NOTICE 'A partition index has been created %', partition;
    END LOOP;

    RETURN 1;
  END;
  $$
  LANGUAGE plpgsql;

  -- run conversion function
  BEGIN;
    SELECT partition_buildroot_listing();
    DROP FUNCTION partition_buildroot_listing();
  COMMIT;

Using SSL with PostgreSQL
-------------------------

The basic :doc:`Koji server walkthrough <server_howto>` and sample
configuration files instruct users to use plaintext TCP/IP connections to the
postgresql server. This is not a good practice, and it is more secure to use
SSL. You'll need to configure the postgresql server to accept SSL connections,
and then configure the Koji Hub to only use a trusted SSL connection.

Enabling SSL on the PostgreSQL server
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

Edit ``/var/lib/pgsql/data/postgresql.conf``:

 * Enable SSL with ``ssl = on``
 * The ``listen_addresses`` option cannot be empty. ``listen_addresses = '*'``
   will make postgres listen on every network interface (simpler), or you can
   restrict it to only certain network interfaces.

Create two files:

 * ``/var/lib/pgsql/data/server.crt`` - This is the public signed certificate.
   It should include the full chain (including the CA and any intermediates).
 * ``/var/lib/pgsql/data/server.key`` - The private key.

Set the ownership appropriately::

  chown postgres:postgres /var/lib/pgsql/data/server.{crt,key}
  chmod 0600 /var/lib/pgsql/data/server.key

Restart postgresql for the new settings to take effect::

  systemctl restart postgresql

Configuring the Koji hub to use SSL to Postgres
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

Once you've enabled SSL on the PostgreSQL server, you can test it with the
CLI::

  psql 'postgresql://koji:example_password@db.example.com/koji?sslmode=verify-full&sslrootcert=/etc/pki/tls/certs/ca-bundle.trust.crt'

You should be able to list tables, run queries, etc.

Edit ``/etc/koji-hub/hub.conf`` to use this connection string::

  DBConnectionString = postgresql://koji:example_password@kojidev.example.com/koji?sslmode=verify-full&sslrootcert=/etc/pki/tls/certs/ca-bundle.trust.crt

Restart the hub and verify the new ``DBConnectionString`` is working::

  systemctl restart httpd

  koji hello
=====================
Defining Hub Policies
=====================

Defining a policy on the hub allows you fine control over certain activities
in the system. At present, policy allows you to control:

* tag/untag/move operations
* allowing builds from srpm
* allowing builds from SCM, and managing properties/behaviors related to the SCM
  if it is allowed
* allowing builds from expired repos
* managing the package list for a tag
* managing which channel a task goes to
* altering task priority

In the future, we expect to add more policy hooks for controlling more aspects
of the system.

Policy configuration is optional. If you don't define one, then by default:

* tag/untag/move operations are governed by tag locks/permissions
* builds from srpm are only allowed for admins
* builds from any SCM are only allowed for admins. It's used when
  ``allowed_scms_use_policy`` is ``true`` in ``/etc/kojid.conf`` of the builders
  (``false`` by default). And the SCM's properies: ``use_common`` and
  ``source_cmd`` are set to their default values: ``False`` and
  ``['make', 'source']``
* builds from expired repos are only allowed for admins
* only admins and users with ``tag`` permission may modify package lists
* tasks go to the default channel
* vm tasks need ``admin`` or ``win-admin`` permission
* content generator import can be done by anyone
* all content ends in ``DEFAULT`` volume.

Configuration
=============

The hub policy is configured in the ``hub.conf`` file, which is an ini-style
configuration file. Policies are defined in the section named ``[policy]``.
Each ``name = value`` pair defines the policy of that name. With multiple line
policies, successive lines should be indented so that the parser treats them
as part of the whole.

Consider the following simple (and strict) example:

::

    [policy]
    tag =
        has_perm admin :: allow
        tag *-candidate :: allow
        all :: deny

This policy section defines a single policy (named 'tag'). The policy is a
series of rules, one per line. The rule lines must be indented. Each rule is
a test and an action, separated by a double colon. The valid actions for
current policies are 'allow' and 'deny'. There are many tests available,
though not all of them are applicable for all policies. Each test is specified
by giving the name of the test followed by any arguments the test accepts.

Each rule in the policy is checked until a match is found. Upon finding a
match, the action is applied. Our example above limits non-admins to tags
ending in -candidate.

Getting a bit more complicated
------------------------------

The example above is very simple. The policy syntax also supports compound
tests, negated tests, and nested tests. Consider the following example:

::

    [policy]
    tag =
        buildtag *epel* :: {
            tag *epel* !! deny
        }
        tag *-updates :: {
            operation move :: {
                fromtag *-updates-candidate :: allow
                fromtag *-updates-testing :: allow
                all :: deny Tagging from some tags to *-updates is forbidden.
            }
            operation tag && hastag *-updates-candidate *-updates-testing :: deny
        }
        all :: allow

This policy sets up some rules concerning tags ending in -updates and tags
containing epel, but is otherwise permissive.

The first nested rule limits builds built from a tag matching ``epel``  to only
such tags. Note the use of !! instead of :: negates the test.

For tags matching ``*-updates``, a particular work-flow is enforced. Moving is
only allowed if the move is coming from a tag matching ``*-updates-candidate``
or ``*-updates-testing``. Conversely, a basic tag operation (not a move) is
denied if the build also has such a tag (the policy requires a move instead).

For denied operations some clarifying message is sent to user. If there is no
specific message (everything after action keyword), only generic 'policy
violation (policy_name)' is sent, so it could be helpful to specify such
messages in more complicated cases.

General format
==============
The general form of a basic policy line is one of the following

::

    test [params] [&& test [params] ...] :: action-if-true
    test [params] [&& test [params] ...] !! action-if-false

And for nested rules:

::

    test [params] [&& ...] [::|!!] {
        test [params] [&& ...] [::|!!] action
        test [params] [&& ...] [::|!!] {
            ...
            }
    }

Note that each closing brace must be on a line by itself.
Using ``!!`` instead of ``::`` negates the entire test.
Tests can only be joined with &&, the syntax does not support ``||``.

Available policies
==================
The system currently looks for the following policies

* ``tag``: checked during tag/untag/move operations
* ``build_from_srpm``: checked when a build from srpm (not an SCM reference) is
  requested.
* ``build_from_scm``: checked when a build task from SCM is executing on builder
* ``build_from_repo_id``: checked when a build from a specified repo id is
  requested
* ``package_list``: checked when the package list for a tag is modified
* ``channel``: consulted when a task is created
* ``cg_import``: consulted during content generator imports
* ``volume``: determine which volume a build should live on

These policies are set by assigning a rule set to the given name in the policy
section.

Note that the use of tag policies does not bypass tag locks or permissions

Note that an admin can bypass the tag policy by using ``--force``.

Actions
=======

Most of the policies are simply allow/deny policies. They have two possible
actions: ``allow`` or ``deny``.

The **channel** policy is used to determine the channel for a task. It supports
the following actions:

``use <channel>``
    * use the given channel

``req``
    * use the requested channel
    * generally this means the default, though some calls allow the client to
      request a channel

``parent``
    * use the parent's channel
    * only valid for child tasks
    * recommend using the ``is_child_task`` test to be sure

The **priority** policy is used to alter task's priority. In most cases you
should manage priorities by different channels and builders assigned to them.
There is nevertheless few corner-cases which can benefit from altering task's
priority.

Note, that you can easily get to deadlock situation if this is not handled with
caution (lower priority tasks will get assigned only if there is no higher
priority task for given channel).

.. note::
    For example OSBS use this mechanism to propagate higher priority tasks to
    its plugin.  Deadlock problem is here mitigated by limiting policy to
    ``buildContainer`` tasks only. These tasks are consumed only by dedicated
    builders/channel, so they will not take priority over other types of tasks
    (e.g. ``newRepo`` or ``tagBuild`` tasks which could be blocked otherwise.

Technically it is very similar to **channel** policy. Only actions are
different:

``stay``
    * don't touch the default priority of the task

``set <int>``
    * set priority to this value

``adjust +<int>``
    * increment default priority

``adjust -<int>``
    * decrement default priority

The **build_from_scm** policy is used to assert if the SCM is allowed or not,
like the basic allow/deny one. It is also used to manage the SCM's properties as
the same as the ``allowed_scms`` option of the koji builder. The actions could
be defined as:

``allow [use_common] [<source_cmd>]``
    * allow the SCM
    * use(clone) the /common repo when ``use_common`` follows ``allow``
    * ``<source_cmd>`` is a *optional* shell command for preparing the source
      between checkout and srpm build. If it is omitted, it will follow the
      default value: ``make source``. The explicit value: ``none`` means **No**
      ``source_cmd`` is defined.

``deny [<reason>]``
    * disallow the SCM
    * ``<reason>`` is the error message which is shown as the task result

Available tests
===============
``true``
    * always true. no arguments

``all``
    * an alias of true

``false``
    * always false. no arguments

``none``
    * an alias of false

``operation``
    * for tag operations, the operation is one of: tag, untag, move. This test
      checks its arguments against the name of the operation and returns true if
      there is a match. Accepts glob patterns.
    * only applicable to the tag policy

``package``
    * Matches its arguments against the package name. Accepts glob patterns.

``version``
    * Matches its arguments against the build version. Accepts glob patterns.

``release``
    * Matches its arguments against the build release. Accepts glob patterns.

``tag``
    * matches its arguments against the tag name. Accepts glob patterns.
    * for move operations, the tag name tested is the destination tag (see
      fromtag)
    * for untag operations, the tag name is null and this test will always be
      false (see fromtag)
    * for the build_from_* policies, tests the destination tag for the build
      (which will be null is --skip-tag is used)

``fromtag``
    * matches against the tag name that a build is leaving. Accepts glob
      patterns
    * for tag operations, the tag name is null and this test will always be
      false
    * for move operations, the tag name test is the one that the build is
      moving from
    * for untag operations, tests the tag the build is being removed from
    * only applicable to the tag policy

``target``
    * matches against the build's target name. Accepts glob patterns.

``hastag``
    * checks the current tags for the build in question against the arguments.

``buildtag``
    * checks the build tag name against the arguments
    * for the build_from_* policies the build tag is determined by the build
      target requested
    * for the tag policies, determines the build tag from the build data,
      which will by null for imported builds

``buildtag_inherits_from``
    * check if some tag is in inheritance chain of the buildtag
    * All parents are checked excluding the buildtag itself

``buildtype``
    * checks the build type(s) against the arguments

``skip_tag``
    * checks to see if the --skip-tag option was used
    * only applicable to the build_from_* policies

``imported``
    * checks to see if the build in question was imported
    * takes no arguments
    * true if any of the component rpms in the build lacks buildroot data
    * only applicable to the tag policy

``is_build_owner``
    * Check if requesting user owns the build (not the same as package
      ownership)
    * take no arguments

``user_in_group``
    * matches the users groups against the arguments
    * true if user is in /any/ matching group

``has_perm``
    * matches the user's permissions against the arguments
    * true is user has /any/ matching permission

``source``
    * test the build source against the arguments
    * for the build_from_* policies, this is the source specified for the build
    * for the tag policy, this comes from the task corresponding to the build
      (and will be null for imported builds)

``policy``
    * takes a single argument, which is the name of another policy to check
    * checks the named policy. true if the resulting action is one of: yes,
      true, allow
    * additional policies are defined in the [policy] section, just like the
      others

``is_new_package``
    * true if the package being added is new to the system
    * intended for use with the package_list policy

``is_child_task``
    * true if the task is a child task
    * for use with the channel policy

``method``
    * matches the task method name against glob pattern(s)
    * true if the method name matches any of the patterns
    * for use with the channel policy

``user``
    * checks the username against glob patterns
    * true if any pattern matches
    * the user matched is the user performing the action

``match``
    * matches a field in the data against glob patterns 
    * true if any pattern matches
======================
Exporting repositories
======================

Koji provides some *limited* features for exporting repositories of RPMs.
Please note that Koji is a build system, not a repository manager, and these
features are secondary.
If you need more robust repository generation than Koji provides, then you may
want to look into using `pungi <https://pagure.io/pungi/>`_.


Koji's internal repositories
============================

Koji uses yum repositories as part of its build process for RPMs.
These repositories are used by the builders to generate buildroots.
Their generation is focused on that purpose, and they are not really suitable
for export.
However, they can be useful for simple cases.

Koji's internal repositories can be accessed at
``<topdir>/repos/<tag_name>/<repo_id>``
For a given tag name, there will be multiple repo ids over time as tag content
changes.
The current repo for a given tag can be determined with a call to ``getRepo``.

For convenience, Koji also maintains a "latest" symlink for each tag:
``<topdir>/repos/<tag_name>/latest``.
Please note that this symlink changes over time, which could break a yum transaction.


Dist-repos
==========

The simplest way to create a distribution-ready repo is to use the ``koji dist-repo``
command.
It allows users with access to generate a more robust yum repository from the
contents of a given tag.
These repos differ from the internal ones in several key ways:

* generation is user-controlled via the ``dist-repo`` command
* supports using signed rpms
* supports multilib
* allows for more customized comps data
* supports deltarpm generation
* can split debuginfo into separate repos
* can generate zchunk files

**Access control**

In order to use the ``dist-repo`` command, a user must satisfy one of the
following:

* have the ``dist-repo`` permission
* have the ``admin`` permission
* satisfy the requirements of the ``dist_repo`` policy

For more information about hub policies, see :doc:`defining_hub_policies`.


**Usage**

The ``dist-repo`` command takes a tag name and one or more key ids for signing keys.

..

    koji dist-repo [options] <tag> <key_id> [<key_id> ...]

The ``tag`` argument must be a valid tag name.
The resulting repository will be based on the contents of that tag.
Any valid tag will work, whether or not it has an associated target.

Koji will attempt to find a signed copy for each rpm matching one
of the given ``key_id`` arguments (searching in the order given).
Normally, Koji will error if there is no matching signed copy for any of the
rpms.
This behavior can be modified with the ``--allow-missing-signatures`` or
``--skip-missing-signatures`` options.
The ``key_id`` argument may be omitted entirely if the
``--allow-missing-signatures`` option is specified.

Koji will export the repository to ``<topdir>/repos-dist/<tag_name>/<repo_id>``
The current dist repo for a given tag can be determined with a call to
``getRepo(dist=True)``. Similar to internal build repos, Koji also maintains a
"latest" symlink for each tag: ``<topdir>/repos-dist/<tag_name>/latest``.

Various features of repo generation (e.g. multilib support, delta rpms, or
zchunk files) are controlled via command options.
For a full list of options, see ``koji dist-repo --help``.

**Koji Hub plugin**

Fedora release engineering uses a hub plugin `tag2distrepo
<https://pagure.io/releng/tag2distrepo>`_ to automatically export dist-repos
for certain tags.

Beyond Koji
===========

If you're aiming to have more control about repositories, varieties of
distribution flavours, etc. use `pungi <https://pagure.io/pungi/>`_ which can
create whole composes and which uses Koji for some of the subtasks.
Pungi + koji is what Fedora currently uses for composes.
====================================
External Repository Server Bootstrap
====================================

Bootstrapping a new External Repo Koji build environment
========================================================

These are the steps involved in pointing a new Koji server at external
repositories so that it can be used for building. This assumes that the Koji
hub is up, appropriate authentication methods have been configured, the Koji
repo administration daemon (``kojira``) is properly configured and running,
and at least one Koji builder (``kojid``) is properly configured and running.
All koji cli commands assume that the user is a Koji ``admin``.  If you need
help with these tasks, see the :doc:`server_howto`.

* Create a new tag. ::

    $ koji add-tag dist-foo

* Create a build tag with the desired arches, and the previously created tag
  as a parent. ::

    $ koji add-tag --parent dist-foo --arches "i386 x86_64 ppc ppc64" dist-foo-build

* Add an external repository to your build tag. Koji substitutes $arch for the
  arches in your build tag. The ``$`` needs to be escaped for the shell to send
  it though without interpreting it. See below for some additional examples of
  external repo URLs. ::

    $ koji add-external-repo -t dist-foo-build dist-foo-external-repo http://repo-server.example.com/path/to/repo/for/foo/\$arch/

  .. note::
    If you are adding multiple external repos, koji assigns a priority to each
    repo in FIFO order. This may cause updated packages to not be visible if a
    repo with older packages is ranked at a higher priority (lower numeric
    value). Use the ``-p`` flag to set specific repo priorities.

  .. note::
    This uses $arch NOT $basearch

  Koji behaves differently to the external repository based on which ``merge``
  mode is set for that. Merge modes can be set via ``-m`` option and are as
  follows:

    ``koji``
        Basic mode - koji expects, that that repo is complete and
        doesn't contain mixed content. It means that only rpms from one SRPM can
        be present in repo for given package.

        It runs ``mergerepos_c --koji`` Repositories generated via ``dist-repo``
        command (and all other repositories coming from koji` has these
        properties.

    ``bare``
        It runs ``mergerepos_c --pkgorigins --all``. It includes all rpms with
        same package name and architecture even if version or release is
        different. This one is needed for modular repos. ``createrepo_c`` 0.14+
        compiled with ``libmodule`` support needs to be installed on the
        builder. Nevertheless, only the first rpm with identical NEVRA is
        accepted.

    ``simple``
        It runs ``mergerepos_c --mode simple`` - we're least restrivtive with
        this type of repo. Even packages with identical NEVRA are accepted in
        such case. Simple mode is in ``createrepo_c`` suite from version 0.13.
        Reasons to use this compared to ``bare`` mode is a) you have older
        ``createrepo_c``, b) you really want to have all this identical NEVRAs
        in the repo.

* Create a build target that includes the tags you've already created. ::

    $ koji add-target dist-foo dist-foo-build

  At this point you can verify that your external repository is set with the
  "taginfo" command. You should see it listed under "External repos". Here is
  an example with several CentOS external repos::

    $ koji taginfo dist-foo-build
      Tag: dist-foo-build [740]
      Arches: x86_64
      Groups: build, srpm-build
      Tag options:
      This tag is a buildroot for one or more targets
      Current repo: repo#55077: 2017-11-29 03:34:18.847127
      Targets that build from this tag:
        dist-foo
      External repos:
          2 centos7-cr (http://mirror.centos.org/centos/7/cr/$arch/)
          3 centos7-extras (http://mirror.centos.org/centos/7/extras/$arch/)
          5 centos7-updates (http://mirror.centos.org/centos/7/updates/$arch/)
         10 centos7-os (http://mirror.centos.org/centos/7/os/$arch/)

* Create a ''build'' and ''srpm-build'' group associated with your build tag. ::

    $ koji add-group dist-foo-build build
    $ koji add-group dist-foo-build srpm-build

* Populate the ``build`` and ``srpm-build`` group with packages that will be
  installed into the minimal buildroot. You can find out what the is in the
  current groups for Fedora by running ``koji list-groups dist-f9-build``
  against the Fedora Koji instance. This is probably a good starting point for
  your minimal buildroot and srpm creation buildroot. If you are rebuilding
  Fedora packages, it is recommended that you add all packages listed in the
  Fedora Koji instance. ::

    $ koji add-group-pkg dist-foo-build build <pkg1> <pkg2> .....

* Add packages that you intend to build to your tag. ::

    $ koji add-pkg --owner <kojiuser> dist-foo <pkg1> <pkg2> .....

* Wait for the repo to regenerate, and you should now be able to run a build
  successfully.

Regenerating your repo
======================

koji doesn't monitor external repositories for changes by default.
Administrators can enable such bejaviour with setting ``check_external_repos =
true`` in ``kojira.conf`` (for details see :doc:`utils`). If it is not
enabled new repositories will be generated when packages you build land in a tag
that populates the buildroot or you manually regenerate the repository. you
should be sure to regularly regenerate the repositories manually to pick up
updates.

::

    $ koji regen-repo dist-foo-build

Examples of urls to use for external Repositories
=================================================

all these examples use mirrors.kernel.org please find the closest mirror
to yourself. Note that the Fedora minimal buildroots download ~100Mb
then build dependencies on top. these are downloaded each build you can
save a lot of network bandwidth by using a local mirror or running
through a caching proxy.

NOTE: this uses $arch **NOT** $basearch

Fedora 10
---------

::

    https://mirrors.kernel.org/fedora/releases/10/Everything/\$arch/os/
    https://mirrors.kernel.org/fedora/updates/10/\$arch/

CentOS 5 and EPEL
-----------------

::

    https://mirrors.kernel.org/centos/5/os/\$arch/
    https://mirrors.kernel.org/centos/5/updates/\$arch/
    https://mirrors.kernel.org/fedora-epel/5/\$arch/

Example tags and targets
========================

In the simplest setup, where you just want to build against what is
available in the external repositories, you may want to go with a simple
layout of *dist-f\ **X**-build* tags inheriting one another, and
*dist-f\ **X**-updates* tags and targets that inherit the
*dist-f\ **X**-build* tag and have external repos attached to them. This
way, a *dist-f\ **Y**-build* or *dist-f\ **Y**-updates* tag will not
automatically inherit the external repos of your *dist-f\ **X*** tags.

Tags
----

::

    dist-f10-updates               - This is where the external repos for f10 release and f10 updates are attached
     `- dist-f10-build             - This is the f10 build target with the 'build' and 'srpm-build' group inherited from dist-f9-build,
         |                           so that your buildroot gets populated but you do not have to maintain these groups for each
         |                           separate release.
         `- dist-f9-build          - etc.
             `- dist-f8-build      - etc.

Targets
-------

Each *dist-f\ **X**-build* tag has a *dist-f\ **X**-updates* child tag,
and each *dist-f\ **X**-updates* tag has a corresponding
*dist-f\ **X**-updates-candidate* build target.
Hub Configuration Options
-------------------------
The hub can be configured via one or more ini-style configuration files.
Options below should be placed in the ``[hub]`` section of these files.

These files can also contain a ``[policy]`` section for configuring policies.
This is described in :doc:`policy <defining_hub_policies>`.

File locations
^^^^^^^^^^^^^^

By default, koji-hub will read a primary configuration file at
``/etc/koji-hub/hub.conf`` and any number of supplemental configuration
files in the ``/etc/koji-hub/hub.conf.d`` directory.
Options in the primary configuration file take precedence in case of duplication.

If needed, these locations can be changed by setting the environment variables
``koji.hub.ConfigFile`` and ``koji.hub.ConfigDir`` respectively, e.g. by using
the SetEnv directive in the httpd configuration.

A common pattern is to place policy rules and/or sensitive values in separate configuration
files.


File permissions
^^^^^^^^^^^^^^^^

Note some configuration options (e.g. database password) are sensitive values.
Our default packaging installs ``hub.conf`` with 0640 root/apache file permissions.
If you're installing some other way, we recommend that you double-check these permissions.

Basic options
^^^^^^^^^^^^^
.. glossary::
   DBName
      Type: string

      Default: ``None``

      The name of the database that the hub should connect to.

   DBHost
      Type: string

      Default: ``None``

      The hostname that the database is running on.

      Note: If your database server is running locally and you would like to connect
      via Unix socket instead of TCP, then omit the ``DBHost`` and ``DBPort`` options.
      If you set ``DBHost`` to ``localhost``, then the connection will be over TCP.

   DBPort
      Type: string

      Default: ``None``

      The port to use when connecting to the database over TCP.

   DBUser
      Type: string

      Default: ``None``

      The database user to connect as.

   DBPass
      Type: string

      Default: ``None``

      The password for connecting to the database.

      Please ensure that your hub configuration file(s) have appropriate file permissions
      before placing sensitive data in them.

   DBConnectionString
      Type: string

      Default: ``None``

      The connection string (dsn) for connecting to the database.
      This overrides the other ``DB*`` options.
      This value is passed through to psycopg2 and would typically look something like:
      ``dbname=koji user=koji host=db.example.com port=5432 password=example_password``

   KojiDir
      Type: string

      Default: ``/mnt/koji``

      This is the root directory for koji files.

The database connection can be specified either by the single ``DBConnectionString``
option, or by other individual ``DB*`` options.
If given, the ``DBConnectionString`` option takes precedence.

General authentication options
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
.. glossary::
   CheckClientIP
      Type: boolean

      Default: ``True``

      Use user IP in session management.

   LoginCreatesUser
      Type: boolean

      Default: ``True``

      Whether or not to automatically create a new user from valid ssl or gssapi credentials.

GSSAPI authentication options
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

The following options control aspects of authentication when using ``mod_auth_gssapi``.

.. glossary::
   ProxyPrincipals
      Type: string

      Default: ``None``

      A comma separated list of principals that are allowed to perform proxy authentication.
      This ability is only intended for kojiweb.

   HostPrincipalFormat
      Type: string

      Default: ``None``

      This format string is used to set the principal when adding new hosts.
      The ``%s`` is expanded to the hostname.
      If a specific principal is given to the ``add-host`` command then this option
      is not used.

   AllowedKrbRealms
      Type: string

      Default: ``*``

      Allowed Kerberos Realms. The default value "*" indicates any Realm is allowed.
      This is a comma separated list.

   DisableGSSAPIProxyDNFallback
      Type: boolean

      Default: ``False``

      If True, enables backwards compatible behavior in the handling of the ``ProxyDNs``
      option.
      The default value of False is recommended.

   DisableURLSessions
      Type: boolean

      Default: ``False``

      If set to ``False``, it enables older clients to log in via session parameters
      encoded in URL. New behaviour uses header-based parameteres. This default
      will be changed in future to ``True`` effectively disabling older clients. It is
      encouraged to set it to ``True`` as soon as possible when no older clients are
      using the hub. (Added in 1.30, will be removed in 1.34)

Enabling gssapi auth also requires settings in the httpd config.

SSL client certificate auth configuration
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
If using SSL auth, these settings need to be valid and in line with other services configurations
for kojiweb to allow logins.

.. glossary::
   DNUsernameComponent
      Type: string

      Default: ``CN``

      The client username is the common name of the subject of their client certificate.

   ProxyDNs
      Type: string

      Default: ``''``

      If specified, the given DNs are allowed to perform proxy authentication.
      This ability is only intended for kojiweb.
      Multiple DNs can be separated with the vertical bar character, ``|``.

Enabling ssl auth also requires editing the httpd config (conf.d/kojihub.conf).

Notification options
^^^^^^^^^^^^^^^^^^^^
.. glossary::
   KojiWebURL
      Type: string

      Default: ``http://localhost.localdomain/koji``

      This specifies the URL address of Koji web.
      This setting affects the links that appear in notification messages.

   EmailDomain
      Type: string

      Default: ``None``

      Email domain name that koji will append to usernames when creating email notifications.

   NotifyOnSuccess
      Type: boolean

      Default: ``True``

      Whether to send the task owner and package owner email or not on success.
      This still goes to watchers.

   DisableNotifications
      Type: boolean

      Default: ``False``

      Disables all notifications

For more on notifications in Koji, see :ref:`notification-basics`

Resource limits
^^^^^^^^^^^^^^^
If configured, the following resource limits are applied by the hub at startup.
Each value defaults to ``None``, meaning no limit is applied.
If given, the value should be either a single integer or a pair of integers separated by whitespace.

If a pair of integers is given, this sets both the soft and hard limits for the resource.
If a single integer is given, only the soft limit is set.

.. glossary::
   RLIMIT_AS
      Type: string

      Default: ``None``

      This is the maximum size of the process's virtual memory (address space).
      The limit is specified in bytes, and is rounded down to the system page size.

   RLIMIT_CORE
      Type: string

      Default: ``None``

      This is the maximum size of a core file in bytes that the process may dump.
      When 0, no core dump file are created. When nonzero, larger dumps are truncated to this size.

   RLIMIT_CPU
      Type: string

      Default: ``None``

      This is a limit, in seconds, on the amount of CPU time that the process can consume.

   RLIMIT_DATA
      Type: string

      Default: ``None``

      This is the maximum size of the process's data segment (initialized data,
      uninitialized data, and heap). The limit is specified in bytes,
      and is rounded down to the system page size.

   RLIMIT_FSIZE
      Type: string

      Default: ``None``

      This is the maximum size in bytes of files that the process may create.

   RLIMIT_MEMLOCK
      Type: string

      Default: ``None``

      This is the maximum number of bytes of memory that may be  locked into RAM.
      This limit is in effect rounded down to the nearest multiple of the system page size.

   RLIMIT_NOFILE
      Type: string

      Default: ``None``

      This specifies a value one greater than the maximum file descriptor number that
      can be opened by this process.

   RLIMIT_NPROC
      Type: string

      Default: ``None``

      This is a limit on the number of extant process (or, more precisely on Linux, threads)
      for the real user ID of the  calling process.

   RLIMIT_OFILE
      Type: string

      Default: ``None``

      This specifies a value one greater than the maximum file descriptor number that
      can be opened by this process. This limit was on BSD.

   RLIMIT_RSS
      Type: string

      Default: ``None``

      This is a limit (in bytes) on the process's resident set (the number of virtual pages resident in RAM).

   RLIMIT_STACK
      Type: string

      Default: ``None``

      This is the maximum size of the process stack, in bytes.

Additionally, the following options govern resource-related behavior.

.. glossary::
   MemoryWarnThreshold
      Type: int

      Default: ``5000``

      If memory consumption rises more than this value (in kilobytes) while handling a single
      request, a warning will be emitted to the log

   MaxRequestLength
      Type: int

      Default: ``4194304``

      This sets the maximum request length that the hub will process.
      If a longer request is encountered, the hub will stop reading it and return an error.

Extended features
^^^^^^^^^^^^^^^^^
Koji includes limited support for building via Maven or under Windows.

.. glossary::
   EnableMaven
      Type: boolean

      Default: ``False``

      This option enables support for building with Maven.

   EnableWin
      Type: boolean

      Default: ``False``

      This option enables support for :doc:`Windows builds <winbuild>`.

Koji hub plugins
^^^^^^^^^^^^^^^^
The hub supports plugins, which are loaded from the ``PluginPath`` directory.

.. glossary::
   PluginPath
      Type: string

      Default: ``/usr/lib/koji-hub-plugins``

      The path where plugins are found.

   Plugins
      Type: string

      Default: ``''``

      A space-separated list of plugins to load.
      Each entry should be the name of a plugin file (without the ``.py``).
      Only plugins from the configured ``PluginPath`` can be loaded.

Koji debugging
^^^^^^^^^^^^^^
The following options are primarily intended for debugging Koji's behavior.

.. glossary::
   KojiDebug
      Type: boolean

      Default: ``False``

      If KojiDebug is on, the hub will be /very/ verbose and will report exception details to
      clients for anticipated errors (i.e. koji's own exceptions -- subclasses of koji.GenericError).

      This option is only intended for specialized debugging and should be left off in production
      environments.

   KojiTraceback
      Type: string

      Default: ``None``

      This determines how much detail about exceptions is reported to the client (via faults).
      The meaningful values are:

      * normal - a basic traceback (format_exception)
      * extended - an extended traceback (format_exc_plus)

      If left unset, the default behavior is to omit the traceback and just report the error
      class and message.

      Note: the extended traceback is intended for debugging only and should NOT be used in production,
      since it may contain sensitive information.

   LogLevel
      Type: string

      Default: ``WARNING``

      This option controls the log level the hub logs.
      Koji uses the standard Python logging module and the standard log level names.

      Setting multiple log levels for different parts of the hub code is possible.
      The option is treated as a space-separated list log level directives, which
      can be either a single log level name (sets the default log level) or a
      logger:level pair (sets the log level for the given logger namespace).

      For example, you could set:

         ``LogLevel = INFO koji.db:DEBUG``

      To see debug logging only for the db module.

   LogFormat
      Type:string

      Default: ``%(asctime)s [%(levelname)s] m=%(method)s u=%(user_name)s p=%(process)s r=%(remoteaddr)s %(name)s: %(message)s``

      This sets the log format string for log messages issued by the hub code.
      In addition to the normal values available from the logging module, the hub's log formatter provides:

      * method -- the method name for the call being processed
      * user_id -- the id of the user making the call
      * user_name -- the name of the user making the call
      * session_id -- the session_id of the call
      * callnum -- the callnum value for the session
      * remoteaddr -- the ip address and port (colon separated) that the call is coming from

Hub Policy
^^^^^^^^^^
.. glossary::
   VerbosePolicy
      Type: boolean

      Default: ``False``

      If VerbosePolicy (or KojiDebug) is on, 'policy violation' messages will report the
      policy rule which caused the denial.

   MissingPolicyOk
      Type: boolean

      Default: ``True``

      If MissingPolicyOk is on, and a given policy is not defined, the policy check will return
      'allow', otherwise such cases will result in 'deny'.

Koji outages options
^^^^^^^^^^^^^^^^^^^^
These options are intended for planned outages.

.. glossary::
   ServerOffline
      Type: boolean

      Default: ``False``

      If ServerOffline is True, the server will always report a ServerOffline fault
      (with OfflineMessage as the fault string).

   OfflineMessage
      Type: string

      Default: ``None``

      This controls the error message that is reported when ``ServerOffline`` is set.

   LockOut
      Type: boolean

      Default: ``False``

      If Lockout is True, the server will report a ServerOffline fault for most non-admin requests.

Name verification
^^^^^^^^^^^^^^^^^
Currently we have two groups for name verification:
 - internal names
 - user names

Group internal names is currently used for:
 - archive type
 - btype
 - channel
 - external repo
 - group
 - host
 - kerberos
 - permission
 - tag
 - target
 - volume

Group user names is currently used for:
 - user
 - host

Host names are listed in both groups because hosts always have an associated user entry.

.. glossary::
   MaxNameLengthInternal
      Type: string

      Default: ``256``

      Set length of internal names. By default there is allowed length set up to 256.
      When length is set up to 0, length verifying is disabled.

   RegexNameInternal
      Type: string

      Default: ``^[A-Za-z0-9/_.+-]+$``

      Set regex for verify an internal names. When regex string is empty, verifying
      is disabled.

   RegexUserName = ^[A-Za-z0-9/_.@-]+$
      Type: string

      Default: ``^[A-Za-z0-9/_.@-]+$``

      Set regex for verify a user name and kerberos. User name and kerberos have
      in default set up allowed '@' and '/' chars on top of basic name regex
      for internal names. When regex string is empty, verifying is disabled.
=======================
Building Images in Koji
=======================

Image Building
==============

Koji is capable of building many different types of images or appliances. They
broadly fall into a few categories: LiveMedias, Disk Images, and Containers. All
types of images end up with a Name-Version-Release just like an RPM build.  What
you need to provide Koji to perform a build varies by category, and the
specifics will be explained below.

For additional questions and information, ask around in ``#koji`` on FreeNode
IRC and sign up to the right mailing lists as the `Koji project website`_
dictates.

Kickstart First
===============

No matter which type of image you want to build, you need to feed Koji a
`kickstart`_ file, so we touch on how to create them and how to get them to Koji
first. Once you have a kickstart to try out, look over the kickstart-specific
caveats for each image type in the later sections.

Kickstart Reference
-------------------

If you're new to automated installations, you should read the `Anaconda
Kickstart Guide`_ first.

Getting your Kickstart to Koji
------------------------------

For scratch builds, you can get away with just using the ``--kickstart``
parameter, which accepts a path (relative or absolute) to your kickstart file
on disk. For non-scratch builds though, the kickstart file needs to live in a
remote Source Control Manager (SCM) such as git or Subversion. To access that,
you need to pass the ``--ksurl`` parameter, which accepts a wacky string in
this form:

::

    git://git.fedorahosted.org/git/spin-kickstarts.git?fedora22#68c40eb7

Here we pointed to a repository hosted at
``git://git.fedorahosted.org/git/spin-kickstarts.git``. Note the ``?`` in
there, this delimiter separates the host URL with a directory structure to
look in for the kickstart file. In other words, if you were to clone the
repository, you should expect to see these (sub)directories laid out below the
root of the clone. The important thing to realize is the behavior of
``--kickstart`` changes if ``--ksurl`` is specified. Instead of indicating a
path to a kickstart file, you just indicate a filename. The file itself is not
specified in the fragment of the string between ``?`` and ``#`` that is passed
to ``--ksurl``, Koji still uses the ``--kickstart`` option for that. After the
directory structure comes a ``#``, which indicates the start of the commit ID.
This is the commit Koji will reset the repository to after cloning. If you
created a git repository which had just the spec file template in it, your SCM
URL would have the ``?`` and ``#`` right next to each other.

If we passed ``--ksurl`` the string above, and gave
``--kickstart "server-ec2.ks"``, then Koji would do the following:

* Clone the repository locally
* Call ``git reset --hard 68c40eb7``
* Search in the "fedora22" subdirectory of the clone for a file called
  ``server-ec2.ks``, and pass that to Anaconda
* Perform an automated install

.. _building-livemedia:

Building LiveMedias
===================

Let's describe building LiveMedias and the mechanics behind it. We also still
suport older method LiveCD which is deprecated now.

Getting Started
---------------

What you need before proceeding:

* a name for your LiveMedia, and a version number
* a Koji build target
* what architectures you want
* a flattened kickstart file
* you may also need yum repositories generated by release engineering if you
  require signed packages be installed into the LiveMedia
* the permission for building these in Koji - in default installation permission
  is not needed, but in production environments there will be probably some
  permission or at least policy in place

The Koji command that creates a LiveMedia is called ``spin-livemedia``. It calls
out to `livemedia-creator`_ from `lorax`_ project in a mock chroot to construct
the LiveMedia. To run a LiveMedia build, use the ``spin-livemedia`` command like
so:

::

    $ koji spin-livemedia --release 20200922.n.0  \
                          --ksurl 'git+https://pagure.io/fedora-kickstarts.git?#b4956c05028a088c641d0ce6e1a31b6d8b20176f' \
                          --install-tree-url 'https://kojipkgs.fedoraproject.org/compose/branched/Fedora-33-20200922.n.0/compose/Everything/$basearch/os' \
                          --repo 'https://kojipkgs.fedoraproject.org/compose/branched/Fedora-33-20200922.n.0/compose/Everything/$basearch/os' \
                          --can-fail ppc64le,aarch64 \
                          Fedora-Workstation-Live 33 f33 x86_64,ppc64le,aarch64 fedora-live-workstation.ks

In this example (real task is `here
<https://koji.fedoraproject.org/koji/taskinfo?taskID=52013359>`_) a LiveMedia will
be created with the N-V-R of ``Fedora-Workstation-Live-33-20200922.n.0``. If
``--release`` was not included an incrementing value would be computed by Koji
instead. spin-livemedia takes a minimum of 4 arguments:

Name
    the name of the image, without versioning information. Examples could be
    ``Fedora-Live`` or ``Fedora-Workstation-Live``.

Version
    an arbitrary version string. For Fedora images, this usually matches the
    release version, such as 31, 32, or 33.

Build Target
    just like RPM builds Koji must be told which target to use. This controls
    what tag the image will be tagged into, and what packages are available to
    install into the image.

Kickstart File
    this is a recipe file that performs drives the construction of the image.
    Note, that you're can either upload your kickstart or just point to some SCM
    (``--ksurl``).  In our example fedora repo contains
    ``fedora-live-workstation.ks`` kickstart file.

Use ``--help`` to discover more options to ``spin-livemedia``. You can override
the Release field with ``--release``, or ``--repo`` to override what yum repo is
used to provide packages to install to the image. Note that regardless of what
repo you use, it must contain RPMs built by Koji, otherwise you will get an
error. (unless you are building a scratch image). ``--can-fail`` options says
which architectures are optional and can fail without failing the whole build.

Mechanics
---------

The way LiveMedias are created in Koji is fairly straightforward. A chroot is
initialized and populated with the packages and their dependencies from the
``livemedia-build`` package group. Next, the kickstart file is copied into it if
it was provided from local storage. If not, it is checked out into it from an
SCM. It is then modified to use the repo associated with the build tag for the
target specified in the command unless the ``--repo`` option was given. Both the
original and the modified kickstart files are saved as part of the output for
the task for later review. A `livemedia-creator`_ command is executed using the
``mock('--chroot', ...)`` method.

.. note::
    This process runs as root. This produces the desired image which is uploaded
    to ``/mnt/koji/images/<image>/$imageID`` if it is not a scratch image.

.. _caveats-for-livemedias:

Caveats for LiveMedias
----------------------

There are some known caveats with using the spin-livemedia command that users
should be aware of.

%include macros in the kickstart
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

A word of caution about kickstart files and the ``%include`` macro.
`livemedia-creator`_ is smart enough to search the current directory of the
submitted kickstart file if it has ``%include`` macros. If the kickstart
specified to koji is from local storage, only that kickstart file will be copied
into the chroot, and this creates a problem if it has ``%include`` macros,
because the other kickstart files it needs will be inaccessible. This issue is
not present when the kickstart file is retrieved from a remote SCM (such as the
fedora-kickstarts git repo), because the entire repository is checked out.
Presumably it will include any other kickstart files the specified one is
including in the same directory. A workaround for the issue would be to use
``ksflatten`` (from pykickstart) on kickstart files with ``%include`` macros
that are going to be submitted to koji from the user's local disk.

Package Groups in the Kickstart File
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

Package Groups in the kickstart file cause a problem if the Koji repos do not
define them, which they most likely don't since Koji's comps.xml is based on the
"groups" set up from the CLI. `livemedia-creator`_'s behavior is to ignore
package groups that are not defined in the repo it is using, so this can be
troublesome when creating the image since packages could be left out. There are
a couple possible workarounds:

* do not use package groups in the kickstart file and just specify a huge list
  of packages
* use ``--repo`` and specify a repo that does have a comps.xml that defines
  the groups it uses

Only Include RPMs Built in Koji
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

The image building tasks will fail if your image tries to include a package
that was not built in your build system. This is because the package does not
have any origin information stored in Koji's database. The repos defined in the
kickstart will automatically be overridden with the repo of the build tag for
the build target, unless you use the ``--repo`` option. Since only packages
you have built (or include from an external repo) should be there, you should
never have this problem unless you use ``--repo``.

No Signed or Debuginfo RPMs in Koji's Build Tags
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

If you need signed RPMs or debuginfo RPMs, you will run into trouble because
Koji does not keep those in its build tag repos. The only work around for this
is to create a repo yourself that includes these RPMs and then use ``--repo``.
This will force the image to take RPMs from that repo. Remember, the task will
fail if Koji detects RPMs were installed that were not built in the build
system.

%post Section in Kickstart
^^^^^^^^^^^^^^^^^^^^^^^^^^

While `livemedia-creator`_ does support building on SELinux disabled hosts, you
can run into denials when booting if you create and use new files in the
``%post`` section of your kickstart file. If you do, you should either set the
labels appropriately at the end of the ``%post`` section, or instigate an
autorelabel at boot time.

Troubleshooting
---------------

If your build fails, you will be notified on the command line. In the output
will be a URL to the Koji UI, visit that and click on the red subtask link.
From that page review ``root.log`` and ``livemedia-out.log`` for errors. Often
errors are caused by packages being missing, or malformed kickstart files. The
log files are at the bottom of the page. If problem occurs later during
installation `livemedia-creator`_ will also upload a lot of other logs like
anaconda's, etc. If you're stuck, contact Release Engineering.

Build System Preparation
------------------------

This section assumes you have know-how required to install and configure a new
instance of Koji, and that you have already done so. You can learn how to do so
:doc:`here <server_howto>` if you need to. Please ensure you are using the
latest version of the software and that your database schema is updated as
well. You should also have some familiarity with how `livemedia-creator`_ works.
This section only covers preparation for LiveMedia builds.

Follow this procedure step by step to get things prepared they way they need to
be.

#.  ``koji add-host-to-channel <your-host> livemedia``
        add a builder to the livemedia channel
#.  ``koji grant-permission livemedia <user>``
        grant the permission to build an image type to a user. This step is
        optional since admins have all permissions.
#.  You will need a tag and target to build the images from. The yum repo
    generated for the build tag of the target is what Koji will use to populate
    the LiveMedias with by default. (the alternative is to use the ``--repo``
    option, more on that later)
#.  ``koji add-group <build-tag> livemedia-build``
    add the livemedia-build group
#.  ``koji add-group-pkg <build-tag> livemedia-build <pkg> ...``
        add packages to the livemedia-build group. These package lists vary has
        packages and dependencies change. As of September, 2020 for Fedora 33 the
        needed packages for each image type are:

        * bash, coreutils, glibc-all-langpacks, lorax-lmc-novirt,
          selinux-policy-targeted, shadow-utils, util-linux


Building Disk Images
====================

Disk images are files that represent virtual disks. They have a partition table
and filesystems on them, and are available in a variety of formats: qcow2,
vmdk, ova, Hyper-V, raw, "base" container images, and more.

Getting Started
---------------

What you need:

* a name for your image, and a version number
* what architectures you want
* a Koji build target
* kickstart file
* installation tree
* you may also need yum repositories generated by Rel-Eng if you require signed
  packages be installed into the image

The Koji command to build a disk image is called ``image-build``. The
``image-build`` command uses `ImageFactory`_ and `Oz`_ to start a VM guest and
perform an automated Anaconda installation. Here is a (lengthy) example for
building a disk image.

::

    $ koji image-build --repo 'https://alt.fedoraproject.org/pub/alt/releases/22/Cloud/$arch/os/' --kickstart fedora-server.ks --scratch --distro Fedora-22 --format qcow2 fedora-server-kvm 22 'https://alt.fedoraproject.org/pub/alt/releases/22/Cloud/$arch/os/' x86_64

This example builds a scratch qcow2 disk image using packages from an
additional yum repository. Without this option the yum repo to populate the
build root would be used instead. If this was the first image with the N-V of
fedora-server-kvm-22, then the N-V-R would be fedora-server-kvm-22-1, because
Koji uses an incrementing number for the release if you do not provide one.
Like all Koji commands, use ``--help`` to see more options that are available.

For Docker, Koji only supports Base Images right now using a kickstart file as
described above. In the future it will support layered images, but not before
some Docker requirements are met, and Koji is maintaining a Registry of its
own. This scoping effort is ongoing.

``image-build`` takes a minimum of 5 positional arguments, and 2 options must
be specified. They are reviewed in the list below, with the positional
arguments first.

Name
    the name of the image, without versioning information. Examples could be
    ``fedora-server`` or ``fedora-workstation``.

Version
    an arbitrary version string. For Fedora images, this usually matches the
    release version, such as 22 or 23.

Build Target
    just like RPM builds Koji must be told which target to use. This controls
    what tag the image will be tagged into, and what packages are available to
    install into the image.

Installation Tree URL
    this is a URL to a location you can install an operating system from. It is
    the same place you would direct a PXE-booted system to go. In 99% of cases
    this location is provided by Release Engineering. It should have an
    "isolinux" subdirectory and yum metadata somewhere within.

Architecture
    only x86_64 or i386 is supported, and you can specify both on the command
    line. This will cause two subtasks to be run, allowing you to build for
    both arches in parallel. If either fail, the whole build will fail.

Kickstart File
    this is a recipe file that performs drives the construction of the image.
    Pass in the path to a kickstart file, which must be flattened.

Kickstart URL
    in a non-scratch build, you'll need this too. For more details, see the
    Getting your Kickstart to Koji section.

Distro
    a string that indicates what OS is being built. These always follow the
    convention of "Fedora-X", where X is the release number.

Since this command can get very long, a configuration file can drive the task
as well, using the ``--config`` option. It accepts a path to a configuration
file written in the Python ConfigParser format (like a Windows .ini). The
options are all named the same with one caveat, see below. Here's what one
could look like:

::

    [image-build]
    name = fedora-server-docker
    version = 22
    target = f22-candidate
    install_tree = https://alt.fedoraproject.org/pub/alt/releases/22/Cloud/$arch/os/
    arches = x86_64

    format = qcow2,rhevm-ova,vsphere-ova
    distro = Fedora-22
    repo = https://alt.fedoraproject.org/pub/alt/releases/22/Cloud/$arch/os/
    disk_size = 20

    ksversion = DEVEL
    kickstart = fedora-22-server-docker.ks
    ksurl = git://git.fedorahosted.org/git/spin-kickstarts.git?fedora22#68c40eb7
    specfile = git://git.fedorahosted.org/git/spin-kickstarts.git?spec_templates/fedora22#68c40eb7

A few notes on the syntax:

* it allows for comments too, the lines start with a hash (#)
* options on the command line that can be used multiple times can accept
  values here as comma-separated strings
* options with a hyphen need to use an underscore instead. ``--disk-size`` for
  example would be ``disk_size`` in the config file

OVA Features
------------

If you're building OVAs, either for RHEVM or vSphere, you can specify OVA
options with a special section in the configuration file. It looks something
like this:

::

    [ova-options]
    vsphere_product_version=22
    rhevm_description=Fedora Cloud 22
    vsphere_product_vendor_name=Fedora Project
    ovf_memory_mb=6144
    rhevm_default_display_type=1
    vsphere_product_name=Fedora Cloud 22
    ovf_cpu_count=4
    rhevm_os_descriptor=Fedora-22

or this:

::

    [ova-options]
    vsphere_ova_format = vagrant-virtualbox
    rhevm_ova_format = vagrant-libvirt
    vagrant_sync_directory = /home/vagrant/sync

The second one is actually the secret sauce for generating an image for use in
Vagrant. At this time, you would need rename the image file extension from .ova
to .box, but otherwise this should work fine.

Kickstart Preparation
---------------------

Kickstarts for the image-build command have some specific requirements which
are covered in this section.

Required Kickstart Arguments
^^^^^^^^^^^^^^^^^^^^^^^^^^^^

Anaconda of course requires many commands to be defined in the kickstart file.
If you're starting from scratch you should review the reference linked above,
or use an existing kickstart file in the spin-kickstarts git repo. It is
critically important that the installation be completely automated, if
Anaconda has to prompt for input for any reason, the build will fail because
you cannot send input to the guest. Some of the kickstart commands are optional
to Anaconda, but are required in Koji for your build to succeed. Here's the
list and the reasons why.

zerombr
    You must tell Anaconda to wipe out the MBR in the virtual block device, if
    you don't Anaconda will ask you.

clearpart --all --initlabel
    Anaconda has to be told to wipe out all data on the virtual block device we
    install on otherwise it will ask for confirmation to do so. Since it is
    blank anyway this is harmless.

reboot
    When the installation completes, the guest is rebooted. `ImageFactory`_ is
    specifically looking for this behavior to conclude the installation
    completed. Anaconda's default behavior is to wait for a key press to reboot
    the system, but this is impossible from outside of Koji.

locking the root account
    You have to lock the root account (rootpw --lock) or create a non-root user
    (user), otherwise Anaconda will prompt for one.

Do not use the url command
    The repo commands are overridden by Koji to point to internal Koji repos,
    or what you specified on the command line with ``--repo``, it does not
    override the url command if you provided it. Anaconda has a behavior where
    it will prefer packages from the repositories given with the url command
    over those with the repo command, and this is generally not what you want.
    If Koji sees an RPM was installed that was not built in the system, it will
    fail the build.

Recommended Kickstart Arguments
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

Often you want a ``%post`` section in your kickstart to perform
post-installation configuration steps. Review that section of the reference
and note that you can specify ``--log`` and ``--interpreter``. Both of these
are recommended (but not required) to assist with the development and debugging
process. Here are some other recommendations:

* You probably want the network to use dhcp, sshd to be started, and port 22
  opened in the firewall to allow access as well.
* If you're building an image that will be shipped with a product, SELinux
  should be enabled.
* Images that will be used in cloud deployments like OpenStack or EC2 should
  have ``cloud-init`` in the package list.
* It is discouraged to have root passwords in plaintext in your kickstart file.
* If your %post section is written in bash, consider setting -x.
* For images that have multiple partitions, use the ``--asprimary`` option for
  the part command that defines the root file system. This will ensure it is
  the first partition on the image, which is a requirement in some cloud
  environments like EC2.

Troubleshooting
---------------

If your disk image build fails, follow the link in the command line output
that takes you to the task page in the Koji web UI. Click on the failed
createImage subtask in red. On that page review the screenshot.ppm file if it
was provided, or oz.log. Most failures are from Anaconda rejecting a malformed
kickstart file, which will be indicated in the screenshot. Your installation
must be completely automatic, there can be no interactivity at all, otherwise
Anaconda will sit there indefinitely until Koji (actually ImageFactory) kills
the task.

It is very easy to write a kickstart file with bugs or that results in a system
that does not boot. This section will present a series of questions to ask
yourself and examples to help diagnose where the problem lies. Once you know
that, it should be easier to understand what you can do to inspect further.

There are 4 steps in the process:

#.  create a guest
#.  perform an automated installation in the guest
#.  boot the guest and extract the list of installed RPMs
#.  upload and archive the disk image of the guest

Is it a problem with guest creation?
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

There have been unusual cases where libvirt, ImageFactory, or Oz was
misconfigured and guests could not be started properly. A misconfiguration with
Puppet or whatever Fedora Infrastructure is up to can cause this. So far the
errors have been clear in the task output, look either in the results string
or oz.log. The bad news is that in this case you really can only inform Rel-Eng
about the issue and wait for a resolution. The good news is these cases are
very rare.

Did the installation fail?
^^^^^^^^^^^^^^^^^^^^^^^^^^

The Anaconda installation can fail for many reasons: missing packages, network
problems, or syntax errors in %post. Tasks will also fail if Anaconda prompts
for input for any reason. If Koji detects a lack of disk activity in the guest
for more than 5 minutes, it will fail the build and tear down the guest.
Looking in oz.log may have the answer: dracut, anaconda, and yum logs are all
printed there.

These sorts of failures often have a screenshot taken and saved with the task
output called screenshot.ppm. Viewing this will usually tell you what Anaconda
is complaining about if the installer detected an issue or prompted for input.
The string in the results output that says "No disk activity in 300 seconds,
failing." This almost always means Anaconda hit an issue and either gave up or
waited.

If Anaconda claims it is missing packages, confirm they exist in the repos you
are using with ``--repo``, if you are using that option. If you are not,
confirm the builds you expect are in the tag inheritance for the target you are
running. This is a lot like checking whether an RPM will build against the
right libraries, except we're building an image instead.

If you get the rare Anaconda dialog box that says something like "An unexpected
error occurred", try using the ``text`` command in kickstart, which will have
Anaconda boot in text mode. Sometimes the Python traceback (or whatever the
error condition is) will be printed there. I have also seen cases where
text-mode yields a black screen, but booting in graphical mode (the default)
does produce a useful dialog box. Issues like this stem from syntax errors in
the kickstart file, or bugs in pykickstart itself. If you think it is a
pykickstart bug, then someone in Rel-Eng needs to update pykickstart on the
builders.

Did the guest boot?
^^^^^^^^^^^^^^^^^^^

Koji waits 5 minutes for a guest to boot in this step. It unfortunately does
not give a lot of insight to why a guest may not boot, so these are a tougher
class of issues to work through. You can usually answer this question by
looking in results string. If you see "Timed out waiting for guest to boot",
then this is your problem. You can also confirm this in oz.log.

For now, the best way to investigate an issue like this is to drive a guest
installation locally using something like Gnome's Virtual Machine Manager
(VMM). The steps to perform are:

* Select a Network Install
* For the Operating System Install URL use the same one you gave to Koji. It
  will be something like
  https://alt.fedoraproject.org/pub/alt/releases/22/Cloud/x86_64/os/
* Set the Kickstart URL to where your kickstart file is. You may need to make
  it available over http.
* Bump the memory to 2048M for good measure
* Launch the guest and let it complete installation
* Open a VNC session and watch what happens when the guest attempts to boot.

If the console is not providing enough information, we have to get more
creative. Anaconda supports starting an SSH daemon while the installation is
happening with the sshpw command in kickstart. Set that and comment out the
reboot command. This will let the installation complete locally and wait for a
keystroke to reboot the guest. At this point you should be able to ssh in and
inspect the environment to figure out what is going on. You should also
consider making use of the ``--log`` option to %post so that output from the
script is saved somewhere.

Another option would be to scp logs and other files off of the guest as part of
the ``%post`` script.

Other Guest Misconfigurations
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

If the guest boots but you're having problems accessing it I'd suggest
following same procedure as when the guest fails to boot. This could be a
result of firewall misconfigurations or SSH not being available for some
reason. Usually in this case the build is succeeding in Koji, but there's
something still fundamentally broken in the image. If the issue is something
you can investigate while the guest is online (you can log in), then I'd
suggest importing it locally using the libvirt.xml and the disk image provided
in Koji's task output.

You can also do investigative work in an offline mode by mounting the image
locally or using something like libguestfs to poke around without starting the
guest. The fast, dirty way to do it is by mounting it. This can often pollute
your guest environment. Here's how to do it:

* Download the image from Koji
* If the image format is not raw, you have to convert it first with qemu-img.
  Something like:
  ::

        $ qemu-img convert -O raw <image-file> <output-file></pre>

* Now mount it up using loopback devices. (as root) If your image has multiple
  partitions in it, you may need to pass in a different mapped loopback device
  like ``loop0p2``. Whichever one you think is the root partition or has the
  issue you're trying to fix.
  ::

        # kpartx -av <raw-image>
        # mount -o loop /dev/mapper/loop0p1 /mnt/my_directory

Hopefully at this point you figure out the issue. To tear down the image you'll
run commands as root like so:

::

    # umount /mnt/my_directory
    # dmsetup remove loop0p1
    # losetup -d /dev/loop0

Again, if you used different loopback devices, substitute those in to the
dmsetup and losetup commands.

Build System Preparation
------------------------

Follow this guide if you're a Koji admin and would like to enable image
building or want to set up some testing before enabling the integration.

When moving to ImageFactory to do image builds Koji lost the ability to easily
reproduce the build environment for images the way we do for RPMs using Mock.
This section will document how to set up an image builder for Koji. This is a
lengthy task, it will take first-timers about a week to have a useful instance,
and it is painful because it requires a bare metal system be provisioned since
`ImageFactory`_ provisions VMs to build the image. There is a significant
performance penalty for using nested virtualization.

Follow the steps below to set up your builder. 

.. note::
    You do not have to stand up a complete Koji instance to test the way Koji
    builds images. However, if you want to test image builds with an accurate
    representation of how Koji does it, or you want to test code changes
    related to image builds in Koji, you should follow all the steps below.

ImageFactory/Oz Preparation
^^^^^^^^^^^^^^^^^^^^^^^^^^^

#.  Provision a system with at least 4G of memory with the current release of
    RHEL 6 or later. Sometimes builders lag behind a month or two before taking
    in updates, but this will still get you pretty close to where you want to
    be. For Fedora, use the latest release.
#.  Install the following packages to the builder.
    #.  oz
    #.  imagefactory
    #.  imagefactory-plugins-TinMan
    #.  imagefactory-plugins-vSphere
    #.  imagefactory-plugins-ovfcommon
    #.  imagefactory-plugins-docker
    #.  imagefactory-plugins
    #.  imagefactory-plugins-OVA
    #.  imagefactory-plugins-RHEVM
    #.  python-psphere => 0.5
    #.  VMDKStream => 0.2
    #.  pykickstart
#.  Edit ``/etc/kojid/kojid.conf``, and set an second value, eg: 7200 for
    ``oz_install_timeout``. It's a timeout waiting guest installing. Default
    value is 0, that means oz will use its default value. Since ``oz-0.16.0``,
    it can be configured in ``/etc/oz/oz.cfg`` as ``install`` in ``[timeouts]``
    section.
#.  Edit ``/etc/oz/oz.cfg``, and set the memory value in the ``[libvirt]``
    section to at least 2048. Set ``safe_generation`` under ``[icicle]`` to yes.
#.  Run: ``mkdir -p ~root/.psphere/templates``, and then copy the following
    code into ``~root/.psphere/config.yaml``. Do not worry about the server,
    username, and password credentials; they are not used anywhere.

    ::

        general:
          server: 10.16.120.224
          username: Administrator
          password: whatever
          template_dir: ~/.psphere/templates/
        logging:
          destination: ~/.psphere/psphere.log
          level: DEBUG # DEBUG, INFO, etc

Start up the services and ImageFactory/Oz should be ready to go. You should
read more about `how to use Oz`_ and `how to use ImageFactory`_. If you want
to try calling `ImageFactory`_ as if from a Koji Builder (but not set up a
whole Koji instance), you can use the code below to emulate that. If you want
to test the Koji integration with a full Koji instance, proceed to the next
section instead.

::

    #!/usr/bin/python -tt

    import logging
    import os.path
    import random
    import sys

    from imgfac.BuildDispatcher import BuildDispatcher
    from imgfac.PluginManager import PluginManager
    from imgfac.ReservationManager import ReservationManager
    plugin_mgr = PluginManager('/etc/imagefactory/plugins.d')
    plugin_mgr.load()
    from imgfac.ApplicationConfiguration import ApplicationConfiguration

    # logging
    handler = logging.StreamHandler(sys.stdout)
    tlog = logging.getLogger()
    tlog.setLevel(logging.DEBUG)
    tlog.addHandler(handler)

    # configuration
    ks = open('oztest.ks').read()
    workdir = '/tmp/koji/test'
    config =  {
        #Oz specific
        'oz_data_dir': os.path.join(workdir, 'oz_data'),
        'oz_screenshot_dir': os.path.join(workdir, 'oz_screenshots'),
        #IF specific
        'imgdir': os.path.join(workdir, 'scratch_images'),
        'tmpdir': os.path.join(workdir, 'oz-tmp'),
        'verbose' : True,
        'timeout': 3600,
        'output': 'log',
        'raw': False,
        'debug': True,
        'image_manager': 'file',
        'plugins': '/etc/imagefactory/plugins.d',
        'tdl_require_root_pw': False,
        'image_manager_args': {
            'storage_path': os.path.join(workdir, 'output_image')},
    }
    random.seed() # necessary to ensure a unique mac address
    rm = ReservationManager()
    rm._listen_port = random.randint(rm.MIN_PORT, rm.MAX_PORT)
    ApplicationConfiguration(configuration=config)
    params = {'install_script': ks}
    template = """<template>
        <name>test-appliance</name>
            <os>
                <name>Fedora</name>
                <version>22</version>
                <arch>x86_64</arch>
                <install type='url'>
                    <url>https://alt.fedoraproject.org/pub/alt/releases/22/Cloud/x86_64/os/</url>
                </install>
                <icicle>
                    <extra_command>rpm -qa --qf '%{NAME},%{VERSION},%{RELEASE},%{ARCH},%{EPOCH},%{SIZE},%{SIGMD5},%{BUILDTIME}\n'</extra_command>
                </icicle>
            </os>
        <description>test-appliance OS</description>
        <disk>
            <size>16G</size>
        </disk>
    </template>
    """
    bd = BuildDispatcher()

    # build the image
    base = bd.builder_for_base_image(template, parameters=params)
    base.base_thread.join()
    tlog.removeHandler(handler)

This script is run as root with no arguments. It uses ``oztest.ks`` in your
local directory as the kickstart you want to try to use. The URL in the XML
template is where the RPM packages will be installed from, and what the guest
will be booted with.

Koji Preparation
^^^^^^^^^^^^^^^^

#.  :doc:`Install Koji <server_howto>` if you need it. This will easily be the
    most time-consuming part of the process; my first time took 3 days to get
    it working properly. Follow the guide closely, and go with the SSL
    authentication method. SSL is a lot easier to set up locally. You will need
    to install every Koji component (except koji-vmd) on the same system.
    Proceed to the next step after you've had a successful Kojira repository
    generated.
#.  At this point, you have a system that should be ready to build images. We
    just have to do some Koji configuration so that your instance is pulling
    content from Koji. Replace the base tag names with whatever fits your
    conventions.

        #.  Add tags, tag inheritance, new pkg (with pkg owner), new external repo, and regen the repo

            ::

                koji add-tag fedora22
                koji add-tag jay-fedora22
                koji add-tag jay-fedora22-override --parent jay-1-fedora22
                koji add-tag jay-fedora22-build --arches x86_64 --parent jay-fedora22-override
                koji add-tag jay-fedora22-candidate --parent jay-fedora22
                koji add-tag-inheritance --priority 40 jay-fedora22-build fedora22
                koji add-pkg --owner kojiadmin jay-fedora22 fedora-server-ec2 fedora-server-kvm
                koji add-external-repo -t fedora::20 fedora22 'https://alt.fedoraproject.org/pub/alt/releases/22/Cloud/$arch/os/'
                koji add-target jay-fedora22-candidate jay-fedora22-build
                koji regen-repo jay-fedora22-build
        #.  Grab a kickstart file from an image task in Koji that relates to what you want to test.
        #.  Finally, kick off a build!

            ::

                koji image-build fedora-server-ec2 22 --distro Fedora-22 jay-fedora22-candidate --kickstart fedora-server-starter-ec2.ks 'https://alt.fedoraproject.org/pub/alt/releases/22/Cloud/$arch/os/'


Building Appliances
===================

This section is here for the sake of legacy. Unless you are trying to build
ARM images, you should use the image-build command described in the previous
section.

.. note::
    The spin-appliance command, described herein, is deprecated.

Getting Started
---------------

Here's what you need before proceeding:

* a name for your Appliance, and a version number
* a Koji build target
* what architectures you want
* a flattened kickstart file
* you may also need yum repositories generated by release engineering if you
  require signed packages be installed into the appliance
* the appliance permission in Koji

The Koji command that creates an Appliance is called ``spin-appliance``. It
calls out to appliance-creator in a mock chroot to construct the Appliance.
To run an Appliance build, use the spin-appliance command like so:

::

    $ koji spin-appliance --release 4 fedora-workstation 23 f23-build fedora-workstation.ks

In this example an Appliance will be created with the N-V-R of
``fedora-workstation-23-4``. If ``--release`` was not included an incrementing
value would be computed by Koji instead. spin-appliance takes a minimum of 5
arguments:

Name
    the name of the image, without versioning information. Examples could be
    ``fedora`` or ``fedora-workstation``.

Version
    an arbitrary version string. For Fedora images, this usually matches the
    release version, such as 21, 22, or 23.

Build Target
    just like RPM builds Koji must be told which target to use. This controls
    what tag the image will be tagged into, and what packages are available to
    install into the image.

Architecture
    only arm, x86_64, or i386 are supported

Kickstart File
    this is a recipe file that performs drives the construction of the image.

Use ``--help`` to discover more options to spin-appliance. You can override the
Release field with ``--release``, or ``--repo`` to override what yum repo is
used to provide packages to install to the image. Note that regardless of what
repo you use, it must contain RPMs built by Koji, otherwise you will get an
error. (unless you are building a scratch image)

Mechanics
---------

The way Appliances are created in Koji is the same as :ref:`building-livemedia`.

Caveats for Appliances
----------------------

Caveats for using ``spin-appliance`` are the same as using ``spin-livemedia`` to
:ref:`caveats-for-livemedias`.

Troubleshooting
---------------

If your build fails, you will be notified on the command line. In the output
will be a URL to the Koji UI, visit that and click on the red subtask link.
From that page review ``root.log`` and ``appliance.log`` for errors. Often
errors are caused by packages being missing, or malformed kickstart files. The
log files are at the bottom of the page. If you're stuck, contact Release
Engineering.

Build System Preparation
------------------------

This section assumes you have know-how required to install and configure a new
instance of Koji, and that you have already done so. You can learn how to do so
:doc:`here <server_howto>` if you need to. Please ensure you are using the
latest version of the software and that your database schema is updated as
well. You should also have some familiarity with how `appliance-creator`_
works. This section only covers preparation for Appliance builds.

Follow this procedure step by step to get things prepared they way they need
to be.

#.  Add a builder to the appliance channel
        ::

            koji add-host-to-channel <your-host> appliance

#.  Grant the permission to build an appliance to a user. This step is optional
    since admins have all permissions.

        ::

            koji grant-permission appliance <user>

#.  You will need a tag and target to build the images from. The yum repo
    generated for the build tag of the target is what Koji will use to populate
    the Appliances with by default. (the alternative is to use the ``--repo``
    option, more on that later)

#.  Add the appliance-build group

        ::

            koji add-group <build-tag> appliance-build``

#.  Add packages to the appliance-build group. These package lists vary has
    packages and dependencies change. As of October, 2015 for Fedora 24 the
    needed packages for appliances:

        * appliance-tools, bash, coreutils, grub, parted, perl, policycoreutils,
          selinux-policy, shadow-utils, sssd-client

        ::

            koji add-group-pkg <build-tag> appliance-build <pkg> ...

.. _Koji project website: https://fedorahosted.org/koji/wiki
.. _kickstart:
    https://github.com/rhinstaller/pykickstart/blob/master/docs/kickstart-docs.rst
.. _Anaconda Kickstart Guide:
    https://github.com/rhinstaller/pykickstart/blob/master/docs/kickstart-docs.rst
.. _lorax: https://github.com/weldr/lorax
.. _livemedia-creator: https://weldr.io/lorax/livemedia-creator.html
.. _ImageFactory: http://imgfac.org/
.. _Oz: https://github.com/clalancette/oz
.. _how to use Oz: https://github.com/clalancette/oz/wiki
.. _how to use ImageFactory: http://imgfac.org/documentation/
.. _appliance-creator: https://fedoraproject.org/wiki/Features/ApplianceTools
.. Koji documentation master file, created by
   sphinx-quickstart on Tue Nov  3 12:11:18 2015.
   You can adapt this file completely to your liking, but it should at least
   contain the root `toctree` directive.

================================
Welcome to Koji's documentation!
================================

Koji
====

Koji is the software that builds `RPM packages for the Fedora project`_. It
uses `Mock`_ to create chroot environments to perform builds. To download the
source code, report bugs, join the mailing list etc., see the `Koji project
website`_.

Contents
========

.. toctree::
    :maxdepth: 2

    HOWTO
    access_controls
    permissions
    defining_hub_policies
    kerberos_gssapi_debug
    external_repo_server_bootstrap
    image_build
    winbuild
    exporting_repositories
    tag_inheritance
    misc
    release_notes/release_notes
    migrations/migrations
    CVEs/CVEs
    runs_here
    server_bootstrap
    server_howto
    signing
    database_howto
    kojid_conf
    hub_conf
    using_the_koji_build_system
    setting_rpm_macros
    profiles
    plugins
    volumes
    writing_koji_code
    content_generators
    content_generator_metadata
    configuring_jenkins
    utils
    supported_platforms

HowTos
======

Setting up and using Koji on Fedora:

* :doc:`Using the Koji build system <using_the_koji_build_system>`
* :doc:`Run Your Own Koji Build Server <server_howto>`
* :doc:`Building Images in Koji <image_build>`
* :doc:`Defining hub policies <defining_hub_policies>`

Koji Architecture
=================

Terminology
-----------
In Koji it is sometimes necessary to distinguish between a package in general,
a specific build of a package, and the various rpm files created by a build.
When precision is needed, these terms should be interpreted as follows:

Package
    The name of a source rpm. This refers to the package in general and not
    any particular build or subpackage. For example: kernel, glibc, etc.

Build
    A particular build of a package. This refers to the entire build: all
    arches and subpackages. For example: kernel-2.6.9-34.EL, glibc-2.3.4-2.19.

RPM
    A particular rpm. A specific arch and subpackage of a build. For example:
    kernel-2.6.9-34.EL.x86_64, kernel-devel-2.6.9-34.EL.s390,
    glibc-2.3.4-2.19.i686, glibc-common-2.3.4-2.19.ia64


Quick overview of architecture and history is covered by Ken's `video
<https://www.youtube.com/watch?v=B0xfsJ1G4v0>`_.

Koji Components
===============

.. figure:: koji_structure1.svg
   :scale: 50 %
   :alt: Koji component structure diagram

Koji is comprised of several components:

Koji-Hub
--------
koji-hub is the center of all Koji operations. It is an XML-RPC server running
under mod_wsgi in Apache. koji-hub is passive in that it only receives XML-RPC
calls and relies upon the build daemons and other components to initiate
communication. koji-hub is the only component that has direct access to the
database and is one of the two components that have write access to the file
system.

Kojid
-----
kojid is the build daemon that runs on each of the build machines. Its primary
responsibility is polling for incoming build requests and handling them
accordingly. Essentially kojid asks koji-hub for work. Koji also has support
for tasks other than building. Creating install images is one example. kojid
is responsible for handling these tasks as well. kojid uses mock for building.
It also creates a fresh buildroot for every build. kojid is written in Python
and communicates with koji-hub via XML-RPC.

Koji-Web
--------
koji-web is a set of scripts that run in mod_wsgi and use the Cheetah
templating engine to provide a web interface to Koji. It acts as a client to
koji-hub providing a visual interface to perform a limited amount of
administration. koji-web exposes a lot of information and also provides a means
for certain operations, such as cancelling builds.

Koji-client
-----------
koji-client is a CLI written in Python that provides many hooks into Koji. It
allows the user to query much of the data as well as perform actions such as
adding users and initiating build requests.

Kojira
------
kojira is a daemon that keeps the build root repodata updated. It is
responsible for removing redundant build roots and cleaning up after a build
request is completed.

Package Organization
====================

Tags and Targets
----------------

Koji organizes packages using tags:

* Tags are tracked in the database but not on disk
* Tags support multiple inheritance
* Each tag has its own list of valid packages (inheritable)
* Package ownership can be set per-tag (inheritable)
* Tag inheritance is more configurable
* When you build you specify a target rather than a tag

A build target specifies where a package should be built and how it should be
tagged afterwards. This allows target names to remain fixed as tags change
through releases. You can get a full list of build targets with the following
command:

::

    $ koji list-targets

You can see just a single target with the --name option:

::

    $ koji list-targets --name dist-fc7

    Name                           Buildroot                      Destination
    ---------------------------------------------------------------------------------------------
    dist-fc7                       dist-fc7-build                 dist-fc7

This tells you a build for target dist-fc7 will use a buildroot with packages
from the tag dist-fc7-build and tag the resulting packages as dist-fc7.

You can get a list of tags with the following command:

::

    $ koji list-tags

Package lists
-------------

As mentioned above, each tag has its own list of packages that may be placed in
the tag. To see that list for a tag, use the list-pkgs command:

::

    $ koji list-pkgs --tag dist-fc7

    Package                 Tag                     Extra Arches     Owner
    ----------------------- ----------------------- ---------------- ----------------
    ElectricFence           dist-fc6                                 pmachata
    GConf2                  dist-fc6                                 rstrode
    lucene                  dist-fc6                                 dbhole
    lvm2                    dist-fc6                                 lvm-team
    ImageMagick             dist-fc6                                 nmurray
    m17n-db                 dist-fc6                                 majain
    m17n-lib                dist-fc6                                 majain
    MAKEDEV                 dist-fc6                                 clumens
    [...]

The first column is the name of the package, the second tells you which tag the
package entry has been inherited from, and the third tells you the owner of the
package.

Latest Builds
-------------

To see the latest builds for a tag, use the latest-build command:

::

    $ koji latest-build --all dist-fc7

    Build                                     Tag                   Built by
    ----------------------------------------  --------------------  ----------------
    ConsoleKit-0.1.0-5.fc7                    dist-fc7              davidz
    ElectricFence-2.2.2-20.2.2                dist-fc6              jkeating
    GConf2-2.16.0-6.fc7                       dist-fc7              mclasen
    ImageMagick-6.2.8.0-3.fc6.1               dist-fc6-updates      nmurray
    MAKEDEV-3.23-1.2                          dist-fc6              nalin
    MySQL-python-1.2.1_p2-2                   dist-fc7              katzj
    NetworkManager-0.6.5-0.3.cvs20061025.fc7  dist-fc7              caillon
    ORBit2-2.14.6-1.fc7                       dist-fc7              mclasen

The output gives you not only the latest builds, but which tag they have been
inherited from and who built them (note: for builds imported from beehive the
"built by" field may be misleading).

Documentation
-------------

We've tried to make Koji self-documenting wherever possible. The command line
tool will print a list of valid commands and each command supports --help.
For example:

::

    $ koji help

    Koji commands are:
    build                Build a package from source
    cancel-task          Cancel a task
    help                 List available commands
    latest-build         Print the latest builds for a tag
    [...]

::

    $ koji build --help

    usage: koji build [options]  tag URL
    (Specify the --help global option for a list of other help options)

    options:
    -h, --help            show this help message and exit
    --skip-tag            Do not attempt to tag package
    --scratch             Perform a scratch build
    --nowait              Don't wait on build
    [...]

You can see administrator-only command help with --admin. Most users will
never use these additional commands, but if you're setting up your own Koji
system, you may find them very useful.

::

    $ koji help --admin
    Available commands:
            add-external-repo         Create an external repo and/or add one to a tag
            add-group                 Add a group to a tag
            add-group-pkg             Add a package to a group's package listing
    [...]

Koji Deployments
================

Koji is also known to be used in many places, and we :doc:`track them on this
page <runs_here>`. Feel free to add your entry. There is no additional
obligation to you for doing so. :)

Koji Contributor Guides
=======================

If you're interested in submitting patches, writing documentation, or filing
bugs this section is for you. In time this will be the best place to learn
how to get involved.

* :doc:`Getting Started as a Developer <writing_koji_code>`

Indices and tables
==================

* :ref:`genindex`
* :ref:`modindex`
* :ref:`search`

.. _Mock: https://github.com/rpm-software-management/mock/wiki
.. _RPM packages for the Fedora project: https://koji.fedoraproject.org/koji/
.. _Koji project website: https://pagure.io/koji
==============================
Koji Kerberos/GSSAPI debugging
==============================

Run basic command Koji with debug option isn't help to debug Kerberos/GSSAPI issue.
In successful call you'll see what authentication method was used and was is the
koji server connected to:

::

    $ kinit my_account@FEDORAPROJECT.ORG
    Password for my_account@FEDORAPROJECT.ORG:

    $ koji hello
    successfully connected to hub
    dobrý den, my_account!

    You are using the hub at https://koji.fedoraproject.org/kojihub
    Authenticated via GSSAPI

...or you can get some error:

::

    $ koji hello
    2021-04-23 09:45:22,732 [ERROR] koji: (gssapi auth failed: requests.exceptions.HTTPError: 401 Client Error: Unauthorized for url: https://koji.fedoraproject.org/kojihub/ssllogin)
    Use following documentation to debug kerberos/gssapi auth issues. https://docs.pagure.org/koji/kerberos_gssapi_debug/
    2021-04-23 09:45:22,734 [ERROR] koji: GSSAPIAuthError: unable to obtain a session (gssapi auth failed: requests.exceptions.HTTPError: 401 Client Error: Unauthorized for url: https://koji.fedoraproject.org/kojihub/ssllogin)
    Use following documentation to debug kerberos/gssapi auth issues. https://docs.pagure.org/koji/kerberos_gssapi_debug/

In this case we see not one but two errors while trying to authenticate. Let's
see what more info we can get:

::

    $ KRB5_TRACE=/dev/stdout python koji -d hello
    2021-04-23 09:47:58,730 [DEBUG] koji: Opening new requests session
    2021-04-23 09:47:58,731 [DEBUG] koji: Opening new requests session
    2021-04-23 09:47:59,446 [DEBUG] koji: Opening new requests session
    2021-04-23 09:47:59,446 [ERROR] koji: (gssapi auth failed: requests.exceptions.HTTPError: 401 Client Error: Unauthorized for url: https://koji.fedoraproject.org/kojihub/ssllogin)
    Use following documentation to debug kerberos/gssapi auth issues. https://docs.pagure.org/koji/kerberos_gssapi_debug/
    Traceback (most recent call last):
      File "/usr/bin/koji", line 335, in <module>
        rv = locals()[command].__call__(options, session, args)
      File "/home/tkopecek/cvs/koji/cli/koji_cli/commands.py", line 7496, in handle_moshimoshi
        activate_session(session, options)
      File "/home/tkopecek/cvs/koji/cli/koji_cli/lib.py", line 749, in activate_session
        session.gssapi_login(proxyuser=runas)
      File "/home/tkopecek/cvs/koji/koji/__init__.py", line 2513, in gssapi_login
        raise GSSAPIAuthError(err)
    koji.GSSAPIAuthError: unable to obtain a session (gssapi auth failed: requests.exceptions.HTTPError: 401 Client Error: Unauthorized for url: https://koji.fedoraproject.org/kojihub/ssllogin)
    Use following documentation to debug kerberos/gssapi auth issues. https://docs.pagure.org/koji/kerberos_gssapi_debug/

We see no output from krbV library. It indicates that there was no significant
interaction with the library. Main reason would be missing ``kinit``. Fixing
this problem is pretty easy. What about other problems?

You can see wrong URL for the hub. Check if it is coming from system-wide config
``/etc/koji.conf`` or anything in ``/etc/koji.conf.d`` or ``~/.koji/config``.
Maybe URL for your instance has changed or some old config was forgotten in
place.

Compare with successful output when krbV library does its work:

::

    $ KRB5_TRACE=/dev/stdout koji -d hello
    2021-04-23 09:50:34,442 [DEBUG] koji: Opening new requests session
    2021-04-23 09:50:34,442 [DEBUG] koji: Opening new requests session
    [36315] 1619164236.523876: TXT record _kerberos.koji.fedoraproject.org. not found
    [36315] 1619164236.523877: TXT record _kerberos.fedoraproject.org. found: FEDORAPROJECT.ORG
    [36315] 1619164236.523878: ccselect module realm chose cache FILE:/tmp/krb5cc_1000 with client principal my_account@FEDORAPROJECT.ORG for server principal HTTP/koji.fedoraproject.org@FEDORAPROJECT.ORG
    [36315] 1619164236.523879: Getting credentials my_account@FEDORAPROJECT.ORG -> HTTP/koji.fedoraproject.org@ using ccache FILE:/tmp/krb5cc_1000
    [36315] 1619164236.523880: Retrieving my_account@FEDORAPROJECT.ORG -> HTTP/koji.fedoraproject.org@ from FILE:/tmp/krb5cc_1000 with result: -1765328243/Matching credential not found (filename: /tmp/krb5cc_1000)
    [36315] 1619164236.523881: Retrying my_account@FEDORAPROJECT.ORG -> HTTP/koji.fedoraproject.org@FEDORAPROJECT.ORG with result: -1765328243/Matching credential not found (filename: /tmp/krb5cc_1000)
    [36315] 1619164236.523882: Server has referral realm; starting with HTTP/koji.fedoraproject.org@FEDORAPROJECT.ORG
    [36315] 1619164236.523883: Retrieving my_account@FEDORAPROJECT.ORG -> krbtgt/FEDORAPROJECT.ORG@FEDORAPROJECT.ORG from FILE:/tmp/krb5cc_1000 with result: 0/Success
    [36315] 1619164236.523884: Starting with TGT for client realm: my_account@FEDORAPROJECT.ORG -> krbtgt/FEDORAPROJECT.ORG@FEDORAPROJECT.ORG
    [36315] 1619164236.523885: Requesting tickets for HTTP/koji.fedoraproject.org@FEDORAPROJECT.ORG, referrals on
    [36315] 1619164236.523886: Generated subkey for TGS request: aes256-cts/A108
    [36315] 1619164236.523887: etypes requested in TGS request: aes256-cts, aes128-cts, aes256-sha2, aes128-sha2, rc4-hmac, camellia128-cts, camellia256-cts
    [36315] 1619164236.523889: Encoding request body and padata into FAST request
    [36315] 1619164236.523890: Sending request (977 bytes) to FEDORAPROJECT.ORG
    [36315] 1619164236.523891: Sending DNS URI query for _kerberos.FEDORAPROJECT.ORG.
    [36315] 1619164236.523892: URI answer: 10 1 "krb5srv:m:kkdcp:https://id.fedoraproject.org/KdcProxy/"
    [36315] 1619164236.523893: Resolving hostname id.fedoraproject.org
    [36315] 1619164236.523894: TLS certificate name matched "id.fedoraproject.org"
    [36315] 1619164236.523895: Sending HTTPS request to https 152.19.134.142:443
    [36315] 1619164237.522076: Received answer (929 bytes) from https 152.19.134.142:443
    [36315] 1619164237.522077: Terminating TCP connection to https 152.19.134.142:443
    [36315] 1619164237.522078: Response was from master KDC
    [36315] 1619164237.522079: Decoding FAST response
    [36315] 1619164237.522080: FAST reply key: aes256-cts/F6DC
    [36315] 1619164237.522081: TGS reply is for my_account@FEDORAPROJECT.ORG -> HTTP/koji.fedoraproject.org@FEDORAPROJECT.ORG with session key aes256-cts/A3FF
    [36315] 1619164237.522082: TGS request result: 0/Success
    [36315] 1619164237.522083: Received creds for desired service HTTP/koji.fedoraproject.org@FEDORAPROJECT.ORG
    [36315] 1619164237.522084: Storing my_account@FEDORAPROJECT.ORG -> HTTP/koji.fedoraproject.org@ in FILE:/tmp/krb5cc_1000
    [36315] 1619164237.522086: Creating authenticator for my_account@FEDORAPROJECT.ORG -> HTTP/koji.fedoraproject.org@, seqnum 742219461, subkey aes256-cts/1CC1, session key aes256-cts/A3FF
    successfully connected to hub
    dobrý den, my_account!

    You are using the hub at https://koji.fedoraproject.org/kojihub
    Authenticated via GSSAPI


Most problems could be tracked down by this command plus kinit/klist tools. for
list of these consult next section.

General problems
================
* *No KRB5_TRACE output* - You've not run ``kinit`` in first place, run ``kinit``.
* If yes, maybe ticket is no longer valid - check ``klist`` + run ``kinit``
* *Hub URL is wrong* - check configs, find out right URL for your environment,
  update related packages (e.g. ``fedora-packager``)
* *Wrong service ticket* - e.g. because your instance is hidden behind a proxy.
  In such a case, you'll see a wrong principal in the output, such as e.g.
  ``HTTP/proxy10.fedoraproject.org@FEDORAPROJECT.ORG``. Kerberos
  authentication will fail because ``krbV`` will try to fetch a service ticket for
  PTR instead of a DNS record, effectively asking for a wrong service.
  The correct form is ``HTTP/koji.fedoraproject.org@FEDORAPROJECT.ORG``
  (also listed as "Ticket Server" in klist output). This problem is usually
  caused by a wrong value of ``dns_canonicalize_hostname`` in ``/etc/krb5.conf``.
  Please try setting it to ``true``, ``fallback`` and ``false`` in turn,
  as different values may be required depending on your situation.
* *You can't get service ticket at all*. You've not set up the ``/etc/krb5.conf``
  for relevant KDC/REALM. It shouldn't happen if you were able to ``kinit`` with
  the correct credentials (It means that you've already set up something).
  Anyway, you'll see following in the debug output
* *Your user account was disabled* This error is not krbV specific. But you can
  hit it. In such case you will see simple message ``koji: AuthError: unable to
  obtain a session``. From security reasons we don't display it differently from
  non-existent account. If you've suspicion that it could be the reason you need
  to check with your koji instance admin.
kojid.conf Options
------------------
kojid.conf is standard .ini-like configuration file. Its main section is
called ``[kojid]`` and contains following options. They are split by function
to section but can occur anywhere in ``[kojid]`` section in config file.

General
^^^^^^^
.. glossary::
   chroot_tmpdir=/chroot_tmpdir
      The temporary directory in buildroot. It is advised to not use ``/tmp``
      as it is bound in mock and its content would change in random moments
      during the build process.

   keepalive=True
      noop - it is still allowed in config file for backward compatibility.

   log_level=None
      Set logging level to one of the standard level names in Python's logging
      module. Valid values are: CRITICAL, ERROR, WARNING, INFO, and DEBUG.
      The default log level is WARNING.

   maxjobs=10
      The maximum number of jobs that kojid will handle at a time. This
      serves as a backup to the capacity check and prevents a tremendous
      number of low weight jobs from piling up.

   buildroot_basic_cleanup_delay=120
      Time after which successfully finished task's buildroot is deleted (2
      minutes in seconds). Some logs and directories are left in place until
      buildroot_final_cleanup_delay

   buildroot_final_cleanup_delay=86400
      Time after which buildroot (pre-cleand after
      ``buildroot_basic_cleanup_delay``) is deleted completely. (1 day in
      seconds)

   max_retries=120
      Set the maximum number of times that an individual hub call can be
      retried.

   minspace=8192
      The minimum amount of free space (in MBs) required for each build root.
      If this amount of space is not available, no new jobs will be taken.

   no_ssl_verify=False
      Turn off SSL verification for https calls. It is strongly advised to
      not turn off this verification.

   offline_retry=True
      The hub returns a special error code when it is placed in offline
      mode or when the database is unavailable. This setting controls
      whether calls should be retried in these cases.

   offline_retry_interval=120
      Controls the wait time for retrying hub calls when the hub reports
      as offline. Such calls are only retried if the offline_retry setting
      is set to True. The value is in seconds.

   pkgurl=None
      Obsoleted, use ``topurl`` instead.

   plugin=''
   plugins=''
      Space-separated list of builder plugins which should be enabled. By
      default no plugins are used. For list of available plugins check
      :doc:`this page <plugins>`. Both spellings plugin/plugins are allowed
      in config, but don't mix them as order is not binding.

   pluginpath=/usr/lib/koji-builder-plugins
      Colon-separated list of directories to check for builder plugins.
      They are not used by default, use ``plugins`` to enable them.

   retry_interval=60
      If there is an unsuccessful call to hub, this is how many seconds to
      wait before trying new call.

   server=http://hub.example.com/kojihub
      The URL for the koji xmlrpc server.

   sleeptime=15
      The number of seconds to sleep between checking for new tasks.

   topurl=http://hub.example.com/kojifiles
      The URL where the main Koji volume can be accessed. The builder uses
      this url for most file access.

   topdir=/mnt/koji
      The location where the main Koji volume is mounted. This mount is
      mainly used during createrepo tasks, and should be read-only.

   use_fast_upload=True
      Enables faster uploading (bypassing XMLRPC overhead). Changing it makes
      sense only in weird combination of very old hub and newer builders.

   workdir=/tmp/koji
      The directory root for temporary storage on builder.

Building
^^^^^^^^
.. glossary::
   allowed_scms=scm.example.com:/cvs/example git.example.org:/example svn.example.org:/users/\*:no
      Controls which source control systems the builder will accept. It is a
      space-separated list of entries in one of the following forms:

      .. code::

          hostname:path[:use_common[:source_cmd]]
          !hostname:path


      Incorrectly-formatted tuples will be ignored.

      If ``use_common`` is not present, kojid will attempt to checkout a ``common/``
      directory from the repository.  If ``use_common`` is set to ``no``, ``off``, ``false``, or ``0``,
      it will not attempt to checkout a ``common/`` directory.

      ``source_cmd`` is a shell command (args separated with commas instead of spaces)
      to run before building the srpm. It is generally used to retrieve source
      files from a remote location.  If no ``source_cmd`` is specified, ``make sources``
      is run by default.

      The second form (``!hostname:path``) is used to explicitly block a host:path
      pattern. In particular, it provides the option to block specific subtrees of
      a host, but allow from it otherwise. This explicit block syntax was added in
      version 1.13.0.


   build_arch_can_fail=False
      If set to ``True``, failing subtask will not automatically cancel other siblings.

   createrepo_skip_stat=True
      If set to ``True``, append ``--skip-stat`` to all createrepo commands.

   createrepo_update=True
      Recycle old repodata (if they exist) in createrepo.

   failed_buildroot_lifetime=14400
      Failed tasks leave buildroot content on disk for debugging purposes.
      They are removed after 4 hours by default. This value is specified
      in seconds.

   literal_task_arches=''
      Space-separated list of globs (``fnmatch``) for architectures which
      will not be converted to canonical archs when choosing builder.

   log_timestamps=False
      If set to ``True`` additional logs with timestamps will get created and
      uploaded to hub. It could be useful for debugging purposes, but creates
      twice as many log files.

   maven_repo_ignore='\*.md5 \*.sha1 maven-metadata\*.xml _maven.repositories resolver-status.properties \*.lastUpdated'
      Space-separated globs of repo files which should be ignored when
      gathering maven result artifacts.

   oz_install_timeout=7200
      Install timeout in seconds for image build. Default value is 0, which
      means using the number in ``/etc/oz/oz.cfg``. Supported since oz-0.16.0.

   use_createrepo_c=False
      Use ``createrepo_c`` rather than ``createrepo`` command. There is
      generally no reason to not use createrepo_c in modern depolyments. It
      is disabled by default only to be backward-compatible. This default
      would change in future.

   task_avail_delay=300
      [Added in 1.17.0]

      This delay works around a deficiency in task scheduling. The default
      delay is 300 seconds. It is unlikely that admins will need to adjust
      this setting.

      Despite the name, this does not introduce any new delay compared to the
      old behavior. The setting controls how long a host will wait before
      taking a task in a given channel-arch “bin” when that host has an
      available capacity lower than the median for that bin. Previously, such
      hosts could wait forever.

   timeout=None
      This value is used for waiting on all xmlrpc calls to hub. By default
      there is no timeout set.

   xz_options=-z6T0
      Image builds with ``raw-xz`` type will use this setting when compressing
      the image. Default value is compromise between speed and resource usage.
      Only one option (not space-separated) is allowed here for now.

RPM Builds
^^^^^^^^^^
.. glossary::
   distribution=Koji
      The distribution to use in rpm headers. Value is propagated via macros
      to rpmbuild.

   packager=Koji
      The packager to use in rpm headers. Value is propagated via macros to
      rpmbuild.

   support_rpm_source_layout=True
      Originally, when building an SRPM from source control, Koji expected
      the contents to be flattened (e.g. the spec and sources files directly
      in the checkout directory). When this option is enabled (the default),
      Koji will also accept these contents in separate ``SPECS`` and
      ``SOURCES`` directories.

   vendor=Koji
      The vendor to use in rpm headers. Value is propagated via macros to
      rpmbuild.

Mock
^^^^
.. glossary::
   mockdir=/var/lib/mock
      The directory root for mock.

   mockhost=koji-linux-gnu
      The _host string to use in mock.

   mockuser=kojibuilder
      The user to run as when performing builds. Note, that user must exist on
      the build host and must have permission to use mock.

   rpmbuild_timeout=86400
      Timeout for build duration (24 hours). Propagated to mock, not
      controlled by koji directly.

   yum_proxy=None
      Address of proxy server which will be passed via mock to yum.

Notifications
^^^^^^^^^^^^^
.. glossary::
   admin_emails=''
      Space-separated list of addresses for sending logs.

   from_addr=Koji Build System <buildsys@example.com>
      The From address used when sending email notifications.

   smtphost=example.com
      The mail host to use for sending email notifications.

Kerberos Authentication
^^^^^^^^^^^^^^^^^^^^^^^
.. glossary::
   ccache=/var/tmp/kojid.ccache
      Credentials cache used for krbV login.

   host_principal_format=compile/\%s\@EXAMPLE.COM
      The format of the principal used by the build hosts.
      The %s will be replaced by the FQDN of the host.

   keytab=/etc/kojid/kojid.keytab
      Location of the keytab.


SSL Authentication
^^^^^^^^^^^^^^^^^^
.. glossary::
   ca=''
      noop, obsoleted, will be removed soon.

   cert=/etc/kojid/client.crt
      Client certificate.

   serverca=/etc/kojid/serverca.crt
      This specifies the CA (or CA bundle) that the builder should use to
      verify the ssl connection to the hub. If the default value of
      ``/etc/kojid/serverca.crt`` exists, then that file is used.
      Otherwise the default system bundle is used.


Insecure Authentication Options
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

These options are only intended for simple development environments
and should never be used in production.
Please use Kerberos or SSL authentication instead.

.. glossary::
   user=None
       Username for authentication

   password=None
       Clear-text password (I've told you.)
Migrating to Koji 1.10
======================

.. reStructured Text formatted

The 1.10 release of Koji includes a few changes that you should consider when
migrating.

DB Updates
----------

The new ``tag_extra`` table tracks extra data for tags.

There is a new entry in the ``channels`` table and some additions and updates to
the ``archivetypes`` table.

As in previous releases, we provide a migration script that updates the
database.

::

    # psql koji koji  </usr/share/doc/koji-1.10.0/docs/schema-upgrade-1.9-1.10.sql


Command line changes
--------------------

A few commands support new arguments

``maven-build``
    * ``--ini``     : Pass build parameters via a .ini file
    * ``--section`` : Get build parameters from this section of the .ini

``wrapper-rpm``
    * ``--ini``     : Pass build parameters via a .ini file
    * ``--section`` : Get build parameters from this section of the .ini

``import``
    * ``--link``    : Attempt to hardlink instead of uploading

``list-tagged``
    * ``--latest-n``: Only show the latest N builds/rpms

``list-history``
    * ``--watch``   : Monitor history data

``edit-tag``
    * ``--extra``   : Set tag extra option

``list-tasks``
    * ``--user``    : Only tasks for this user
    * ``--arch``    : Only tasks for this architecture
    * ``--method``  : Only tasks of this method
    * ``--channel`` : Only tasks in this channel
    * ``--host``    : Only tasks for this host

``download-build``
    * ``--task-id`` : Interpret id as a task id

And there are three new commands

* ``image-build-indirection``
* ``maven-chain``
* ``runroot``


Other Configuration changes
---------------------------

The Koji web interface can now treat ``extra-footer.html`` as a Cheetah
template. This behavior can be enabled by setting the ``LiteralFooter`` option
to ``False`` in the kojiweb config.


RPC API Changes
---------------

The ``readTaggedBuilds`` and ``readTaggedRPMS`` now treat an integer value for
the optional latest argument differently. Before it was simply treated as a
boolean flag, which if true caused the call to return only the latest build for
each package. Now, if the value is a positive integer N, it will return the N
latest builds for each package. The behavior is unchanged for other values.

New rpc calls: ``chainMaven``, ``buildImageIndirection``, and ``mergeScratch``
Migrating to Koji 1.11
======================

.. reStructured Text formatted

The 1.11 release of Koji includes a several changes that you should consider when
migrating.

DB Updates
----------

There are a number of new tables and columns to support content generators. Here is a summary:
    * The ``btype`` table tracks the known btypes [LINK] in the system
    * The ``build_types`` table links builds to their btype(s)
    * The ``content_generator`` table tracks the known content generators in the system
    * The ``cg_users`` table tracks which users have access to which content generators
    * The ``buildroot`` table now tracks more generic buildroots
    * The ``standard_buildroot`` table tracks data for "normal" koji buildroots
    * Several tables now have an ``extra`` column that stores json data
    * There are several new entries in the ``archivetypes`` table
    * The ``image_listing`` table has been replace by the more general ``archive_rpm_components`` table
    * The new ``archive_components`` complements this and tracks non-rpm components

As in previous releases, we provide a migration script that updates the
database.

::

    # psql koji koji  </usr/share/doc/koji-1.11.0/docs/schema-upgrade-1.10-1.11.sql

Note: prior to this release, we had some interim update scripts:
    * schema-update-cgen.sql
    * schema-update-cgen2.sql

Most users should not need these scripts. The new schema upgrade script includes
those changes.


Command line changes
--------------------

The ``help`` command now shows a categorized list of commands.

The ``hello`` command now reports the authentication type.

Several commands support new arguments. Here are the notable changes:

``add-tag``
    * ``--extra``       : Set an extra option for tag at creation

``watch-task``
    * Supports several new task selection options

``download-build``
    * ``--rpm``         : Used to download a particular rpm by name

``runroot``
    * ``--new-chroot``  : Run command with the --new-chroot (systemd-nspawn) option to mock


And there are five new commands

* ``assign-task``
* ``import-cg``
* ``grant-cg-access``
* ``revoke-cg-access``
* ``spin-livemedia``


Client configuration options
----------------------------

The command line and several other tools support the following new configuration options:
    * ``use_old_ssl``   : Use the old ssl code instead of python-requests
    * ``no_ssl_verify``   : Disable certificate verification for https connections
    * ``upload_blocksize`` : Override the blocksize for uploads

The ``ca`` option is deprecated and no longer required for ssl authentication (``serverca`` is still required).

Even if not using ssl authentication, the ``serverca`` option, if specified, is used to verify the certificate of the
server.


Other Configuration changes
---------------------------

The Koji web interface supports the following new configuration options:
    * ``KrbRDNS``       : Use the fqdn of the server when authenticating via kerberos
    * ``LoginDisabled`` : Hide the login link at the top of the page


RPC API Changes
---------------

New rpc calls:
    * ``CGImport``      : Used by content generators
    * ``getBuildType``  : Returns typeinfo for a build
    * ``listBTypes``    : List the known btypes for the system
    * ``addBType``      : Adds a new btype
    * ``grantCGAccess`` : Grants a user content generator access
    * ``revokeCGAccess`` : Revokes content generator access

Changes to calls
    * Several information calls now return additional fields
    * ``getRPMDeps`` returns optional deps
    * ``listTasks`` supports new selection options
    * ``getLoggedInUser`` includes an authtype field
Migrating to Koji 1.12
======================

..
  reStructured Text formatted

The 1.12 release of Koji includes a several changes that you should consider when
migrating.

DB Updates
----------

There is a minor update to support the dist-repos feature:
    * The ``repo`` table now has a ``dist`` column

Additionally, the schema explicitly adds the ``image`` permission to the
permissions table, correcting an old oversight.

As in previous releases, we provide a migration script that updates the
database.

::

    # psql koji koji  </usr/share/doc/koji/docs/schema-upgrade-1.11-1.12.sql


Command line changes
--------------------

The ``import-sig`` command now now supports a ``--write`` option to immediately
write out a signed copy.

The ``write-signed-rpm`` command previously (and confusingly) only accepted
nvrs as arguments (i.e. builds not rpms). Now it accepts either nvras or nvrs
(rpms or builds).

The ``clone-tag`` command has been refactored. It supports many more options
and should execute much faster than before.

The new ``dist-repo`` command creates rpm repos suitable for distribution.

The new ``save-failed-tree`` command allows the a task owner (or admin)
to download information from the buildroot of a failed build. This feature
requires the ``save_failed_tree`` plugin to be enabled on the hub and builders.


Configuration
-------------

The only configuration changes are for the ``save-failed-tree`` plugins (hub
and builder). Each has its own configuration file. See :doc:`../plugins`

The hub accepts a new ``CheckClientIP`` option (default True) to indicate
whether authentication credentials should be tied to the client's IP address.
(For some proxy setups, this may need to be set to False).


RPC API Changes
---------------

New rpc calls:

``listPackagesSimple``
    handles a limited subset of the
    functionality provided by the ``listPackages`` call

``distRepo``
    triggers generation of a distribution repo

Changes to calls:
    * repo related calls (e.g. ``repoInfo`` now include a boolean ``dist``
      field
    * the ``editTag2`` call can now remove tag_extra data if the
      ``remove_extra`` keyword argument is used
    * the listTaskOutput call supports a new ``all_volumes`` keyword argument.
      If true, the results are extended to deal with files in same relative paths
      on different volumes.
    * the ``getTaskResult`` call takes an optional boolean ``raise_fault``
      argument
    * the ``taskWaitResults`` call takes an optional ``canfail`` argument
      to indicate subtasks which can fail without raising an exception
Migrating to Koji 1.13
======================

..
  reStructured Text formatted

The 1.13 release of Koji includes several changes that you should consider when
migrating.

DB Updates
----------

We have increased the length limit for tag names and there is a minor schema
change to support this.

As in previous releases, we provide a migration script that updates the
database.

::

    # psql koji koji  </usr/share/doc/koji/docs/schema-upgrade-1.12-1.13.sql


Packaging changes
-----------------

Because the CLI and base library now support both python2 and python3, the core
libs and most of the cli code have moved to separate packages for each major
Python version:

    * python2-koji
    * python3-koji

The main koji package still contains the (now much smaller) koji script, and
requires either python2-koji or python3-koji, depending on whether python3
support is enabled.

The CLI now also supports plugins, and two commands (runroot and
save-failed-tree) have moved to the `python[23]-koji-cli-plugins`
subpackages. If you need these subcommands, you may need to explicitly install
the appropriate koji-cli-plugins package.


Other changes
-------------

There are numerous other changes in 1.13 that should not have a direct impact
on migration. For details see:
:doc:`../release_notes/release_notes_1.13`
Migrating to Koji 1.14
======================

..
  reStructured Text formatted

You should consider the following changes when migrating to 1.14:

DB Updates
----------

The schema updates this time are minor

   * dropped unused ``log_messages`` table
   * new standard entries in the ``archivetypes`` table

As in previous releases, we provide a migration script that updates the
database.

::

    # psql koji koji  </usr/share/doc/koji/docs/schema-upgrade-1.13-1.14.sql


Dropped mod_python support
--------------------------

Koji's support for mod_python has been deprecated for many years. If you are
still relying on mod_python, you will need to switch to mod_wsgi.

See: :doc:`migrating_to_1.7`


Other changes
-------------

There are numerous other changes in 1.14 that should not have a direct impact
on migration. For details see:
:doc:`../release_notes/release_notes_1.14`
Migrating to Koji 1.15
======================

..
  reStructured Text formatted

The update from to 1.15 is comparatively light from a migration perspective.

DB Updates
----------

There are no schema updates in 1.15.


Other changes
-------------

There are numerous other changes in 1.15 that should not have a direct impact
on migration. For details see:
:doc:`../release_notes/release_notes_1.15`
Migrating to Koji 1.16
======================

..
  reStructured Text formatted

You should consider the following changes when migrating to 1.16:

DB Updates
----------

This release has schema changes to support tracking history for hosts.

    * new table: ``host_config``
    * some fields from the ``host`` table have moved to ``host_config``
    * the ``host_channels`` table now has versioning data like the other
      versioned tables

As in previous releases, we provide a migration script that updates the
database.

::

    # psql koji koji  </usr/share/doc/koji/docs/schema-upgrade-1.15-1.16.sql


Other changes
-------------

There are numerous other changes in 1.16 that should not have a direct impact
on migration. For details see:
:doc:`../release_notes/release_notes_1.16`
Migrating to Koji 1.17
======================

..
  reStructured Text formatted

You should consider the following changes when migrating to 1.17:

DB Updates
----------

This release some minor schema changes

    * the ``tag_external_repos`` table has a new ``merge_mode`` column
    * the ``build_target.name`` column is now a TEXT field rather than VARCHAR

As in previous releases, we provide a migration script that updates the
database.

::

    # psql koji koji  </usr/share/doc/koji/docs/schema-upgrade-1.16-1.17.sql


Other changes
-------------

There are numerous other changes in 1.17 that should not have a direct impact
on migration. For details see:
:doc:`../release_notes/release_notes_1.17`
Migrating to Koji 1.18
======================

..
  reStructured Text formatted

You should consider the following changes when migrating to 1.18:

DB Updates
----------

This release has a few schema changes:

    * Several new indexes to speed operations
    * A ``cg_id`` field has been added to the ``build`` table
    * A new ``build_reservations`` table
    * A new ``build_notifications_block`` table
    * Updates to the data in the ``archivetypes`` table

As in previous releases, we provide a migration script that updates the
database.

::

    # psql koji koji  </usr/share/doc/koji/docs/schema-upgrade-1.17-1.18.sql


More granular permissions
-------------------------

The new ``host``, ``tag``, ``target`` permissions allow access to a number of
actions that were previously admin-only.
Koji administrators should consider using these to reduce the number of users
with full ``admin`` permission.


Other changes
-------------

There are numerous other changes in 1.18 that should not have a direct impact
on migration. For details see:
:doc:`../release_notes/release_notes_1.18`
Migrating to Koji 1.19
======================

..
  reStructured Text formatted

You should consider the following changes when migrating to 1.19:

DB Updates
----------

This release has a few schema changes:

    * A new ``tag_package_owners`` table
    * A new ``user_krb_principals`` table
    * Updates to the data in the ``archivetypes`` table
    * Updates to the data in the ``permissions`` table
    * The ``content_generator`` table now enforces unique strings in the ``names`` field

As in previous releases, we provide a migration script that updates the
database.

::

    # psql koji koji  </usr/share/doc/koji/docs/schema-upgrade-1.18-1.19.sql


Other changes
-------------

There are numerous other changes in 1.19 that should not have a direct impact
on migration. For details see:
:doc:`../release_notes/release_notes_1.19`
Migrating to Koji 1.20
======================

You should consider the following changes when migrating to 1.20:

DB Updates
----------

There are no big database schema changes in this release. There is only cleanup
of potentially not `dropped old constraint <https://pagure.io/koji/issue/1789>`_.

As in previous releases, we provide a migration script that updates the
database.

::

    # psql koji koji < /usr/share/doc/koji/docs/schema-upgrade-1.19-1.20.sql


Other changes
-------------

There are numerous other changes in 1.20 that should not have a direct impact on
migration. For details see: :doc:`../release_notes/release_notes_1.20`
Migrating to Koji 1.21
======================

You should consider the following changes when migrating to 1.21:

DB Updates
----------

There are no big database schema changes in this release.

We've updated table with events and ``get_event()`` function to better handle
timeline (`PR#2068 <https://pagure.io/koji/pull-request/2068>`_) and set default
merge mode for external repos (`PR#2051 <https://pagure.io/koji/pull-request/2051>`_)

As in previous releases, we provide a migration script that updates the
database.

::

    # psql koji koji < /usr/share/doc/koji/docs/schema-upgrade-1.20-1.21.sql


Other changes
-------------

There are numerous other changes in 1.21 that should not have a direct impact on
migration. For details see: :doc:`../release_notes/release_notes_1.21`
Migrating to Koji 1.22
======================

You should consider the following changes when migrating to 1.22:

DB Updates
----------

There are two minor schema changes in this release.

* we've updated all ``timestamp`` fields to ``timestamptz`` with the return value of
  ``get_event_time()`` function to avoid unexpected time offset caused by PostgreSQL timezone
  setting (`PR#2237 <https://pagure.io/koji/pull-request/2237>`_).
* we've updated the ``sessions_active_and_recent`` index for the ``session`` table for better
  performance (`PR#2334 <https://pagure.io/koji/pull-request/2334>`_)

As in previous releases, we provide a migration script that updates the database.

::

    # psql koji koji < /usr/share/doc/koji/docs/schema-upgrade-1.21-1.22.sql


Dropped python2 support of hub and web
--------------------------------------

Python 2 was `sunset <https://www.python.org/doc/sunset-python-2/>`_ on January 1, 2020.
Starting with Koji 1.22, hub and web will only support Python 3.
The CLI, builder, and utils retain Python 2 support for now.
For more information see: `PR#2218 <https://pagure.io/koji/pull-request/2218>`_

.. _migration_krbv:

Dropped krbV authentication support
-----------------------------------

We have dropped all the code related to the old python-krbV library, and are now only
providing GSSAPI auth.
For ``ClientSession`` objects, ``krb_login()`` is redirected to
``gssapi_login()`` with a printed warning.
Any code still calling ``krb_login()`` directly should be updated.

The newer gssapi authentication mechanism requires either ``python-requests-kerberos`` or
``python-requests-gssapi``.

For more information see: `PR#2244 <https://pagure.io/koji/pull-request/2244>`_ and
`PR#2280 <https://pagure.io/koji/pull-request/2280>`_

As part of this, the ``krbservice`` and ``krb_rdns`` options have been dropped.
These options were accepted in several configuration files and also as command
line options (``--krbservice`` and ``--krb-rdns``) in the cli and some utility
scripts.
In the Web UI configuration (``web.conf``), these options were named
``KrbService`` and ``KrbRDNS``.
Users and admins should remove these options from their configuration.

These options will cause an error if given on the command line.
They will also cause an error if used in the following configuration files:

* kojid.conf
* kojira.conf


Other changes
-------------

There are numerous other changes in 1.22 that should not have a direct impact on migration. For
details see: :doc:`../release_notes/release_notes_1.22`
Migrating to Koji 1.23
======================

You should consider the following changes when migrating to 1.23:

DB Updates
----------

This release includes some minor schema changes.

We've dropped the ``NOT NULL`` restriction on the ``value`` column of the
``tag_extra`` table as part of the changes to allow blocking these values in
the inheritance (see `PR#2495 <https://pagure.io/koji/pull-request/2495>`_).

We've also added a new ``proton_queue`` table that is used by the ``protonmsg``
plugin when it is configured to queue messages
(see `PR#2441 <https://pagure.io/koji/pull-request/2441>`_)

Lastly, we've added a new index for the ``task`` table to improve performance
(see `PR#2419 <https://pagure.io/koji/pull-request/2419>`_)

As in previous releases, we provide a migration script that updates the database.

::

    # psql koji koji < /usr/share/doc/koji/docs/schema-upgrade-1.22-1.23.sql


Other changes
-------------

There are numerous other changes in 1.23 that should not have a direct impact on migration. For
details see: :doc:`../release_notes/release_notes_1.23`
Migrating to Koji 1.24
======================

You should consider the following changes when migrating to 1.24:

DB Updates
----------

This release includes one minor schema change.

As we now can have architectures defined for individual external repos, we need
to reflect it in db. (see `PR#2564
<https://pagure.io/koji/pull-request/2564>`_).

As in previous releases, we provide a migration script that updates the database.

::

    # psql koji koji < /usr/share/doc/koji/docs/schema-upgrade-1.23-1.24.sql


Other changes
-------------

There are numerous other changes in 1.24 that should not have a direct impact on migration. For
details see: :doc:`../release_notes/release_notes_1.24`
Migrating to Koji 1.25
======================

You should consider the following changes when migrating to 1.25:

DB Updates
----------

This release includes one schema change.

Each repo now contains a link to the task which was used to create it (`Issue #888
<https://pagure.io/koji/issue/888>`_)


As in previous releases, we provide a migration script that updates the database.

::

    # psql koji koji < /usr/share/doc/koji/docs/schema-upgrade-1.24-1.25.sql


Other changes
-------------

There are numerous other changes in 1.25 that should not have a direct impact on migration. For
details see: :doc:`../release_notes/release_notes_1.25`
Migrating to Koji 1.26
======================

You should consider the following changes when migrating to 1.26:

DB Updates
----------

This release includes one schema change.

Channels now can have description and can be enabled/disabled. (Issues `#1711
<https://pagure.io/koji/issue/1711>`_, `#1849 <https://pagure.io/koji/issue/1849>`_
`#1851 <https://pagure.io/koji/issue/1851>`_)


As in previous releases, we provide a migration script that updates the database.

::

    # psql koji koji < /usr/share/doc/koji/docs/schema-upgrade-1.25-1.26.sql


Other changes
-------------

There are numerous other changes in 1.26 that should not have a direct impact on migration. For
details see: :doc:`../release_notes/release_notes_1.26`
Migrating to Koji 1.27
======================

You should consider the following changes when migrating to 1.27:

DB Updates
----------

This release doesn't include any schema change.

Other changes
-------------

There are numerous other changes in 1.27 that should not have a direct impact on migration. For
details see: :doc:`../release_notes/release_notes_1.27`
Migrating to Koji 1.28
======================

You should consider the following changes when migrating to 1.28:

DB Updates
----------

There is a simple schema change adding descriptions to individual permissions.

As in previous releases, we provide a migration script that updates the database.

::

    # psql koji koji < /usr/share/doc/koji/docs/schema-upgrade-1.27-1.28.sql


Other changes
-------------

There are numerous other changes in 1.28 that should not have a direct impact on migration. For
details see: :doc:`../release_notes/release_notes_1.28`
Migrating to Koji 1.29
======================

You should consider the following changes when migrating to 1.29:

DB Updates
----------

There is no schema change this time.

As in previous releases, we provide a migration script which is no-op in this case.

Other changes
-------------

There are numerous other changes in 1.29 that should not have a direct impact on migration. For
details see: :doc:`../release_notes/release_notes_1.29`
Migrating to Koji 1.30
======================

You should consider the following changes when migrating to 1.30:

DB Updates
----------

There is a simple schema change adding descriptions to individual permissions.

As in previous releases, we provide a migration script that updates the database.

::

    # psql koji koji < /usr/share/doc/koji/docs/schema-upgrade-1.29-1.30.sql


Other changes
-------------

There are numerous other changes in 1.30 that should not have a direct impact on migration. For
details see: :doc:`../release_notes/release_notes_1.30`
Migrating to Koji 1.31
======================

You should consider the following changes when migrating to 1.31:

DB Updates
----------

There is a simple schema change adding new index.

As in previous releases, we provide a migration script that updates the database.

::

    # psql koji koji < /usr/share/doc/koji/docs/schema-upgrade-1.30-1.31.sql


Other changes
-------------

There are numerous other changes in 1.31 that should not have a direct impact on migration. For
details see: :doc:`../release_notes/release_notes_1.31`
Migrating to Koji 1.7
=====================

.. reStructured Text formatted

The 1.7 release of Koji contains changes that will require a little extra
work when updating. These changes are:

* DB schema updates to support storage volumes
* The change from mod_python to mod_wsgi
* The introduction of a separate configuration file for koji-web
* Changes to url options

DB Schema Updates
-----------------

The 1.7 release adds two new tables to the database. The ``volume`` table tracks
the names of available storage volumes, and the ``tag_updates`` table tracks
changes to tags that are not easily calculated from other tables. There is
also a new field in the ``build`` table, ``volume_id``, which indicates which
volume a build is stored on.

As in previous releases, we provide a migration script that updates the
database.

::

    # psql koji koji  </usr/share/doc/koji-1.7.0/docs/schema-upgrade-1.5-1.7.sql


mod_python and mod_wsgi
-----------------------

Koji now defaults to using mod_wsgi to interface with httpd. Support for
mod_python is _deprecated_ and will disappear in a future version of Koji.
Koji administrators can opt to stay on mod_python for now, but some minor
configuration changes will be required.

Migrating to mod_wsgi
^^^^^^^^^^^^^^^^^^^^^

The mod_wsgi package is now required for both koji-hub and koji-web. Folks
running RHEL5 can find mod_wsgi in EPEL.

You will need to adjust your http config for both koji-hub and koji-web. Our
example config files default to mod_wsgi. To adapt your existing config, you
will need to:

* For both the koji-hub and koji-web/scripts directories:
    * add ``Options ExecCGI``
    * change ``SetHandler`` from mod_python to wsgi-script
* Ensure that the koji-web Alias points to wsgi_publisher.py
* If you have not already, migrate all koji-hub PythonOptions to hub.conf
* Migrate all koji-web PythonOptions to web.conf (see later section)

Staying on mod_python
^^^^^^^^^^^^^^^^^^^^^

Support for mod_python is _deprecated_ and will disappear in a future version
of Koji.

While we have made efforts to maintain mod_python compatibility, there are
a few configuration changes you will need to make.

The koji-hub http config should continue to function without modification.

The koji-web http config will, at minimum, require the following changes:

* Ensure that the koji-web ``Alias`` points to wsgi_publisher.py
* Change koji-web's ``PythonHandler`` setting to wsgi_publisher

Our example http configurations contain commented examples of mod_python
configuration.

Even if you stay on mod_python, we recommend that you migrate away from using
PythonOptions and place your configuration in web.conf and hub.conf.


Web Configuration
-----------------

Starting with version 1.7, koji-web uses a separate configuration file, rather
than PythonOptions embedded in the httpd config. The location of the new file
is:

::

    /etc/kojiweb/web.conf

The web.conf file is an ini-style configuration file. Options should be placed
in the [web] section. All previous options accepted via PythonOptions are
accepted in web.conf. Please see the example web.conf file.


Custom Config File Location
^^^^^^^^^^^^^^^^^^^^^^^^^^^

The location of web.conf can be specified in the httpd configuration. To
specify the location under mod_wsgi, use:

::

    SetEnv koji.web.ConfigFile /path/to/web.conf

Under mod_python, use:

::

    PythonOption koji.web.ConfigFile /path/to/web.conf

If you opt to stay on mod_python, the server will continue to process the old
PythonOptions. To ease migration, it does so by default unless the
koji.web.ConfigFile PythonOption is specified. In order to use web.conf under
mod_python, you *must* specify koji.web.ConfigFile in your http config.

We strongly recommend moving to web.conf. The server will issue a warning at
startup if web.conf is not in use.


Changes to url options
----------------------

The pkgurl option has been removed from the koji command line tool and from
the build daemon (kojid). The url for packages is deduced from the topurl
option, which should point to the top of the /mnt/koji tree.

Any config files that specify pkgurl (e.g. ~/.koji/config, /etc/koji.conf, or
/etc/kojid/kojid.conf) will need to be adjusted.

Similarly, the kojiweb config options KojiPackagesURL, KojiMavenURL, and
KojiImagesURL have been dropped in favor of the new option KojiFilesURL.


Additional Notes
----------------

Split Storage
^^^^^^^^^^^^^

Apart from the schema changes, no other migration steps are required for the
split storage feature. By default, builds are stored in the normal location.

Web Themes
^^^^^^^^^^

Using the old method (httpd aliases for koji static content) should continue
to work. For (brief) instructions on the new method, see the README file under
koji-web/static/themes.
Migrating to Koji 1.8
=====================

.. reStructured Text formatted

The 1.8 release of Koji refactors how images (livecd and appliance) are stored
in the database and on disc. These changes will require a little extra work
when updating.

There have also been some changes to the command line.

Finally, kojira accepts some new options.

DB Schema Updates
-----------------

Previous to 1.8, images were stored in separately from other builds, both in
the database and on disc. The new schema adds new tables: ``image_builds``,
``image_listing``, and ``image_archives``.

The following tables are now obsolete: ``imageinfo`` and ``imageinfo_listing``.
However you should not drop these tables until you have migrated your image
data.

As in previous releases, we provide a migration script that updates the
database.

::

    # psql koji koji  </usr/share/doc/koji-1.8.0/docs/schema-upgrade-1.7-1.8.sql

Note that the SQL script does not (and can not) automatically migrate your old
image data to the new tables. After applying the schema changes, you can
migrate old images using the ``migrateImage`` hub call. This method is necessary
because the new schema requires each image to have a name, version, and release
value. The values for name and version cannot be automatically guessed.


Migrating your old images
-------------------------

If you have old images, you can migrate them to the new system using the
``migrateImage`` hub call. This call requires admin privilege and must also be
enabled with the ``EnableImageMigration`` configuration option in ``hub.conf``.

The signature of the call is:

::

    migrateImage(old_image_id, name, version)

This call can made from the command line:

::

    # koji call migrateImage 45 my_livecd 1.1


Cleaning up
-----------

After you have migrated any necessary images to the new system, you may want to
remove the old database tables and filesystem directories. This step is
*optional*. If you want to leave the old data around, it will not affect Koji.

Before you take any of the following actions, please *make sure* that you have
migrated any desired images.

Removing the old data is simply a matter of dropping tables and deleting files.

::

    koji=> DROP TABLE imageinfo_listing;
    koji=> DROP TABLE imageinfo;
    # rm -rf /mnt/koji/images


Command line changes
--------------------

For clarity and consistency, all of the ``-pkg`` commands have been renamed to
``-build`` commands.

::

    latest-pkg -> latest-build
    move-pkg -> move-build
    tag-pkg -> tag-build
    untag-pkg -> untag-build

For backwards compatibility, the old commands names are also recognized.

A new command has been added, ``remove-pkg``.

Several commands have been modified to support images.

The ``spin-livecd`` and ``spin-appliance`` commands now require additional
arguments. These arguments specify the name and version to use for the image.


New kojira options
------------------

The following options are new to kojira:

::

    max_delete_processes
    max_repo_tasks_maven

Previously, kojira ran as a single process and repo deletions could potentially
slow things down (particularly for Maven-enabled repos). Now kojira spawns
a separate process to handle these deletions. The ``max_delete_processes``
determines how many such processes it will launch at one time.

When Maven-enabled repos are in use, they can potentially take a very long time
to regenerate. If a number of these pile up it can severely slow down
regeneration of non-Maven repos. The ``max_repo_tasks_maven`` limits how many
Maven repos kojira will attempt to regenerate at once.

Also the following kojira option has been removed:

::

    prune_batch_size
Migrating to Koji 1.9
=====================

.. reStructured Text formatted

The 1.9 release of Koji includes a few changes that you should consider when
migrating.

DB Updates
----------

ImageFactory support introduced some new archive types. These have been added to
the ``archivetypes`` table. The inaccurate ``vmx`` entry has been removed.

As in previous releases, we provide a migration script that updates the
database.

::

    # psql koji koji  </usr/share/doc/koji-1.9.0/docs/schema-upgrade-1.8-1.9.sql


Command line changes
--------------------

The command line interface handles configuration files a little differently. Old
configs should work just fine, but now there are new options and enhancements.

In addition to the main configuration files, the koji cli now checks for
``/etc/koji.conf.d`` and ``~/.koji/config.d`` directories and loads any
``*.conf`` files contained within. Also if the user specifies a directory with
the ``-c/--config`` option, then that directory will be processed similarly.

The command line supports a new ``-p/--profile`` option to select alternate
configuration profiles without having to link or rename the koji executable.

The new ``image-build`` command is used to generate images using ImageFactory.
The older spin-appliance command is now deprecated.

The ``mock-config`` command no longer requires a name argument. You can still
specify one
if you want to override the default choice. It also supports new options. The
``--latest`` option causes the resulting mock config to reference the
``latest`` repo (a varying symlink). The ``--target`` option allows generating
the config from a target name.

Other command line changes include:
* a new ``download-logs`` command
* the ``list-groups`` command now accepts event args
* the ``taginfo`` command now reports the list of comps groups for the tag
* the fast upload feature is now used automatically if the server supports it

Other Configuration changes
---------------------------

There are also some minor configuration changes in other parts of Koji.

In ``kojid`` the time limit for rpm builds is now configurable via the
``rpmbuild_timeout`` setting in ``kojid.conf``. The default is 24 hours.

The ``koji-gc`` tool supports two new configuration options. The ``krbservice``
option allows you to specify the kerberos service for authentication, and the
``email_domain`` option allows you to specify the email domain for sending gc
notices.

The messagebus hub plugin now supports ``timeout`` and ``heartbeat`` options
for the message bus connection.


RPC API Changes
---------------

Most of these changes are extensions, though some of the host-only call
changes are incompatible.

The ``tagHistory`` call accepts a new named boolean option (``active``) to
select only active/inactive entries. It also now reports the additional fields
``maven_build_id`` and ``win_build_id`` if builds are maven or win builds
respectively.

New rpc calls: ``buildImageOz``, ``host.completeImageBuild``, and
``host.evalPolicy``.

The host-only calls ``host.moveImageBuildToScratch`` and ``host.importImage``
no longer accept the ``rpm_results`` argument. The rpm results can be embedded
in the regular ``results`` argument.
==========
Migrations
==========

.. toctree::
    :maxdepth: 1

    migrating_to_1.31
    migrating_to_1.30
    migrating_to_1.29
    migrating_to_1.28
    migrating_to_1.27
    migrating_to_1.26
    migrating_to_1.25
    migrating_to_1.24
    migrating_to_1.23
    migrating_to_1.22
    migrating_to_1.21
    migrating_to_1.20
    migrating_to_1.19
    migrating_to_1.18
    migrating_to_1.17
    migrating_to_1.16
    migrating_to_1.15
    migrating_to_1.14
    migrating_to_1.13
    migrating_to_1.12
    migrating_to_1.11
    migrating_to_1.10
    migrating_to_1.9
    migrating_to_1.8
    migrating_to_1.7
========================
Koji Miscellaneous Notes
========================


Notes and miscellaneous details about Koji
==========================================

This document is intended to illuminate some of the small quirks that
you may encounter while running your own koji server.

Koji CLI
--------

Getting Help
~~~~~~~~~~~~

-  There are multiple ways to receive help from the koji command, each
   of them gives you something a little different:

   -  List of user commands:
      ::

          koji help

   -  Command-specific help:
      ::

          koji [command] --help

   -  List of admin commands:
      ::

          koji help --admin

Building from SRPM
~~~~~~~~~~~~~~~~~~

Koji only allows admins access to build directly from srpm. It expects
normal users to build out of an SCM.

Building from SCM
~~~~~~~~~~~~~~~~~

SCM Layout
^^^^^^^^^^

When building out of an SCM, Koji expects there will be a Makefile in
the project root that has a 'sources' target. Koji will call 'make
sources' on the checked out files.

This target simply needs to download all the sources for the SRPM that
are not already included in the SCM repository.

SCM URI Structure
^^^^^^^^^^^^^^^^^

In Fedora, this detail is typically handled by the fedpkg tool. Outside
of Fedora, you may need to know how to manually submit builds from an
SCM to koji.

Koji accepts an SCM URI in this format:

::

    koji build [target] [scheme]://[user]@[hostname]/[path/to/repository]?[path/to/project]#[revision]

Note the division between repository path and project path. During setup
of kojid, the allowed\_scms parameter is configured in
/etc/kojid/kojid.conf. This value of this parameter should match the
path to the repository.

In some SCM configurations, there isn't a difference between repository
path and project path. In these cases, it should be understood that
dividing your SCM path into two components, URI path and URI query, can
seem somewhat arbitrary. An easy way to remember this detail is that the
path specified in allowed\_scms is the portion of your SCM path that
goes before the URI query, any sub-directories not specified in
allowed\_scms is given to koji as the URI query.

Koji tasks
----------

BuildNotifications
~~~~~~~~~~~~~~~~~~

-  Koji sends build notifications to the package owner and the user who
   submitted the build. For BuildNotifications to work successfully, the
   package owner's username needs to match a valid username for an
   e-mail address, because kojihub sends to
   username@domain\_in\_kojihub.conf.

   -  For SSL authentication, this means that your CN must be valid as
      the user portion of an e-mail address.

Koji server administration
--------------------------

Importing an rpm without srpm
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

If you are running a private koji instance, you may run into a situation
where you need to integrate a proprietary rpm into your build system. To
do this, it is similar to the package imports you did when bootstrapping
your koji instance:

::

    koji import --create-build [package]

Multi-arch builds
~~~~~~~~~~~~~~~~~

There are some packages that need to build across multiple arches. For
example, the kernel package no longer builds an i386 kernel, kernels are
built on i586 and above. To instruct koji to build these additional
arches, use this command:

::

    koji set-pkg-arches [options] <arches> <tag> <package> [<package2> ...]

Note: is a single entity, so denote multiple arches within quotes (e.g.
'i386 i586 i686').

How noarch sub-packages are built
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

It is a technical detail, but can make sense for someone debugging
noarch issues. If there are multiple architectures defined, multiple
`buildArch` tasks are spawned. Each of them will produce arch and also
noarch packages. While arch packages are unique compared to other
subtasks, noarch packages should be the same. It shouldn't matter on
which arch you're building it. If there is a difference, it is a bug.
Koji internally checks some data from these noarch packages and
chooses randomly one, which will appear in final builds. If
differences are found, build will fail. Koji is using simple embedded
rpmdiff variant, which checks for each file included in rpm: mode,
device, vflags, user, group, digest. Furthermore, Name, Summary,
Description, Group, License, URL, PreIn, PostIn, PreUn, PostUn,
Provides, Requires, Conflicts and Obsoletes are compared.  In case of
failing this test, `BuildError` is raised.

Manually submitting a task
~~~~~~~~~~~~~~~~~~~~~~~~~~

Occasionally, you may need to manually submit a task to koji. One likely
usage is to manually trigger a createRepo. To do this, use this command:

::

    koji make-task [options] <arg1> [<arg2>...]

The make-task command is a bit of under-documented black magic. Task
parameters are defined in kojihub.py. The easiest way I have found to
figure out the right incantations for make-task is to query the *task*
table in the koji database directly. Find a similar task to the one you
want to create, and look in the request field for the parameters the
task used, and mimic those.

So, citing the createRepo case above, here is an example:

::

    kojiadmin@localhost$ koji make-task --channel=createrepo --priority=15 \
    newRepo dist-foo-build

Managing your tags
~~~~~~~~~~~~~~~~~~

Occasionally an unwanted package or version of a package will be built
by koji. Don't fret. There are two mechanisms to handle rescinding a
package or specific package version.

-  To remove a specific version of a package, you can untag it:

::

    koji untag-build [options] <tag> <pkg> [<pkg>...]

 supports either %name or %name-%version-%release

-  To remove all versions of a package, you can untag it as above or you
   can administratively block it from being listed in a tag:

::

    koji block-pkg [options] tag package [package2 ...]

Spec file processing
--------------------

Macro processing
~~~~~~~~~~~~~~~~

Macros in the spec file are expanded before Requires and BuildRequires
are processed. If there are any custom macros in the spec file, the
package that drops those macros into /etc/rpm must be tagged under your
dist-build tag

%dist tags
^^^^^^^^^^

For packages that incorporate the %dist tags in their filename, they
expect %dist to be defined in /etc/rpm/macros.dist, which was added in
Fedora 7. For building on RHEL5/FC6 and earlier, koji needs the
`https://buildsys.fedoraproject.org/buildgroups/
buildsys-macros <https://buildsys.fedoraproject.org/buildgroups/ buildsys-macros>`__
package tagged under the dist-build tag.
=================
Permission system
=================

Permissions are used by Koji to control access in a number of ways.
Some permissions are built-in (e.g. ``admin``, ``repo``), but new ones can be
created by administrators.

The ``admin`` permission is special.
It grants superuser access and can stand in for any other permission.

Most of the built-in permissions control access to various hub calls.
For example, the ``dist-repo`` permission allows access to create dist repos.

Custom permissions can used as the required permission for a tag, or they can be
referenced in :doc:`hub policies <defining_hub_policies>`. Note, that you need
to first understand the policy mechanism as most permissions are reflected in
policy rules.


Permission management
=====================

Granting or removing permissions requires the ``admin`` permission.
A user with sufficient access can use the following koji CLI commands:

``koji grant-permission [--new] <permission> <user> [<user> ...]``\
    Grants permission to one or more users. It can be also used to create
    a new permission with the ``--new`` option.

``koji revoke-permission <permission> <user> [<user> ...]``
    Removes the named permission from users.

``koji list-permissions [--user <user>] [--mine]``
    Lists permissions in the system.


Built-in permissions
====================

Administration
--------------

The following permissions govern access to key administrative actions.


``admin``
  This is a superuser access without any limitations, so grant with caution.
  Users with admin effectively have every other permission.
  We recommend granting the smallest effective permission.

``host``
  Restricted admin permission for handling host-related management tasks.

``tag``
  Permission for adding/deleting/editing tags.  Allows use of the
  ``tagBuildBypass`` and ``untagBuildBypass`` API calls also. Note, that this
  name could be confusing as it is not related to tagging builds but to editing
  tags themselves. Tagging builds (and adding/removing packages from package
  lists for given tags) is handled by ``tag`` and ``package_list`` policies
  respectively.

``target``
  Permission for adding/deleting/editing targets


Tasks
-----

The following permissions grant access to trigger specialized tasks.

``appliance``
  appliance tasks (``koji spin-appliance``)

``dist-repo``
  distRepo tasks (``koji dist-repo``)

``image``
  image tasks (``koji image-build``)

``livecd``
  livecd tasks (``koji spin-livecd``)

``livemedia``
  livemedia tasks (``koji spin-livemedia``)

``regen-repo``
  This permission grants access to regenerate repos (i.e. to trigger
  ``newRepo`` tasks).

``win-admin``
  The default ``vm`` policy requires this permission to trigger Windows builds.


Data Import
-----------

The following import permissions allow a user to directly import build
artifacts of different types.
We recommend caution when granting these.
In general, it is better to use the
:doc:`content generator interface <content_generators>` rather than the direct
import calls these govern.

``image-import``
  used for importing external maven artifacts
  (``koji import-archive --type maven``)

``maven-import``
  used for importing external maven artifacts
  (``koji import-archive --type maven``)

``win-import``
  used for importing external maven artifacts
  (``koji import-archive --type win``)


Other
-----

These remaining permissions don't fit into other categories.

``build``
  Defined in the database but currently unused

``repo``
  This special permission is only intended to be granted to the user that
  ``kojira`` runs as.
  It grants access to regenerate and expire repos, as well as flag them as
  deleted or broken.
  Do not grant this permission to normal users.
  The ``regen-repo`` permission can be used to grant access for regeneration
  only.

``sign``
  This permission grants access to add signatures to rpms and to write out
  signed copies (``koji import-sig`` and ``koji write-signed-rpm``).
=======
Plugins
=======

Following plugins are available in default koji installation.

Runroot
=======

Plugin for running any command in buildroot. It has three parts as most of the
others (hub, builder and CLI).

Builder
-------
You enable plugin by editing ``/etc/kojid.conf`` by adding ``plugin = runroot``
there. Plugin itself has separate configuration file on each builder located at
``/etc/kojid/plugins/runroot.conf`` There is a sample configuration file
with option descriptions installed.

Hub
---
On the hub side ``Plugins = runroot_hub`` needs to be added to
``/etc/koji-hub/hub.conf``. Note, that by default policy runroot tasks are
assigned to ``runroot`` channel. As this is a plugin, we don't create it
automatically. There are three options - create channel when adding first builder
there via ``koji add-host-to-channel --new hostname runroot`` or by changing the
default channel policy according to :doc:`defining_hub_policies`. Last option is
to use ``--channel-override`` option in CLI to drive task to channel of choice.

CLI
---
CLI is looking for available plugins every run, so it if it is installed, you'll
see new command ``runroot`` with options described in its help. No config
options are needed to enable it.

Save Failed Tree
================

In some cases developers want to investigate exact environment in which their
build failed. Reconstructing this environment via mock needn't end with
exactly same structure (due to builder settings, etc.). In such case this
plugin can be used to retrieve tarball with complete mock tree.

Additional feature is that some paths from buildroot can be left out from
tarball. Feature can be configured via
`/etc/kojid/plugins/save_failed_tree.conf` file. Currently only field
filters.paths is used and it consists of globs (standard python's fnmatch is
used) separated by whitespaces.

.. code-block:: ini

  [filters]
  paths = /etc/*.keytab /tmp/secret_data

.. warning::
  For security reasons, currently all ``/tmp/krb5cc*`` and ``/etc/*.keytab``
  files are removed from tarball. If we found some other dangerous pieces,
  they can be added to this blacklist.

Special task method is created for achieving this which is called
``SaveFailedTree``. This task can be created via CLI:
``koji save-failed-tree <taskID>``. Additional options are:

.. option:: --full

   directs koji to create tarball with complete tree.

.. option:: --nowait

   exit immediately after creating task

.. option:: --quiet

   don't print any information to output

After task finishes, one can find the tarball on relevant task web page (URL
will be printed to stdout until ``--quiet`` is used.

Plugin allow to save trees only for tasks defined in config
``/etc/koji-hub/plugins/save_failed_tree.conf``. Option
``allowed_methods`` contains list of comma-delimited names of tasks. Default
configuration contains line: ``allowed_methods = buildArch``. Anybody
is allowed to create this type of task (and download tarball).

.. warning::
  Don't forget that this type of task can generate huge amount of data, so use
  it wisely.

TODO
----
 * Separate volume/directory on hub
 * garbage collector + policy for retaining generated tarballs

Sidetag
=======

Sidetag plugin is originally work of Mikolaj Izdebski and was pulled into base
koji due to easier integration with rest of the code.

It is used for managing `sidetags` which are light-weight short-lived build tags
for developer's use. Sidetag creation is governed by hub's policy.

Hub
---

Example for `/etc/koji-hub/hub.conf`:

.. code-block:: ini

    PluginPath = /usr/lib/koji-hub-plugins
    Plugins = sidetag_hub

    [policy]
    sidetag =
        # allow maximum of 10 sidetags per user for f30-build tag
        tag f30-build && compare number_of_tags <= 10 :: allow
        # forbid everything else
        all :: deny

    package_list =
        # allow blocking for owners in their sidetags
        match action block && is_sidetag_owner :: allow
        all :: deny

There are two special policy tests `is_sidetag` and `is_sidetag_owner` with
expectable behaviour.

Now Sidetag Koji plugin should be installed.  To verify that, run
`koji list-api` command -- it should now display `createSideTag`
as one of available API calls.

Plugin has also its own configuration file
``/etc/koji-hub/plugins/sidetag.conf`` which contains following options:

.. glossary::
   remove_empty = off
       If this is set, sidetag is automatically deleted when
       last package is untagged from there.

   allowed_suffixes =
       List of strings delimited by commas. These suffixes are then allowed to
       be requested via ``createSideTag``

   name_template = {basetag}s-side-{tag_id}d
       Python string template to be used for generation of sidetag name. It needs
       to contain both basetag/tag_id placeholders.

CLI
---

For convenient handling, also CLI part is provided. Typical session would look
like:

.. code-block:: shell

   $ koji add-sidetag f30-build --wait
   f30-build-side-123456
   Successfully waited 1:36 for a new f30-build-side-123456 repo

   $ koji remove-sidetag f30-build-side-123456

API
---
And in scripts, you can use following calls:

.. code-block:: python

    import koji
    ks = koji.ClientSession('https://koji.fedoraproject.org/kojihub')
    ks.gssapi_login()
    ks.createSideTag('f30-build')

.. _protonmsg-config:

Proton messaging
================

The ``protonmsg`` plugin for the hub will, if enabled, send a wide range of
messages about Koji activity to the configured amqps message brokers.
Most callback events on the hub are translated into messages.

In order to enable this plugin, you must:

* add ``protonmsg`` to the ``Plugins`` setting in ``/etc/koji-hub/hub.conf``

* provide a configuration file for the plugin at
  ``/etc/koji-hub/plugins/protonmsg.conf``

The configuration file is ini-style format with three sections: broker,
queue and message.
The ``[broker]`` section defines how the plugin connects to the message bus.
The following fields are understood:

* ``urls`` -- a space separated list of amqps urls. Additional urls are
  treated as fallbacks. The plugin will send to the first one that accepts
  the message
* ``cert`` -- the combined client cert and key file for authenticating koji to
  the broker.
* ``cacert`` -- the CA certificate to verify the broker server TLS connection
* ``topic_prefix`` -- Koji uses this string as a prefix for all message
  topics. For example, if you choose ``topic://koji``, then Koji
  will publish messages on ``topic://koji.package.add`` when an user runs
  ``kojidev add-pkg`` etc. Use ``topic://`` prefixes for ActiveMQ brokers,
  ``/topic/`` for RabbitMQ brokers.
* ``connect_timeout`` -- the number of seconds to wait for a connection before
  timing out
* ``send_timeout`` -- the number of seconds to wait while sending a message
  before timing out

The ``[message]`` section sets parameters for how messages are formed.
Currently only one field is understood:

* ``extra_limit`` -- the maximum allowed size for ``build.extra`` fields that
  appear in messages. If the ``build.extra`` field is longer (in terms of 
  json-encoded length), then it will be omitted. The default value is ``0``
  which means no limit.

The ``[queue]`` section controls how (or if) the plugin will use the database
to queue messages when they cannot be immediately sent.
The following fields are understood:

* ``enabled`` -- if true, then the feature is enabled
* ``batch_size`` -- the maximum number of queued messages to send at one time
* ``max_age`` -- the age (in hours) at which old messages in the queue are discarded

It is important to note that the database queue is only a fallback mechanism.
The plugin will always attempt to send messages as they are issued.
Messages are only placed in the database queue when they cannot be immediately
sent on the bus (e.g. if the amqps server is offline).

Admins should consider the balance between the ``batch_size`` and
``extra_limit`` options, as both can affect the total amount of data that the
plugin could attempt to send during a single call.


Image builds using Kiwi
=======================

**This is just a tech-preview. API/usage can drastically change in upcoming
releases**

Plugin for creating images via `kiwi <http://osinside.github.io/kiwi/>`_
project. Minimal supported version of kiwi is ``kiwi-9.24.2``.

All three parts (cli/hub/builder) needs to be installed. There is currently no
configuration except allowing the plugins (name is 'kiwi' for all components).

Builders don't need to have any specific library installed (kiwi
invocation/usage is only in buildroots not on builder itself). (Temporarily
``python3-kiwi`` needs to be installed on builder for kojid to be able to parse
kiwi output. It will be changed to json in next version and this requirement
will be dropped.)

``image`` channel is the default one and ``channel`` policy can be used to
request other channel for this type of tasks as usual.

Buildtag needs to be configured by adding special group ``kiwi`` which should
contain at least ``kiwi-cli``, potentially ``jing`` for better description files
validation and any ``kiwi-systemdeps-*`` packages for requested image types. So,
most simple configuration will look like:

.. code-block:: shell

   $ koji add-group kiwi-build-tag kiwi-build
   $ koji add-group-pkg kiwi-build-tag kiwi-build kiwi-cli kiwi-systemdeps

Another thing we need to ensure is that we're building in chroot and not in
container.

.. code-block:: shell

   $ koji edit-tag kiwi-build-tag -x mock.new_chroot=False

Calling the build itself is a matter of simple CLI call:

.. code-block: shell

   $ koji kiwi-build kiwi-target git+https://my.git/image-descriptions#master my_image_path

Selecting other than default kiwi profile can be done by ``--kiwi-profile``
option. Similarly to other image tasks, alternative architecture failures can be
ignored for successful build by ``--can-fail`` option. ``--arch`` can be used to
limit build tag architectures.

There are some limitation to used kiwi configuration:

 * ``include`` node can use only ``this://`` protocol. Other types like ``file://``
   or ``https://`` could reach out of the repo preventing reproducible build.
 * All repositories from description (and included files) are removed and replaced
   by buildroot repo and other repositories specified by ``--repo`` option.

Driver Update Disks building
============================

**This is just a tech-preview. API/usage can drastically change in upcoming
releases**

Plugin for creating Driver Update Disks with ``xorrisofs``.

All three parts (cli/hub/builder) needs to be installed. There is currently no
configuration except allowing the plugins (name is 'dud' for all components).

Builders don't need to have any specific library installed (xorrisofs
invocation/usage is only in buildroots not on builder itself).

Buildtag needs to be configured by adding special group ``dud-build`` which should contain
the following packages:

.. code-block:: shell


   $ koji add-group dud-build-tag dud-build
   $ koji add-group-pkg dud-build-tag dud-build xorriso
   $ koji add-group-pkg dud-build-tag dud-build createrepo_c
   $ koji add-group-pkg dud-build-tag dud-build dnf
   $ koji add-group-pkg dud-build-tag dud-build dnf-plugins-core

Another thing we need to ensure is that we're building in chroot and not in
container.

.. code-block:: shell

   $ koji edit-tag dud-build-tag -x mock.new_chroot=False

Calling the build itself is a matter of simple CLI call:

.. code-block: shell

   $ koji dud-build dud-target --scmurl=git+https://my.git/image-descriptions#master myamazingdud 1 package1 package2

The command options allows to bring all the package dependencies into the DUD
ISO with ``--alldeps``. ``--scmurl`` allows to include non-RPM related content
inside the produced ISO.

Similarly to other image tasks, alternative architecture failures can be
ignored for successful build by ``--can-fail`` option. ``--arch`` can be used
to limit build tag architectures.

tag2distrepo
============

Koji plugin to automatically regenerate distrepos on tag operations

It uses the following options on a tag to control behaviour:

- ``tag2distrepo.enabled``: set to "true" to enable automatic distrepos
- ``tag2distrepo.keys``: set to a space-separated list of keys to use for distrepos

Following parameters correspond to relevant ``brew dist-repo`` options.

- ``tag2distrepo.inherit``: follow inheritance (default: False)
- ``tag2distrepo.latest``: use only latest tagged builds (default: False)
- ``tag2distrepo.split_debuginfo``: separate directory for debuginfo default: False

The tag must have at least one arch configured on it.

Installing plugin on Koji Hub
-----------------------------

1. Edit the following settings in ``/etc/koji-hub/hub.conf`` to enable the plugin:

.. code-block:: ini

         PluginPath = /usr/lib/koji-hub-plugins
         Plugins = tag2distrepo

2. Reload Apache

.. code-block:: shell

        $ systemctl reload httpd

Example usage
-------------

Here is an example of enabling the plugin on an "f33-infra" tag. Create the tag and ensure it has
at least one arch and a package list (direct or inherited) so we can tag packages into it.

.. code-block:: shell

    $ koji add-tag f33-infra --arches=x86_64
    $ koji add-pkg --owner kdreyer f33-infra bash

Set the extra options on the tag so the plugin will generate the repository:

.. code-block:: shell

    $ koji edit-tag -x tag2distrepo.enabled=True -x tag2distrepo.keys=47dd8ef9 f33-infra

Tag a new build to trigger the plugin:

.. code-block:: shell

    $ koji tag f33-infra bash-5.0.17-2.fc33

The hub will immediately queue a new distRepo task, using the tagBuild task host as the distRepo
task owner. When the distRepo task completes, you can find the new repository under the ``topurl``
for your Koji instance.

To confirm that the tag has the correct options set, use the `koji taginfo` command:

.. code-block:: shell

    $ koji taginfo f33-infra
    Tag: f33-infra [18680]
    Arches: x86_64
    Tag options:
      tag2distrepo.enabled : 'true'
      tag2distrepo.keys : '47dd8ef9'

To disable the plugin for the same tag:

.. code-block:: shell

    $ koji edit-tag -r tag2distrepo.enabled -r tag2distrepo.keys f33-infra

Using Multiple Keys
-------------------

If you want to create a repository that contains builds signed by more than one key, list your
desired key IDs ordered by preference.

For example:

.. code-block:: shell

    $ koji edit-tag coreos-pool -x tag2distrepo.keys="45719a39 9867c58f 38ab71f4 5323552a"

For each RPM in the tag, Koji will use the first signed copy that it finds. In other words,
Koji will try the first key (`45719a39`), and if Koji does not have the first key's signature
for that RPM, then it will try the second key (`9867c58f`), third key (`38ab71f4`), and so on.
=============
Koji Profiles
=============

Koji clients can connect to multiple Koji instances via "profiles". These are
configuration sections that describe how to connect to different environments.


Command Line Interface
======================

By default, the ``koji`` CLI will use a profile named ``koji``. (This profile
is Fedora's production Koji environment.)

You can choose a different profile on the CLI. For example, to choose CentOS's
"cbs" profile instead:

 * Run ``koji`` with ``--profile=cbs``, for example ``koji --profile cbs
   list-hosts``.

 * Symlink or alias the profile name to the ``koji`` executable. For example,
   ``ln -s /usr/bin/koji /usr/bin/cbs`` or ``alias cbs=koji``. The CLI will
   use the executable name as the profile name, so you can simply run ``cbs
   list-hosts``.

Configuration Files
===================

The Koji client searches for profile definitions in the following locations:

 * ``/etc/koji.conf``
 * ``/etc/koji.conf.d/*.conf``
 * ``~/.koji/config.d/*.conf``
 * The ``--config=FILE`` option on the CLI

Koji reads all these files and searches for ``[$profile_name]`` sections. For
example, if you use a profile named ``cbs``, the Koji client will search for a
section titled ``[cbs]`` in the files.

Using Koji Profiles in Python
=============================

Instead of using the ``koji`` Python module directly, you can get a
profile-specific module by calling::

    mykoji = koji.get_profile_module("cbs")

This ``mykoji`` module is clone of the ``koji`` module with additional
profile-specific tweaks.

You can read all the settings in the profile configuration with the
``.config`` property::

    mykoji.config        # optparse.Values object
    vars(mykoji.config)  # plain python dict


Examples
--------

Print configurations for multiple profiles::

    import koji

    fedora_koji = koji.get_profile_module("koji")
    stage_koji = koji.get_profile_module("stg")

    for this_koji in (fedora_koji, stage_koji):
        print("PROFILE: %s" % this_koji.config.profile)
        for key, value in sorted(vars(this_koji.config).items()):
            print("    %s = %s" % (key, value))
        print("")


Use ``ClientSession`` to send RPCs to the hub::

    import koji

    mykoji = koji.get_profile_module("koji")
    client = mykoji.ClientSession(mykoji.config.server)
    print(client.listTags())
=============
Release Notes
=============

.. toctree::
    :maxdepth: 1

    release_notes_1.31
    release_notes_1.30.1
    release_notes_1.30
    release_notes_1.29.1
    release_notes_1.29
    release_notes_1.28.1
    release_notes_1.28
    release_notes_1.27.1
    release_notes_1.27
    release_notes_1.26.1
    release_notes_1.26
    release_notes_1.25.1
    release_notes_1.25
    release_notes_1.24.1
    release_notes_1.24
    release_notes_1.23.1
    release_notes_1.23
    release_notes_1.22.1
    release_notes_1.22
    release_notes_1.21.1
    release_notes_1.21
    release_notes_1.20.1
    release_notes_1.20
    release_notes_1.19.1
    release_notes_1.19
    release_notes_1.18.1
    release_notes_1.18
    release_notes_1.17
    release_notes_1.16.2
    release_notes_1.16.1
    release_notes_1.16
    release_notes_1.15.1
    release_notes_1.15
    release_notes_1.14
    release_notes_1.13

For releases before 1.13, see the migration guides:

:doc:`../migrations/migrations`
Koji 1.13 Release Notes
=======================

Migrating from Koji 1.12
------------------------

For details on migrating see :doc:`../migrations/migrating_to_1.13`


Client Changes
--------------

Python 3 client support
^^^^^^^^^^^^^^^^^^^^^^^

| PR: https://pagure.io/koji/pull-request/417

The koji command and core library now support Python 3 (as well as 2). The
default spec now produces both `python2-koji` and `python3-koji`
subpackages. The `koji` package still contains the (now much smaller)
``/usr/bin/koji`` file.

Some older features are not supported by the Python 3 client

    * the `use_old_ssl` option is not supported, python-requests must be used
    * the old kerberos auth mechanism is not supported, use gssapi instead

CLI Plugins
^^^^^^^^^^^

| PR: https://pagure.io/koji/pull-request/199

The command line interface now has basic plugin support. The primary use case
is for plugins to be able to add new subcommands.
For details see: :ref:`plugin-cli-command`

list-channels CLI command
^^^^^^^^^^^^^^^^^^^^^^^^^

| PR: https://pagure.io/koji/pull-request/442

The new `list-channels` command lists the known channels for the system.

.. code-block:: text

    Usage: koji list-channels
    (Specify the --help global option for a list of other help options)

    Options:
      -h, --help  show this help message and exit
      --quiet     Do not print header information

hostinfo CLI command
^^^^^^^^^^^^^^^^^^^^

| PR: https://pagure.io/koji/pull-request/399
| Issue: https://pagure.io/koji/issue/364

The new ``hostinfo`` command shows basic information about a build host,
similar to the web interface.

.. code-block:: text

    Usage: koji hostinfo [options] <hostname> [<hostname> ...]
    (Specify the --help global option for a list of other help options)

    Options:
      -h, --help  show this help message and exit

Enhancements to restart-hosts
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

| PR: https://pagure.io/koji/pull-request/472

The `restart-hosts` command is used by admins to safely restart the build hosts
after a configuration change.

Because multiple restarts can conflict, the command will now exit with a error
if a restart is already underway (can be overridden with --force).

There are now options to limit the restart to a given channel or arch.

The command now has a timeout option, which defaults to 24hrs.

User-Agent header
^^^^^^^^^^^^^^^^^

| PR: https://pagure.io/koji/pull-request/393
| Issue: https://pagure.io/koji/issue/392

Previously the Koji client library reported a confusingly out-of-date value
in the ``User-Agent`` header. Now it simply reports the major version.

raise error on non-existing profile
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

| PR: https://pagure.io/koji/pull-request/375
| Issue: https://pagure.io/koji/issue/370

If the requested client profile is not configured, the library will raise an
error, rather than proceeding with default values.

See also: :doc:`../profiles`


Changes to the Web interface
----------------------------

Build Log Display
^^^^^^^^^^^^^^^^^

| PR: https://pagure.io/koji/pull-request/471

The build info pages now display the log files for a build (instead of linking
directly to the directory on the download server). This works for all builds,
including those imported by content generators.


Builder changes
---------------

Configuring mock chroot behavior
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

| PR: https://pagure.io/koji/pull-request/400
| Issue: https://pagure.io/koji/issue/398

Koji now supports using mock's --new-chroot option on a per-tag basis.
For details see: :ref:`tuning-mock-per-tag`

pre/postSCMCheckout callbacks
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

The callback interface is used by plugins to hook into various Koji operations.
With this release we have added callbacks in the builder daemon for before and
after source checkout: ``preSCMCheckout`` and ``postSCMCheckout``.

Extended allowed_scms format
^^^^^^^^^^^^^^^^^^^^^^^^^^^^

| PR: https://pagure.io/koji/pull-request/421

The allowed_scms option now accepts entries like:

::

    !host:repository

to explicitly block a host:repository pattern.

See also: :ref:`scm-config`


System changes
--------------

mod_auth_gssapi required
^^^^^^^^^^^^^^^^^^^^^^^^

| PR: https://pagure.io/koji/pull-request/444

On modern platforms, both koji-hub and koji-web now require
mod_auth_gssapi instead of mod_auth_kerb.


Longer tag names
^^^^^^^^^^^^^^^^

| PR: https://pagure.io/koji/pull-request/388
| Issue: https://pagure.io/koji/issue/369

Previously, tag names were limited to 50 characters. They are now limited
to 256 characters.
Koji 1.14 Release Notes
=======================

Migrating from Koji 1.13
------------------------

For details on migrating see :doc:`../migrations/migrating_to_1.14`


Client Changes
--------------


Fail fast option for builds
^^^^^^^^^^^^^^^^^^^^^^^^^^^

| PR: https://pagure.io/koji/pull-request/432


When builders are configured with ``build_arch_can_fail = True`` then the
failure of a single buildArch task does not immediately cause the build
to fail. Instead, the remaining buildArch tasks are allowed to complete,
at which point the build will still fail.

Sometimes developers would rather a build fail immediately, so we have added
the ``--fail-fast`` option to the build command, which overrides this setting.
The option only has an effect if the builders are configured to fail slow.


Custom Lorax templates
^^^^^^^^^^^^^^^^^^^^^^

| PR: https://pagure.io/koji/pull-request/419

Koji now supports custom Lorax templates for the ``spin-livemedia`` command.
The command accepts two new options:

.. code-block:: text

      --lorax_url=URL       The URL to the SCM containing any custom lorax
                            templates that are to be used to override the default
                            templates.
      --lorax_dir=DIR       The relative path to the lorax templates directory
                            within the checkout of "lorax_url".


The Lorax templates must come from an SCM, and the ``allowed_scms`` rules
apply.

When these options are used, the templates will be fetched and an appropriate
``--lorax-templates`` option will be passed to the underlying livemedia-creator
command.


Allow profiles to request a specific python version
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

| PR: https://pagure.io/koji/pull-request/566

On platforms with python3 available, the Koji client is built to execute
with the python3 binary. However, there are a few client features that do not
work under python3, notably old-style (non-gssapi) Kerberos authentication.

If this issue is affecting you, you can set ``pyver=2`` in your Koji
configuration. This can be done per profile. When Koji sees this setting
at startup, it will re-execute itself under the requested python binary.


New list-builds command
^^^^^^^^^^^^^^^^^^^^^^^

| PR: https://pagure.io/koji/pull-request/526

The command line now has a ``list-builds`` command that has similar
functionality to the builds tab of the web interface.

.. code-block:: text

    Usage: koji list-builds [options]
    (Specify the --help global option for a list of other help options)

    Options:
      -h, --help            show this help message and exit
      --package=PACKAGE     List builds for this package
      --buildid=BUILDID     List specific build from ID or nvr
      --before=BEFORE       List builds built before this time
      --after=AFTER         List builds built after this time
      --state=STATE         List builds in this state
      --type=TYPE           List builds of this type.
      --prefix=PREFIX       Only list packages starting with this prefix
      --owner=OWNER         List builds built by this owner
      --volume=VOLUME       List builds by volume ID
      -k FIELD, --sort-key=FIELD
                            Sort the list by the named field
      -r, --reverse         Print the list in reverse order
      --quiet               Do not print the header information


New block-group command
^^^^^^^^^^^^^^^^^^^^^^^

| PR: https://pagure.io/koji/pull-request/509

The ``block-group`` command allows admins to block package group entries
without having to resort to the ``call`` command.

.. code-block:: text

    Usage: koji block-group <tag> <group>
    (Specify the --help global option for a list of other help options)

    Options:
      -h, --help  show this help message and exit


Exit codes for some commands
^^^^^^^^^^^^^^^^^^^^^^^^^^^^

| PR: https://pagure.io/koji/pull-request/558
| PR: https://pagure.io/koji/pull-request/559

Several more commands will now return a non-zero exit code
when an error occurs:

    * the various image building commands
    * the ``save-failed-tree`` command (provided by a plugin)


Easier for scripts to use activate_session
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

| PR: https://pagure.io/koji/pull-request/493

In Koji 1.13.0, it became possible for scripts to ``import koji_cli.lib`` and
gain access to the ``activate_session`` function that the command line tool
uses to authenticate.

In this release, this function has been made easier for scripts to use:

    * the options argument can now be a dictionary
    * less options need to be specified


Builder changes
---------------


Normalize paths for scms
^^^^^^^^^^^^^^^^^^^^^^^^

| PR: https://pagure.io/koji/pull-request/591


For many years, kojid has supported the ``allowed_scms`` option
(see: :ref:`scm-config`) for controlling which scms can be used for building.
In 1.13.0, Koji added the ability to explicitly block a host:path pattern.

Unfortunately, 1.13.0 did not normalize the path before checking the pattern,
making it possible for users to use equivalent paths to route around the
block patterns.

Now, Koji will normalize these paths before the ``allowed_scms`` check.


Graceful reload
^^^^^^^^^^^^^^^

| PR: https://pagure.io/koji/pull-request/565


For a long time kojid has handled the USR1 signal by initiating a graceful restart.
This change exposes that in the systemd service config (and the init script
on older platforms).

Now, ``service kojid reload`` will trigger the same sort of restart that the
``restart-hosts`` command accomplishes, but only for the build host you run it
on. When this happens, kojid will:

    * stop taking new tasks
    * wait for current tasks to finish
    * restart itself once all its tasks are completed


Friendlier runroot configuration
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

| PR: https://pagure.io/koji/pull-request/539
| PR: https://pagure.io/koji/pull-request/528

Two changes make it easier to write a configuration for the runroot plugin.

The ``path_subs`` option is now more forgiving about whitespace:

    * leading and trailing whitespace is ignored for each line
    * blank lines are ignored

The ``[pathNN]`` sections are no longer required to have sequential numbers.
Previously, the plugin expected a sequence like ``[path0]``, ``[path1]``,
``[path2]``, etc, and would stop looking for entries if the next number
was missing. Now, any set of distinct numbers is valid and all ``[pathNN]``
sections will be processed.


System changes
--------------

Deprecations
^^^^^^^^^^^^

| PR: https://pagure.io/koji/pull-request/554
| PR: https://pagure.io/koji/pull-request/597

The following features are deprecated and will be removed in a future release:

    * the ``importBuildInPlace`` rpc call
    * the ``use_old_ssl`` client configuration option (and the underlying
      ``koji.compatrequests`` library)


Removed calls
^^^^^^^^^^^^^

| PR: https://pagure.io/koji/pull-request/497
| PR: https://pagure.io/koji/pull-request/507

The deprecated ``buildFromCVS`` hub call has been removed. It was replaced
by the ``buildSRPMFromCVS`` call many years ago and has been deprecated since
version 1.6.0.

The ``add_db_logger`` function has been removed from the koji library, along
with the ``log_messages`` table in the db. This extraneous call has never been
used in Koji.


Dropped mod_python support
^^^^^^^^^^^^^^^^^^^^^^^^^^

| PR: https://pagure.io/koji/pull-request/508


Koji no longer supports mod_python. This option has been deprecated since
mod_wsgi support was added in version 1.7.0.

See also: :doc:`../migrations/migrating_to_1.7`


Large integer support
^^^^^^^^^^^^^^^^^^^^^

| PR: https://pagure.io/koji/pull-request/571


Koji uses xmlrpc for communications with the hub, and unfortunately the
baseline xmlrpc standard only supports 32-bit signed integers. This
results in errors when larger integers are encountered, typically
when a file is larger than 2 GiB.

Starting with version 1.14.0, Koji will emit ``i8`` tags when encoding
large integers for xmlrpc. Integers below the limit are still encoded
with the standard ``int`` tag. The only time this makes a difference
is when Koji would previously have raised an ``OverflowError``.

The ``i8`` tag comes from the
`ws-xmlrpc <https://ws.apache.org/xmlrpc/types.html>`__
spec. Python's xmlrpc decoder has
for many years accepted and understood this tag, even though its encoder
would not emit it.

Previous versions of Koji worked around such size issues by converting
large integers to strings in a few targeted places. Those targeted
workarounds have been left in place on the hub for the sake of backward
compatibility.


Test mode for protonmsg plugin
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

| PR: https://pagure.io/koji/pull-request/538

The ``protonmsg`` plugin now accepts a boolean ``test_mode`` configuration
option. When this option is enabled, the plugin will not actually
send messages, but will instead log them (at the DEBUG level).

This option allows testing environments to run with the plugin enabled, but
without requiring a message bus to be set up for that environment.


Handling of debugsource rpms
^^^^^^^^^^^^^^^^^^^^^^^^^^^^

| PR: https://pagure.io/koji/pull-request/524

Koji will now treat rpms ending in ``-debugsource`` the same way that it does
other debuginfo rpms. Such rpms are:

    * omitted from Koji's normal yum repos
    * listed separately when displaying builds
    * not downloaded by default in the ``download-build`` command


Added kojifile component type for content generators
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

| PR: https://pagure.io/koji/pull-request/506

Content generator imports now accept entries with type equal to ``kojifile``
in the component lists for buildroots and images/archives. This type provides
a more reliable way to reference archive that come from Koji.

See: :ref:`Example metadata <metadata-kojifile>`.
Koji 1.15.1 Release Notes
=========================

Koji 1.15.1 is a bugfix release for Koji 1.15. The most important change
is the fix for :doc:`../CVEs/CVE-2018-1002150`.

Please see: :doc:`release_notes_1.15`

Issues fixed in 1.15.1
----------------------

- `Issue 850 <https://pagure.io/koji/issue/850>`_ --
  CVE-2018-1002150

- `Issue 846 <https://pagure.io/koji/issue/846>`_ --
  error occurs in SCM.get_source since subprocess.check_output is not supported by python 2.6-

- `Issue 724 <https://pagure.io/koji/issue/724>`_ --
  buildNotification of wrapperRPM fails because of task["label"] is None

- `Issue 786 <https://pagure.io/koji/issue/786>`_ --
  buildSRPMFromSCM tasks fail on koji 1.15

- `Issue 803 <https://pagure.io/koji/issue/803>`_ --
  Email notifications makes build tasks fail with "KeyError: 'users_usertype'"

- `Issue 742 <https://pagure.io/koji/issue/742>`_ --
  dict key access fail in koji_cli.commands._build_image

- `Issue 811 <https://pagure.io/koji/issue/811>`_ --
  AttributeError: 'dict' object has no attribute 'hub.checked_md5'

- `Issue 813 <https://pagure.io/koji/issue/813>`_ --
  cg imports fail with "Unsupported checksum type"
Koji 1.15 Release Notes
=======================

Updates
-------

- :doc:`Koji 1.15.1 <release_notes_1.15.1>` is a security update for Koji 1.15

Migrating from the previous release
-----------------------------------

For details on migrating see :doc:`../migrations/migrating_to_1.15`


Client Changes
--------------


Display license Info
^^^^^^^^^^^^^^^^^^^^

| PR: https://pagure.io/koji/pull-request/686


The ``rpminfo`` command now displays the ``License`` field from the rpm.


Keytabs for GSSAPI authentication
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

| PR: https://pagure.io/koji/pull-request/708

Previously keytabs were only supported by the older kerberos auth method, which
is not available on Python 3. Now the gssapi method supports them as well.


Add krb_canon_host option
^^^^^^^^^^^^^^^^^^^^^^^^^

| PR: https://pagure.io/koji/pull-request/653

This release adds a ``krb_canon_host`` option that tells Koji clients
to use the dns canonical hostname for kerberos auth.

This option allows kerberos authentication to work in situations where
the hub is accessed via a cname, but the hub's credentials are under
its canonical hostname.

If specified, this option takes precedence over the older
option named ``krb_rdns``. That option caused Koji clients to perform a
reverse name lookup for kerberos auth.

When configuring kojiweb (in web.conf), the option is named ``KrbCanonHost``.

Both options only affect the older kerberos authentication path, and not
gssapi.


Watch-task return code
^^^^^^^^^^^^^^^^^^^^^^

| PR: https://pagure.io/koji/pull-request/703

Previously, the ``watch-task`` command would return a non-zero exit status
if any subtask failed, even if this did not cause the parent task to fail.

Now that we have cases where subtasks are optional, this no longer makes sense.
The exit code is now based solely on the results of
the top level tasks it is asked to watch.


New runroot options
^^^^^^^^^^^^^^^^^^^

| PR: https://pagure.io/koji/pull-request/633

The ``runroot`` command now supports options similar to the various build commands. These new
options are:


.. code-block:: text

  --nowait              Do not wait on task
  --watch               Watch task instead of printing runroot.log
  --quiet               Do not print the task information


New watch-logs options
^^^^^^^^^^^^^^^^^^^^^^

| PR: https://pagure.io/koji/pull-request/625

The ``watch-logs`` command now supports the following new options:

.. code-block:: text

  --mine      Watch logs for all your tasks
  --follow    Follow spawned child tasks


Web UI changes
--------------

Archive component display
^^^^^^^^^^^^^^^^^^^^^^^^^

| PR: https://pagure.io/koji/pull-request/610

Previously, the web UI only displayed component lists for image builds.
However, new build types can also have component lists.

Now the interface will display components for any archive that has them.


Display license Info
^^^^^^^^^^^^^^^^^^^^

| PR: https://pagure.io/koji/pull-request/686


The ``rpminfo`` page now displays the ``License`` field from the rpm.


Show suid bit
^^^^^^^^^^^^^

| PR: https://pagure.io/koji/pull-request/617

The web UI will now display the setuid bit when displaying rpm/archive file contents.




Builder changes
---------------


Alternate tmpdir for mock chroots
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

| PR: https://pagure.io/koji/pull-request/602


Recent versions of mock (1.4+) default to ``use_nspawn=True``, which results
in /tmp being a fresh tmpfs mount on every run. This means the /tmp
directory no longer persists outside of the mock invocation.

Now, the builder will use /builddir/tmp instead of /tmp for persistent data.


Store git commit hash
^^^^^^^^^^^^^^^^^^^^^

| PR: https://pagure.io/koji/pull-request/674

In Koji, for builds from an SCM, the source is specified as an
scm url.
For git urls, the revision in that url can be anything that git
will recognize, including:

    - a sha1 ref
    - an abbreviated sha1 ref
    - a branch name
    - a tag
    - HEAD

With this change:

    * the revision is replaced with the full sha1 ref for git urls
    * the scm url is stored in build.source
    * the original scm url is saved in build.extra

Previously, this source url was not properly stored for rpm builds. It
appeared in the task parameters, but the build.source field remained blank.
If a symbolic git ref (e.g. HEAD) was given in the url, the underlying
sha1 value was only recorded in the task logs.



System changes
--------------


Volume policy support
^^^^^^^^^^^^^^^^^^^^^

| PR: https://pagure.io/koji/pull-request/622

Koji has for many years had the ability to split its storage across multiple
volumes. However, there is no automatic process for placing builds onto
volumes other than the primary. To do so often requires a lot of manual work
from an admin.

This feature:

    * adds a volume policy check to the key import pathways
    * adds an applyVolumePolicy call to apply the policy to existing builds

The hub consults the volume policy at various points to
determine where a build should live. This allows admins to make rules like:

    - all kernel builds go to the volume named kstore
    - all builds built from the epel-7-build tag go to the volume named epel7
    - all builds from the osbs content generator go to the volume named osbs

The default policy places all builds on the default volume.

See also: :doc:`../volumes`

Messagebus plugin changes
^^^^^^^^^^^^^^^^^^^^^^^^^

| PR: https://pagure.io/koji/pull-request/537

There are two notable changes to the messagebus plugin this release:


Deferred sending
""""""""""""""""

Similar to the current behavior of the protonmsg plugin, messages are queued
up during hub calls and only sent out during the ``postCommit`` callback.

This avoids sending messages about failed calls, which can be confusing to
message consumers (e.g. build state change messages about a build that does
not exist because it failed to import).

Test mode
"""""""""

The plugin now looks for a boolean ``test_mode`` option. If it is true, then
the messages are still queued up, but not actually sent. This makes it
possible to enable the plugin in test environments without having to set up a
separate message bus.


Protonmsg plugin changes
^^^^^^^^^^^^^^^^^^^^^^^^

| PR: https://pagure.io/koji/pull-request/657
| PR: https://pagure.io/koji/pull-request/651

There are two changes to how the protonmsg plugin handles rpmsign events:

    1. The arch of the rpm is included in messages
    2. The message are omitted when the sigkey is empty



No notifications for disabled users or hosts
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

| PR: https://pagure.io/koji/pull-request/615


Koji will no longer send out email notifications to disabled users or
to users corresponding to a host.


Replace pycurl with requests
^^^^^^^^^^^^^^^^^^^^^^^^^^^^

| PR: https://pagure.io/koji/pull-request/601

All uses of the pycurl library have been replaced with calls
to python-requests, so pycurl is no longer required.


Drop importBuildInPlace call
^^^^^^^^^^^^^^^^^^^^^^^^^^^^

| PR: https://pagure.io/koji/pull-request/606

The deprecated ``importBuildInPlace`` call has been dropped.

This call was an artifact of a particular bootstrap event that happened a long
time ago. It was never really documented or recommended for use.
Koji 1.16.1 Release Notes
=========================

Koji 1.16.1 is a point release for Koji 1.16. The major changes include:

- Allow target info to be read for different type tasks in channel policy.
- Create symlinks for builds imported onto non-default volumes.
- Fix RPMdiff issues found in Koji 1.16.0.

Please see: :doc:`release_notes_1.16`

Issues fixed in 1.16.1
----------------------

- `Issue 847 <https://pagure.io/koji/issue/847>`_ --
  spin-livecd failed with "Could not resolve host"

- `Issue 932 <https://pagure.io/koji/issue/932>`_ --
  Fix use_host_resolv with new mock version

- `Issue 1010 <https://pagure.io/koji/issue/1010>`_ --
  koji fails runroot because of `UnicodeDecodeError`

- `Issue 998 <https://pagure.io/koji/issue/998>`_ --
  cancel build doesn't work for images

- `Issue 994 <https://pagure.io/koji/issue/994>`_ --
  rpmdiff calculate wrong results

- `Issue 1025 <https://pagure.io/koji/issue/1025>`_ --
  missing default volume symlink for imported builds affected by volume policy

- `Issue 1007 <https://pagure.io/koji/issue/1007>`_ --
  decode_args() might result in --package parameter missing in runroot command

- `Issue 150 <https://pagure.io/koji/issue/150>`_ --
  no target info in channel policy for non-rpm tasks

- `PR: 973 <https://pagure.io/koji/pull-request/973>`_ --
  Check empty arches before spawning dist-repo

- `Issue 958 <https://pagure.io/koji/issue/958>`_ --
  Notification for tagBuildBypass is writing message untagged from, expected message tagged into

- `Issue 968 <https://pagure.io/koji/issue/968>`_ --
  Default enable python3 on RHEL8

- `Issue 916 <https://pagure.io/koji/issue/916>`_ --
  `clone-tag` doesn't preserve tagging order

- `Issue 949 <https://pagure.io/koji/issue/949>`_ --
  cli: [rpminfo] KeyError: 'license' for external RPM

- `Issue 876 <https://pagure.io/koji/issue/876>`_ --
  koji clone-tag raises "UnboundLocalError"

- `Issue 945 <https://pagure.io/koji/issue/945>`_ --
  Koji build fail due to ambiguous python shebang
Koji 1.16.2 Release Notes
=========================

Koji 1.16.2 is a bugfix release for Koji 1.16.
The purpose of this release is address  :doc:`../CVEs/CVE-2018-1002161`.

See also:

- :doc:`release_notes_1.16.1`

- :doc:`release_notes_1.16`


Issues fixed in 1.16.2
----------------------

- `Issue 1183 <https://pagure.io/koji/issue/1183>`_ --
  CVE-2018-1002161
Koji 1.16.0 Release notes
=========================


Migrating from Koji 1.15
------------------------

For details on migrating see :doc:`../migrations/migrating_to_1.16`



Security Fixes
--------------

**CVE-2018-1002150 - distRepoMove missing access check**

This release includes the fix for :doc:`../CVEs/CVE-2018-1002150`.


Client Changes
--------------

**CLI commands to manage notifications**

| PR: https://pagure.io/koji/pull-request/688

The change adds new cli sub-commands:

    - list-notifications
    - add-notification
    - remove-notification
    - edit-notification

Previously this functionality was only available through the web ui or
by making direct api calls.


**Add --old-chroot option to runroot command**

| PR: https://pagure.io/koji/pull-request/823

This option causes the runroot handler to pass the same-named option
to the mock command. This complements the existing ``--new-chroot``
option.

If neither ``--old-chroot`` or ``--new-chroot`` is given, then mock will
follow its default behavior. This default varies across mock versions.
For newer versions of mock, ``--new-chroot`` is the default (uses a
systemd nspawn container).


**Fix runroot output on py3**

| PR: https://pagure.io/koji/pull-request/828

The runroot command should now work under python3.


**Honor runroot --quiet**

| PR: https://pagure.io/koji/pull-request/806

The ``--quiet`` option was added to the runroot command in version 1.15,
but it only took effect when the ``--watch`` option was given. Now it is
honored in all cases.


**Drop old ssl code**

| PR: https://pagure.io/koji/pull-request/498

The old ``koji.ssl`` module has been removed, and the ``use_old_ssl`` option
has been removed from client code.

Because these files (which were originally from
`Plague <https://fedoraproject.org/wiki/Plague>`_) were the only parts
of Koji that were licensed as GPLv2+, Koji is now simply licensed as
LGPLv2.


Builder Changes
---------------

**Configure install timeout for imagefactory**

| PR: https://pagure.io/koji/pull-request/841

Previously the install timeout parameter for imagefactory was set
to a fixed value of 7200 by Koji. Now it can be controlled by
setting the ``oz_install_timeout`` option in ``kojid.conf``.

A value of ``0`` will disable the timeout.


**Record log timestamps**

| PR: https://pagure.io/koji/pull-request/777

If the ``log_timestamps`` option is enabled in ``kojid.conf``, then
the builder will record a separate timestamp file for each log file
in a build.

The filename for the timestamp file is generated by taking the name
of the log file and appending ``-ts.log``. So ``build.log`` will have
timestamp data in ``build.log-ts.log``.

The format of the timestamp log is plain text with each line showing
a numeric timestamp and a line offset.


**Builder option: chroot_tmpdir**

| PR: https://pagure.io/koji/pull-request/787

The new ``chroot_tmpdir`` option controls which directory within buildroots
is used for various temporary data by the Koji builder daemon.
Previously this was hardcoded to ``/builddir/tmp``, which created problems
with modern versions of mock.

The default value is ``/chroot_tmpdir``.


**Add internal_dev_setup option to runroot config**

| PR: https://pagure.io/koji/pull-request/824

The ``internal_dev_setup`` config option for the runroot builder plugin
controls whether the mock option of the same name is set for runroot
tasks.



System Changes
--------------


**Add option to configure DB port**

| PR: https://pagure.io/koji/pull-request/884

The hub now accepts a ``DBPort`` option in ``hub.conf``, which specifies
which port the hub should use when connecting to the database.


**Split debuginfo for dist repos**

| PR: https://pagure.io/koji/pull-request/914

Dist repos can now be generated with debuginfo files split into a separate
repo. The behavior is controlled by passing the ``--split-debuginfo`` option
to the ``dist-repo`` subcommand.

When this option is in effect, the main repo will be in the normal location.
The debuginfo repo will be in the ``debug`` subdirectory. So, you will
see a directory structure like:

.. code-block:: text


    Packages/
    repodata/
    debug/
    debug/repodata

Regardless of the split, all the rpms are located in the top level
``Packages`` directory.


**Notifications in [un]tagBuildBypass**

| PR: https://pagure.io/koji/pull-request/691

Previously the ``tagBuildBypass`` and ``untagBuildBypass`` calls did not trigger
notifications. Now they will do so by default. The call now accepts a
``notify`` option (defaults to True) which controls the behavior.


**Track history for host data**

| PR: https://pagure.io/koji/pull-request/778

Koji now tracks changes to host data similarly to the way it tracks
changes for other data. This includes

    - enabled state
    - arches
    - capacity
    - description & comment
    - channels

The ``list-history`` cli command now supports ``--host`` and ``--channel``
options to select history entries for a host or channel.

The versioned host data is stored in the ``host_config`` and ``host_channels``
tables.


**Fix block-group functionality**

| PR: https://pagure.io/koji/pull-request/678

The ``block-group`` command and its underlying api call now actually work.


**Strict option for archive listing calls**

| PR: https://pagure.io/koji/pull-request/734
| PR: https://pagure.io/koji/pull-request/748

The ``list_archives``, ``get_archive_file()``, and ``list_archive_files()``
hub functions now accept a strict option, which defaults to False. When
the option is True, the call will raise an exception if there is no
match.


**Search build by source**

| PR: https://pagure.io/koji/pull-request/765

The ``listBuilds()`` api call now supports a source option. This is
treated as a glob pattern and matched against the ``source`` field of the build.


**Option to ignore tags in kojira**

| PR: https://pagure.io/koji/pull-request/695

Kojira now supports an ``ignore_tags`` option. This is treated as a
space-separated list of glob patterns. Tags that match are ignored
by kojira (it will not generate newRepo tasks for them).


**Improve kojira throughput**

| PR: https://pagure.io/koji/pull-request/797

Kojira should be much more responsive in triggering ``newRepo`` tasks.


**Drop migrateImage call**

| PR: https://pagure.io/koji/pull-request/632

The ``migrateImage`` call hub call has been removed.

This call was added in version 1.8 (April 2013)
as a one-time tool for migrating images from the old model (no build entry)
to the new model (image build type). It was only available if the
EnableImageMigration option was set on the hub.
Koji 1.17.0 Release notes
=========================


Migrating from Koji 1.16
------------------------

For details on migrating see :doc:`../migrations/migrating_to_1.17`



Security Fixes
--------------

**CVE-2018-1002161 - SQL injection in multiple remote calls**

| PR: https://pagure.io/koji/pull-request/1274

This release includes the fix for :doc:`../CVEs/CVE-2018-1002161`


Client Changes
--------------

**Volume id option for livemedia and livecd tasks**

| PR: https://pagure.io/koji/pull-request/1227

The ``spin-livecd`` and ``spin-livemedia`` commands now accept a ``--volid``
argument to specify the volume id for the media. If unspecified, the
volume id is chosen via the same heuristic as before.

Volume ids must be 32 characters or less.



**Build order preserved by clone-tag**

| PR: https://pagure.io/koji/pull-request/1014

This is an improvement to the ``clone-tag`` command. Previously, when the
command was used without the ``--latest-only`` option, it could get the
ordering of builds wrong in the destination tag. Now, the order will
match the source tag.



**Configurable authentication timeout**

| PR: https://pagure.io/koji/pull-request/1172

Previously, the network timeout during authentication was hard coded to
60 seconds. It is now configurable via the ``auth_timeout`` configuration
option.


**Additional information from list-channels command**

| PR: https://pagure.io/koji/pull-request/940

The ``list-channels`` command now shows three separate host counts for
each channel:

- the number of enabled hosts in the channel
- the number of ready hosts in the channel
- the number of disabled hosts in the channel


**The free-task command requires at least one task-id**

| PR: https://pagure.io/koji/pull-request/1045

Previously this command was a no-op when given no arguments. Now it will return an
error.



Library Changes
---------------

**Drop encode_int function**

| PR: https://pagure.io/koji/pull-request/852

This is a follow up to the large integer support that we added in version 1.14

See also: :doc:`release_notes_1.14`

The ``encode_int`` function is no longer used
and has been dropped from the library.

Because we no longer call ``encode_int``, the hub will now always use i8 tags
when returning large integers, rather than returning them as strings in some
cases.


**Use custom Kerberos context with krb_login**

| PR: https://pagure.io/koji/pull-request/1187

Clients can now pass in their own Kerberos context to
``ClientSession.krb_login()`` using
the ``ctx`` parameter. This is intended for multi-threaded clients.


**Custom keyboard interrupt handling in watch_tasks**

| PR: https://pagure.io/koji/pull-request/981

The new ``ki_handler`` option for the ``koji_cli.lib.watch_tasks()`` function
allows other cli tools to set their own handler for keyboard interrupts.
If specified, the value should be callable and will be called when a
keyboard interrupt is encountered.
If unspecified, the original behavior is retained.


**_unique_path() -> unique_path**

| PR: https://pagure.io/koji/pull-request/980

The ``_unique_path`` function is deprecated. It has been replaced
by ``unique_path``.


Web UI Changes
--------------

**Additional info on builders in channelinfo page**

| PR: https://pagure.io/koji/pull-request/989

The channelinfo page now shows enabled/ready status for each host and a count
for each.



Builder Changes
---------------

**Builder task_avail_delay check**

| PR: https://pagure.io/koji/pull-request/1176

This delay works around a deficiency in task scheduling. The default
delay is 300 seconds and can be adjusted with the ``task_avail_delay``
option to kojid. However, it is unlikely that admins will need to
adjust this setting.

Despite the name, this does not introduce any new delay compared to the
old behavior. The setting controls how long a host will wait before taking
a task in a given channel-arch "bin" when that host has an available
capacity lower than the median for that bin. Previously, such hosts
could wait forever.



System Changes
--------------


**Python 3 Support**

| PR: https://pagure.io/koji/pull-request/1117
| PR: https://pagure.io/koji/pull-request/891
| PR: https://pagure.io/koji/pull-request/921
| PR: https://pagure.io/koji/pull-request/1184
| PR: https://pagure.io/koji/pull-request/1019
| PR: https://pagure.io/koji/pull-request/685
| ...and many fixes

Support for Python 3 has been extended to all components of Koji. Including:

- Hub
- Builder
- Web UI
- Utils



**No more messagebus plugin**

| PR: https://pagure.io/koji/pull-request/1043

The messagebus plugin has been dropped. The protonmsg plugin is still
available.



**Simple mode for mergerepos**

| PR: https://pagure.io/koji/pull-request/1066

External repos now have a ``merge_mode`` option. Valid values are
either ``koji`` (the old way) or ``simple`` (a new alternative). This
option can be set with the ``--mode`` option to the ``add-external-repo``
or ``edit-external-repo`` commands.

When an external repo is merged with simple mode, a number of the complex
filters that Koji normally applies are skipped. This mode still honors
the block list from Koji and ignores duplicate NVRAs, but otherwise
it simply merges the repo in.

Multiple merge modes cannot be combined in a single tag. If a tag
has two external repos with different modes, then the repo will
fail to generate.


**Avoid "unknown task" errors in Kojira**

| PR: https://pagure.io/koji/pull-request/1175

This is a bug fix for a minor race condition in Kojira that could cause
errors in the log and redundant repo regens.



**Full filename display for kojifiles directory indexes**

| PR: https://pagure.io/koji/pull-request/1156

This is simply a change to the default httpd configuration for serving
/mnt/koji. It adds ``NameWidth=*`` to ``IndexOptions`` so that long filenames
are fully displayed.



**Broader support for target/source/scratch tests in channel policy**

| PR: https://pagure.io/koji/pull-request/962

It is now possible to write channel policy rules based on
build target, source, and scratch options for task types other
than ``build``.



**Longer Build Target names**

| PR: https://pagure.io/koji/pull-request/925

Build target names can now be up to 256 characters, the same length
restriction as for tag names.
Koji 1.18.1 Release Notes
=========================

Koji 1.18.1 is a bugfix release for Koji 1.18.
The purpose of this release is address  :doc:`../CVEs/CVE-2019-17109`.


Issues fixed in 1.18.1
----------------------

- `Issue 1634 <https://pagure.io/koji/issue/1634>`_ --
  possible to upload file to a path other than work directory
Koji 1.18.0 Release notes
=========================


Migrating from Koji 1.17
------------------------

For details on migrating see :doc:`../migrations/migrating_to_1.18`



Security Fixes
--------------



Client Changes
--------------

**Add option for custom cert location**

| PR: https://pagure.io/koji/pull-request/1253

The CLI now has an option for setting a custom SSL certificate, similar to the
options for Kerberos authentication.


**Load client plugins from ~/.koji/plugins**

| PR: https://pagure.io/koji/pull-request/892


This change allows users to load their own cli plugins from ``~/.koji/plugins``
or from another location by using the ``plugin_paths`` setting.


**Show load/capacity in list-channels**

| PR: https://pagure.io/koji/pull-request/1449

The ``list-channels`` display has been expanded to show overall totals for load
and capacity.


**Allow taginfo cli to use tag IDs**

| PR: https://pagure.io/koji/pull-request/1476

The ``taginfo`` command can now accept a numeric tag id on the command line.


**Add option to show channels in list-hosts**

| PR: https://pagure.io/koji/pull-request/1425

The ``list-hosts`` command will now display channel subscriptions if the
``--show-channels`` option is given.


**Remove merge option from edit-external-repo**

| PR: https://pagure.io/koji/pull-request/1499

This option was mistakenly added to the command and never did anything.
It is gone now.


**Honor mock.package_manager tag setting in mock-config cli**

| PR: https://pagure.io/koji/pull-request/1374

The ``mock-config`` command will now honor this setting just as ``kojid`` does.




Library Changes
---------------

**New multicall interface**

| PR: https://pagure.io/koji/pull-request/957

This feature implements a new and much better way to use multicall in the Koji
library.
These changes create a new implementation outside of ClientSession.
The old way will still work.

With this new implementation:

* a multicall is tracked as an instance of `MultiCallSession`
* the original session is unaffected
* multiple multicalls can be managed in parallel, if desired
* `MultiCallSession` behaves more or less like a session in multicall mode
* method calls return a `VirtualCall` instance that can later be used to access the result
* `MultiCallSession` can be used as a context manager, ensuring that the calls are executed

Usage examples can be found in the :doc:`Writing Koji Code <../writing_koji_code>`
document.




Web UI Changes
--------------

**Retain old search pattern in web ui**

| PR: https://pagure.io/koji/pull-request/1258

The search results page of the web ui now retains a search form with the
current search pre-filled.
This makes it easier for users to refine their searches.


**Display task durations in webui**

| PR: https://pagure.io/koji/pull-request/1383


The ``taskinfo`` page in the web ui now shows task durations in addition to
timestamps.



Builder Changes
---------------

**Rebuild SRPMS before building**

| PR: https://pagure.io/koji/pull-request/1462

For rpm builds from an uploaded srpm, Koji will now rebuild the srpm in the
build environment first.
This ensures that the NVR is correct for the resulting build.

The old behavior can be requested by setting ``rebuild_srpm=False`` in the tag
extra data for the build tag in question.


**User createrepo_c by default**

| PR: https://pagure.io/koji/pull-request/1278


The ``use_createrepo_c`` configuration option for ``kojid`` now defaults to True.


**Use createrepo update option even for first repo run**

| PR: https://pagure.io/koji/pull-request/1363

If there is no older repo for a tag, Koji will now attempt to find
a related repo to use ``createrepo --update`` with.
This will speed up first-time repo generations for tags that
predominantly inherit their content from another build tag.


**Scale task_avail_delay based on bin rank**

| PR: https://pagure.io/koji/pull-request/1386

This is an adjustment to Koji's decentralized scheduling algorithm.
It should result in better utilization of host capacity, particularly when
a channel has hosts that are very heterogeneous in capacity.

The meaning of the ``task_avail_delay`` setting is different now.
Within a channel-arch bin, the hosts with highest capacity will take the task
immediately, while hosts lower down will have a delay proportional to their
rank.
The "rank" here is a float between 0.0 and 1.0 used as a multiplier.
So ``task_avail_delay`` is the maximum time that any host will wait to
take a task.

Hosts with higher available capacity will be more likely to claim a
task, resulting in better utilization of the highest capacity hosts.


**Use RawConfigParser for kojid**

| PR: https://pagure.io/koji/pull-request/1544

The use of percent signs is common in ``kojid.conf`` because of the
``host_principal_format`` setting.
This causes an error in python3 if ``SafeConfigParser`` is used, so we use
``RawConfigParser`` instead.


**Handle bare merge mode**

| PR: https://pagure.io/koji/pull-request/1411
| PR: https://pagure.io/koji/pull-request/1516
| PR: https://pagure.io/koji/pull-request/1502


This feature adds a new merge mode for external repos named ``bare``.
This mode is intended for use with modularity.

Use of this mode requires createrepo_c version 0.14.0 or later on the builders
that handle the createrepo tasks.




System Changes
--------------


**API for reserving NVRs for content generators**

| PR: https://pagure.io/koji/pull-request/1464
| PR: https://pagure.io/koji/pull-request/1597
| PR: https://pagure.io/koji/pull-request/1601
| PR: https://pagure.io/koji/pull-request/1602
| PR: https://pagure.io/koji/pull-request/1606

This feature allows content generators to reserve NVRs earlier in the build
process similar to builds performed by ``kojid``. The NVR is reserved by
calling ``CGInitBuild()`` and finalized by the ``CGImport()`` call.



**Per-tag configuration of rpm macros**

| PR: https://pagure.io/koji/pull-request/898

This feature allows setting rpm macros via the tag extra field. These macros
will be added to the mock configuration for the buildroot. The system
looks for extra values of the form ``rpm.macro.NAME``.

For example, to set the dist tag for a given tag, you could use a command like:

::

    $ koji edit-tag f30-build -x rpm.macro.dist=MYDISTTAG



**Per-tag configuration for module_hotfixes setting**

| PR: https://pagure.io/koji/pull-request/1524
| PR: https://pagure.io/koji/pull-request/1578

Koji now handles the field ``mock.yum.module_hotfixes`` in the tag extra.
When set, kojid will set ``module_hotfixes=0/1`` in the yum portion of the
mock configuration for a buildroot.


**Allow users to opt out of notifications**

| PR: https://pagure.io/koji/pull-request/1417
| PR: https://pagure.io/koji/pull-request/1580

This feature lets users opt out of notifications that they would otherwise
automatically recieve, such as build and tag notifications for:

- the build owner (the user who submitted the build)
- the package owner within the given tag

These opt-outs are user controlled and can be managed with the new
``block-notification`` and ``unblock-notificiation`` commands.


**Allow hub policy to match version and release**

| PR: https://pagure.io/koji/pull-request/1513


This feature adds new policy tests to match ``version`` and ``release``.
This tests are glob pattern matches.


**Allow hub policy to match build type**

| PR: https://pagure.io/koji/pull-request/1415


Koji added btypes in version 1.11 along with content generators.
Now, all builds have one or more btypes.

This change allows policies to check the btype value using the ``buildtype`` test.



**More granular admin permissions**

| PR: https://pagure.io/koji/pull-request/1454

A number of actions that were previously admin-only are now governed by
separate permissions:

    ``host``
        This permission governs most host management operations, such as
        adding, editing, enabling/disabling, and restarting.

    ``tag``
        This permission governs adding, editing, and deleting tags.

    ``target``
        This permission governs adding, editing, and deleting targets.

Koji administrators may want to consider reducing the number of users with
full ``admin`` permission.


**Option to generate separate source repo**

| PR: https://pagure.io/koji/pull-request/1273

The (non-dist) yum repos that Koji generates for building normally don't
include srpms.
An old option allowed them to be included in some cases, but they were simply
added to each repo.
Newer options have been added that instruct Koji to include them as a separate
src repo.

In the cli, the ``regen-repo`` command now accepts a ``--separate-source``
option that triggers this behavior.

In ``kojira``, the ``separate_source_tags`` option is a list of tag patterns.
Build tags that match any of these patterns will have their repos generated
with a separate src repo.



**Add volume option for dist-repo**

| PR: https://pagure.io/koji/pull-request/1327

Dist repos can now be generated on volumes other than the main one.
Use the ``--volume`` option to the ``dist-repo`` command to do so.

Generally you want the repo to be on the same volume as the rpms it will
contain.
Dist repos hard link (same volume) or copy (different volume) their rpms into
place.
Using the appropriate volume can drastically improve the efficiency, both in
generation time and space consumption.


**Minor gc optimizations**

| PR: https://pagure.io/koji/pull-request/1337
| PR: https://pagure.io/koji/pull-request/1442
| PR: https://pagure.io/koji/pull-request/1437

This change speeds up portions of garbage collection by making the
``build_references`` check lazy by default.



**Rollback errors in multiCall**

| PR: https://pagure.io/koji/pull-request/1358

If one of the calls in a multicall raises an error, then the transaction will
be rolled back to the start of that call before Koji proceeds to the next call.
This matches the behavior of normal calls more closely.

Multicalls are still handled within single database transaction.



**Support tilde in search**

| PR: https://pagure.io/koji/pull-request/1297


The tilde character is no longer prohibited in search terms.



**Remove 'keepalive' option**

| PR: https://pagure.io/koji/pull-request/1277

The ``keepalive`` setting is no longer used anywhere in koji.
It has been removed.
Koji 1.19.1 Release notes
=========================

This is a small bugfix release for significant 1.19 bugs.

Client Changes
--------------

**Fix permissions to check tag/target/host permissions**

| PR: https://pagure.io/koji/pull-request/1733

In previous release the tag/target/host permissions were not being properly checked
by the client, this change includes those checks.



System Changes
--------------

**Fix hub reporting of incorrect ownership data**

| PR: https://pagure.io/koji/pull-request/1753

This change fixes package owner listing; in previous release, information returned by ``list-pkgs``
was incorrect.


**Fix issue with listing users with old versions of Postgres**

| PR: https://pagure.io/koji/pull-request/1751

``array_remove`` was removed and replaced to support Postgres versions older than 9.4.
Koji 1.19.0 Release notes
=========================


Migrating from Koji 1.18
------------------------

For details on migrating see :doc:`../migrations/migrating_to_1.19`



Security Fixes
--------------

**GSSAPI authentication checks kerberos principal**

| PR: https://pagure.io/koji/pull-request/1419

When using GSSAPI authentication the user's kerberos principal will be checked
for their username to avoid a potential username and kerberos principal mismatch.



Client Changes
--------------

**Add user edit**

| PR: https://pagure.io/koji/pull-request/902
| PR: https://pagure.io/koji/pull-request/1701
| PR: https://pagure.io/koji/pull-request/1713

A new ``edit-user`` command and API call was added, allowing for user rename,
and changing, adding, or removing the kerberos principal of a user.


**Add remove group**

| PR: https://pagure.io/koji/pull-request/923

A new ``remove-group`` command was added, allowing the removal of a group
from a tag. It uses the existing ``groupListRemove`` API call.


**Query builds per chunks in prune-signed-builds**

| PR: https://pagure.io/koji/pull-request/1589

For bigger installations querying all builds can cause the hub to run out of memory.
``prune-signed-builds`` now queries these in 50k chunks.


**Show inheritance flags in list-tag-inheritance output**

| PR: https://pagure.io/koji/pull-request/1120

While not often used, tag inheritance can be modified with a few different options (e.g. maxdepth).
These options are shown in the ``taginfo`` display, but not the ``list-tag-inheritance`` display.
This change adds basic indicators to the latter.


**Return usage information in make-task**

| PR: https://pagure.io/koji/pull-request/1157

``make-task`` now returns usage information if no arguments are provided.


**Clarify clone-tag usage**

| PR: https://pagure.io/koji/pull-request/1623

The ``clone-tag`` help text now clarifies that the destination tag will be created
if it does not already exist.


**Add option check for list-signed**

| PR: https://pagure.io/koji/pull-request/1631

The ``list-signed`` command will now fail if no options are provided.



Library Changes
---------------

**Consolidate config reading style**

| PR: https://pagure.io/koji/pull-request/1296

Changes have been made to make configuration handling more consistent.

With this new implementation:

* ``read_config_files`` is extended with a strict option and directory support
* ``ConfigParser`` is used for all invokings except kojixmlrpc and ``kojid``
* ``RawConfigParser`` is used for ``kojid``


**list_archive_files handles multi-type builds**

| PR: https://pagure.io/koji/pull-request/1508

If ``list_archive_files`` is provided a build with multiple archive types it now correctly
handles them instead of failing.


**Disallow archive imports that don't match build type**

| PR: https://pagure.io/koji/pull-request/1627
| PR: https://pagure.io/koji/pull-request/1633

The ``importArchive`` call now refuses to proceed if the build does not have the given type.


**Add listCG RPC**

| PR: https://pagure.io/koji/pull-request/1160

``listCGs`` has been added to list new content generator records.

The purpose of this change is to make it easier for administrators to determine what
content generators are present and what user accounts have access to those.


**Add method to cancel CG reservations**

| PR: https://pagure.io/koji/pull-request/1662

The new ``CGRefundBuild`` call allows CGs to cancel build reservations, such as in the case
of a failing build.


**Allow ClientSession objects to get cleaned up by the garbage collector**

| PR: https://pagure.io/koji/pull-request/1653

This change ensures ``koji.ClientSession`` objects are destroyed once their requests are complete.


**Add missing package list check**

| PR: https://pagure.io/koji/pull-request/1244
| PR: https://pagure.io/koji/pull-request/1702

The ``host.tagBuild`` method was missing a check to ensure the package was actually listed in the
destination tag. This should now be checked as expected.


**Increase buildReferences SQL performance**

| PR: https://pagure.io/koji/pull-request/1675

The performance for ``build_references`` has been improved.


**ensuredir does not duplicate directories**

| PR: https://pagure.io/koji/pull-request/1197

``koji.ensuredir`` no longer creates duplicate directories if provided a path ending in a
forward slash.


**Warn users if buildroot uses yum instead of dnf**

| PR: https://pagure.io/koji/pull-request/1595

This change sets the mock config ``dnf_warning`` to True for buildroots using yum.


**Tag permission can be used for tagBuildBypass and untagBuildBypass**

| PR: https://pagure.io/koji/pull-request/1685

The ``tag`` permission can now be used in place of admin to call ``tagBuildBypass``
and ``untagBuildBypass``. Admin is still required to use the ``--force`` option.


**Rework update of reserved builds**

| PR: https://pagure.io/koji/pull-request/1621

This change reworks and simplifies the code that updates reserved build entries for cg imports.
It removes redundancy with checks in ``prep_build`` and avoids duplicate ``*BuildStateChange``
callbacks.


**Use correct top limit for randint**

| PR: https://pagure.io/koji/pull-request/1612

The top limit for ``randint`` has been set to 255 from 256 to prevent ``generate_token`` from
creating unneccesarily long tokens.


**Add strict option to getRPMFile**

| PR: https://pagure.io/koji/pull-request/1068

``getRPMFile`` now has a ``strict`` option, failing when the RPM or filename does not exist.


**Stricter groupListRemove**

| PR: https://pagure.io/koji/pull-request/1173
| PR: https://pagure.io/koji/pull-request/1678

``groupListRemove`` now returns an error if the provided group does not exist for the tag.


**Clarified docs for build.extra.source**

| PR: https://pagure.io/koji/pull-request/1677

The usage for ``build.extra.source`` has now been clarified in the ``getBuild`` call.


**Use bytes for debug string**

| PR: https://pagure.io/koji/pull-request/1657

This change fixes debug output for Python 3.


**Removed host.repoAddRPM call**

| PR: https://pagure.io/koji/pull-request/1680

The ``host.repoAddRPM`` call has been removed because it was unused and broken.



Web UI Changes
--------------

**Made difference between Builds and Tags sections more clear**

| PR: https://pagure.io/koji/pull-request/1676

The search page results for packages now has a clearer delineation between builds and tags.



Builder Changes
---------------

**Use preferred arch when builder provides multiple**

| PR: https://pagure.io/koji/pull-request/1684

When using ExclusiveArch for noarch builds the build task will now use the
arch specified instead of randomly picking from the arches the builder provides.

This change adds a ``preferred_arch`` parameter to ``find_arch``.


**Log insufficient disk space location**

| PR: https://pagure.io/koji/pull-request/1523

When ``kojid`` fails due to insufficient disk space, the directory which needs more
disk space is now included as part of the log message.


**Allow builder to attempt krb if gssapi is available**

| PR: https://pagure.io/koji/pull-request/1613

``kojid`` will now use ``requests_kebreros`` for kerberos authentication when available.


**Add support for new mock exit codes**

| PR: https://pagure.io/koji/pull-request/1682

``kojid`` now expects mock exit code 10 for failed builds (previously 1).


**Fix kickstart uploads for Python 3**

| PR: https://pagure.io/koji/pull-request/1618

This change fixes the file handling of kickstarts for Python 3.



System Changes
--------------

**Package ownership changes do not trigger repo regens**

| PR: https://pagure.io/koji/pull-request/1473
| PR: https://pagure.io/koji/pull-request/1643

Changing tag or package owners no longer cause repo regeneration. A new
``tag_package_owners`` table has been added for this purpose.


**Support multiple realms by kerberos auth**

| PR: https://pagure.io/koji/pull-request/1648
| PR: https://pagure.io/koji/pull-request/1696
| PR: https://pagure.io/koji/pull-request/1701

This change adds a new table ``user_krb_principals`` which tracks a list of ``krb_principals``
for each user instead of the previous one-to-one mapping. In addition:

* all APIs related to user or krb principals are changed
* ``userinfo`` of ``getUser`` will contain a new list ``krb_principals``
    * ``krb_principals`` will contain all available principals if ``krb_princs=True``
* there is a new hub option ``AllowedKrbRealms`` to indicate which realms are allowed
* there is a new client option ``krb_server_realm`` to allow krbV login to set server realm
    * Previously same as client principal realm before, supported by all clients
* ``QueryProcessor`` has a new queryOpt ``group``, which is used to generate ``GROUP BY`` section
    * By default, this feature is disabled by arg ``enable_group=False``


**Added cronjob for sessions table maintenance**

| PR: https://pagure.io/koji/pull-request/1492

The sessions table is now periodically cleaned up via script (handled by cron by default).
Without this the sessions table can grow large enough to affect Koji performance.


**Added basic email template for koji-gc**

| PR: https://pagure.io/koji/pull-request/1430

The email message koji-gc uses has been moved to ``/etc/koji-gc/email.tpl`` for
easier customization.


**Add all permissions to database**

| PR: https://pagure.io/koji/pull-request/1681

Permissions previously missing from schema have been added, including ``dist-repo``, ``host``,
``image-import``, ``sign``, ``tag``, and ``target``.


**Add new CoreOS artifact types**

| PR: https://pagure.io/koji/pull-request/1616

This change adds the new CoreOS artifact types ``iso-compressed``, ``vhd-compressed``,
``vhdx-compressed``, and ``vmdk-compressed`` to the database.


**Enforce unique content generator names in database**

| PR: https://pagure.io/koji/pull-request/1159

Set a uniqueness constraint on the content generator name in the database.
Prior to this change, we were only enforcing this in the hub application layer.
Configure this in postgres for safety.


**Fix typo preventing VM builds**

| PR: https://pagure.io/koji/pull-request/1666

This change fixes the options passed to ``verifyChecksum`` which was preventing VM builds.


**Fix verifyChecksum for non-output files**

| PR: https://pagure.io/koji/pull-request/1670

``verifyChecksum`` now accepts files under the build requires path as well as the output path.
Other paths can be added as needed.


**Set f30+ python-devel default**

| PR: https://pagure.io/koji/pull-request/1683

When installed on a Fedora 30+ host with Python 2 support, Koji will now require
``python2-devel`` instead of ``python-devel``.


**Handle sys.exc_clear for Python 3**

| PR: https://pagure.io/koji/pull-request/1642

The method ``sys.exc_clear`` does not exist in Python 3, so it has been escaped for those instances.


**Remove deprecated koji.util.relpath**

| PR: https://pagure.io/koji/pull-request/1458

``koji.util.relpath`` was deprecated in 1.16 and has been removed from 1.19.


**Remove deprecated BuildRoot.uploadDir**

| PR: https://pagure.io/koji/pull-request/1511

``BuildRoot.uploadDir`` was deprecated in 1.18 and has been removed from 1.19.


**Remove deprecated koji_cli.lib_unique_path**

| PR: https://pagure.io/koji/pull-request/1512

``koji_cli.lib_unique_path`` was deprecated in 1.17 and has been removed from 1.19.


**Deprecation of sha1_constructor and md5_constructor**

| PR: https://pagure.io/koji/pull-request/1490

``sha1_constructor`` and ``md5_constructor`` have been deprecated in favor of ``hashlib``.
Koji 1.20.1 Release notes
=========================

This is first regular minor release. We're trying new release cycle. It should
involve of regular releases (1.x.0) roughly every three months. In between there
should be one minor release (1.x.1) with bugfixes and documentation updates.

Overall policy for minor release is, that it shouldn't contain anything changing
API or any compatibility features. Neither it should touch the db schema. Client
has to be completely compatible with 1.x.0 version.

Reason to introduce them is to make quicker delivery of simple fixes to end
users. As an administrator of koji instance you're free to not/update as you
wish. There also needs to be clear update path from 1.x via 1.x.1 to 1.x+1.

Anyway, if some security or important bugfix is found anywhere during 1.x cycle,
we're going to do additional minor release addressing this problem. We will
announce it properly through standard channels (mainly koji-devel mailing list).

All changes can be found at `pagure <https://pagure.io/koji/roadmap/1.20.1/>`_.
Most important changes are listed here.

Migrating from Koji 1.20
------------------------

No special actions are needed.

Security Fixes
--------------
None

Client Changes
--------------
**Fix flags display for list-tag-inheritance**

| PR: https://pagure.io/koji/pull-request/1929

Fix of the bug with garbage output for given command.

**Don't use full listTags in list-groups call**

| PR: https://pagure.io/koji/pull-request/1967

Speed improvement - sidetags introduced large tag sets which slowed down some
calls.

Library Changes
---------------
**Always use stream=True when iterating over a request**

| PR: https://pagure.io/koji/pull-request/1993

Bug introduced in 1.20 could cause kojid running out of memory.

API Changes
-----------
None

Web UI Changes
--------------
**Display params also for malformed tasks in webui**

| PR: https://pagure.io/koji/pull-request/1488

**Display some taskinfo for deleted buildtags**

| PR: https://pagure.io/koji/pull-request/1920

**Expect, that hub is returning GM time**

| PR: https://pagure.io/koji/pull-request/1919

Information on taskinfo page could have wrong times.

Builder Changes
---------------

**Ensure that all keys in distrepo are lowered**

| PR: https://pagure.io/koji/pull-request/1982

Distrepo now should treat sigkeys as case-insensitive.

System Changes
--------------
**Improve sql speed in build_references**

| PR: https://pagure.io/koji/pull-request/1962

``build_references`` was using one of the slowest SQL calls in koji. It was
rewritten now to be faster.

Utilities Changes
-----------------

Garbage Collector
.................
None

DB Sweeper
..........
**Analyze/vacuum all affected tables**

| PR: https://pagure.io/koji/pull-request/1944

There was a mistake in vacuumed table and one another was missing.

Kojikamid
.........
None

Documentation Changes
---------------------
**Fix help message for list-groups**

| PR: https://pagure.io/koji/pull-request/1947

**Fix usage message for add-pkg**

| PR: https://pagure.io/koji/pull-request/1946

**improve search() API documentation**

| PR: https://pagure.io/koji/pull-request/1995

**Docs for kojira and koji-gc**

| PR: https://pagure.io/koji/pull-request/1935
Koji 1.20.0 Release notes
=========================

Announcement: We're going to drop python 2 support for hub and web in
koji 1.22. Please, prepare yourself for deploying python 3 versions of
these. Both are already supported and this is the next step in
retiring python 2 codebase.

All changes can be found at `pagure <https://pagure.io/koji/roadmap/1.20/>`_.
Most important changes are listed here.

Migrating from Koji 1.19
------------------------

For details on migrating see :doc:`../migrations/migrating_to_1.20`

Security Fixes
--------------
None

Client Changes
--------------
**Add basic zchunk support for dist-repo**

| PR: https://pagure.io/koji/pull-request/1743

Fixes: https://pagure.io/koji/issue/1198

The ``dist-repo`` supports new options ``--zck``, which enables createrepo's
zchunk generation, and ``--zck-dict-dir``, which indicates the directory
the builder that contains zchunk dictionaries to use.

**Add repo waiting options to the build command**

| PR: https://pagure.io/koji/pull-request/1626
| PR: https://pagure.io/koji/pull-request/1889

New options ``--wait-build`` and ``--wait-repo`` for the ``build`` command
cause the build to wait for a repo regeneration.
This is similar to using ``wait-repo`` + ``build`` in succession, except
that the repo monitoring is handled in the build task itself.

**Remove title option for livemedia-creator**

| PR: https://pagure.io/koji/pull-request/1781

livemedia-creator dropped ``--title`` option, so we are.

**Add --disabled option to list-hosts command**

| PR: https://pagure.io/koji/pull-request/1738

This option is simply an alias for the existing ``--not-enabled`` option.

**Unify return values for permission denied**

| PR: https://pagure.io/koji/pull-request/1785

Some places were using ``print`` + ``return 1``, some `parser.error` calls.
Let's unify it to ``parser.error``.

Library Changes
---------------
**Raise error when we have not configuration**

| PR: https://pagure.io/koji/pull-request/1767
| PR: https://pagure.io/koji/pull-request/1787

Previously, Koji would proceed with only the coded defaults,
which is no longer sensible.

**Sanity check on remotely opened RPMs**

| PR: https://pagure.io/koji/pull-request/1829

Sometimes RPMs are not downloaded correctly into buildroot and it results in
weird errors. A simple check was added to detect corruption of downloaded files.

**Include profile name in parsed config options**

| PR: https://pagure.io/koji/pull-request/1525

Fix behaviour to be in line with docs examples.

**Make rpm import optional in koji/__init__.py**

| PR: https://pagure.io/koji/pull-request/1773
| PR: https://pagure.io/koji/pull-request/1795

``koji/__init__.py`` is being used more and more often in virtualenv. As rpm is
always the pain here and most users don't need those specific functions, we can
make it optional (and require only on spec level). Distribution via PyPi will be
less painful.


API Changes
-----------

**Default krb_princs value changed to True in getUser**

| PR: https://pagure.io/koji/pull-request/1872

This argument was added in PR #1648, with a default value of ``False``.  It is
used to control whether the ``krb_principals`` field is included in the result of
``getUser``.

**Drop buildMap API call**

| PR: https://pagure.io/koji/pull-request/1755

It was designed for GC, but it is not used anymore.

**New addArchiveType API call**

| PR: https://pagure.io/koji/pull-request/1149

Adds a new hub method for inserting new archivetype records.

**Raise GenericError on existing build reservation in in CGInitBuild**

| PR: https://pagure.io/koji/pull-request/1893

Previously a database exception was propagated. Now, it is raising a more informative
exception.


Web UI Changes
--------------
**Browsable api**

| PR: https://pagure.io/koji/pull-request/1821

The web ui now offers API introspection similar to the ``koji list-api`` command.

**Cluster health info page**

| PR: https://pagure.io/koji/pull-request/1551

New web page showing current usage of build cluster.

**Show build link(s) on buildContainer task page**

| PR: https://pagure.io/koji/pull-request/284

This is a temporary solution to the problem of connecting the builds and tasks
generated by the
`containerbuild plugin <https://github.com/containerbuildsystem/koji-containerbuild>`.

**Human-friendly file sizes in taskinfo page**

| PR: https://pagure.io/koji/pull-request/1820


Builder Changes
---------------

**Provide for passing credentials to SRPMfromSCM**

| PR: https://pagure.io/koji/pull-request/1640

Builder's conf can now contain ``scm_credentials_dir`` option, where can be
stored authentication certificates or other data for use inside the mock when
building SRPMs for fetching data from authenticated SCMs.

**Log kernel version used for buildroot**

| PR: https://pagure.io/koji/pull-request/821
| PR: https://pagure.io/koji/pull-request/1850

**Use --update for dist-repos if possible**

| PR: https://pagure.io/koji/pull-request/1037

Improves speed of new distrepos.

**Don't send notifications in case of deleted tag**

| PR: https://pagure.io/koji/pull-request/1380

In some cases (sidetags) tag can be deleted before untag notifications are sent,
so don't send them if tag is already deleted.

**Check existence of maven symlink**

| PR: https://pagure.io/koji/pull-request/1742

In recent Fedora's maven is alternatives symlink. Original check now failed even
if maven was installed.


System Changes
--------------

**QueryProcessor: fix countOnly for group sql**

| PR: https://pagure.io/koji/pull-request/1845

WebUI returned an error on Users tab after multiple kerberos realms per user
were introduced.

**Limit distRepo tasks per tag**

| PR: https://pagure.io/koji/pull-request/1869
| PR: https://pagure.io/koji/pull-request/1912

Introduces ``distrepo.cancel_others`` extra flag for tags. If enabled, new
distRepo task will cancel previous non-finished ones leaving only new one.

**Fix CGRefundBuild to release build properly**
| PR: https://pagure.io/koji/pull-request/1853

Fixes for refunding failed/cancelled build.

**Use BulkInsertProcessor for hub mass inserts**

| PR: https://pagure.io/koji/pull-request/1714
| PR: https://pagure.io/koji/pull-request/1847

Speed up mass inserts.

**Use comma delimiter for allowed_methods**

| PR: https://pagure.io/koji/pull-request/1745

The ``allowed_methods`` configuration option for the ``save_failed_tree`` plugin
now expects comma-separated values, as the example config indicates.

**Fix issue with listing users and old versions of Postgres**

| PR: https://pagure.io/koji/pull-request/1751

Utilities Changes
-----------------
**Add koji-gc/kojira/koji-shadow to setup.py**

| PR: https://pagure.io/koji/pull-request/1428

Koji utilities are now installlable from PyPi.

Garbage Collector
.................
**Speedup untagging/moving to trashcan**

| PR: https://pagure.io/koji/pull-request/1873

Rewrite of how koji-gc handles untagging. Multicalls are used now and some
speedup of related API calls is also included.

DB Sweeper
..........
**New options to clean database**

| PR: https://pagure.io/koji/pull-request/1824

Last release introduced new tool ``koji-sweep-db`` which is used to clean the
database. Few new options were added now like cleaning scratch builds, CG
reservations, notification tasks or unused buildroots.

Note, that these new features are more technical preview. You need to use
``--force`` flag to run them for a good reason. They can a) take insane time to
finish b) remove data you never wanted to delete.  Always test these commands in
safe environment, before running them in production.

Cleaning sessions and reservations are still safe and they are primary goals of
the script.

Kojikamid
.........
**A few fixes for kojikamid**

| PR: https://pagure.io/koji/pull-request/1837

kojikamid (the daemon that runs in VMs) needs a few updates to be consistent
with changes to the the Koji data model, and Python 3 compatibility.

Documentation Changes
---------------------
Lot of documentation was added in last release in API and also in docs pages.


**Documentation**

| PR: https://pagure.io/koji/pull-request/1716
| PR: https://pagure.io/koji/pull-request/1794
| PR: https://pagure.io/koji/pull-request/1801
| PR: https://pagure.io/koji/pull-request/1802
| PR: https://pagure.io/koji/pull-request/1803
| PR: https://pagure.io/koji/pull-request/1804
| PR: https://pagure.io/koji/pull-request/1805
| PR: https://pagure.io/koji/pull-request/1806
| PR: https://pagure.io/koji/pull-request/1817
| PR: https://pagure.io/koji/pull-request/1823
| PR: https://pagure.io/koji/pull-request/1875
| PR: https://pagure.io/koji/pull-request/1917

**API**

| PR: https://pagure.io/koji/pull-request/1799
| PR: https://pagure.io/koji/pull-request/1832
| PR: https://pagure.io/koji/pull-request/1868

**CLI**

| PR: https://pagure.io/koji/pull-request/1775
| PR: https://pagure.io/koji/pull-request/1918
Koji 1.21.1 Release notes
=========================

All changes can be found at `pagure <https://pagure.io/koji/roadmap/1.21.1/>`_.
Most important changes are listed here.

Migrating from Koji 1.21
------------------------

No special actions are needed.

Security Fixes
--------------
None

Client Changes
--------------

**Don't use listTagged(tag, *) for untag-build**

| PR: https://pagure.io/koji/pull-request/2038

Simple change which speeds up ``untag-build`` call.

**Fix list-signed --tag memory issues**

| PR: https://pagure.io/koji/pull-request/2103

Another performance improvement - rpms are filtered on hub's side on client's.

**Fix variable name**

| PR: https://pagure.io/koji/pull-request/2224

Koji buildinfo could have failed on missing content generator fiels.

**Fix un/lock-tag permission handling**

| PR: https://pagure.io/koji/pull-request/2223

Bug caused unlock-tag to fail on missing permission for admin.

Library changes
---------------

**don't decode signature headers**

| PR: https://pagure.io/koji/pull-request/2268

Some rpm signature headers were not handled properly.

Hub Changes
-----------

**Admin can force tag now**

| PR: https://pagure.io/koji/pull-request/2203

Previously admin cannot force policy for tagging. Now admins can override the
policy and force tagging.

Utilities Changes
-----------------

Garbage Collector
.................

**fix query order**

| PR: https://pagure.io/koji/pull-request/2279

New ``queryHistory`` method doesn't sort output, so sorting on client side is
needed. It is returned in DB preferred order which seems to work for PG, but
better safe than sorry.

**koji-gc: various typos in maven path**

| PR: https://pagure.io/koji/pull-request/2153

This is the important regression fix for bug which caused koji-gc to fail in
many cases.

**koji-gc: test existence of trashcan tag**

| PR: https://pagure.io/koji/pull-request/2211

We assumed that ``trashcan`` tag exists. If it is not the case GC will notice it
relatively late. This is additional check during the start.

Kojira
......

**kojira: use cached getTag for external repos**

| PR: https://pagure.io/koji/pull-request/2157

Use cached values for external repos checks.

Documentation Changes
---------------------

**Links to copr builds**

| PR: https://pagure.io/koji/pull-request/2248


**Fix sidetag enablement typo**

| PR: https://pagure.io/koji/pull-request/2178


**Document removeExternalRepoFromTag arguments**

| PR: https://pagure.io/koji/pull-request/2174


**Extend docs for --before/--after options**

| PR: https://pagure.io/koji/pull-request/2245


**API docs**

| PR: https://pagure.io/koji/pull-request/2241
| PR: https://pagure.io/koji/pull-request/2242


**Document addExternalRepoToTag arguments**

| PR: https://pagure.io/koji/pull-request/2158


**Remove obsoleted note**

| PR: https://pagure.io/koji/pull-request/2194

**Update for mod_auth_gssapi configuration**

| PR: https://pagure.io/koji/pull-request/2141
Koji 1.21.0 Release notes
=========================

Announcement: We're going to drop python 2 support for hub and web in koji 1.22.
Please, prepare yourself for deploying python 3 versions of these. Both are
already supported and this is the next step in retiring python 2 codebase.

All changes can be found at `pagure <https://pagure.io/koji/roadmap/1.21/>`_.
Most important changes are listed here.

Migrating from Koji 1.20
------------------------

For details on migrating see :doc:`../migrations/migrating_to_1.21`

Security Fixes
--------------
None

Client Changes
--------------
**Add --no-delete option to clone-tag**

| PR: https://pagure.io/koji/pull-request/1385

``clone-tag`` command was enhanced to produce 'copy' operation without deleting
what is in the target tag. See PR for detailed semantics, as it could be
confusing a bit.

**display merge mode for external repos**

| PR: https://pagure.io/koji/pull-request/2097

Merge modes are now listed in taginfo command (and also in web ui)

**koji download-build resuming downloads**

| PR: https://pagure.io/koji/pull-request/2080

``download-build`` could often break for bigger builds. Resuming truncated
download after relaunch is now default behaviour.

**add-host work even if host already tried to log in**

| PR: https://pagure.io/koji/pull-request/2042

Previously, if builder contacted before its user was created in db, it was hard
to fix it. Now, it could be forced via cli's ``--force``.

**Allow to skip SRPM rebuild for scratch builds**

| PR: https://pagure.io/koji/pull-request/2083

Rebuilding SRPMs doesn't make much sense in most of scratch builds. There is an
option ``--no-rebuild-srpm`` which can be used to skip this step. Note, that it
doesn't work for regular builds, which needs to adhere to policy set by
rel-engs.

**Deprecating list-tag-history and tagHistory**

| PR: https://pagure.io/koji/pull-request/938

These commands are superseded by ``list-history`` resp. ``queryHistory`` and
will be removed in near future.

**Add detail about known koji signatures to buildinfo**

| PR: https://pagure.io/koji/pull-request/2016

If koji knows about any signatures, they are now also printed.

**deprecation of krb_login**

| PR: https://pagure.io/koji/pull-request/1992

gssapi_login should be now used wherever possible

Library Changes
---------------

**Remove deprecated functions**

| PR: https://pagure.io/koji/pull-request/1984
| PR: https://pagure.io/koji/pull-request/2001

md5/sha1 constructors and cgi.escape functions were removed.

API Changes
-----------

**editTagExternalRepo is able to set merge_mode**

| PR: https://pagure.io/koji/pull-request/2051

Removing and re-adding external repo is no more needed if user just needs to
change merge strategy.

**Remove debugFunction API**

| PR: https://pagure.io/koji/pull-request/1863

Removed deprecated call

Builder Changes
---------------

**make xz options configurable**

| PR: https://pagure.io/koji/pull-request/2028

xz compression for images now can be configured on builder level. It can be
tuned accordingly to CPU/memory ratio available.

**Delete oldest failed buildroot when there is no space**

| PR: https://pagure.io/koji/pull-request/2082

If there is no space on builder, we try to delete buildroots from oldest to
newest. It could be harder to debug some failed builds, as those data can be
already deleted, on the other hand, builders will not refuse to work due to lack
of space.

System Changes
--------------

**new policy for dist-repo**

| PR: https://pagure.io/koji/pull-request/2081

Previously only users with ``dist-repo`` permission were allowed to run it. Now,
there could be a policy defined, mostly based on tag or user names.

**Add 'target' policy**

| PR: https://pagure.io/koji/pull-request/1058

We used it before, but with generic tests like ``match``. Now we have proper
``target`` policy test.

**always set utf8 pg client encoding**

| PR: https://pagure.io/koji/pull-request/2105

We're now forcing utf8 client encoding for database connection.

**Limit final query by prechecking buildroot ids**

| PR: https://pagure.io/koji/pull-request/2074

Significant performance improvement for ``query_buildroots``.

**use real time for events**

| PR: https://pagure.io/koji/pull-request/2068

Events now should be created with real-world time, not with the beginning of
transaction. It could have led to non-clear history in some cases, it should be
better now.

**log --force usage by admins**

| PR: https://pagure.io/koji/pull-request/2019

Using ``--force`` to override policies is now logged.

**Add smtp authentication support**

| PR: https://pagure.io/koji/pull-request/692

SMTP authentication is now available in kojid and koji-gc.

Plugins
-------

**Sidetag plugin is now part of koji**

| PR: https://pagure.io/koji/pull-request/1956
| PR: https://pagure.io/koji/pull-request/2006
| PR: https://pagure.io/koji/pull-request/2004

We've integrated sidetag plugin to koji, so we can add more integrated
functionality to it.

**allow debuginfo for sidetag repos**

| PR: https://pagure.io/koji/pull-request/1990

sidetag repos can now contain debuginfo packages (``--debuginfo`` option for
``add-sidetag`` command).

**New call editSideTag**

| PR: https://pagure.io/koji/pull-request/2054

New API call allowing users of sidetags to modify certain values (debuginfo,
package lists).

**Emit user in PackageListChange messages**

| PR: https://pagure.io/koji/pull-request/1059

protonmsg now sends also user name and id.

**limit size of extra field in proton msgs**

| PR: https://pagure.io/koji/pull-request/2047

``extra`` field can be omitted from proton message if it exceeds configured
threshold. Some content generators can create very big ``extra`` data which
needn't to be sent via message bus and can be queried on demand via API.

Utilities Changes
-----------------

Garbage Collector
.................

**file locking for koji-gc**

| PR: https://pagure.io/koji/pull-request/1333

As GC can run for long periods of time, ensuring, that there is only one
instance running is worthwile. ``--lock-file`` and ``--exit-on-lock``

Kojira
......

**kojira monitors external repos changes**

| PR: https://pagure.io/koji/pull-request/516

External repositories are now monitored and kojira will trigger ``newRepo``
tasks when their content changed.

**reverse score ordering for tags**

| PR: https://pagure.io/koji/pull-request/2022

Fixed bug which regenerated repositories in least-important-first order.

Documentation Changes
---------------------

Lot of documentation was added in last release in API and also in docs pages.

**Documentation**

| PR: https://pagure.io/koji/pull-request/2057
| PR: https://pagure.io/koji/pull-request/2129
| PR: https://pagure.io/koji/pull-request/2128
| PR: https://pagure.io/koji/pull-request/2078
| PR: https://pagure.io/koji/pull-request/2079
| PR: https://pagure.io/koji/pull-request/2034
| PR: https://pagure.io/koji/pull-request/1975

**API**

| PR: https://pagure.io/koji/pull-request/1987
| PR: https://pagure.io/koji/pull-request/2000

**CLI**

| PR: https://pagure.io/koji/pull-request/2071
Koji 1.22.1 Release notes
=========================

All changes can be found at `pagure <https://pagure.io/koji/roadmap/1.22.1/>`_.
Most important changes are listed here.

Migrating from Koji 1.22
------------------------

No special actions are needed.

Security Fixes
--------------
None

Library changes
---------------
**Fix time formatting for timezone values**

| PR: https://pagure.io/koji/pull-request/2409

Some datetime values returned from hub were not properly parsed which resulted
in failing CLI/web. We've replaced it with GMT timestamps internally, so we are
more sure about their proper timezones.

Hub Changes
-----------
**ensure that cursors are closed in QueryProcessor.iterate()**

| PR: https://pagure.io/koji/pull-request/2436

In some cases (especially ``hastag`` policy in combination with ``clone-tag``)
there were allocated db cursors but not freed.

**stricter config file permissions**

| PR: https://pagure.io/koji/pull-request/2474

Hub and web config files contains sensitive values. We've made permissions
stricter by default and encourage existing users to review theirs.

Builder Changes
---------------
**builder: handle btrfs subvolumes in ApplianceTask**

| PR: https://pagure.io/koji/pull-request/2365

When using btrfs, the / mountpoint can be associated with a subvolume; if that's
the case, return the btrfs partition as the root device. Note that this
implementation assumes there's only one btrfs partition defined in kickstart.

**fix extra-boot-args option**

| PR: https://pagure.io/koji/pull-request/2452

``bootloader append`` directive in kickstart wasn't properly passed to lorax.

API Changes
-----------
**editTag: make compat perm_id option an alias for perm**

| PR: https://pagure.io/koji/pull-request/2409

It is a backward compatible change.


Documentation Changes
---------------------
**setting rpm macros for build tags**

| PR: https://pagure.io/koji/pull-request/2410


**more info about permission system**

| PR: https://pagure.io/koji/pull-request/2415

**migration note regarding dropped krb configuration options**

| PR: https://pagure.io/koji/pull-request/2427
Koji 1.22.0 Release notes
=========================

Important: python 2 support for hub and web have been dropped in koji 1.22,
meanwhile CLI and builder are still supporting python2. Please prepare your hub
and web service for python3 if you are going to upgrade them to koji 1.22.

All changes can be found at `pagure <https://pagure.io/koji/roadmap/1.22/>`_.
Most important changes are listed here.


Migrating from Koji 1.21/1.21.1
-------------------------------

For details on migrating see :doc:`../migrations/migrating_to_1.22`


Security Fixes
--------------

None


Client Changes
--------------

**Output extra['rpm.macro.*'] to mock-config**

| PR: https://pagure.io/koji/pull-request/2255

The ``mock-config`` command honors 'rpm.macro.*' options in tag's extra config now.

**--ca option has been deprecated**

| PR: https://pagure.io/koji/pull-request/2182
| PR: https://pagure.io/koji/pull-request/2246

This option is deprecated for a while and not used internally. We added the
deprecation warning and will finally remove it in 1.24.
Notes: It is deprecated in koji-gc as well.

**Flush stdout during watch-logs**

| PR: https://pagure.io/koji/pull-request/2228

Calling ``flush()`` immediately to display the output faster for PY3.

**Do not try unnecessary authentication**

| PR: https://pagure.io/koji/pull-request/2228

In some CLI commands we used ``active_session()`` which will try its best to
login, but it is not necessary. Now, we only ensure the connection without
authentication.

**Unify --debug options**

| PR: https://pagure.io/koji/pull-request/2085

The cli accepts a global ``--debug`` option before the command name.
Some commands accepted a separate ``--debug`` option local to the command,
which was confusing.
Now these commands take their cue from the global option.
The local option is still accepted for backwards compatibility, though
it has been hidden in the help output.

The following commands were affected:

* ``prune-sigs``
* ``list-signed``
* ``list-tag-history``
* ``list-history``

**New option --wait for download-task**

| PR: https://pagure.io/koji/pull-request/2346

This is a UE enhancement to let the command be able to wait for the tasks to be
finished as the same as the behavior of ``build`` command.

**Fix image-build-indirection --wait**

| PR: https://pagure.io/koji/pull-request/2347

Previously, the ``image-build-indirection`` command accepted the ``--wait``
option, but did not honor it.
This oversight has been fixed.

**Fix event option handling in clone-tag**

| PR: https://pagure.io/koji/pull-request/2364

The ``getTag()`` call for fetching source tag info in ``clone_tag`` didn't use event
before. Now, it does.


Library Changes
---------------

**Correctly identify "hostname doesn't match" errors**

| PR: https://pagure.io/koji/pull-request/2266

"hostname doesn't match" can be identified as a certificate error, so that
client will not retry the request.

**openRemoteFile retries and checks downloaded content**

| PR: https://pagure.io/koji/pull-request/2254

Sometimes we hit a problem with incorrect downloads caused by various
malfunctions, like cache, filesystem, network, etc. Now, in
``openRemoteFile``, we are going to

* compare http's ``Content-Length`` header with the data we really downloaded
* check the rpm header is valid if the file is an RPM
* do 3 times retries if it fails


API Changes
-----------

**filterResults and countAndFilterResults raise GenericError**

| PR: https://pagure.io/koji/pull-request/2150

API ``filterResults`` and ``countAndFilterResults`` now raise
``koji.GenericError`` instead of ``xmlrpc.client.Fault`` when method's keyword
argument is not expected.

**Deprecation of host.getTask call**

| PR: https://pagure.io/koji/pull-request/2238
| PR: https://pagure.io/koji/pull-request/2264

This host API will be finally removed in 1.23

**Optimizations to the listBuildroots call**

| PR: https://pagure.io/koji/pull-request/2299
| PR: https://pagure.io/koji/pull-request/2303
| PR: https://pagure.io/koji/pull-request/2301

For the optimization purpose, the ``listBuildroots`` API call avoids
unnecessary checks when the return will be empty.

Additionally, the call avoids some table joins that can slow down the queries
in some cases.
As a result, the return value will no longer include the ``is_update`` field
when querying by ``rpmID``.

**Disable notifications by default in [un]tagBuildBypass calls**

| PR: https://pagure.io/koji/issue/2292

The ``notify`` option to the ``tagBuildBypass`` and ``untagBuildBypass`` now defaults to False.
Tools that wish to generate email notifications will need to explicitly pass ``notify=True``.

**Fix a typo in the error message of getChangelogEntries**

| PR: https://pagure.io/koji/pull-request/2338

**A new option - pattern for listTags call**

| PR: https://pagure.io/koji/pull-request/2320
| PR: https://pagure.io/koji/pull-request/2348
| PR: https://pagure.io/koji/pull-request/2387

This option is a GLOB match pattern for the name of tag. You can now directly
call ``session.listTags(pattern='prefix-*-postfix')`` for example, to filter the
result list on server side. The ``list-tags`` command tries its best to call it with
``pattern`` as well.


Builder Changes
---------------

**Koji now supports Mock's bootstrap chroot and image**

| PR: https://pagure.io/koji/pull-request/2166
| PR: https://pagure.io/koji/pull-request/2212
| PR: https://pagure.io/koji/pull-request/2372
| PR: https://pagure.io/koji/pull-request/2328

Koji now supports Mock's ``--bootstrap-chroot`` and ``--bootstrap-image``
options. See:

* `Bootstrap chroot <https://github.com/rpm-software-management/mock/wiki/Feature-bootstrap>`_
* `Container for bootstrap <https://github.com/rpm-software-management/mock/wiki/Feature-container-for-bootstrap>`_

For the configuration on koji, please refer to :doc:`../using_the_koji_build_system`.
The bootstrap buildroot will be pruned automatically by kojid as the same as the
normal buildroot.

**Pass bootloader append option to livemedia builds**

| PR: https://pagure.io/koji/pull-request/2262

Koji is now able to pass ``--extra-boot-args --append="bootloader --append"``
options to ``livemedia-creator`` tool for livemedia builds.

**Per-tag environment variables in Mock's buildroot**

| PR: https://pagure.io/koji/pull-request/2064

Now, you can set ``rpm.env.*`` in build tag's ``extra`` to specify environment
variables in mock's buildroot. See :doc:`../using_the_koji_build_system`.

**Support specific per-settings for Mock's sign plugin**

| PR: https://pagure.io/koji/pull-request/1932
| PR: https://pagure.io/koji/pull-request/2337

We are now providing ``mock.plugin_conf.sign_enable``,
``mock.plugin_conf.sign_opts.cmd`` and ``mock.plugin_conf.sign_opts.opts`` in
build tag's ``extra`` for enabling and configuring the sign plugin of mock. For
more details, see :doc:`../using_the_koji_build_system`.

**Per-tag settings of yum's depsolver policy for Mock**

| PR: https://pagure.io/koji/pull-request/1932

``mock.yum.best=0/1`` is available in tag's extra config for the corresponding
setting of mock config.

**Use mergerepo_c for all merge modes**

| PR: https://pagure.io/koji/pull-request/2376

As ``mergerepo_c`` has supported ``simple`` mode since 0.13.0, we now use it on
python3 or ``use_createrepo_c=True`` kojid for repo creation. And as `issues/213
<https://github.com/rpm-software-management/createrepo_c/issues/213>`_ of
``createrepo_c`` has been fixed in 0.15.11, we also append ``--arch-expand`` on
demand. Therefore, koji are now able to use ``mergerepo_c`` for all 3 modes: koji,
simple, bare. Nevertheless, we are still providing ``mergerepos`` scripts for
python2.

**Turn off dnf_warning in mock.cfg**

| PR: https://pagure.io/koji/pull-request/2353

In `PR #1595 <https://pagure.io/koji/pull-request/1595>`_, we set
``dnf_warning=True`` when we started to add this configuration. But since Mock
2.0, ``bootstrap_chroot`` is set to ``True`` by default, we need to set
``dnf_warning`` to ``False`` accordingly. For the details, please refer to
`issue #2026 <https://pagure.io/koji/issue/2026>`_.

**BuildSRPMFromSCMTask: Support auto-selecting a matching specfile name**

| PR: https://pagure.io/koji/pull-request/2257

When building SRPM from SCM, if there are more than one ``*.spec`` found in root
directory, or ``support_rpm_source_layout=yes`` in ``/etc/kojid/kojid.conf`` and
there are more than one ``*.spec`` found in ``SPECS`` directory, the builder is
going to use the specfile with the SCM repo's name in root or ``SPECS`` dir.

**Pass buildroot to preSCMCheckout and postSCMCheckout where applicable**

| PR: https://pagure.io/koji/pull-request/2123

The ``preSCMCheckout`` and ``postSCMCheckout`` callbacks for kojid now include
a ``buildroot`` field that provides access to the internal ``BuildRoot``
object, when such an object is available.
This change impacts ``BuildMavenTask``, ``WrapperRPMTask``, ``ImageTask`` and
``BuildSRPMfromRPMTask``.
The current exceptions are ``OzImageTask`` and ``BuildIndirectionImageTask``,
which do not use this type of buildroot.

Any plugins that use this field should be aware that the behavior of this class
may change across releases.


Web UI Changes
--------------

**A new repoinfo page**

| PR: https://pagure.io/koji/pull-request/2193

The new page displays basic information of a normal repo, linked by the repo id
on taskinfo and buildrootinfo page.


Win Builder Changes
-------------------

**Clone mac address via xml**

| PR: https://pagure.io/koji/pull-request/2290

We've hit a problem that while VM is being cloned, the mac address cloning is
refused and a new one is assigned instead. We are now using the xml file for mac
address setup.


System Changes
--------------

**Drop python2 support for hub and web**

| PR: https://pagure.io/koji/pull-request/2218
| PR: https://pagure.io/koji/pull-request/2342

Finally, python2 support for hub and web have been dropped in this release.

**Drop krbV support**

| PR: https://pagure.io/koji/pull-request/2244
| PR: https://pagure.io/koji/pull-request/2151

``krbV`` support has been finally removed from this release. For more information, please refer to
:ref:`migration_krbv`.

**Use requests_gssapi for GSSAPI authentication**

| PR: https://pagure.io/koji/pull-request/2244
| PR: https://pagure.io/koji/pull-request/2401

``requests_gssapi`` is supported in this release. In all of the components we provide, we now try to
use ``request_gssapi`` at first, if it isn't installed, fallback to ``requests_kerberos`` then.

**DB: Use timestamps with timezone**

| PR: https://pagure.io/koji/pull-request/2237
| PR: https://pagure.io/koji/pull-request/2366

We have updated all our timestamp fields to include timezone.
This prevents time inconsistencies when the database has a timezone setting
other than UTC.

**DB: Update sessions_active_and_recent index**

| PR: https://pagure.io/koji/pull-request/2334

We have adjusted the ``sessions_active_and_recent`` index so that the planner
will actually use it.

**Log tracebacks for multicall**

| PR: https://pagure.io/koji/pull-request/2225

The exceptions inside multicall were not logged before. These tracebacks will
benefit us for debugging purpose, as we are often using multicall more and more.

**Fix build_notification crashing caused by recipients check**

| PR: https://pagure.io/koji/pull-request/2308
| PR: https://pagure.io/koji/pull-request/2309

This change fixes an inconsistency in the function where it would return
``None`` instead of an empty list as expected.

**Allow packagelist changes with 'tag' permission by the default policy**

| PR: https://pagure.io/koji/pull-request/2275

The ``tag`` permission was introduced in version 1.18 as part of an effort to
make admin permissions more granular.
This permission now grants access to make package list changes for tags
via the default ``package_list`` policy.

**Improve race condition for getNextRelease call and images**

| PR: https://pagure.io/koji/pull-request/2263

It was possible to meet the race condition in the old logic of image building.
We are now calling ``get_next_release()`` in the ``initImageBuild`` call if there is
ino release passed in, rather than calling ``getNextRelease`` in the ImageBuild
task individually. This would notably reduce the possibility of the race
condition.

**Replace MD5 with SHA-256 in most places**

| PR: https://pagure.io/koji/pull-request/2317

Koji should work on the FIPS enabled system where MD5 is disabled for security
reason. We are now using SHA-256 to replace MD5 for web token and file uploading,
but only keeping MD5 for RPM file processing.

**Remove "GssapiSSLonly Off" option**

| PR: https://pagure.io/koji/pull-request/2162

We have removed the ``GssapiSSLonly`` option from our example httpd
configuration.
It was previously shown in the example, set to ``Off``.
This is also the default in mod_auth_gssapi, but *it is not the recommended
setting*.
For more information, see `mod_auth_gssapi doc
<https://github.com/gssapi/mod_auth_gssapi#gssapisslonly>`_

**Remove "GssapiLocalName Off" option**

| PR: https://pagure.io/koji/pull-request/2351
| PR: https://pagure.io/koji/pull-request/2358

We have also removed the ``GssapiLocalName`` option from our example httpd
configurations.
Similar to the above, our example setting was already the default.

**Provide task-based data to volume policy**

| PR: https://pagure.io/koji/pull-request/2306

For builds with associated tasks, more information is now available to the volume policy.
In particular, the ``buildtag`` policy test should work for such builds.

Note that some builds (e.g. content generator builds and other imported builds) do not
have associated tasks.

For more information on hub policies, see :doc:`../defining_hub_policies`.

**Honor volume policy in host.importImage**

| PR: https://pagure.io/koji/pull-request/2359

This fixes a bug where an underlying function as ignoring the volume policy result.


Plugins
-------

sidetag
.......

**listSideTags also returns user info**

| PR: https://pagure.io/koji/pull-request/2132

We now provide an easier way to find the owner of sidetags

**Give koji admins the permission to operate sidetags**

| PR: https://pagure.io/koji/pull-request/2322


Users with the ``admin`` permission can now manage sidetags even if they are
not their own.

**Fix is_sidetag_owner and is_sidetag policy tests**

| PR: https://pagure.io/koji/pull-request/2326

These policy tests would previously always return a null result.
Now they return the correct one.


Utilities Changes
-----------------

Garbage Collector
.................

**Systemd units for koji-gc**

| PR: https://pagure.io/koji/pull-request/2199

The systemd units(service and timer) are now installed by default.

**Allow specifying CC and BCC address for email notifications**

| PR: https://pagure.io/koji/pull-request/2195
| PR: https://pagure.io/koji/pull-request/2278

New options ``cc_addr``, ``bcc_addr`` in config file, or CLI options
``--cc-addr``, ``--bcc-addr`` are available now.

**Set smtp_host to localhost by default**

| PR: https://pagure.io/koji/pull-request/2253

The previous the default value was ``None``, which would cause failures
if notifications were enabled.

Kojira
......

**New option: queue_file for task queue monitoring**

| PR: https://pagure.io/koji/pull-request/2024

With a writable filepath specified, the state information will be saved into
this file in each cycle. For more information, please refer to
:ref:`utils-kojira`.

**Use mtime of repo directory to determine the age**

| PR: https://pagure.io/koji/pull-request/2154

Kojira should now do a better job of determining the age of a repo at startup.

**Fix logic detecting directories for pruneLocalRepos**

| PR: https://pagure.io/koji/pull-request/2323

The condition was opposite before.

**Totally drop SysV support**

| PR: https://pagure.io/koji/issue/2171

Thus, we won't provide kojira service on <=EL6 platform.

**Repo deletion within thread**

| PR: https://pagure.io/koji/pull-request/2340
| PR: https://pagure.io/koji/pull-request/2397

Kojira are now able to delete repos in a separated thread.
The old ``delete_batch_size`` option is no longer used and has been removed.

koji-sidetag-cleanup
....................

**Set the shebang to /usr/bin/python2 on RHEL<=7**

| PR: https://pagure.io/koji/pull-request/2209

Otherwise, the build will fail on RHEL<=7.

koji-sweep-db
.............

**use "Type=oneshot" for systemd**

| PR: https://pagure.io/koji/pull-request/2187

``oneshot`` is the appropriate choice for periodic cleanup scripts, see `systemd
docs
<https://www.freedesktop.org/software/systemd/man/systemd.service.html#Type=>`_.
Koji 1.23.1 Release notes
=========================

All changes can be found at `pagure <https://pagure.io/koji/roadmap/1.23.1/>`_.
Most important changes are listed here.

Migrating from Koji 1.23
------------------------

No special actions are needed.

PR#2579: Install into /usr/lib rather than /usr/lib64/

Security Fixes
--------------

**web: XSS vulnerability**

| PR: https://pagure.io/koji/pull-request/2652

CVE-2020-15856 - Web interface can be abused by XSS attack. Attackers can supply
subversive http links containing malicious javascript code. Such links were not
controlled properly, so attackers can potentially force users to submit actions
which were not intended. Some actions which can be done via web UI can be
destructive, so updating to this version is highly recommended.

System Changes
--------------
**Revert "timezones for py 2.7"**

| PR: https://pagure.io/koji/pull-request/2569

We've returned some behaviour which prevented time operations on py 2.7

Library Changes
---------------
**lib: better argument checking for eventFromOpts**

| PR: https://pagure.io/koji/pull-request/2517

``eventFromOpts`` can now properly parse ``after`` and ``before`` arguments.

Hub Changes
-----------
**hub: use CTE for build_references**

| PR: https://pagure.io/koji/pull-request/2567

This should improve kojira's performance in some cases.

Builder Changes
---------------
**mergerepo uses workdir as tmpdir**

| PR: https://pagure.io/koji/pull-request/2547

Until now mergerepo used /tmp instead of workdir. It could lead to space
exhaustion if there is not enough space there. Workdir gets cleaned more often.

Web Changes
-----------
**disable links to deleted tags**

| PR: https://pagure.io/koji/pull-request/2558

**Only redirect back to HTTP_REFERER if it points to kojiweb**

| PR: https://pagure.io/koji/pull-request/2504

Utilities Changes
-----------------
**kojira: don't expire ignored tags with targets**

| PR: https://pagure.io/koji/pull-request/2548

Ignored tags' repos were expired even in case when they've had targets. It is
fixed now and ignored tags are really ignored.

**kojira: cache external repo timestamps by arch_url**

| PR: https://pagure.io/koji/pull-request/2533

Fix of bug which could have missed some split repositories updates.

Documentation Changes
---------------------

**assign multicall to "m" in code example**

| PR: https://pagure.io/koji/pull-request/2593

**api docs**

| PR: https://pagure.io/koji/pull-request/2509

**python support matrix**

| PR: https://pagure.io/koji/pull-request/2528
Koji 1.23.0 Release notes
=========================

All changes can be found at `pagure <https://pagure.io/koji/roadmap/1.23/>`_.
Most important changes are listed here.


Migrating from Koji 1.22/1.22.1
-------------------------------

For details on migrating see :doc:`../migrations/migrating_to_1.23`


Security Fixes
--------------

None


Client Changes
--------------

**cli: clone-tag fails on failed multicalls**

| PR: https://pagure.io/koji/pull-request/2464

Previously some errors could have been hidden which could have led to missing
content in target tag. Now we are failing on any error.

**improved download_file**

| PR: https://pagure.io/koji/pull-request/2395
| PR: https://pagure.io/koji/pull-request/2461
| PR: https://pagure.io/koji/pull-request/2471

In the previous version we introduced unified ``download_file`` method which also
checks downloaded content. There are minor updates improving this function
especially in reaction to errors.

**cli: add --task and --source options to list-builds command**

| PR: https://pagure.io/koji/pull-request/2458

Builds now can be queried via ``task`` or ``source`` fields.

**show log urls for failed tasks**

| PR: https://pagure.io/koji/pull-request/2505

When watching for task progress, failed tasks will also display links to
relevant logs.

**load plugins also from /usr/lib64**

| PR: https://pagure.io/koji/pull-request/2525

Fix of the bug which missed plugins in /usr/lib64 prefix.

**clone-tag --config also clones extra info**

| PR: https://pagure.io/koji/pull-request/2472

Formerly cloning skipped extra values. Nevertheless, extra values are becoming
an important part of config, so from now we are cloning them.

Library Changes
---------------

**getRPMHeaders now fetch all headers by default**

| PR: https://pagure.io/koji/pull-request/2388

Simple change of library function default behaviour.

API Changes
-----------

**lowercase sigkeys during import/query where needed**

| PR: https://pagure.io/koji/pull-request/2459

Case of signature hashes is properly ignored.

**tagChangedSince reacts on changes in extra**

| PR: https://pagure.io/koji/pull-request/2439

It is probably mostly used by kojira, but it can be of some interest to
automation scripts.

Because of this, kojira will now correctly regenerate repos when tag extra
values change.

**getAverageBuildDuration sliding window**

| PR: https://pagure.io/koji/pull-request/2421

The ``getAverageBuildDuration`` hub call returned an average for all builds for
the given package.
However, old data could be irrelevant to new versions of packages.
Now the call offers an ``age`` option to limit the query (specified as number
of months).

The koji builder daemon now uses this option with a value of 6 months when
adjusting the weight of ``buildArch`` tasks.

**getBuildConfig returns inheritance history**

| PR: https://pagure.io/koji/pull-request/2493

Additional option return inheritance chain for extra values and architectures.
This behaviour is meant to be used with next change.

**blocking inherited extra**

| PR: https://pagure.io/koji/pull-request/2495

Inherited tag extra fields could have been overridden but not removed. Now it can
be done via CLIs ``edit-tag`` or ``editTag2`` respectively which has a new
``block_extra`` option.

**deprecate getGlobalInheritance**

| PR: https://pagure.io/koji/pull-request/2407

This call was never used in Koji.
Clients should instead use the ``readFullInheritance`` call.
The ``getGlobalInheritance`` call will be completely removed in Koji 1.25.

**remove deprecated list-tag-history / tagHistory**

| PR: https://pagure.io/koji/pull-request/2405

Final removal.

**Remove deprecated host.getTask call**

| PR: https://pagure.io/koji/pull-request/2406

Final removal.


Builder Changes
---------------

**builder: configurable TTL for buildroots**

| PR: https://pagure.io/koji/pull-request/2485

Previously these times were hard coded.
The ``buildroot_basic_cleanup_delay`` setting controls how long the builder will
wait before basic cleanup of the buildroot (removing most content but leaving
the directory). The default value is two minutes.
The ``buildroot_final_cleanup_delay`` setting controls how long the build will
wait before final cleanup of the buildroot (removing the rest).
The default value is one day.
Both values are specified in seconds.

For historical context on why there are two separate delays, see
`this bug <https://bugzilla.redhat.com/show_bug.cgi?id=192153>`.

**livemedia-creator: pass --nomacboot on non-x86_64**

| PR: https://pagure.io/koji/pull-request/2373

Additional option was needed for booting on non-x86_64 archs.

**builder: handle btrfs subvolumes in ApplianceTask**

| PR: https://pagure.io/koji/pull-request/2365

BTRFS needed special handling in ``ApplianceTask`` to work.

**kojid: fix extra-boot-args option**

| PR: https://pagure.io/koji/pull-request/2452

Bug which prevented proper usage of ``bootloader --append`` in kickstarts.

**kojid: waitrepo on deleted tag**

| PR: https://pagure.io/koji/pull-request/2417

Previously, if a tag was deleted while a ``waitrepo`` task was watching it, the
task would not notice and wait until the timeout expired.
Now it will fail when it detects that the tag has been deleted.

System Changes
--------------

**dropping python 2.6 / RHEL6 / yum support**

| PR: https://pagure.io/koji/pull-request/2490


One of the most significant changes in this release is dropping support for
older python versions.
Koji no longer supports python 2.6, and only supports python 2.7 for the
builder and cli.

This effectively means ending support for RHEL/CentOS 6 builders.
We are dropping yum support (it was used only with dist-repos) as RHEL7 and
newer have full dnf stack.

**report versions of components**

| PR: https://pagure.io/koji/pull-request/2438

There is a new hub API call named ``getVersion`` (don't confuse with
``getAPIVersion``) which returns the version of Koji that the hub is running.
Similarly, the ``koji`` library provides its version in ``koji.__version__``.

Plugins
-------

**proton: persistent message queue**

| PR: https://pagure.io/koji/pull-request/2441

As qpid (or other amqps broker) can be unreachable for long periods of time
we've implemented a fallback queue in the database to avoid lost messages.
This behaviour needs to be enabled - see
:ref:`the documentation <protonmsg-config>`.

Utilities Changes
-----------------

Kojira
......

**parallel rmtree**

| PR: https://pagure.io/koji/pull-request/2443

Deleting old repos is now done in parallel.


Documentation
-------------

**PostgreSQL requirements for partitioning**

| PR: https://pagure.io/koji/pull-request/2508


**release process**

| PR: https://pagure.io/koji/pull-request/2462


**more info about permission system**

| PR: https://pagure.io/koji/pull-request/2415


**setting rpm macros for build tags**

| PR: https://pagure.io/koji/pull-request/2410

**livecd/livemedia updates**

| PR: https://pagure.io/koji/pull-request/2500
Koji 1.24.1 Release notes
=========================

All changes can be found at `pagure <https://pagure.io/koji/roadmap/1.24.1/>`_.
Most important changes are listed here.

Migrating from Koji 1.24
------------------------

No special actions are needed.

Security Fixes
--------------

None

System Changes
--------------
**drop PyOpenSSL usage**

Library is no longer needed (superseded by ``requests``).

| PR: https://pagure.io/koji/pull-request/2753

Library changes
---------------

**revert "requests exception"**

Regression fix for retrying requests to hub.

| PR: https://pagure.io/koji/pull-request/2787

CLI changes
-----------
**adding check for the license header key**

The CLI could sometimes fail on a missing header key. This no longer happens.

| PR: https://pagure.io/koji/pull-request/2692

Hub Changes
-----------
**set correct import_type for volume policy in completeImageBuild**

Policy data provided incorrect ``import_type`` (maven instead of image)

| PR: https://pagure.io/koji/pull-request/2713

Web Changes
-----------
**escape vcs and disturl**

Web page correctly escapes these URLs now.

| PR: https://pagure.io/koji/pull-request/2747

**optional KojiHubCA usage**

Koji 1.24 introduced a bug where the ``KojiHubCA`` option was required even in
cases when SSL auth was not used for web. Fixed.

| PR: https://pagure.io/koji/pull-request/2749

Utilities Changes
-----------------
**repo removal improvements**

A few bugs related to kojira's repo removal have been fixed. In some cases,
these bugs could have stalled delete threads or even blocked repo deletions
altogether.

| PR: https://pagure.io/koji/pull-request/2755
| PR: https://pagure.io/koji/pull-request/2715
| PR: https://pagure.io/koji/pull-request/2699

Documentation/DevTools Changes
------------------------------
**remove "ca" option from server howto**

| PR: https://pagure.io/koji/pull-request/2725

**update kojid steps in server howto**

| PR: https://pagure.io/koji/pull-request/2724

**fix Fedora's koji URL**

| PR: https://pagure.io/koji/pull-request/2777

**jenkins fedora -> centos migration**

| PR: https://pagure.io/koji/pull-request/2754

**document getNextRelease method**

| PR: https://pagure.io/koji/pull-request/2706

**Additional docs for CVE-2020-15856**

| PR: https://pagure.io/koji/pull-request/2717

**Fix small documentation typo**

| PR: https://pagure.io/koji/pull-request/2772

**set WSGIProcessGroup inside Directory**

| PR: https://pagure.io/koji/pull-request/2731

**tests: stop mock in DBQueryTest**

| PR: https://pagure.io/koji/pull-request/2759

**devtools: updated Dockerfiles**

| PR: https://pagure.io/koji/pull-request/2744
Koji 1.24.0 Release notes
=========================

All changes can be found at `pagure <https://pagure.io/koji/roadmap/1.24/>`_.
Most important changes are listed here.


Migrating from Koji 1.23/1.23.1
-------------------------------

For details on migrating see :doc:`../migrations/migrating_to_1.24`


Security Fixes
--------------

None


Client Changes
--------------

**support download-build \-\-type=remote-sources**

| PR: https://pagure.io/koji/pull-request/2608

This wasn't possible via CLI before. The command has been extended for
downloading this additional artifact type.

**hide import-sig \-\-write option**

| PR: https://pagure.io/koji/pull-request/2654

This option is not used anymore. We're hiding it from the user.

**return error if add/remove-tag-inheritance can't be applied**

Previously only a warning was printed but return code implied no problems. Now
it is returning an error-code so it has better problem visibility in scripts.

| PR: https://pagure.io/koji/pull-request/2605

**raise NotImplementedError with btype name**

| PR: https://pagure.io/koji/pull-request/2610

More verbose error when downloading unsupported archives.

**list-tasks \-\-after/\-\-before/\-\-all**

| PR: https://pagure.io/koji/pull-request/2566

New options for list-tasks. Formerly only running tasks could be
displayed. Now closed tasks can also be displayed with ``--all`` and
``--after``/``--before`` options. Use it wisely -- querying all tasks
can hurt the hub's performance.

**list-hosts can display description/comment**

| PR: https://pagure.io/koji/pull-request/2562

The new ``--comment`` and ``--description`` options can be used to display
additional info in host list.

**allow removal of unused external repo even with \-\-alltags**

| PR: https://pagure.io/koji/pull-request/2560

Fixed confusing behaviour for ``koji remove-external-repo --alltags``
when the given external repo is not associated with any tags.

**history query by extra key**

| PR: https://pagure.io/koji/pull-request/2589

The additional filter option ``--xkey`` for list-history limits the results to
history records that affected the given extra key for some tag.


Library Changes
---------------
**better print with debug_xmlrpc**

| PR: https://pagure.io/koji/pull-request/2598

This fixes an unfortunate display bug introduced by the python3 migration.
The ``--debug-xmlrpc`` feature shows details of the xmlrpc calls to the hub,
but most of the data was shown base64-encoded, regardless of whether it was
printable. Now the client will only result to base64 when it is necessary.

API Changes
-----------
**readFullInheritance stops/jumps deprecation**

| PR: https://pagure.io/koji/pull-request/2655

Deprecation of unused options.

**fix nightly getNextRelease format**

| PR: https://pagure.io/koji/pull-request/2630

Additional format allowed for ``getNextRelease`` - ``{str}.{str}.{id}``.

**[listBuilds] add nvr glob pattern support**

| PR: https://pagure.io/koji/pull-request/2555

The ``list-builds`` command now accepts a ``--pattern`` option that
filters the NVRs using the given glob pattern.

The underlying ``listBuilds`` api call on the hub now accepts a ``pattern``
argument that applies the filtration.

Builder Changes
---------------
**Add option to use repos from kickstart for livemedia builds**

| PR: https://pagure.io/koji/pull-request/2571

The new ``--ksrepo`` option tells the builder to not override the repos
given in the kickstart files for livemedia builds.

**Add nomacboot option for spin-livemedia**

| PR: https://pagure.io/koji/pull-request/2540

The new ``--nomacboot`` option is passed through to livemedia-creator.

System Changes
--------------

**make policy test thread safe**

| PR: https://pagure.io/koji/pull-request/2651


**spec: pythonic provides**

| PR: https://pagure.io/koji/pull-request/2667

Spec file now provides python3dist(koji) provides.

**requires python[23]-requests-gssapi for rhel[78]**

| PR: https://pagure.io/koji/pull-request/2664

**explicit encoding for text file operations**

| PR: https://pagure.io/koji/pull-request/2647

In some mod_wsgi configurations, the hub can raise an error because of non-default
encoding when opening text files. The code has been modified to force UTF-8
everywhere.

**Lower default multicall batch values**

| PR: https://pagure.io/koji/pull-request/2644

In high-load environments long-running transactions can lead even to db
deadlocks. We suggest using lower batches for multicalls and have lowered the
default batch sizes we currently have in the code.

If individual multicalls are running longer than a minute or two, we recommend
splitting them into smaller batches.

**require gssapi-requests 1.22**

| PR: https://pagure.io/koji/pull-request/2584

Older versions of library have a bug which breaks the gsaapi login for builders.
Upgrading to this version solves the problem.

**limit CGImport to allow only one CG per import**

| PR: https://pagure.io/koji/pull-request/2574

We've found that nobody is using the option to include multiple CGs output in
one CG import. It makes things easier if we limit it directly to one CG per
import. In such case we know which CG generated which build and policies can
work with this value, etc.

**external repos can have specified arch list**

| PR: https://pagure.io/koji/pull-request/2564
| PR: https://pagure.io/koji/pull-request/2682

Some external repositories can have split architectures (e.g. primary
architectures in one repo and secondary in the second). On the other hand tags
expect that external repo has all the architectures as the tag has.
We've added a new option to tell Koji that an external repo only contains a
subset of tag's architectures.
Multiple external repos with different architectures can then be attached to the
tag. This behaviour can be tuned by ``--arches`` option in ``add-external-repo``
and ``edit-external-repo`` commands.

**remove deprecated \-\-ca option**

| PR: https://pagure.io/koji/pull-request/2529

Formerly deprecated ``--ca`` option is finally removed for all executables.

Web
---

**return correct content-length**

| PR: https://pagure.io/koji/pull-request/2639

Regressions for py3 code - ``Content-Length`` header was erroneously computed so
some browsers fetched incomplete page. It is not visible in most cases (as final
html tags are corrupted and added by the browser) but in some cases it could
led to broken web page.

**order methods by name in select box**

| PR: https://pagure.io/koji/pull-request/2559

With growing number of task types it makes more sense to order them
alphabetically these days compared to previous *importance* ordering.

**more accessible task colors/icons**

| PR: https://pagure.io/koji/pull-request/2653

For higher accessibility we've slightly changed the colors corresponding to task and
build states. We've also added more informative icons to the taskinfo page.

**display VCS/DistURL rpm tags**

| PR: https://pagure.io/koji/pull-request/2683

The buildinfo and rpminfo pages now display also VCS and DistURL tags if they are
present in rpm (srpm for buildinfo page).

Plugins
-------
**handle plugins and generator results in count and countAndFilterResults**

| PR: https://pagure.io/koji/pull-request/2633

These functions couldn't be used for methods provided by plugins or
methods which returned generators. This is now fixed.

**plugin hooks for repo modification**

| PR: https://pagure.io/koji/pull-request/2637

New ``postCreateRepo`` and ``postCreateDistRepo`` plugin hooks were introduced
on builder. They can be used to modify repodata with intent to allow sign the
repodata by plugins but it can be used for additional repodata modification.

Utilities
---------

Kojira
......

**move checkTasks near its usage**

| PR: https://pagure.io/koji/pull-request/2140

We've moved checking running ``newRepo`` tasks to different place. Now, the number
of running tasks should be closer to set capacity as kojira will check
finished tasks just before spawning new ones.

Documentation
-------------
**mention the final destination for new dist-repos**

| PR: https://pagure.io/koji/pull-request/2621

**link to tag2distrepo hub plugin**

| PR: https://pagure.io/koji/pull-request/2617

**types param for content generators**

| PR: https://pagure.io/koji/pull-request/2609

**remove global SSLVerifyClient option**

| PR: https://pagure.io/koji/pull-request/2627

Koji 1.25.1 Release notes
=========================

All changes can be found at `pagure <https://pagure.io/koji/roadmap/1.25.1/>`_.
Most important changes are listed here.

Migrating from Koji 1.25
------------------------

No special actions are needed.

Security Fixes
--------------

None

Library changes
---------------

**return taskLabel for unknown tasks**

Regression fix - some external plugins was represented in web/console UI by
"malformed task" description. Now we're back with proper method name and
architecture.

| PR: https://pagure.io/koji/pull-request/2906


Hub Changes
-----------

**fix SQL condition**

| PR: https://pagure.io/koji/pull-request/2898

Change in 1.25 caused an error in ``listTagged`` with ``type`` option. Fixed.

**use "name" in result of lookup_name for CGs**

| PR: https://pagure.io/koji/pull-request/2916

New ``cg_match`` policy test returned dicts not cg names, so checking was not
working correctly.

**clean noisy error log**

| PR: https://pagure.io/koji/pull-request/2932

Removed debug message which cluttered httpd logs in 1.25.

Web Changes
-----------

**Drop download link from deleted build**

| PR: https://pagure.io/koji/pull-request/2896

If build is deleted we don't display download links to not confue the users.

**Fix getting tag ID for buildMaven taskinfo page.**

| PR: https://pagure.io/koji/pull-request/2900

``buildMaven`` taskinfo page was broken for deleted builds.

Documentation/DevTools Changes
------------------------------
**update .coveragerc to ignore p3 code**

| PR: https://pagure.io/koji/pull-request/2881

**docs for KojiHubCA/ClientCA**

| PR: https://pagure.io/koji/pull-request/2888

**tests - Add support for running tox with specific test(s)**

| PR: https://pagure.io/koji/pull-request/2890
Koji 1.25.0 Release notes
=========================

All changes can be found in `the roadmap <https://pagure.io/koji/roadmap/1.25/>`_.
Most important changes are listed here.


Migrating from Koji 1.24/1.24.1
-------------------------------

For details on migrating see :doc:`../migrations/migrating_to_1.25`


Security Fixes
--------------

None


Client Changes
--------------
**More verbose error/warning messages**

| PR: https://pagure.io/koji/pull-request/2615
| PR: https://pagure.io/koji/pull-request/2694
| PR: https://pagure.io/koji/pull-request/2702
| PR: https://pagure.io/koji/pull-request/2703
| PR: https://pagure.io/koji/pull-request/2709
| PR: https://pagure.io/koji/pull-request/2727
| PR: https://pagure.io/koji/pull-request/2733
| PR: https://pagure.io/koji/pull-request/2738
| PR: https://pagure.io/koji/pull-request/2761
| PR: https://pagure.io/koji/pull-request/2769
| PR: https://pagure.io/koji/pull-request/2770
| PR: https://pagure.io/koji/pull-request/2773
| PR: https://pagure.io/koji/pull-request/2790
| PR: https://pagure.io/koji/pull-request/2792
| PR: https://pagure.io/koji/pull-request/2803
| PR: https://pagure.io/koji/pull-request/2829
| PR: https://pagure.io/koji/pull-request/2850

We've revised many calls which previously returned empty results to raise more
meaningful errors. For example, ``koji list-untagged package_x`` previously did
not distinguish between the case where ``package_x`` had no untagged builds
and the case where ``package_x`` did not exist.
Also, all warning and error messages were unified in their wording.

**Show connection exception for anonymous calls**

| PR: https://pagure.io/koji/pull-request/2705

Calls which don't need authentication were not properly propagating
connection-related exceptions with ``--debug``.

**list-api option for one method**

| PR: https://pagure.io/koji/pull-request/2688

Now you don't need to scroll through ``list-api`` output if you want a single
method signature. The ``koji list-api listBuilds`` command will show just one.

**Add wait/nowait to all calls**

| PR: https://pagure.io/koji/pull-request/2831

Some commands had ``--wait`` and/or ``--nowait`` options. We've made it
consistent, so all eligible commands now contain both variants.
These options override waiting behavior for the command.

If neither option is specified, the default depends on the runtime context.
If the client is running on a TTY, then it defaults to waiting.
If it is running in the background, then it defaults to not waiting.

This is not new behaviour, just adding explicit options in cases
where they were not present.

**Use multicall for cancel, list-hosts and write-signed-rpm commands**

| PR: https://pagure.io/koji/pull-request/2722
| PR: https://pagure.io/koji/pull-request/2808
| PR: https://pagure.io/koji/pull-request/2810

These commands are now much faster when acting on multiple items.

**Using 'call' command without authentication**

| PR: https://pagure.io/koji/pull-request/2734

The ``call`` command authenticates by default, which is not always desired.
This authentication can be disabled with the the global ``--noauth`` option.
The help text now clarifies this.

**Support modules and other btypes in download-build**

| PR: https://pagure.io/koji/pull-request/2678

The ``download-build`` command now downloads all regular archives. Previously it
was limited to a subset of file types.


Library Changes
---------------
**Better failed authentication/connections**

| PR: https://pagure.io/koji/pull-request/2723
| PR: https://pagure.io/koji/pull-request/2735
| PR: https://pagure.io/koji/pull-request/2794
| PR: https://pagure.io/koji/pull-request/2824
| PR: https://pagure.io/koji/pull-request/2826

Connection errors now provide a bit more information for debugging.
Some errors were masked by more generic ones. We're now trying to display more
information about these, and to help with debugging krbV/GSSAPI errors. We have
added a :doc:`documentation page <../kerberos_gssapi_debug>` for common GSSAPI
errors, which the CLI now refers to.

**More portable BadStatusLine checking**

| PR: https://pagure.io/koji/pull-request/2819

Python 3 versions had some problems detecting expired keepalive connections.
We've made the code a bit more portable so you shouldn't see these errors now.

**Missing default values in read_config**

| PR: https://pagure.io/koji/pull-request/2689

Some default values were missing in the config parsing which could have led to
mysterious behaviour. The configuration values in question were related to
retry behavior when making calls to the hub.

**Use parse_task_params for taskLabel**

| PR: https://pagure.io/koji/pull-request/2771

The ``taskLabel`` function (used in various places to generate a short label
for a given task) has been updated to use the newer ``parse_task_params``
function instead of parsing the parameters itself.
As a result, the function is simpler and more accurate.

The ``parse_task_params`` function is currently the preferred way to interpret
task parameters. We recommend that developers use it rather than ad-hoc code.

API Changes
-----------
**Fail early on wrong info in create*Build methods**

| PR: https://pagure.io/koji/pull-request/2721
| PR: https://pagure.io/koji/pull-request/2732
| PR: https://pagure.io/koji/pull-request/2736

The ``createWinBuild``, ``createImageBuild``, and ``createMavenBuild`` hub
calls will now raise an exception when some data in buildinfo are missing,
and their exception text should be clearer than before.

**getVolume with strict option**

| PR: https://pagure.io/koji/pull-request/2796

This new option brings ``getVolume`` in line with other similar calls.
When the requested volume does not exist, the call will either return ``None``
(when ``strict=False``, the default) or raise an exception (when
``strict=True``).

**getLastHostUpdate ts option**

| PR: https://pagure.io/koji/pull-request/2766

Historically we've passed around dates as text strings. Almost everywhere we're
now also sending GMT timestamps to better handle timezone problems. This new
option in the ``getLastHostUpdate`` call allows you to get a timestamp instead
of a string value.

**Be tolerant with duplicate parents in _writeInheritanceData**

| PR: https://pagure.io/koji/pull-request/2782

Regression fix - call will now not raise an exception if there are duplicated
parents in inheritance chain.

**with_owners options for readPackageList and listPackages**

| PR: https://pagure.io/koji/pull-request/2791

Performance improvement. Most of the calls to these functions don't need
information about the package owner. Dropping this data simplifies the
underlying query to a faster one. If you're using this call in your automation
give it a chance to lower your database load.

System Changes
--------------
**Task priority policy**

| PR: https://pagure.io/koji/pull-request/2711

There is a new ``priority`` policy which can be used to alter task priorities
based on data about task/build.
See :doc:`../defining_hub_policies` for details.

**Python egginfo**

| PR: https://pagure.io/koji/pull-request/2821

After years of struggling with pip/setuptools/rpm packaging we should finally
have something compatible. So, now egginfo, etc. should be properly installed
and usable in virtualenvs.

**Task ID for repos**

| PR: https://pagure.io/koji/pull-request/2802
| PR: https://pagure.io/koji/pull-request/2823

When debugging buildroot content issues it is often important to find out which
repo was used and when it was created, investigate createrepo and mergerepo
logs, etc. It was not easy to find the task that created a given repo.
This information is now tracked in the database, and reported in the web and CLI
interfaces.
It is also recorded in the ``repo.json`` file for the repo.

**Add squashfs-only and compress-arg options to livemedia**

| PR: https://pagure.io/koji/pull-request/2833

Livemedia tasks can now use these options for passing to ImageFactory.

Web
---
**Show VCS and DistURL tags as links when appropriate**

| PR: https://pagure.io/koji/pull-request/2756

Previously these values were shown as plain text in the web interface.
Now they should appears as links.

**Don't use count(*) on first tasks page**

| PR: https://pagure.io/koji/pull-request/2827

The tasks list page can be quite slow in many cases.
The primary cause of this is pagination and the underlying ``count(*)``
query.
As PostgreSQL is very slow for this type of query we've removed the page count
for the first page which is loaded most often.
Subsequent pages will continue to show the count (and therefore also the
performance penalty in some situations).

**Additional info on API page**

| PR: https://pagure.io/koji/pull-request/2828

We've added some simple example client code to the API page, to help users get
started without having to dig through the rest of the documentation.


Plugins
-------
**Configurable sidetags suffixes**

| PR: https://pagure.io/koji/pull-request/2730

The sidetag plugin now allows defining a set of allowed suffixes which can be used
when creating the sidetag. You can distinguish between different types (private,
gating, ...)

**Protonmsg: fixes for persistent queue**

| PR: https://pagure.io/koji/pull-request/2844

The persistent message queue option (see :ref:`protonmsg-config`) was
broken. Now it should work correctly.


Utilities
---------

Kojira
......
**Faster startup**

| PR: https://pagure.io/koji/pull-request/2764

Multicall is used to prefetch tag data from hub. It significantly improves
startup time for larger installations.

**Check repo.json before deleting**

| PR: https://pagure.io/koji/pull-request/2765

Previously kojira refused to delete repositories if the path did not match
the name of the tag the repo was based on.
This kept kojira from cleaning up repos after a build tag was renamed.
Now kojira also consults the ``repo.json`` which records the name of the tag
at the time the repo was created.

**Tolerate floats in metadata timestamps**

| PR: https://pagure.io/koji/pull-request/2784

External repositories sometimes can use float timestamp. We now correctly parse
that.

**Fix fork-related issues**

| PR: https://pagure.io/koji/pull-request/2853
| PR: https://pagure.io/koji/pull-request/2855
| PR: https://pagure.io/koji/pull-request/2856

Forking while deleting is causing some issues (mostly with loggign module)
especially with the latest python.  These can result in kojira not deleting
repos at all.

Garbage Collector
.................
**Allow specifying all CLI options in config**

| PR: https://pagure.io/koji/pull-request/2816

Every option that can be specified on the command line can now be also put into
the configuration file.

**Implement hastag policy for koji-gc**

| PR: https://pagure.io/koji/pull-request/2817

The gc policy now offers a ``hastag`` test for builds, similar to the test
offered by hub policies.

This can be used in numerous ways.
One example (a ``protected`` tag that protects builds from pruning) is
described in the `original request <https://pagure.io/koji/issue/2813>`_.

Documentation
-------------
**Updated docs and devtools**

| PR: https://pagure.io/koji/pull-request/2724
| PR: https://pagure.io/koji/pull-request/2725
| PR: https://pagure.io/koji/pull-request/2772
| PR: https://pagure.io/koji/pull-request/2799
| PR: https://pagure.io/koji/pull-request/2843

Numerous small updates.
Koji 1.26.1 Release notes
=========================

All changes can be found at `pagure <https://pagure.io/koji/roadmap/1.26.1/>`_.
Most important changes are listed here.

Migrating from Koji 1.26
------------------------

No special actions are needed.

Security Fixes
--------------

None

Client changes
--------------

**CLI channels, hosts methods works with older hub**

| PR: https://pagure.io/koji/pull-request/2991

Backward compatibility of new CLI and older hubs


**Fix disabled/enabled option for empty results**

| PR: https://pagure.io/koji/pull-request/3016

Simple fix for list-hosts filters resulting in empty set.


Hub Changes
-----------

**move btypes from headers to body of proton message**

| PR: https://pagure.io/koji/pull-request/3022

AMQP-compatible headers. New values (btypes dicts) were moved to message body.


**create symlink before import and check spec_url for wrapperRPM task**

| PR: https://pagure.io/koji/pull-request/3004
| PR: https://pagure.io/koji/pull-request/3047

``wrapperRPM`` method needed to be updated due to previous policy handling changes.

Builder changes
---------------
**import guestfs before dnf**

| PR: https://pagure.io/koji/pull-request/2993

Problems in jsonc/jansson libraries symbol handling led us to workaround this
issue (until it is solved in upstream libs) so docker images can be built again.


Kojivmd
-------

**change opts allowed_scms_by_* to allowed_scms_use_***

| PR: https://pagure.io/koji/pull-request/3050

Fix for new ``allowed_scm_*`` options.

Documentation/DevTools Changes
------------------------------

 * `README: add koji-hs project <https://pagure.io/koji/pull-request/2997>`_
 * `add instructions for SSL DB connections <https://pagure.io/koji/pull-request/2996>`_
 * `fix "koji" CLI command name in signing instructions <https://pagure.io/koji/pull-request/2994>`_
 * `fix "an user" -> "a user" grammar in help text and errors <https://pagure.io/koji/pull-request/2995>`_
 * `update profiles documentation <https://pagure.io/koji/pull-request/3029>`_
 * `document listHosts method <https://pagure.io/koji/pull-request/3013>`_
 * `fix getBuild documented parameter name <https://pagure.io/koji/pull-request/3021>`_
 * `add all types to docs latest-build and readTaggedBuilds <https://pagure.io/koji/pull-request/3000>`_
 * `fix docs for listBuilds "state" parameter name <https://pagure.io/koji/pull-request/3023>`_
 * `add warnings for remove-sig <https://pagure.io/koji/pull-request/3026>`_
Koji 1.26.0 Release notes
=========================

All changes can be found in `the roadmap <https://pagure.io/koji/roadmap/1.26/>`_.
Most important changes are listed here.


Migrating from Koji 1.25/1.25.1
-------------------------------

For details on migrating see :doc:`../migrations/migrating_to_1.26`


Security Fixes
--------------

None


Client Changes
--------------
**New command userinfo**

| PR: https://pagure.io/koji/pull-requests/2840

Similarly to web user info page, we've added command userinfo listing basic
informations like principals, permissions and activity statistics.

**Add noverifyssl option to oz image builds**

| PR#: http://pagure.io/koji/pull-requests/2860

Image builds could want to use development repos which are not recognized by CAs
installed inside the image. In such case there is a way to add ``--noverifyssl``
into the generated kickstart. The option must be explicitly enabled on builders
which will handle those tasks.

**download-build now pre-check non-existing sigkey**

| PR: https://pagure.io/koji/pull-requests/2864

When downloading builds with specified sigkey it could have been confusing that
404 error returned for non-existing sigkeys. Raising explicit error eliminates
network suspicion.

API Changes
-----------
**Remove jump/stops options from readFullInheritance**

| PR#: http://pagure.io/koji/pull-requests/2847

These unused options were finally removed.

**listBuilds accept also package name and user name**

| PR: https://pagure.io/koji/pull-request/2867
| PR: https://pagure.io/koji/pull-request/2922

It is an extension to previously required IDs.

**Remove deprecated readGlobalInheritance**

| PR: https://pagure.io/koji/pull-request/2879

**add_rpm_sign catches IntegrityError**

| PR: https://pagure.io/koji/pull-request/2909

Better error-handling in case of insert race-condition.

**Get info for deleted tag entries**

| PR: https://pagure.io/koji/pull-request/2923

There was no simple way how to obtain information about tag which are deleted.
Extension of ``getTag``'s ``event`` option for value ``auto`` will now return
existing tag or for deleted tag - last known configuration.


Builder Changes
---------------

**Livecd handler set up release if release not given**

| PR: https://pagure.io/koji/pull-request/2877

Standard ``getNextRelease`` call is used in such case now.

**Prune user repos for image tasks**

| PR: https://pagure.io/koji/pull-request/2967

If multiple repos are specified in task, they'll get pruned. This could happen
if task is created by some automation. Using multiple repos with same url just
consumes memory of anaconda's ramdisk and can result in failed tasks.

**Use getNextRelease for scratch image builds**

| PR: https://pagure.io/koji/pull-request/2974

System Changes
--------------
**Policy test buildtag_inheritance**

| PR: https://pagure.io/koji/pull-request/2872

This new test can be used to check if tag's inheritance contains other specific
tag.

**Fix SQL condition**

| PR: https://pagure.io/koji/pull-request/2898

``listTagged`` was broken in regression from https://pagure.io/koji/pull-request/2791

**Channels can now be disabled and described**

| PR: https://pagure.io/koji/pull-request/2905
| PR: https://pagure.io/koji/pull-request/2933

**dist-repo takes inherited arch when arch is not set**

| PR: https://pagure.io/koji/pull-request/2912

If tag for ``dist-repo`` doesn't have any configured architectures, koji will
look into the inheritance chain and try to find something there.

**Extend SCM.assert_allowed with hub policy**

| PR: https://pagure.io/koji/pull-request/2951

SCM policy can be defined at the hub now. It also allow more granular policies
like allowing some SCMs for scratch builds while others also for regular ones.
This approach can be combined with the current ``kojid.conf`` ``allowed_scms``
option. See example ``kojid.conf`` for more details.

**DBConnectionString/dsn option for db connection**

| PR: https://pagure.io/koji/pull-request/2958

Alternative method for specifying DB connection is now provided via single `DSN
connection string
<https://www.postgresql.org/docs/current/libpq-connect.html#LIBPQ-CONNSTRING>`_.

**Add remove-sig CLI and deleteRPMSig hub call**

| PR: https://pagure.io/koji/pull-request/2965

The ``deleteRPMSig`` hub call removes RPM signatures from Koji. Only use this
method in extreme situations, because it goes against Koji's design of
immutable, auditable data. This call requires ``admin`` permission (``sign``
is not sufficient).

VM
--
**py3 kojikamid fixes**

| PR: https://pagure.io/koji/pull-request/2977

Python 3 port of ``kojikamid`` had a few regressions.

Web
---
**Drop download link from deleted build**

| PR: https://pagure.io/koji/pull-request/2896

It was confusing that link was there even for non-existing files.

**Fix getting tag ID for buildMaven taskinfo page.**

| PR: https://pagure.io/koji/pull-request/2900

Maven task info page was broken for some time due to wrong tag ID handling.

**Hosts page with more filters and added channel column**

| PR: https://pagure.io/koji/pull-request/2910

Simple extension for hosts list page.

**Update webUI number of tasks**

| PR: https://pagure.io/koji/pull-request/2937

As we've dropped number of results on first task info page due to speed reasons,
it is now a bit confusing for users. We've added a bit more indicative result.

Plugins
-------
**Configurable naming template for sidetags**

| PR: https://pagure.io/koji/pull-request/2894

Sidetags now can be named according to set of templates in the config. These
templates then can be used in hub policies for differentiating among the
different side tag types.

**Add btype to protonmsg**

| PR: https://pagure.io/koji/pull-request/2934
| PR: https://pagure.io/koji/pull-request/2955

Build types are now part of proton messages.


Utilities
---------

Kojira
......
**Don't fail on deleted needed tag**

| PR: https://pagure.io/koji/pull-request/2936

Deleted tags could have caused kojira's thread crash which could have been seen
only in the log but kojira still have run. Repository cleanup then could have
failed without notice.

**Do not ever clean up repositories where 'latest' points to**

| PR: https://pagure.io/koji/pull-request/2950
| PR: https://pagure.io/koji/pull-request/2970

We now skip all "latest" repos.

Sweep DB
........

**Read options from main hub config and its config dir**

| PR: https://pagure.io/koji/pull-request/2887

``koji-sweep-db`` now properly reads whole config structure, not only basic
``kojihub.conf``


Documentation
-------------
**Update irc info**

| PR: https://pagure.io/koji/pull-request/2884

**Docs for KojiHubCA/ClientCA for web**

| PR: https://pagure.io/koji/pull-request/2888

**Remove old mod_ssl instructions from server howto**

| PR: https://pagure.io/koji/pull-request/2960

**Document readTaggedRPMS method**

| PR: https://pagure.io/koji/pull-request/2971

**Add signing documentation**

| PR: https://pagure.io/koji/pull-request/2986


**"download-logs --help" fixes**

| PR: https://pagure.io/koji/pull-requests/2952

**cli: improve --config and --profile help text**

| PR: https://pagure.io/koji/pull-requests/2985


Koji 1.27.1 Release notes
=========================

All changes can be found in `the roadmap <https://pagure.io/koji/roadmap/1.27.1/>`_.
Most important changes are listed here.

Migrating from Koji 1.27
------------------------

No special actions are needed.


Security Fixes
--------------

None

Client Changes
--------------
**Return mistakenly dropped option (--keytab)**

| PR: https://pagure.io/koji/pull-request/3172

In 1.27.0 improper merge led to this missing option.

**Use error function instead of print with sys.exit in CLI commands**

| PR: https://pagure.io/koji/pull-request/3113

Internal-only change unifying handling of CLI exit.

Hub Changes
-----------
**Don't fail on missing buildroot tag in policies**

| PR: https://pagure.io/koji/pull-request/3186

buildtag and buildtag_inherits_from will fail in case build doesn't have it.
Such situation can easily happen with content generators.

**Only raise error when authtype is not proxyauthtype**

| PR: https://pagure.io/koji/pull-request/3164

Backward compatibility for new proxyauthtype option.

**Handle dictionary parameter in get_tag()**

| PR: https://pagure.io/koji/pull-request/3118

Unification of internal handling of get_tag/getTag parameters.

Kojira
------
**Don't fail on deleted items**

| PR: https://pagure.io/koji/pull-request/3166

In case that tag was deleted during last run unnecessary error was raised as
relict of py2->py3 conversion.

Web Changes
-----------

**Style channelinfo hosts table**

| PR: https://pagure.io/koji/pull-request/3139

Default CSS was not handling this page well.

Documentation/DevTools Changes
------------------------------

 * `buildtag_inherits_from docs <https://pagure.io/koji/pull-request/3189>`_
 * `Make setup.py executable <https://pagure.io/koji/pull-request/3104>`_
 * `Add unit test for get_options <https://pagure.io/koji/pull-request/3180>`_
 * `Add all options to hub_conf.rst <https://pagure.io/koji/pull-request/3098>`_
 * `Document getBuildLogs method <https://pagure.io/koji/pull-request/3174>`_
 * `Pytest instead of nose in unittest <https://pagure.io/koji/pull-request/3157>`_
 * `Fix spelling in comments for archive handling <https://pagure.io/koji/pull-request/3161>`_
 * `Add and update CLI unit tests <https://pagure.io/koji/pull-request/3115>`_
 * `Print fakeweb listening URL <https://pagure.io/koji/pull-request/3142>`_
 * `Improve protonmsg SSL parameter descriptions <https://pagure.io/koji/pull-request/3138>`_
 * `Rewrite Acceptable keys to Requested keys in missing_signatures log <https://pagure.io/koji/pull-request/3150>`_
Koji 1.27.0 Release notes
=========================

All changes can be found in `the roadmap <https://pagure.io/koji/roadmap/1.27/>`_.
Most important changes are listed here.


Migrating from Koji 1.26/1.26.1
-------------------------------

For details on migrating see :doc:`../migrations/migrating_to_1.27`


Security Fixes
--------------

None


Client Changes
--------------
**set-task-priority permission error fix**

| PR: https://pagure.io/koji/pull-request/2999

CLI confusingly reported "closed task" even in case when user was "just" missing
the admin permission.

**Honour --force-auth for anonymous commands**

| PR: https://pagure.io/koji/pull-request/3010

This option was not respected in some cases.

**Propagate error in write-signed-copies**

| PR: https://pagure.io/koji/pull-request/3020
| PR: https://pagure.io/koji/pull-request/3093
| PR: https://pagure.io/koji/pull-request/3107

Due to speedup changes some errors could have been hidden. Now we're catching
them all and displaying properly in write-signed-copies command.

**Be tolerant of stops/jumps kwargs in list-tag-inheritance**

| PR: https://pagure.io/koji/pull-request/3055

Older client can emit errors when used against a newer hub as it could use
already deprecated options. We're now more tolerant to these.

**Dist-repo with write-signed-rpm option**

| PR: https://pagure.io/koji/pull-request/3058

New option for dist-repo --write-signed-rpms. As there could be
garbage-collected signed copies, dist-repo can fail and these rpms must be
manually reconstructed. This new option will allow user with `sign` permission
to prepare required rpms to be ready for the ``distRepo`` task.

**call --json default option set up to str**

| PR: https://pagure.io/koji/pull-request/3066


Bugfix for converting some datetime values to proper json when using ``call``
command.

**Add option for UTC time in list-history**

| PR: https://pagure.io/koji/pull-request/3090

``list-history`` now accepts ``--utc`` option which will display dates in UTC
instead of local timezone.

API Changes
-----------
**listBuilds/list-builds filtering via CG**

| PR: https://pagure.io/koji/pull-request/3009
| PR: https://pagure.io/koji/pull-request/3111

API call ``listBuilds`` (and its CLI counterpart) now accepts ``cgID``
(``--cg``) option which adds another filter based on content generator type.

**Deprecate taskReport**

| PR: https://pagure.io/koji/pull-request/3036

This call will be removed in the future.

**queryRPMSigs accepts RPM ID, NVRA and dict**

| PR: https://pagure.io/koji/pull-request/3064

To be aligned with other calls, ``queryRPMSigs`` now accepts all rpm ID
specifications (integer ID, NVRA string or NVRA dict)

**Deprecate koji.listFaults**

| PR: https://pagure.io/koji/pull-request/3082

Another candidate for removal

**Deprecated force option in groupReqListRemove call**

| PR: https://pagure.io/koji/pull-request/3089

And another one

**getBuildType: ensure id exists in buildinfo dict**

| PR: https://pagure.io/koji/pull-request/3092

More robust handling of input (NVR dict is sufficient now)

**Add strict option to listTagged, listTaggedRPMS, listTaggedArchives**

| PR: https://pagure.io/koji/pull-request/3095

For better differentiation between empty results for correct inputs and wrong
inputs (non-existing tag, etc.) ``strict`` option was added to these calls.

Builder Changes
---------------
**Import guestfs before dnf**

| PR: https://pagure.io/koji/pull-request/2993

Linking conflicts between json-parsing libraries used by guestfs and dnf led us
to include a small hack. Now it should be again possible to build docker images
via ``oz``.

**Better error messages for Task.lock()**

| PR: https://pagure.io/koji/pull-request/3007

Improved error logging related to multiple builders competing in task
allocation.

**Restart kojid and kojira services automatically**

| PR: https://pagure.io/koji/pull-request/3040

``systemd`` services were updated to automatically restart on failure with one
minute delay.

**Retry get_next_release to avoid race condition**

| PR: https://pagure.io/koji/pull-request/3103

In rare cases there was still race condition with starting image/maven builds
with same auto-incremented release. This should fix this behaviour completely.

System Changes
--------------
**Honour taginfo option in policy_get_build_tags**

| PR: https://pagure.io/koji/pull-request/2989

Deleted buildtags can break some policies. As these are more frequent these days
(via sidetag usage patterns) we have also hit this problem. It should be fixed
now.

**Fix scripts for koji pkg and drop utils from py2**

| PR: https://pagure.io/koji/pull-request/3002

Packaging fixes for regression. ``koji-utils`` are py3-only.

**Create symlink before import**

| PR: https://pagure.io/koji/pull-request/3004

Windows builds were not properly handling importing builds back to hub if volume
policy put the build to non-default volume.

**Speedup untagged_builds query**

| PR: https://pagure.io/koji/pull-request/3005

``untagged_builds`` call is used by garbage collection and it is now about 100%
faster. It doesn't matter that much to GC itself as it needn't to be
particularly fast but other queries/users are not blocked by this query lock.

**Support packages that are head-signed**

| PR: https://pagure.io/koji/pull-request/3012

DSA and RSA header signatures (RPMv4 scheme) support.

**Tasks respect disabled channels**

| PR: https://pagure.io/koji/pull-request/3030
| PR: https://pagure.io/koji/pull-request/3124

In last release option to "disable" channel was introduced. Anyway, tasks were
happily requesting those channels and were never executed as no builder picked
them. Now they fail immediately if the channel is disabled/non-existent in the
moment of task creation.

**Check spec_url for wrapperRPM task source policy test**

| PR: https://pagure.io/koji/pull-request/3047

``wrapperRPM`` tasks now properly propagate data for ``source`` policy test.

**Allow user on git://, git+http://, git+https://, and git+rsync:// scheme**

| PR: https://pagure.io/koji/pull-request/3067

We've not propagated username (or token) to builder. As this data are already
visible in task it doesn't make sense to conceal them on the builder. There are
legitimate cases when token is used, so we are now propagating it without
restriction.

**Logging warning messages about deleteBuild or deletedRPMSig**

| PR: https://pagure.io/koji/pull-request/3076

Last release introduced ``deleteRPMSig`` API call. It is the second call which
irrecoverably destroys build data so it should be better logged. We're now
capturing more data in the logs (especially the user).

**Remove translation stub functions**

| PR: https://pagure.io/koji/pull-request/3077

We've never used the i18n ``_`` call and we don't plan to introduce any
translation. So, we've decided to remove these stubs to make code a bit more
readable and consistent.

**Add specfile log to wrapperRPM**

| PR: https://pagure.io/koji/pull-request/3078

As specfile in ``wrapperRPM`` is modified from template it is nice to store this
file in similar way to modified kickstarts in oz tasks.

Web
---
**Allow kojiweb to proxy users obtained via different mechanisms**

| PR: https://pagure.io/koji/pull-request/3008

New ``proxyauthtype`` option is introduced to ``gssapi_login`` and ``ssl_login``
methods. It allows user1 (typically web interface) to proxy another user2 (via
standard ``proxyuser`` option) with different authentication mechanism than
user1. E.g. user is authenticated to webui by gssapi, while webui itself
authenticates via SSL certificate.

Utilities
---------

Kojira
......
**Don't throw exception when auth fails**

| PR: https://pagure.io/koji/pull-request/3099

More proper exit when authentication fails to not trigger abrt.

**Implement ignore_other_volumes option**

| PR: https://pagure.io/koji/pull-request/3126

Option to forbid kojira to delete repos on non-default volumes.


Documentation
-------------
**Some documentation updates**
| PR: https://pagure.io/koji/pull-request/2994
| PR: https://pagure.io/koji/pull-request/2995
| PR: https://pagure.io/koji/pull-request/2996
| PR: https://pagure.io/koji/pull-request/2997
| PR: https://pagure.io/koji/pull-request/3000
| PR: https://pagure.io/koji/pull-request/3013
| PR: https://pagure.io/koji/pull-request/3023
| PR: https://pagure.io/koji/pull-request/3029
| PR: https://pagure.io/koji/pull-request/3038
| PR: https://pagure.io/koji/pull-request/3051
| PR: https://pagure.io/koji/pull-request/3062
| PR: https://pagure.io/koji/pull-request/3070
| PR: https://pagure.io/koji/pull-request/3085
| PR: https://pagure.io/koji/pull-request/3086
| PR: https://pagure.io/koji/pull-request/3096
| PR: https://pagure.io/koji/pull-request/3102
| PR: https://pagure.io/koji/pull-request/3021
| PR: https://pagure.io/koji/pull-request/3026

**New tests**
| PR: https://pagure.io/koji/pull-request/3027
| PR: https://pagure.io/koji/pull-request/3037
| PR: https://pagure.io/koji/pull-request/3056
| PR: https://pagure.io/koji/pull-request/3075

**Basic security checks with bandit**

| PR: https://pagure.io/koji/pull-request/3043
Koji 1.28.1 Release notes
=========================

All changes can be found in `the roadmap <https://pagure.io/koji/roadmap/1.28.1/>`_.
Most important changes are listed here.

Migrating from Koji 1.28
------------------------

No special actions are needed.


Security Fixes
--------------

None

Library Changes
---------------
**Fix parsing of URLs with port numbers**

| PR: https://pagure.io/koji/pull-request/3262

1.28 feature allowing authentication tokens as part of git urls broke urls with
specified port.

Client Changes
--------------
**Same format output for list-buildroot with --verbose for py3/py2**

| PR: https://pagure.io/koji/pull-request/3300

Output format for python 2.x and 3.x were unified.

Hub Changes
-----------
**Fix type option handling in readTaggedRPMS**

| PR: https://pagure.io/koji/pull-request/3298

As option is called ``type`` it masked builtins.type call used in exception
handling.

**Improve inheritance priority collision error message**

| PR: https://pagure.io/koji/pull-request/3208

More descriptive exception message.

**Set dst permissions same as src permissions**

| PR: https://pagure.io/koji/pull-request/3265

Another regression introduced in 1.28 - newly header/payload split rpms were not
writtend with the same file permissions so they weren't readable for some tasks.

Kojira
------
**Don't call listTags more than once**

| PR: https://pagure.io/koji/pull-request/3259

Speed improvement - one of the caching calls is now run only at the start and
not in every check cycle. It should bring a less more stress to the hub and a
bit faster kojira noticing repo changes.

Web Changes
-----------
**Fix syntax error**

| PR: https://pagure.io/koji/pull-request/3263

Broken HTML at hostedit page.

**Fix attribute test**

| PR: https://pagure.io/koji/pull-request/3303

Search with empty field led to traceback - 1.28 regression.

**Encode filename as UTF-8**

| PR: https://pagure.io/koji/pull-request/3290

Fix for rpminfo page.

**Fix tag and target shows as string, not as dict to string**

| PR: https://pagure.io/koji/pull-request/3252

Fixed wrong target representation for some tasks in tasks overview page.

Documentation/DevTools Changes
------------------------------

 * `Task flow diagram <https://pagure.io/koji/pull-request/3292>`_
 * `Fix readTaggedRPMs rpmsigs option description <https://pagure.io/koji/pull-request/3297>`_
 * `Increase CLI test cases <https://pagure.io/koji/pull-request/3270>`_
Koji 1.28.0 Release notes
=========================

All changes can be found in `the roadmap <https://pagure.io/koji/roadmap/1.28/>`_.
Most important changes are listed here.


Migrating from Koji 1.27/1.27.1
-------------------------------

For details on migrating see :doc:`../migrations/migrating_to_1.28`


Security Fixes
--------------

None


Client Changes
--------------
**Deprecated --paths option in list-buildroot**

| PR: https://pagure.io/koji/pull-request/3105

This option doesn't have any effect, so it should be removed in the near future.

**Remove rename-channel CLI and use editChannel in renameChannel**

| PR: https://pagure.io/koji/pull-request/3108

As we've extended ``edit-channel`` command this one is a redundant now. Same can
be achieved with ``edit-channel --name``. 

**mock-config: if topdir option is used, remove topurl**

| PR: https://pagure.io/koji/pull-request/3146

``--topdir`` option is treated as a superior to ``--topurl``. Specifying both
doesn't have semantical value, so we're overriding the second one.


API Changes
-----------
**Deprecated force option in groupPackageListRemove call**

| PR: https://pagure.io/koji/pull-request/3145

``force`` doesn't affect the call and as such can be confusing. User could
expect something which is not really happening (e.g. overriding some locks).
We're going to remove this option in near future.

**Deprecated remove-channel CLI and removeChannel API**

| PR: https://pagure.io/koji/pull-request/3158

``edit-channel`` and ``editChannel`` can do the same now.

**listPackages has now default with_blocked=True**

| PR: https://pagure.io/koji/pull-request/3196

It is more a regression fix and returning to original behaviour with this new
option.


Builder Changes
---------------
**AuthExpire returns code 1 in kojid**

| PR: https://pagure.io/koji/pull-request/3119

For automation of ``kojid`` restarts it is useful to retun exit code instead of
raising an exception.

System Changes
--------------
**Add limits on name values**

| PR: https://pagure.io/koji/pull-request/3028

Historically we were not casting any limits on names used throughout the koji.
We've trusted admins to create meaningful names. Nevertheless, some automation
tools are now having permissions which allow them to create tags, targets, etc.
As we're not able to control their code, we've introduced name templates which
can be used to limit which characters (via regexps) can be part of names. For
details see the documentation for hub options ``RegexNameInternal``,
``RegexUserName`` and ``MaxNameLengthInternal``.

**RLIMIT_OFILE alias for RLIMIT_NOFILE**

| PR: https://pagure.io/koji/pull-request/3116

``RLIMIT_OFILE`` is an old deprecated name, so we're using it as an alias for
now.

**Deprecated hub option DisableGSSAPIProxyDNFallback**

| PR: https://pagure.io/koji/pull-request/3117

Option doesn't affect anything anymore.

**Centralize name/id lookup clauses**

| PR: https://pagure.io/koji/pull-request/3123

Code improvement to make SQL layer more consistent.

**only raise error when authtype is not proxyauthtype**

| PR: https://pagure.io/koji/pull-request/3164

Fixing regression for webUIs having same authentication mechanism as user.

**Add description for permissions**

| PR: https://pagure.io/koji/pull-request/3214

All permissions now can have description visible via ``list-permissions``, so
they can be documented inside the koji not in external docs.

**Provide meaningful message when importing image files fails**

| PR: https://pagure.io/koji/pull-request/3223

More verbose error message for file types non-existent in the db.

**Allow password in SCM url with new builder option**

| PR: https://pagure.io/koji/pull-request/3212

Token/password can now be part of SCM url when submitting the build. Note, that
this will be visible everywhere and should be used only with caution (public
tokens). Otherwise, it can quite easily leak passwords. For the same reason this
behaviour must be explicitly allowed via ``allow_password_in_scm_url`` kojid
option.

**lib: refactor variables in is_conn_err()**

| PR: https://pagure.io/koji/pull-request/3204

Simple code cleanup

Web
---
**Rpminfo/fileinfo/imageinfo/archiveinfo page shows human-readable filesize**

| PR: https://pagure.io/koji/pull-request/3137

**Taginfo page shows packages with/without blocked**

| PR: https://pagure.io/koji/pull-request/3159

**Show total builds and add two more date options**

| PR: https://pagure.io/koji/pull-request/3215

Buildsbystatus web page is showing a bit more information now.


Plugins
-------
**protonmsg: allow users to specify router-specific topic prefixes**

| PR: https://pagure.io/koji/pull-request/3168

**kiwi**

Kiwi plugin for building images based on XML description files was extended and
refactored a bit, so it is now almost production-ready. We expect that in one or
two releases we can flip it to first-class plugin.

**kiwi: implant releasever into kiwi description**

| PR: https://pagure.io/koji/pull-request/3205

**kiwi: save modified .kiwi files per arch**

| PR: https://pagure.io/koji/pull-request/3211

Documentation
-------------
**Explain IMA signing vs usual RPM signing**

| PR: https://pagure.io/koji/pull-request/3206

**Improve multicall documentation**

| PR: https://pagure.io/koji/pull-request/3226

**Additional explanations for RPM signatures**

| PR: https://pagure.io/koji/pull-request/3218

**Link to koji overview video**

| PR: https://pagure.io/koji/pull-request/3195

**Drop RHEL6 references**

| PR: https://pagure.io/koji/pull-request/3177
Koji 1.29.1 Release notes
=========================

All changes can be found in `the roadmap <https://pagure.io/koji/roadmap/1.29.1/>`_.
Most important changes are listed here.

Migrating from Koji 1.29
------------------------

No special actions are needed.


Security Fixes
--------------

None

Library Changes
---------------

Client Changes
--------------
**Download output for all task types in download-task**

| PR: https://pagure.io/koji/pull-request/3343

It was not possible to download e.g. image scratch task. We've
extended download options, so all task types can be downloaded now.

Hub Changes
-----------
**postgresql hub: date_part instead of EXTRACT**

| PR: https://pagure.io/koji/pull-request/3388

PostgreSQL 14 introduced small changes in formats which led to failing koji on
Fedora rawhide.

**Rename log to cg_import.log and add successful import log message.**

| PR: https://pagure.io/koji/pull-request/3415

Small change renaming log from 1.29 `external_rpm_warning.log` was too
confusing, so we've renamed it to `cg_import.log` and added final SUCCESS
message.

**more verbose default policy denials**

| PR: https://pagure.io/koji/pull-request/3398

Default policies were not verbose enough, so it could be overseen that they are
used instead of local policies from some reason (typically apache can't read the
proper config).

**Fix wrong encoding in changelog entries**

| PR: https://pagure.io/koji/pull-request/3413

There are still some very old rpms which contain broken unicode in their
changelogs (behaviour which is impossible to trigger nowadays). We're going to
replace the broken characters with '?'.

Web Changes
-----------
**Order channels at hosts page**

| PR: https://pagure.io/koji/pull-request/3368

Simple change to use ordered channel listing in place of random order.

Plugins
-------

**Fix arches check in kiwi plugin**

| PR: https://pagure.io/koji/pull-request/3428

As part of 1.29 security improvements we've made a regression about architecture
handling in kiwi plugin which prevented builds. It is fixed now and works as
before.

Documentation/DevTools Changes
------------------------------
 * `Add long description to setup.py <https:/pagure.io/koji/pull-request/3374`_
 * `CGRefundBuild description in CG docs <https://pagure.io/koji/pull-request/3411`_

Koji 1.29.0 Release notes
=========================

All changes can be found in `the roadmap <https://pagure.io/koji/roadmap/1.29/>`_.
Most important changes are listed here.


Migrating from Koji 1.28/1.28.1
-------------------------------

For details on migrating see :doc:`../migrations/migrating_to_1.29`


Security Fixes
--------------

None


Client Changes
--------------
**Retry gssapi_login if it makes sense**

| PR: https://pagure.io/koji/pull-request/3248

Only place in CLI where we did not retried when server was inaccessible was
rewriteen to retry in some cases when there is an expecetation that it could
work.

**Fix more users in userinfo**

| PR: https://pagure.io/koji/pull-request/3325

Userinfo regressions - it now correctly list information for all users specified
on command line.

**Allow untag-build for blocked packages**

| PR: https://pagure.io/koji/pull-request/3255

When package is blocked in the tag but there was already some build for that
package, CLI refused to untag it. It is now fixed.

API Changes
-----------
**Remove taskReport API call**

| PR: https://pagure.io/koji/pull-request/3237

**Add strict option to getRPMHeaders**

| PR: https://pagure.io/koji/pull-request/3256

Similarly to other stricts it will raise an error if rpm doesn't exist. Empty
dictionary was returned instead which was indistunguishable with missing
requested key in existing RPM. This behaviour is still available with
``strict=False``.

**Add extra of builds to listTagged call result**

| PR: https://pagure.io/koji/pull-request/3282

Pattern of ``listTagged`` followed by multicall of ``getBuild`` is used often,
so we decided to put missing info (``extra`` field) directly into ``listTagged``
so only one call is required now.

**Add as_string option to showOpts for raw string or dict output**

| PR: https://pagure.io/koji/pull-request/3313
| PR: https://pagure.io/koji/pull-request/3349

Usable for koji administration - additional format for getting current
configuration in machine-readable format. ``as_string=True`` reflects old
behaviour.

Builder Changes
---------------
**Check ccache size before trying to use it**

| PR: https://pagure.io/koji/pull-request/3234

Machine restart could cause failing authentication for the builder as ccache
file is not properly deleted.

**call git rev-parse before chowning source directory**

| PR: https://pagure.io/koji/pull-request/3355

Newer git (2.35.2) fixed long-standing git security issue but we needed to adapt
our code as a result.

System Changes
--------------
**Remove koji.listFaults**

| PR: https://pagure.io/koji/pull-request/3238

This method was not used anywhere in koji code and could complicate future
exception redesign, so we've removed it.

**Add log file for match_rpm warnings in cg_import**

| PR: https://pagure.io/koji/pull-request/3257

CG import was writing errors to hub logs when rpm were not found in koji. As it
could be a problem in some workflows, we've added more visibility by uploading
separate log to the imported build which lists these errors.

**Return 400 codes when client fails to send a full request**

| PR: https://pagure.io/koji/pull-request/3269

Changing to errcode instead of exception will allow client to retry it. So,
another type of network problems can be handled more transparently.

**Weak dep on httpd.service for kojid/ra - koji**

| PR: https://pagure.io/koji/pull-request/3277

In case more services are running on same node (e.g. hub + kojira) this change
will trigger ``kojid``/``kojira`` after hub is running resulting in no initial
errors due to inaccessible hub.

**Log content-length when we get an error reading request**

| PR: https://pagure.io/koji/pull-request/3289

Additional logging information for unstable networks.

**Hub, plugins and tools inputs validation**

| PR: https://pagure.io/koji/pull-request/3318

Internal changes improving security via better input handling.

**Add admin check when priority has negative value in wrapperRPM**

| PR: https://pagure.io/koji/pull-request/3321
| PR: https://pagure.io/koji/pull-request/3347

It was a bug present in the code, where ``wrapperRPM`` s could have been set to
negative priority even by their owners. Negative priorities are reserved just
for admins.

**Permit forcing releasever/arch within mock per tag**

| PR: https://pagure.io/koji/pull-request/3358

Mock's ``forcearch`` and ``relver`` options are now accesible via tag extras.

Web
---
**Add free task for admin**

| PR: https://pagure.io/koji/pull-request/3272

In addition to cancelling task, admin now has also "free" button available.

**Add blocked option to packages page**

| PR: https://pagure.io/koji/pull-request/3334
| PR: https://pagure.io/koji/pull-request/3329

New filter to see blocked packages in webui.

**Display load/capacity at hosts page**

| PR: https://pagure.io/koji/pull-request/3346

It is sometimes useful to see these values there.

Plugins
-------
**Adding Driver Update Disk building support**

| PR: https://pagure.io/koji/pull-request/3217

Previously Driver Update Disks were done by custom scripts or by `ddiskit
<https://github.com/orosp/ddiskit/blob/master/bin/ddiskit>`_. Now it can be done
in koji, so it benefits from auditability, etc.

**koji-sidetag-cleanup: delete inactive tags**

| PR: https://pagure.io/koji/pull-request/3294

New cleanup option allow to delete sidetags which are no longer active (no new
builds are tagged there).

**Add tag2distrepo plugin to hub**

| PR: https://pagure.io/koji/pull-request/3326

Plugin will trigger ``distrepo`` tasks for configured tags when a new build
arrives.

**Fix age to max_age in protonmsg**

| PR: https://pagure.io/koji/pull-request/3344

Incoherent naming in documentation and code is now unified.


Documentation
-------------
**Clarify rpm imports**

| PR: https://pagure.io/koji/pull-request/3301

**Better description for kiwi channel requirements**

| PR: https://pagure.io/koji/pull-request/3331

**Winbuild documentation updates**

| PR: https://pagure.io/koji/pull-request/3333

**Document "list-signed" requires filesystem access**

| PR: https://pagure.io/koji/pull-request/3342

Koji 1.30.1 Release notes
=========================

All changes can be found in `the roadmap <https://pagure.io/koji/roadmap/1.30.1/>`_.
Most important changes are listed here.


Migrating from Koji 1.30
------------------------

No special actions are needed.

Security Fixes
--------------

None

Hub Changes
-----------
**Use nextval function instead of query 'SELECT nextval'**

| PR: https://pagure.io/koji/pull-request/3484

Incremental step to remove raw SQL queries.

**Return data when query execute asList with transform**

| PR: https://pagure.io/koji/pull-request/3513

Bug removal in part of code which was not used yet.

Client Changes
--------------
**Allow redirects for file size checking**

| PR: https://pagure.io/koji/pull-request/3464

Commands downloading files from the server were not properly checking file size
when trying to append partially downloaded files.

**Download all files, skip downloaded files**

| PR: https://pagure.io/koji/pull-request/3502

Regression fix for 1.30 to have backward-compatible behaviour with
``download-task``.

Builder Changes
---------------
**Various simple updates to windows content builder**

| PR: https://pagure.io/koji/pull-request/3503
| PR: https://pagure.io/koji/pull-request/3504
| PR: https://pagure.io/koji/pull-request/3505
| PR: https://pagure.io/koji/pull-request/3507

Plugins
-------
**kiwi: Handle include protocols**

| PR: https://pagure.io/koji/pull-request/3496

Forbid other include protocols than ``this://`` to prevent directory traversal.

**kiwi: Explicitly use koji-generated description**

| PR: https://pagure.io/koji/pull-request/3498

Koji now requires user to explicitely select descriptions file instead of
leaving it up to kiwi to select the right one.

Web Changes
-----------
**More generic taskinfo parameter handling**

| PR: https://pagure.io/koji/pull-request/3455

Internal change to use standardized parameter handling on ``taskinfo`` page.
This also replace "Parameters are not correct for this method." with data
display.

**Fix dist-repo repo.json url**

| PR: https://pagure.io/koji/pull-request/3469

``repoinfo`` page display correct link to distrepos.

**Fix arch filter in list of hosts webUI**

| PR: https://pagure.io/koji/pull-request/3492

Filtering via arch sometimes returned additional records.


Documentation/DevTools Changes
------------------------------
* `Fix flake8 errors <https://pagure.io/koji/pull-request/3479>`_
* `Fix URLs to pull requests <https://pagure.io/koji/pull-request/3481>`_
* `Block py3 compilation in py2 env <https://pagure.io/koji/pull-request/3486>`_
* `Explain waitrepo tasks in vm channel <https://pagure.io/koji/pull-request/3506>`_
* `Fix missing characters in config example <https://pagure.io/koji/pull-request/3518>`_

Koji 1.30.0 Release notes
=========================

All changes can be found in `the roadmap <https://pagure.io/koji/roadmap/1.30/>`_.
Most important changes are listed here.


Migrating from Koji 1.29/1.29.1
-------------------------------

For details on migrating see :doc:`../migrations/migrating_to_1.30`


Security Fixes
--------------

None

Client Changes
--------------
**Remove --paths option from list-buildroot**

| PR: https://pagure.io/koji/pull-request/3352

This option was not used and was deprecated. Now it was removed.

**list-channels with specific arch**

| PR: https://pagure.io/koji/pull-request/3363

New filtering ``--arch`` option.

**download-task retry download file**

| PR: https://pagure.io/koji/pull-request/3385

Additional place where we retry download in case of temporary network issues.

**Add a utility function to watch builds**

| PR: https://pagure.io/koji/pull-request/3406

For CLI plugin development we've separated ``wait_repo`` function to library.

**Rewritten download-task**

| PR: https://pagure.io/koji/pull-request/3425
| PR: https://pagure.io/koji/pull-request/3430
| PR: https://pagure.io/koji/pull-request/3438
| PR: https://pagure.io/koji/pull-request/3459
| PR: https://pagure.io/koji/pull-request/3462

``download-task`` command was rewritten to solve some long-standing issues. E.g.
downloading image scratch builds or some conflicting files. Command should be
backward-compatible but allows additional options like ``--dir-per-arch`` and
additional filtering.

API Changes
-----------
**Remove force option from groupPackageListRemove hub call**

| PR: https://pagure.io/koji/pull-request/3354

Deprecated unused option was finally removed.

**Remove deprecated remove-channel/removeChannel**

| PR: https://pagure.io/koji/pull-request/3357

Same here - same functionality is available via ``disable-channel/editChannel``.

**Use compression_type in addArchiveType**

| PR: https://pagure.io/koji/pull-request/3391

Archive files had available listing for some specific extensions (zip, tar).
Other archives couldn't been displayed even if they had the same compression
format (e.g. jar which is hidden zip). Explicitly specifying compression type
via ``addArchiveType`` allows also these other types to be inspected.

Library Changes
---------------
**Fix rpm_hdr_size file closing**

| PR: https://pagure.io/koji/pull-request/3423

Simple fix for potential file descriptor leak in user scripts.

**Authtype as enum and getSessionInfo prints authtype name**

| PR: https://pagure.io/koji/pull-request/3437

``koji.AUTHTYPE_*`` were converted to enum like other ``koji.*`` constants. It
unifies the usage + prints human-readable strings instead of numeric IDs.

**parse_arches allows string and list of arches**

| PR: https://pagure.io/koji/pull-request/3440

Utility conversion function now accepts more types than before.

System Changes
--------------
**Server-side clonetag**

| PR: https://pagure.io/koji/pull-request/3308

Major rehaul of ``clone-tag`` command. It was completely removed from CLI-side
and everything happens at hub now. It is immensely faster in real workload (for
big tags from hours to seconds). Nevertheless, we've lost some functionality -
typically verbose mode is no more possible as everything happens in one
transaction now, so there is almost no text output. We believe it is worth the
speed improvements. There are also very minor semantical changes (e.g. event ids
for separate steps) which shouldn't be noticed by vast majority of users.

If target tag doesn't exist, things are even more faster as we don't need to
check what is there, etc.

New API calls related to this behaviour are now available: ``massTag``, ``snapshotTag``,
``snapshotTagModify``. Especially ``massTag`` could be used by many admins as it
is basically batch call of ``tagBuildBypass`` ('tag' permission needed) dropping
need of ``tagBuildBypass`` multicall overhead.

Note, that this is breaking change. If you use new client with older hub, you'll
not be able to run clone-tag command at all. In such a case we would recommend
temporarily using older client (1.29.1 - e.g. installed via pip if it is not
available as rpm)

**Drop old indices**

| PR: https://pagure.io/koji/pull-request/3359

Few unused old indices could still exists in some deployments. Migration script
will drop them.

**Correct getAverageDuration values for most GC builds**

| PR: https://pagure.io/koji/pull-request/3402
| PR: https://pagure.io/koji/pull-request/3457

``getAverageDuration`` was not making much sense for packages which had also
imported content. Now we ignore zero times for imported content getting better
estimation of real koji builds.

**Consistence pre/postPackageListChange sequence**

| PR: https://pagure.io/koji/pull-request/3403

If ``packageListAdd`` ended with no action because package is already in the
list, only ``prePackageListChange`` callback was run. In such case no callback
should be run.

**Check release/version format in cg_import**

| PR: https://pagure.io/koji/pull-request/3422

Failed builds could have had non-sense in release/version. It was never true for
completed builds as koji wouldn't allow such build to finish. Anyway, it was
confusing to see such items in failed builds list, so we've denied it from the
beginning.

**Expect dict for chainmaven builds**

| PR: https://pagure.io/koji/pull-request/3444

Regression fix for ``chainMaven`` API call which was refusing correct input from
1.29.

Builder Changes
---------------
**Catch koji.AuthError and bail out**

| PR: https://pagure.io/koji/pull-request/3364

kojid and kojira now fail on authentication errors and don't try forever.
Anyway, daemons will be restarted via systemd (possibly loading updated
certificates, keytabs, ...) so it could help in some situations.

**Don't propagate SIGHUP ignore to child processes**

| PR: https://pagure.io/koji/pull-request/3404

Some packages are testing SIGHUP behaviour (e.g. cpython) in their test suite.
Previously we've been blocking SIGHUP in child processes (mock), so it needed
some care from packagers. There is no need to do that, so we've dropped this
behaviour.

**Beautify logged commands issued by koji**

| PR: https://pagure.io/koji/pull-request/3405

In few cases (e.g. createrepo) koji logs very long command lines. They are now
wrapped to 80 characters for easier log reading.

**Don't crash in _checkImageState if there's no image.os_plugin**

| PR: https://pagure.io/koji/pull-request/3445

In some cases ImageFactory tried to tear down the VM even in case there wasn't
right code/plugin for that.

Web Changes
-----------
**archivelist and rpmlist raise error when imageID is unknown**

| PR: https://pagure.io/koji/pull-request/3382

Don't crash on non-existing IDs.

**Set SameSite and Set-Cookie2**

| PR: https://pagure.io/koji/pull-request/3390

We've added these http headers to increase the security.

**Convert data to string in escapeHTML first**

| PR: https://pagure.io/koji/pull-request/3450

Better rendering of some non-textual (int, datetime) values.

Plugin Changes
--------------
**proton: save messages when connection fails**

| PR: https://pagure.io/koji/pull-request/3360

Further improvement of handling message bus issues. Some types of errors were
not treated as a connection problem (DNS resolution) thus losing messages.

**kiwi: fix arches check**

| PR: https://pagure.io/koji/pull-request/3428

Regression fix.

Documentation
-------------
**Increase unit tests**

| PR: https://pagure.io/koji/pull-request/3380
| PR: https://pagure.io/koji/pull-request/3383


Koji 1.31.0 Release notes
=========================

All changes can be found in `the roadmap <https://pagure.io/koji/roadmap/1.31/>`_.
Most important changes are listed here.


Migrating from Koji 1.30/1.30.1
-------------------------------

For details on migrating see :doc:`../migrations/migrating_to_1.31`


Security Fixes
--------------

None


Client Changes
--------------
**download-task more specific info for non-CLOSED tasks**

| PR: https://pagure.io/koji/pull-request/3488

A bit more info about state of the task.

**Add count and size for download-build**

| PR: https://pagure.io/koji/pull-request/3516

Unification of output how many files are being downloaded.

**list-hosts fix when list of channels is empty**

| PR: https://pagure.io/koji/pull-request/3533

Command was failing when builder was not assigned to any channel.

**edit-channel set default return value and print error msg to stderr**

| PR: https://pagure.io/koji/pull-request/3535

Unification of return values and error-handling.

**Fix nvr sorting in list-builds**

| PR: https://pagure.io/koji/pull-request/3542

Confusing default ``--sort-key=nvr`` was replaced by ``--sort-key=build_id``.
NVR sorting was/is in reality alphabetic sort, not NVR sort. We're not planning
to introduce NVR comparison anywhere, so we've changed the default here.

**Add regex --filter and --skip option for download-task**

| PR: https://pagure.io/koji/pull-request/3552

``download-task`` command can download many files and default filters like
``--arch`` can be insufficient and lead to downloading much more content than
needed. Two new regexp filters are introduced to further limit bandwith.

API Changes
-----------
**Allow buildTagID and destTagID as string and dict in getBuildTargets**

| PR: https://pagure.io/koji/pull-request/3550

A bit more comfort in specifying these values.

Builder Changes
---------------
**Remove login shell from kojibuilder user**

| PR: https://pagure.io/koji/pull-request/3476

Login shell is not needed for normal ``kojid`` usage. It was meant more for
debugging but it is better to lock it off by default due to potential security
risks.

**Enable fetching any ref from git repo**

| PR: https://pagure.io/koji/pull-request/3509

Support for fetching any refs which were not fetched by original ``git clone``.
Typically merge requests.

**Error on list-tagged --sigs --paths without mount**

| PR: https://pagure.io/koji/pull-request/3531

It was confusing for users that there is no output if they don't have
``/mnt/koji`` (or other topdir) mounted. Such combination of options now fail
instead of printing empty output.

**Fix restartHosts on py 3.5+**

| PR: https://pagure.io/koji/pull-request/3541

Newer python introduced behaviour which leads to non-working ``restartHosts``
task parent.

System Changes
--------------
**Build policy**

| PR: https://pagure.io/koji/pull-request/3407

New ``build_rpm`` policy for specifying which builds are allowed. It is
superceeding ``build_from_srpm`` and ``build_from_repo_id`` policies,
effectively adding capability for ``build_from_scm`` policy and merging these to
one more simple. Former two are now deprecated and will be removed in 1.33.

**Save source for wrapperRPM**

| PR: https://pagure.io/koji/pull-request/3417

``wrapperRPM`` is now more compatible with regular rpm builds storing
``source`` into metadata.

**Header-based sessions**

| PR: https://pagure.io/koji/pull-request/3426

Formerly, we've had session id and key as a part of URL. These values are now
moved to HTTP headers to be more in line with current security practices.
Backward compatibility is still ensured and can be turned off by
``DisableURLSessions`` in config. Old-style session support will be removed in
1.34.

**Move database classes and functions to koji/db.py**

| PR: https://pagure.io/koji/pull-request/3474
| PR: https://pagure.io/koji/pull-request/3489
| PR: https://pagure.io/koji/pull-request/3563
| PR: https://pagure.io/koji/pull-request/3513

Most of the database queries are rewritten to use ``*Processor`` classes which
improves maintanability and allows easier migration to SQLAlchemy or other
library. Also all db code is now in ``koji/db.py``, so also other tools can
utilize it (typically ``koji-sweep-db`` script).

**Emphasize non-working image XML**

| PR: https://pagure.io/koji/pull-request/3490

Koji is supporting more output formats for images than libvirt can utilize. For
these we're adding some more info directly to libvirt's XML, so end-user is more
informed about need to convert the data to some format libvirt supports.

**Log when session ID, session key and hostip is not related**

| PR: https://pagure.io/koji/pull-request/3557

Additional logging for security/audit reasons, so we can more easily detect
e.g. session stealing.

**Fedora 37 compatibility update**

| PR: https://pagure.io/koji/pull-request/3592

Python 3.11 finally dropped ``inspect.getargspec``, so hub/web are not running
on F37. Simple update to ``getfullargspec`` fixes it. Change is backward-compatible to
python 3.6 which is still oldest :doc:`supported version
<../supported_platforms>` for hub/web.

Web
---
**Add active sessions web page**

| PR: https://pagure.io/koji/pull-request/3446

In line with other security/transparency items in this release, we've added
simple web page to list all active sessions user currently have.

**More generic taskinfo parameter handling**

| PR: https://pagure.io/koji/pull-request/3455

Task web page sometimes shows cryptic messages like "Parameters are not right
for this method" and for some less integrated plugins it shows just python dict
of values. This was improved to handle such values more systematically.

Plugins
-------
**kiwi: Fix include path**

| PR: https://pagure.io/koji/pull-request/3555

More safe include handling in kiwi's profiles.

**kiwi: Propagate --type option**

| PR: https://pagure.io/koji/pull-request/3558

New option to select image type.

**kiwi: Bind builders's /dev only in old_chroot**

| PR: https://pagure.io/koji/pull-request/3585

Device-mapper based images needs exposed /dev/mapper/control file, but not whole
dev filesystem. /dev filesystem is now mounted only in ``old_chroot`` buildroots.
Nspawn-based buildroots (``mock.new_chroot=True``) don't bind it and for dm there
is a corresponding mock `change
<https://github.com/rpm-software-management/mock/pull/1005>`_.

Utilities
---------
**koji-gc: Fix check for type cc_addr, bcc_addr**

| PR: https://pagure.io/koji/pull-request/3573

**koji-sweep-db: fix**

| PR: https://pagure.io/koji/pull-request/3566

**Add absolute to clean sessions in koji-sweep-db**

| PR: https://pagure.io/koji/pull-request/3569


VM
--
**Various updates to kojivmd**

| PR: https://pagure.io/koji/pull-request/3503
| PR: https://pagure.io/koji/pull-request/3504
| PR: https://pagure.io/koji/pull-request/3505
| PR: https://pagure.io/koji/pull-request/3507
| PR: https://pagure.io/koji/pull-request/3538
| PR: https://pagure.io/koji/pull-request/3576
| PR: https://pagure.io/koji/pull-request/3577
| PR: https://pagure.io/koji/pull-request/3578

Various updates to changes in libvirt, improving error handling, VM cleanup,
better repo handling, python3 and documentation fixes.

Documentation
-------------
**Explain waitrepo tasks in vm channel**

| PR: https://pagure.io/koji/pull-request/3506

**Change license identifiers to SPDX format**

| PR: https://pagure.io/koji/pull-request/3521

**Increase unit tests**

| PR: https://pagure.io/koji/pull-request/3528
| PR: https://pagure.io/koji/pull-request/3548
| PR: https://pagure.io/koji/pull-request/3546


=========
Runs Here
=========

List of locations that Koji is currently in use
===============================================

Please add yourself here if you run a local version of Koji. Please also
describe what you are using it for.

+---------+----------------------------------------------------------------------------+-------------------------------------------------------------------+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
| #       | Project                                                                    | Site Name                                                         | Used for                                                                                                                                                                                                                                                                  |
+=========+============================================================================+===================================================================+===========================================================================================================================================================================================================================================================================+
| 0       | Grid and HPC Operations Center of A.U.Th                                   | Not-Public                                                        | Building packages for local fabric needs. Maintenance of RPM repositories. Monitoring, messaging and information system software for `LCG <http://cern.ch/lcg>`__.                                                                                                        |
+---------+----------------------------------------------------------------------------+-------------------------------------------------------------------+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
| 1       | pc V Pharmacy                                                              | Not-public                                                        | Compiling proprietary software across Fedora and RHEL. Both the software and any dependencies that are not available in Fedora and/or RHEL that may also be proprietary.                                                                                                  |
+---------+----------------------------------------------------------------------------+-------------------------------------------------------------------+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
| 2       | CERN                                                                       | \*                                                                | All RPMs for SLC6 (Scientific Linux CERN), CC7 (CERN CentOS), and CentOS8 for CERN are built through our own internal instance of koji and consumed via http://linuxsoft.cern.ch                                                                                          |
+---------+----------------------------------------------------------------------------+-------------------------------------------------------------------+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
| 3       | `fedora.danny.cz <http://fedora.danny.cz/danny>`__                         | the hub is not public yet                                         | http://sharkcz.livejournal.com/3988.html                                                                                                                                                                                                                                  |
+---------+----------------------------------------------------------------------------+-------------------------------------------------------------------+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
| 4       | `Ergo Project <http://www.ergo-project.org>`__                             | `Koji Web Interface <http://koji.ergo-project.org>`__             | Fast-Track repositories for Enterprise Linux and Fedora, FTBFS, `Blog posts on Koji <http://planet.ergo-project.org/category/tags/koji>`__                                                                                                                                |
+---------+----------------------------------------------------------------------------+-------------------------------------------------------------------+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
| 5       | `NetNix Estonia OÜ <http://netnix.ee/>`__                                  | Internal                                                          | Packaging commercial and in-house software for RHEL/CentOS/Fedora support. Multiple instances installed, locally and customer sites.                                                                                                                                      |
+---------+----------------------------------------------------------------------------+-------------------------------------------------------------------+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
| 6       | `Kolab Systems AG <http://www.kolabsys.com>`__                             | Not Public (yet)                                                  | Packaging for ISV products                                                                                                                                                                                                                                                |
+---------+----------------------------------------------------------------------------+-------------------------------------------------------------------+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
| 7       | `Lebanon Evangelical School for Boys and Girls <http://www.lesbg.com>`__   | http://koji.lesbg.com/koji                                        | Used to provide configuration RPMS for our three schools                                                                                                                                                                                                                  |
+---------+----------------------------------------------------------------------------+-------------------------------------------------------------------+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
| 8       | `Messinet Secure Services <http://messinet.com>`__                         | http://messinet.com/koji                                          | Building and packaging to support local infrastructure as well as incorporate software not packaged at Fedora or RPMFusion. Most packaged software is available to all. See `Messinet Secure Services Fedora RPM Repository <http://messinet.com/rpms/>`__ for details.   |
+---------+----------------------------------------------------------------------------+-------------------------------------------------------------------+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
| 9       | `ACOSS France <http://www.acoss.fr/>`__                                    | Not-public                                                        | Building and packaging on top of RHEL and CentOS 6 for newer software version.                                                                                                                                                                                            |
+---------+----------------------------------------------------------------------------+-------------------------------------------------------------------+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
| 10      | `Virtual Bridges, Inc. <http://www.vbridges.com/>`__                       | Not-public                                                        | Building `VERDE LEAF <http://www.vbridges.com/products/verde/verde-leaf/>`__ virtualization platform/distribution.                                                                                                                                                        |
+---------+----------------------------------------------------------------------------+-------------------------------------------------------------------+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
| 11      | Caltech                                                                    | http://koji.hep.caltech.edu/koji/                                 | multi-university CERN-based project                                                                                                                                                                                                                                       |
+---------+----------------------------------------------------------------------------+-------------------------------------------------------------------+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
| 12      | Spacewalk                                                                  | http://koji.spacewalkproject.org/koji/                            | building Spacewalk for RHEL 5, 6, and Fedora 14, 15                                                                                                                                                                                                                       |
+---------+----------------------------------------------------------------------------+-------------------------------------------------------------------+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
| 13      | `National Nanotechnology Network Grid <http://www.ngrid.ru/>`__            | http://koji.ngrid.ru/koji/                                        | GridNNN middleware                                                                                                                                                                                                                                                        |
+---------+----------------------------------------------------------------------------+-------------------------------------------------------------------+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
| 14      | TomTom                                                                     | non-public                                                        | used to build rpms that aren't available in the RHEL repos or EPEL                                                                                                                                                                                                        |
+---------+----------------------------------------------------------------------------+-------------------------------------------------------------------+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
| 15      | Amazon                                                                     | non-public                                                        | used to build RPMS for the Amazon Linux AMI                                                                                                                                                                                                                               |
+---------+----------------------------------------------------------------------------+-------------------------------------------------------------------+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
| 16      | Open Science Grid                                                          | http://koji.chtc.wisc.edu/koji/                                   | Used to build RPMs for the Open Science Grid software project. Software built here is deployed across 100+ universities and labs across the U.S.                                                                                                                          |
+---------+----------------------------------------------------------------------------+-------------------------------------------------------------------+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
| 17      | RussianFedora                                                              | http://koji.russianfedora.ru                                      | Building add-on packages for the RFRemix.                                                                                                                                                                                                                                 |
+---------+----------------------------------------------------------------------------+-------------------------------------------------------------------+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
| 18      | Scientific Linux                                                           | Not-public (yet)                                                  | Building Scientific Linux.                                                                                                                                                                                                                                                |
+---------+----------------------------------------------------------------------------+-------------------------------------------------------------------+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
| 19      | Katello                                                                    | http://koji.katello.org/koji/                                     | Building packages for Katello project                                                                                                                                                                                                                                     |
+---------+----------------------------------------------------------------------------+-------------------------------------------------------------------+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
| 20      | NIWA                                                                       | non-public                                                        | Building packages for climate and forecasting project                                                                                                                                                                                                                     |
+---------+----------------------------------------------------------------------------+-------------------------------------------------------------------+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
| 21      | `Xenith Consulting Limited <http://www.xenithconsulting.com>`__            | Not-Public                                                        | Banking, Telecommunications and Embedded Systems                                                                                                                                                                                                                          |
+---------+----------------------------------------------------------------------------+-------------------------------------------------------------------+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
| 22      | Abril Comunicações                                                         | non-public                                                        | Media / Entertainment Company. Building RPM packages for the Digital products                                                                                                                                                                                             |
+---------+----------------------------------------------------------------------------+-------------------------------------------------------------------+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
| 23      | The FireServer Project                                                     | http://www.fireserver.com.br                                      | Building packages for FireServer Project                                                                                                                                                                                                                                  |
+---------+----------------------------------------------------------------------------+-------------------------------------------------------------------+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
| 24      | Seneca Centre for Development of Open Technology                           | http://koji.pidora.ca (info at http://pidora.ca)                  | Pidora (Fedora Remix for Raspberry Pi) and some other projects.                                                                                                                                                                                                           |
+---------+----------------------------------------------------------------------------+-------------------------------------------------------------------+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
| 25      | OpenVZ/Virtuozzo                                                           | `https://openvz.org/Virtuozzo <https://openvz.org/Virtuozzo>`__   | Building OpenVZ packages                                                                                                                                                                                                                                                  |
+---------+----------------------------------------------------------------------------+-------------------------------------------------------------------+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
| 26      | Cloud Linux                                                                | `http://koji.cloudlinux.com/ <http://koji.cloudlinux.com/>`__     | Building Cloud Linux packages                                                                                                                                                                                                                                             |
+---------+----------------------------------------------------------------------------+-------------------------------------------------------------------+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
| 27      | WadersOS                                                                   | non-public                                                        | Building WadersOS (Nokia fork of Fedora rawhide) packages and images                                                                                                                                                                                                      |
+---------+----------------------------------------------------------------------------+-------------------------------------------------------------------+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
| 28      | AnyStor                                                                    | `https://abs.gluesys.com/ <https://abs.gluesys.com/>`_            | Building AnyStor OS (`Gluesys <http://gluesys.com/?lang=en>`__ fork of CentOS) packages and images                                                                                                                                                                        |
+---------+----------------------------------------------------------------------------+-------------------------------------------------------------------+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
| 29      | `XCP-ng virtualization platform <https://xcp-ng.org/>`__                   | https://koji.xcp-ng.org/                                          | Building XCP-ng                                                                                                                                                                                                                                                           |
+---------+----------------------------------------------------------------------------+-------------------------------------------------------------------+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
=====================
Koji Server Bootstrap
=====================

Bootstrapping a new Koji build environment
==========================================

These are the steps involved in populating a new Koji server with
packages so that it can be used for building. This assumes that the Koji
hub is up, appropriate authentication methods have been configured, the
Koji repo administration daemon (``kojira``) is properly configured and
running, and at least one Koji builder (``kojid``) is properly
configured and running. All koji cli commands assume that the user is a
Koji *admin*. If you need help with these tasks, see the
:doc:`ServerHowTo <server_howto>`.

-  Download all source rpms and binary rpms for the arches you're
   interested in

-  Import all source rpms

::

    $ koji import /path/to/package1.src.rpm /path/to/package2.src.rpm ...

If the files are on the same volume as /mnt/koji, you can use
``koji import --link``, which hardlinks the files into place, avoiding
the need to upload them to the hub and **very significantly** increasing
import speed. When using ``--link``, you must run as root. It is
**highly** recommended that you use ``--link``.

If an imported rpm contains an rpm signature, the import does not automatically
write out a signed copy for that signature. The primary copy will be the signed
rpm, and the signature will be noted. If a signed copy is desired (e.g. for
generating :doc:`distrepos <exporting_repositories>`), you can use the koji
write-signed-rpm command.

-  Import all binary rpms using the same method as above

-  Create a new tag

::

    $ koji add-tag dist-foo

-  Tag all of the packages you just imported into the tag you just
   created

You can use ``koji list-untagged`` to get a list of all of the packages
you just imported.

::

    $ koji list-pkgs --quiet | xargs koji add-pkg --owner <kojiuser> dist-foo
    $ koji list-untagged | xargs -n 1 koji call tagBuildBypass dist-foo

We call the *tagBuildBypass* method instead of using ``koji tag-build``
because it doesn't require the builders to process *tagBuild* tasks one
at a time, but does the tagging directly. This will save a significant
amount of time, especially when tagging a large number of packages.

-  Create a build tag with the desired arches, and the previously
   created tag as a parent

::

    $ koji add-tag --parent dist-foo --arches "i386 x86_64 ppc ppc64" dist-foo-build

-  Create a build target that includes the tags you've already created

::

    $ koji add-target dist-foo dist-foo-build

-  Create a *build* group associated with your build tag

::

    $ koji add-group dist-foo-build build

-  Populate the *build* group with packages that will be installed into
   the minimal buildroot

You can find out what the current build group for Fedora is by running
``koji -s https://koji.fedoraproject.org/kojihub list-groups f17-build``
against the Fedora Koji instance. This is probably a good
starting point for your minimal buildroot.

::

    $ koji add-group-pkg dist-foo-build build pkg1
    $ koji add-group-pkg dist-foo-build build pkg2

If you want to fully duplicate Fedora's group data for a tag, then it would be
easier to do it in bulk -- export Fedora's data and import it to your build
tag.

::

    $ koji -s https://koji.fedoraproject.org/kojihub show-groups --comps f17-build > comps.xml
    $ koji import-comps comps.xml dist-foo-build

-  regenerate the repo

::

    $ koji regen-repo dist-foo-build

-  Wait for the repo to regenerate, and you should now be able to run a
   build successfully.
=============
Server How To
=============

Setting Up a Koji Build System
==============================

The Koji components may **live** on separate resources as long as all resources
are able to communicate. This document will cover how to setup each service
individually, however, all services may **live** on the same resource.

Knowledge Prerequisites
=======================

* Basic understanding of SSL and authentication via certificates and/or
  Kerberos credentials
* Basic knowledge about creating a database in PostgreSQL and importing a schema
* Working with psql
* Basic knowledge about Apache configuration
* Basic knowledge about `dnf`_/`yum`_/`createrepo`_/`mock`_ - else you'll not
  be able to debug problems!
* Basic knowledge about using command line
* Basic knowledge about RPM building
* Simple usage of the Koji client
* For an overview of yum, mock, Koji (and all its subcomponents), mash, and how
  they all work together, see the excellent slides put together by `Steve
  Traylen at CERN <http://indico.cern.ch/event/55091>`_.

Package Prerequisites
=====================

On the server (koji-hub/koji-web)
---------------------------------
* httpd
* mod_ssl
* postgresql-server
* mod_wsgi

On the builder (koji-builder)
-----------------------------
* mock
* setarch (for some archs you'll require a patched version)
* rpm-build
* createrepo

A note on filesystem space
==========================
Koji will consume copious amounts of disk space under the primary KojiDir
directory (as set in the kojihub.conf file - defaults to ``/mnt/koji``).
However, as koji makes use of mock on the backend to actually create build
roots and perform the builds in those build roots, it might come to a surprise
to users that a running koji server will consume large amounts of disk space
under ``/var/lib/mock`` and ``/var/cache/mock`` as well. Users should either
plan the disk and filesystem allocations for this, or plan to modify the
default mock build directory in the kojid.conf file. If you change the
location, ensure that the new directories are owned by the group "mock" and
have 02755 permission.

Koji Authentication Selection
=============================
Koji primarily supports Kerberos and SSL Certificate authentication. For basic
koji command line access, plain user/pass combinations are possible.  However,
kojiweb does **not** support plain user/pass authentication and once either
Kerberos or SSL Certificate authentication is enabled for kojiweb, the plain
user/pass method will stop working entirely.  For this reason we encourage
skipping the plain user/pass method altogether and properly configuring either
Kerberos or SSL Certification authentication from the start.

The decision on how to authenticate users will affect all other actions you
take in setting up koji. For this reason it is a decision best made up front.

For Kerberos authentication
    a working Kerberos environment (the user is assumed to either already have
    this or know how to set it up themselves, instructions for it are not
    included here) and the Kerberos credentials of the initial admin user will
    be necessary to bootstrap the user database.

For SSL authentication
    SSL certificates for the xmlrpc server, for the various koji components,
    and one for the admin user will need to be setup (the user need not know
    how to create certificate chains already, we include the instructions for
    this below).

Setting up SSL Certificates for authentication
----------------------------------------------

Certificate generation
^^^^^^^^^^^^^^^^^^^^^^
Create the ``/etc/pki/koji`` directory and copy-and-paste the ssl.cnf listed
here, and save it in the new directory. This configuration file is used along
with the ``openssl`` command to generate the SSL certificates for the various
koji components.

``ssl.cnf``

::

    HOME                    = .
    RANDFILE                = .rand

    [ca] 
    default_ca              = ca_default

    [ca_default] 
    dir                     = .
    certs                   = $dir/certs
    crl_dir                 = $dir/crl
    database                = $dir/index.txt
    new_certs_dir           = $dir/newcerts
    certificate             = $dir/%s_ca_cert.pem
    private_key             = $dir/private/%s_ca_key.pem
    serial                  = $dir/serial
    crl                     = $dir/crl.pem
    x509_extensions         = usr_cert
    name_opt                = ca_default
    cert_opt                = ca_default
    default_days            = 3650
    default_crl_days        = 30
    default_md              = sha256
    preserve                = no
    policy                  = policy_match

    [policy_match] 
    countryName             = match
    stateOrProvinceName     = match
    organizationName        = match
    organizationalUnitName  = optional
    commonName              = supplied
    emailAddress            = optional

    [req] 
    default_bits            = 2048
    default_keyfile         = privkey.pem
    default_md              = sha256
    distinguished_name      = req_distinguished_name
    attributes              = req_attributes
    x509_extensions         = v3_ca # The extensions to add to the self signed cert
    string_mask             = MASK:0x2002

    [req_distinguished_name] 
    countryName                     = Country Name (2 letter code)
    countryName_default             = AT
    countryName_min                 = 2
    countryName_max                 = 2
    stateOrProvinceName             = State or Province Name (full name)
    stateOrProvinceName_default     = Vienna
    localityName                    = Locality Name (eg, city)
    localityName_default            = Vienna
    0.organizationName              = Organization Name (eg, company)
    0.organizationName_default      = My company
    organizationalUnitName          = Organizational Unit Name (eg, section)
    commonName                      = Common Name (eg, your name or your server\'s hostname)
    commonName_max                  = 64
    emailAddress                    = Email Address
    emailAddress_max                = 64

    [req_attributes] 
    challengePassword               = A challenge password
    challengePassword_min           = 4
    challengePassword_max           = 20
    unstructuredName                = An optional company name

    [usr_cert] 
    basicConstraints                = CA:FALSE
    nsComment                       = "OpenSSL Generated Certificate"
    subjectKeyIdentifier            = hash
    authorityKeyIdentifier          = keyid,issuer:always

    [v3_ca] 
    subjectKeyIdentifier            = hash
    authorityKeyIdentifier          = keyid:always,issuer:always
    basicConstraints                = CA:true

Although it is not required, it is recommended that you edit the default values
in the ``[req_distinguished_name]`` section of the configuration to match the
information for your own server. This will allow you to accept most of the
default values when generating certificates later. The other sections can be
left unedited.

Generate CA
^^^^^^^^^^^

The CA is the Certificate Authority.  It's the key/cert pair used to sign all
the other certificate requests.  When configuring the various koji components,
both the client CA and the server CA will be a copy of the CA generated here.
The CA certificate will be placed in the ``/etc/pki/koji`` directory and the
certificates for the other components will be placed in the
``/etc/pki/koji/certs`` directory. The ``index.txt`` file which is created is
a database of the certificates generated and can be used to view the
information for any of the certificates simply by viewing the contents of
``index.txt``.

::

    cd /etc/pki/koji/
    mkdir {certs,private,confs}
    touch index.txt
    echo 01 > serial
    openssl genrsa -out private/koji_ca_cert.key 2048
    openssl req -config ssl.cnf -new -x509 -days 3650 -key private/koji_ca_cert.key \
    -out koji_ca_cert.crt -extensions v3_ca

The last command above will ask you to confirm a number of items about the
certificate you are generating. Presumably you already edited the defaults for
the country, state/province, locale, and organization in the ``ssl.cnf`` file
and you only needed to hit enter. It's the organizational unit and the common
name that we will be changing in the various certs we create. For the CA
itself, these fields don't have a hard requirement. One suggestion for this
certificate is to use the FQDN of the server.

If you are trying to automate this process via a configuration management
tool, you can create the cert in one command with a line like this:

::

    openssl req -config ssl.cnf -new -x509 \
    -subj "/C=US/ST=Oregon/L=Portland/O=IT/CN=koji.example.com" \
    -days 3650 -key private/koji_ca_cert.key -out koji_ca_cert.crt -extensions v3_ca

Generate the koji component certificates and the admin certificate
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

Each koji component needs its own certificate to identify it. Two of the
certificates (kojihub and kojiweb) are used as server side certificates that
authenticate the server to the client. For this reason, you want the common
name on both of those certs to be the fully qualified domain name of the web
server they are running on so that clients don't complain about the common
name and the server not being the same. You can set the OU for these two
certificates to be kojihub and kojiweb for identification purposes.

For the other certificates (kojira, kojid, the initial admin account, and all
user certificates), the cert is used to authenticate the client to the server.
The common name for these certs should be set to the login name for that
specific component. For example the common name for the kojira cert should be
set to kojira so that it matches the username. The reason for this is that the
common name of the cert will be matched to the corresponding user name in the
koji database. If there is not a username in the database which matches the CN
of the cert the client will not be authenticated and access will be denied.

When you later use ``koji add-host`` to add a build machine into the koji
database, it creates a user account for that host even though the user account
doesn't appear in the user list.  The user account created must match the
common name of the certificate which that component uses to authenticate with
the server. When creating the kojiweb certificate, you'll want to remember
exactly what values you enter for each field as you'll have to regurgitate
those into the /etc/koji-hub/hub.conf file as the ProxyDNs entry.

When you need to create multiple certificates it may be convenient to create a
loop or a script like the on listed below and run the script to create the
certificates. You can simply adjust the number of kojibuilders and the name of
the admin account as you see fit. For much of this guide, the admin account is
called ``kojiadmin``.

::

    #!/bin/bash
    # if you change your certificate authority name to something else you will
    # need to change the caname value to reflect the change.
    caname=koji

    # user is equal to parameter one or the first argument when you actually
    # run the script
    user=$1

    openssl genrsa -out private/${user}.key 2048
    cat ssl.cnf | sed 's/insert_hostname/'${user}'/'> ssl2.cnf
    openssl req -config ssl2.cnf -new -nodes -out certs/${user}.csr -key private/${user}.key
    openssl ca -config ssl2.cnf -keyfile private/${caname}_ca_cert.key -cert ${caname}_ca_cert.crt \
        -out certs/${user}.crt -outdir certs -infiles certs/${user}.csr
    cat certs/${user}.crt private/${user}.key > ${user}.pem
    mv ssl2.cnf confs/${user}-ssl.cnf

Generate a PKCS12 user certificate (for web browser)
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
This is only required for user certificates.

::

    openssl pkcs12 -export -inkey private/${user}.key -in certs/${user}.crt \
        -CAfile ${caname}_ca_cert.crt -out certs/${user}_browser_cert.p12

When generating certs for a user, the user will need the ``${user}.pem``, the
``${caname}_ca_cert.crt``, and the ``${user}_browser_cert.p12`` files which
were generated above.  The ${user}.pem file would normally be installed as
``~/.fedora.cert``, the ``${caname}_ca_cert.crt`` file would be installed as
both ``~/.fedora-upload-ca.cert`` and ``~/.fedora-server-ca.cert``, and the
user would import the ``${user}_brower_cert.p12`` into their web browser as a
personal certificate.

Copy certificates into ~/.koji for kojiadmin
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

You're going to want to be able to send admin commands to the kojihub. In order
to do so, you'll need to use the newly created certificates to authenticate
with the hub. Create the kojiadmin user then copy the certificates for the koji
CA and the kojiadmin user to ``~/.koji``:

::

    kojiadmin@localhost$ mkdir ~/.koji
    kojiadmin@localhost$ cp /etc/pki/koji/kojiadmin.pem ~/.koji/client.crt   # NOTE: It is IMPORTANT you use the PEM and NOT the CRT
    kojiadmin@localhost$ cp /etc/pki/koji/koji_ca_cert.crt ~/.koji/clientca.crt
    kojiadmin@localhost$ cp /etc/pki/koji/koji_ca_cert.crt ~/.koji/serverca.crt

.. note::
    See /etc/koji.conf for the current system wide koji client configuration.
    Copy /etc/koji.conf to ~/.koji/config if you wish to change the config on a
    per user basis.

Setting up Kerberos for authentication
--------------------------------------

The initial configuration of a kerberos service is outside the scope of this
document, however there are a few specific things required by koji.

DNS
^^^

The koji builders (kojid) use DNS to find the kerberos servers for any given
realm.

::

    _kerberos._udp    IN SRV  10 100 88 kerberos.EXAMPLE.COM.

The trailing dot denotes DNS root and is needed if FQDN is used.


Principals and Keytabs
^^^^^^^^^^^^^^^^^^^^^^

It should be noted that in general you will need to use the fully qualified
domain name of the hosts when generating the keytabs for services.

You will need the following principals extracted to a keytab for a fully
kerberized configuration, the requirement for a host key for the koji-hub is
currently hard coded into the koji client.

``host/kojihub@EXAMPLE.COM``
    Used by the koji-hub server when communicating with the koji client

``HTTP/kojiweb@EXAMPLE.COM``
    Used by the koji-web server when performing a negotiated Kerberos
    authentication with a web browser. This is a service principal for
    Apache's mod_auth_gssapi.

``koji/kojiweb@EXAMPLE.COM``
    Used by the koji-web server during communications with the koji-hub. This
    is a user principal that will authenticate koji-web to Kerberos as
    "koji/kojiweb@EXAMPLE.COM". Koji-web will proxy the mod_auth_gssapi user
    information to koji-hub (the ``ProxyPrincipals`` koji-hub config
    option).

``koji/kojira@EXAMPLE.COM``
    Used by the kojira server during communications with the koji-hub

``compile/builder1.example.com@EXAMPLE.COM``
    Used on builder1 to communicate with the koji-hub. This
    is a user principal that will authenticate koji-builder to Kerberos as
    "compile/builder1.example.com@EXAMPLE.COM". Each builder host will have
    its own unique Kerberos user principal to authenticate to the hub.

PostgreSQL Server
=================

Once the authentication scheme has been setup your will need to install and
configure a PostgreSQL server and prime the database which will be used to hold
the koji users.

Configuration Files
-------------------

* ``/var/lib/pgsql/data/pg_hba.conf``
* ``/var/lib/pgsql/data/postgresql.conf``

Install PostgreSQL
------------------

Install the ``postgresql-server`` package::

    # yum install postgresql-server

Initialize PostgreSQL DB:
-------------------------

Initialize PostgreSQL::

    # On RHEL 7:
    root@localhost$ postgresql-setup initdb

    # Or RHEL 8 and Fedora:
    root@localhost$ postgresql-setup --initdb --unit postgresql

And start the database service::

    root@localhost$ systemctl enable postgresql --now

Setup User Accounts:
--------------------

The following commands will setup the ``koji`` account and assign it a password

::

    root@localhost$ useradd koji
    root@localhost$ passwd koji

Setup PostgreSQL and populate schema:
-------------------------------------

The following commands will:

* create the koji user within PostgreSQL
* create the koji database within PostgreSQL
* set a password for the koji user
* create the koji schema using the provided
  ``/usr/share/doc/koji*/docs/schema.sql`` file from the ``koji`` package.

::

    root@localhost$ su - postgres
    postgres@localhost$ createuser --no-superuser --no-createrole --no-createdb koji
    postgres@localhost$ createdb -O koji koji
    postgres@localhost$ psql -c "alter user koji with encrypted password 'mypassword';"
    postgres@localhost$ logout
    root@localhost$ yum -y install koji
    root@localhost$ su - koji
    koji@localhost$ psql koji koji < /usr/share/doc/koji*/docs/schema.sql
    koji@localhost$ exit

.. note::
    When issuing the command to import the psql schema into the new database it
    is important to ensure that the directory path
    /usr/share/doc/koji*/docs/schema.sql remains intact and is not resolved to
    a specific version of koji. In test it was discovered that when the path is
    resolved to a specific version of koji then not all of the tables were
    created correctly.

.. note::
    When issuing the command to import the psql schema into the new database it
    is important to ensure that you are logged in as the koji database owner.
    This will ensure all objects are owned by the koji database user. Upgrades
    may be difficult if this was not done correctly.

Authorize Koji-hub to PostgreSQL
--------------------------------

Koji-hub is the only service that needs direct access to the database. Every
other Koji service talks with the koji-hub via the API calls.

Example: Everything on localhost
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

In this example, the koji-hub Apache server is running on the same system
as the PostgreSQL server, so we can use local-only connections over a Unix
domain socket.

#. Edit ``/var/lib/pgsql/data/pg_hba.conf`` to have the following
   contents::

       #TYPE   DATABASE    USER    CIDR-ADDRESS      METHOD
       local   koji        koji                       trust
       local   all         postgres                   peer

   Explanation:

   * The ``local`` connection type means the postgres connection uses a local
     Unix socket, so PostgreSQL is not exposed over TCP/IP at all.

   * The local ``koji`` user should only have access to the ``koji`` database.
     The local ``postgres`` user will have access to everything (in order to
     create the ``koji`` database and user.)

   * The ``CIDR-ADDRESS`` column is blank, because this example only uses
     local Unix sockets.

   * The `trust <https://www.postgresql.org/docs/current/auth-trust.html>`_
     method means that PosgreSQL will permit any connections from any local
     user for this username. We set this for the ``koji`` user because Apache
     httpd runs as the ``apache`` system user rather than the ``koji`` user
     when it connects to the Unix socket. ``trust`` is not secure on a
     multi-user system, but it is fine for a single-purpose Koji system.

     The `peer <https://www.postgresql.org/docs/current/auth-peer.html>`_
     method means that PostgreSQL will obtain the client's operating system
     username and use that as the allowed username. This is safer than
     ``trust`` because only the local ``postgres`` system user will be able to
     access PostgreSQL with this level of access.

#. Edit ``/var/lib/pgsql/data/postgresql.conf`` and set ``listen_addresses``
   to prevent TCP/IP access entirely::

       listen_addresses = ''

Example: Separate PostgreSQL and Apache servers
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

In this example, the PostgreSQL server "db.example.com" is running on one
host, and the koji-hub Apache server talks to this PostgreSQL server over the
network. The koji-hub Apache server has an IP address of 192.0.2.1 (IPv4) and
2001:db8::1 (IPv6), so we authorize connections from both addresses for the
``koji`` user account.

#. Edit ``/var/lib/pgsql/data/pg_hba.conf`` to have the following contents::

       #TYPE   DATABASE    USER    CIDR-ADDRESS      METHOD
       host    koji        koji    192.0.2.1/32       md5
       host    koji        koji    2001:db8::1/128    md5
       local   all         postgres                   peer

   The ``md5`` authentication mechanism is available in PostgreSQL 9 (RHEL 7).
   On PostgreSQL 10 (RHEL 8+ and Fedora), use the stronger ``scram-sha-256``
   mechanism instead, and set ``password_encryption = scram-sha-256`` in
   ``postgresql.conf``.

#. Edit ``/var/lib/pgsql/data/postgresql.conf`` and set ``listen_addresses``
   so that PostgreSQL will listen on all network interfaces::

    listen_addresses = '*'

Activating changes
^^^^^^^^^^^^^^^^^^

You must reload the PostgreSQL daemon to activate changes to
``postgresql.conf`` or ``pg_hba.conf``::

    root@localhost$ systemctl reload postgresql

Bootstrapping the initial koji admin user into the PostgreSQL database
----------------------------------------------------------------------

You must add the initial admin user manually to the user database using sql
commands.  Once you have bootstrapped this initial admin user, you may add
additional users and change privileges of those users via the koji command
line tool.

However, if you decided to use the simple user/pass method of authentication,
then any password setting/changing must be done manually via sql commands as
there is no password manipulation support exposed through the koji tools.

The sql commands you need to use vary by authentication mechanism.

Maintaining database
--------------------

For now, there is one table which needs periodical cleanup. As postgres doesn't
have any mechanism for this, we need to do it via some other mechanism. Default
handling is done by cron, but can be substituted by anything else (Ansible
tower, etc.)

Script is by default installed on hub as `/usr/sbin/koji-sweep-db`.  It has also
corresponding `koji-sweep-db` service and timer. Note, that timer is not enabled
by default, so you need to run usual `systemctl` commands:

::

   systemctl enable --now koji-sweep-db.timer

If you don't want to use this script, be sure to run following SQL with
appropriate age setting. Default value of one day should be ok for most
deployments. As there will be tons of freed records, additional VACUUM can be
handy.

.. code-block:: sql

   DELETE FROM sessions WHERE update_time < NOW() - '1 day'::interval;
   VACUUM ANALYZE sessions;

Optionally (if you're using :ref:`reservation API <cg_api>` for
content generators), you could want to run also reservation cleanup:

.. code-block:: sql

   DELETE FROM build_reservations WHERE update_time < NOW() - '1 day'::interval;
   VACUUM ANALYZE build_reservations;

Set User/Password Authentication
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

::

    root@localhost$ su - koji
    koji@localhost$ psql
    koji=> insert into users (name, password, status, usertype) values ('admin-user-name', 'admin-password-in-plain-text', 0, 0);

Kerberos authentication
^^^^^^^^^^^^^^^^^^^^^^^

The process is very similar to user/pass except you would replace the first
insert above with this:

::

    root@localhost$ su - koji
    koji@localhost$ psql <<EOF
    with user_id as (
    insert into users (name, status, usertype) values ('admin-user-name', 0, 0) returning id
    )
    insert into user_krb_principals (user_id, krb_principal) values (
    (select id from user_id),
    'admin@EXAMPLE');
    EOF

SSL Certificate authentication
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

There is no need for either a password or a Kerberos principal, so this will
suffice:

::

    root@localhost$ su - koji
    koji@localhost$ psql
    koji=> insert into users (name, status, usertype) values ('admin-user-name', 0, 0);

Give yourself admin permissions
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

The following command will give the user admin permissions. In order to do
this you will need to know the ID of the user.

::

    koji=> insert into user_perms (user_id, perm_id, creator_id) values (<id of user inserted above>, 1, <id of user inserted above>);

.. note::
    If you do not know the ID of the admin user, you can get the ID by running the query::

      koji=> select * from users;

You can't actually log in and perform any actions until kojihub is up and
running in your web server.  In order to get to that point you still need to
complete the authentication setup and the kojihub configuration. If you wish
to access koji via a web browser, you will also need to get kojiweb up and
running.

Koji Hub
========

Koji-hub is the center of all Koji operations. It is an XML-RPC server running
under mod_wsgi in the Apache httpd. koji-hub is passive in that it only
receives XML-RPC calls and relies upon the build daemons and other components
to initiate communication. Koji-hub is the only component that has direct
access to the database and is one of the two components that have write access
to the file system.

Configuration Files
-------------------

* ``/etc/koji-hub/hub.conf``
* ``/etc/koji-hub/hub.conf.d/*``
* ``/etc/httpd/conf/httpd.conf``
* ``/etc/httpd/conf.d/kojihub.conf``
* ``/etc/httpd/conf.d/ssl.conf`` (when using ssl auth)

Install koji-hub
----------------

Install the ``koji-hub`` package along with mod_ssl::

    # yum install koji-hub mod_ssl

Required Configuration
----------------------

We provide example configs for all services, so look for ``httpd.conf``, ``hub.conf``,
``kojiweb.conf`` and ``web.conf`` in source repo or related rpms.

/etc/httpd/conf/httpd.conf
^^^^^^^^^^^^^^^^^^^^^^^^^^

The apache web server has two places that it sets maximum requests a server
will handle before the server restarts. The xmlrpc interface in kojihub is a
python application, and processes can sometimes grow outrageously large when it
doesn't reap memory often enough. As a result, it is strongly recommended that
you set both instances of ``MaxConnectionsPerChild`` in ``httpd.conf`` to
something reasonable in order to prevent the server from becoming overloaded
and crashing (at 100 the httpd processes will grow to about 75MB resident set
size before respawning).

::

    <IfModule prefork.c>
    ...
    MaxConnectionsPerChild  100
    </IfModule>
    <IfModule worker.c>
    ...
    MaxConnectionsPerChild  100
    </IfModule>
    <IfModule event.c>
    ...
    MaxRequestsPerChild  100
    </IfModule>

/etc/httpd/conf.d/kojihub.conf
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

The koji-hub package provides this configuration file. You will need to modify
it based on your authentication type. Instructions are contained within the
file and should be simple to follow.

For example, if you are using SSL authentication, you will want to uncomment
the section that looks like this:

::

    # uncomment this to enable authentication via SSL client certificates
    # <Location /kojihub/ssllogin>
    #         SSLVerifyClient require
    #         SSLVerifyDepth  10
    #         SSLOptions +StdEnvVars
    # </Location>


/etc/httpd/conf.d/ssl.conf
^^^^^^^^^^^^^^^^^^^^^^^^^^

If you are configuring your server for httpd (and you really should), then your
``SSLCertificate*`` directives will generally live in the main ``ssl.conf`` file.
This part is mostly independent of Koji.
It's something you would do for any httpd instance.

The part that matters to Koji is this --
if you are using SSL authentication, then the CA certificate you configure
in ``SSLCACertificateFile`` here should be the same one that you use to issue
user certificates.

::

    SSLCertificateFile /etc/pki/koji/certs/kojihub.crt
    SSLCertificateKeyFile /etc/pki/koji/private/kojihub.key
    SSLCertificateChainFile /etc/pki/koji/koji_ca_cert.crt
    SSLCACertificateFile /etc/pki/koji/koji_ca_cert.crt


/etc/koji-hub/hub.conf
^^^^^^^^^^^^^^^^^^^^^^

This file contains the configuration information for the hub. You will need to
edit this configuration to point Koji Hub to the database you are using and to
setup Koji Hub to utilize the authentication scheme you selected in the
beginning.

::

    DBName = koji
    DBUser = koji

    # If PostgreSQL is on another host, set that here:
    #DBHost = db.example.com
    #DBPass = mypassword

    KojiDir = /mnt/koji
    LoginCreatesUser = On
    KojiWebURL = http://kojiweb.example.com/koji

If koji-hub is running on the same server as PostgreSQL and you are using Unix
sockets to query the database, omit the ``DBHost``, ``DBPort``, and ``DBPass``
variables. Do not set ``DBHost`` to ``localhost``, or else PostgreSQL will
attempt to connect with TCP through ``127.0.0.1`` instead of using the Unix
socket.

If koji-hub is running on a separate server from PostgreSQL, you must set the
``DBHost`` and ``DBPass`` options. You must also configure SELinux to allow
Apache to connect to the remote PostgreSQL server::

    root@localhost$ setsebool -P httpd_can_network_connect_db=1

Note, the database password (``DBPass`` parameter) is a sensitive value. The
``hub.conf`` file should have 0640 ``root``/``apache`` file permissions to
restrict access. If you're not installing the hub from RPM, double-check these
permissions.

Furthermore, you can install any config file in ``/etc/koji-hub/hub.conf.d``
directory. These files are read *at first* and main config is allowed to
override all these values. So, you can use e.g.
``/etc/koji-hub/hub.conf.d/secret.conf`` for sensitive values. Typical usecase
for separate config is :doc:`policy <defining_hub_policies>` configuration file.

Doc page about hub options in :doc:`Hub conf <hub_conf>`.

Authentication Configuration
----------------------------

/etc/koji-hub/hub.conf
^^^^^^^^^^^^^^^^^^^^^^

If using Kerberos, these settings need to be valid and inline with other
services configurations.

::

    AuthPrincipal = host/kojihub@EXAMPLE.COM
    AuthKeytab = /etc/koji.keytab
    ProxyPrincipals = koji/kojiweb@EXAMPLE.COM
    HostPrincipalFormat = compile/%s@EXAMPLE.COM

If using SSL auth, these settings need to be valid and inline with other
services configurations for kojiweb to allow logins.

ProxyDNs should be set to the DN of the kojiweb certificate. For example::

    DNUsernameComponent = CN
    ProxyDNs = CN=example.com,OU=kojiweb,O=Example Org,ST=Massachusetts,C=US

Koji filesystem skeleton
^^^^^^^^^^^^^^^^^^^^^^^^

Above in the ``kojihub.conf`` file we set KojiDir to ``/mnt/koji``.  For
certain reasons, if you change this, you should make a symlink from
``/mnt/koji`` to the new location (note: this is a bug and should be fixed
eventually).  However, before other parts of koji will operate properly, we
need to create a skeleton filesystem structure for koji as well as make the
file area owned by apache so that the xmlrpc interface can write to it as
needed.

::

    cd /mnt
    mkdir koji
    cd koji
    mkdir {packages,repos,work,scratch,repos-dist}
    chown apache.apache *

SELinux Configuration
^^^^^^^^^^^^^^^^^^^^^

Configure SELinux to allow Apache write access to ``/mnt/koji``::

    root@localhost$ setsebool -P allow_httpd_anon_write=1
    root@localhost$ semanage fcontext -a -t public_content_rw_t "/mnt/koji(/.*)?"
    root@localhost$ restorecon -r -v /mnt/koji

If you've placed ``/mnt/koji`` on an NFS share, enable a separate boolean to
allow Apache access to NFS::

    root@localhost$ setsebool -P httpd_use_nfs=1

Check Your Configuration
^^^^^^^^^^^^^^^^^^^^^^^^

At this point, you can now restart apache and you should have at least minimal
operation.  The admin user should be able to connect via the command line
client, add new users, etc.  It's possible at this time to undertake initial
administrative steps such as adding users and hosts to the koji database.

So we will need a working client to test with.

Koji cli - The standard client
==============================

The koji cli is the standard client. It can perform most tasks and is essential
to the successful use of any koji environment.

Ensure that your client is configured to work with your server. The system-wide
koji client configuration file is ``/etc/koji.conf``, and the user-specific one
is in ``~/.koji/config``. You may also use the ``-c`` option when using the
Koji client to specify an alternative configuration file.

If you are using SSL for authentication, you will need to edit the Koji client
configuration to tell it which URLs to use for the various Koji components and
where their SSL certificates can be found.

For a simple test, all we need is the ``server`` and authentication sections.

::

    [koji]

    ;url of XMLRPC server
    server = http://koji-hub.example.com/kojihub

    ;url of web interface
    weburl = http://koji-web.example.com/koji

    ;url of package download site
    topurl = http://koji-filesystem.example.com/kojifiles

    ;path to the koji top directory
    topdir = /mnt/koji

    ; configuration for SSL athentication

    ;client certificate
    cert = ~/.koji/client.crt

    ;certificate of the CA that issued the HTTP server certificate
    serverca = ~/.koji/serverca.crt

The following command will test your login to the hub:

::

    root@localhost$ koji moshimoshi

Koji Web - Interface for the Masses
===================================

Koji-web is a set of scripts that run in mod_wsgi and use the Cheetah
templating engine to provide an web interface to Koji. koji-web exposes a lot
of information and also provides a means for certain operations, such as
cancelling builds.

Configuration Files
-------------------

* ``/etc/httpd/conf.d/kojiweb.conf``
* ``/etc/httpd/conf.d/ssl.conf``
* ``/etc/kojiweb/web.conf``
* ``/etc/kojiweb/web.conf.d/*``

Install Koji-Web
----------------

Install the ``koji-web`` package along with mod_ssl::

    # yum install koji-web mod_ssl

Required Configuration
----------------------

/etc/httpd/conf.d/kojiweb.conf
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

The koji-web package provides this configuration file. You will need to modify
it based on your authentication type. Instructions are contained within the
file and should be simple to follow.

For example, if you are using SSL authentication, you would want to uncomment
the section that looks like this:

::

    # uncomment this to enable authentication via SSL client certificates
    # <Location /koji/login>
    #     SSLVerifyClient require
    #     SSLVerifyDepth  10
    #     SSLOptions +StdEnvVars
    # </Location>


/etc/httpd/conf.d/ssl.conf
^^^^^^^^^^^^^^^^^^^^^^^^^^

Similarly to the hub configuration, if you are using https (as you should),
then you will need to configure your certificates.
This is something you might do for any httpd instance and is mostly independent
of Koji

If you are using SSL authentication, then the CA certificate you configure
in ``SSLCACertificateFile`` here should be the same one that you use to issue
user certificates.

::

    SSLCertificateFile /etc/pki/koji/certs/kojihub.crt
    SSLCertificateKeyFile /etc/pki/koji/private/kojihub.key
    SSLCertificateChainFile /etc/pki/koji/koji_ca_cert.crt
    SSLCACertificateFile /etc/pki/koji/koji_ca_cert.crt


/etc/kojiweb/web.conf
^^^^^^^^^^^^^^^^^^^^^

You will need to edit the kojiweb configuration file to tell kojiweb which URLs
it should use to access the hub, the koji packages and its own web interface.
You will also need to tell kojiweb where it can find the SSL certificates for
each of these components. If you are using SSL authentication, the "WebCert"
line below must contain both the public **and** private key. You will also want
to change the last line in the example below to a unique password. Also check
the file permissions (due to Secret value) if you're not installing koji web
from rpm (0640, root/apache by default).

Furthermore, you can install any config file in ``/etc/kojiweb/web.conf.d``
directory. These files are read *at first* and main config is allowed to
override all these values. So, you can use e.g.
``/etc/kojiweb/web.conf.d/secret.conf`` for sensitive values.

::

    [web]
    SiteName = koji
    # KojiTheme = 

    # Necessary urls
    KojiHubURL = https://koji-hub.example.com/kojihub
    KojiFilesURL = http://koji-filesystem.example.com/kojifiles

    ## Kerberos authentication options
    ; WebPrincipal = koji/web@EXAMPLE.COM
    ; WebKeytab = /etc/httpd.keytab
    ; WebCCache = /var/tmp/kojiweb.ccache

    ## SSL authentication options
    ; WebCert = /etc/pki/koji/koji-web.pem
    ; KojiHubCA = /etc/pki/koji/ca_cert.crt

    LoginTimeout = 72

    # This must be set before deployment
    #Secret = CHANGE_ME

    LibPath = /usr/share/koji-web/lib

Filesystem Configuration
------------------------

You'll need to make ``/mnt/koji/`` web-accessible, either here, on the hub, or
on another web server altogether.

This URL will go into various clients such as:
* ``/etc/kojiweb/web.conf`` as KojiFilesURL
* ``/etc/kojid/kojid.conf`` as topurl
* ``/etc/koji.conf`` as topurl

::

    Alias /kojifiles/ /mnt/koji/
    <Directory "/mnt/koji/">
        Options Indexes
        AllowOverride None
        # Apache < 2.4
        #   Order allow,deny
        #   Allow from all
        # Apache >= 2.4
        Require all granted
    </Directory>

Wherever you configure this, please go back and set it correctly in
``/etc/kojiweb/web.conf`` now.

Web interface now operational
-----------------------------

At this point you should be able to point your web browser at the kojiweb URL
and be presented with the koji interface.  Many operations should work in read
only mode at this point, and any configured users should be able to log in.

Koji Daemon - Builder
=====================

Kojid is the build daemon that runs on each of the build machines. Its primary
responsibility is polling for incoming build requests and handling them
accordingly. Koji also has support for tasks other than building such as
creating livecd images or raw disk images, and kojid is responsible for
handling these tasks as well. The kojid service uses mock for creating pristine
build environments and creates a fresh one for every build, ensuring that
artifacts of build processes cannot contaminate each other. All of kojid is
written in Python and communicates with koji-hub via XML-RPC.

Configuration Files
-------------------

* ``/etc/kojid/kojid.conf`` - Koji Daemon Configuration
* ``/etc/sysconfig/kojid`` - Koji Daemon Switches

All options for `kojid.conf` are described :doc:`here <kojid_conf>`.

Install kojid
-------------

Install the ``koji-builder`` package::

    # yum install koji-builder

Required Configuration
----------------------

Add the host entry for the koji builder to the database
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

You will now need to add the koji builder to the database so that they can be
utilized by koji hub. Make sure you do this before you start kojid for the
first time, or you'll need to manually remove entries from the sessions and
users table before it can be run successfully.

::

    kojiadmin@localhost$ koji add-host kojibuilder1.example.com i386 x86_64

The first argument used after the ``add-host`` command should the hostname of
the builder. The second argument is used to specify the architecture which the
builder uses.


/etc/kojid/kojid.conf
^^^^^^^^^^^^^^^^^^^^^

Edit each koji builder's ``kojid.conf`` file to point at the Koji hub::

    ; The URL for the xmlrpc server
    server=http://hub.example.com/kojihub

Set the "user" value to the FQDN of the builder host. For example, if you
added the host with ``koji add-host kojibuilder1.example.com``, set "user" to
kojibuilder1.example.com::

    user = kojibuilder1.example.com

The builder must reach the filesystem over HTTP. Set "topurl" to the same
value that you've configured for Koji clients (above)::

    # The URL for the file access
    topurl=http://koji-filesystem.example.com/kojifiles

If the "topurl" setting uses an HTTPS URL with a cert signed by a custom CA,
the Koji builder must trust the CA system-wide.

You may change "workdir", but it may not be the same as KojiDir on the
``kojihub.conf`` file. It can be something under KojiDir, just not the same as
KojiDir.

::

    ; The directory root for temporary storage
    workdir=/tmp/koji

The root of the koji build directory (i.e., ``/mnt/koji``) must be mounted on
the builder and configured as "topdir". A Read-Only NFS mount is the easiest
way to handle this.

::

    # The directory root where work data can be found from the koji hub
    topdir=/mnt/koji

Authentication Configuration (SSL certificates)
-----------------------------------------------

/etc/kojid/kojid.conf
^^^^^^^^^^^^^^^^^^^^^

If you are using SSL, edit these settings to point to the
certificates you generated at the beginning of the setup process.

::

    ;client certificate
    ; This should reference the builder certificate we created on the kojihub CA, for kojibuilder1.example.com
    ; ALSO NOTE: This is the PEM file, NOT the crt
    cert = /etc/kojid/kojid.pem

    ;certificate of the CA that issued the HTTP server certificate
    serverca = /etc/kojid/koji_ca_cert.crt

Every unique builder host must have its own unique keypair (PEM file) in
``/etc/kojid/``. If you generated the certificates on another host, move them
to each builder.

Authentication Configuration (Kerberos)
---------------------------------------

/etc/kojid/kojid.conf
^^^^^^^^^^^^^^^^^^^^^

If using Kerberos, these settings need to be valid and inline with other
services configurations.

::

    ; the username has to be the same as what you used with add-host
    ;user =

    host_principal_format=compile/%s@EXAMPLE.COM

By default it will look for the Kerberos keytab in ``/etc/kojid/kojid.keytab``

.. note::
    Kojid will not attempt kerberos authentication to the koji-hub unless the
    username field is commented out

.. _scm-config:

Source Control Configuration
----------------------------

/etc/kojid/kojid.conf
^^^^^^^^^^^^^^^^^^^^^

The *allowed_scms* setting controls which source control systems the builder
will accept. It is a space-separated list of entries in one of the following
forms:

::

    hostname:path[:use_common[:source_cmd]]
    !hostname:path

where

    *hostname* is a glob pattern matched against SCM hosts.

    *path* is a glob pattern matched against the SCM path.

    *use_common* is boolean setting (yes/no, on/off, true/false) that indicates
    whether koji should also check out /common from the SCM host. The default
    is on.

    *source_cmd* is a shell command to be run before building the
    srpm, with commas instead of spaces. It defaults to ``make,sources``.

The second form (``!hostname:path``) is used to explicitly block a host:path
pattern. In particular, it provides the option to block specific subtrees of
a host, but allow from it otherwise


::

    allowed_scms=
        !scm-server.example.com:/blocked/path/*
        scm-server.example.com:/repo/base/repos*/:no
        alt-server.example.com:/repo/dist/repos*/:no:fedpkg,sources


The explicit block syntax was added in version 1.13.0.

SCM checkout can contain multiple spec files (checkouted or created by
``source_cmd``). In such case spec file named same as a checkout directory will
be selected.

.. note::
    We provide ``build_from_scm`` hub policy as an equivalent in version 1.26.0.

    For more details, please refer to :ref:`allowed-scms` and
    :doc:`Defining Hub Policies <defining_hub_policies>`.

Add the host to the createrepo channel
--------------------------------------

Channels are a way to control which builders process which tasks.  By default
hosts are added to the ''default'' channel.  At least some build hosts also
needs to be added to the ''createrepo'' channel so there will be someone to
process repo creation tasks initiated by kojira.

::

    kojiadmin@localhost$ koji add-host-to-channel kojibuilder1.example.com createrepo

A note on capacity
------------------

The default capacity of a host added to the host database is 2. This means that
once the load average on that machine exceeds 2, kojid will not accept any
additional tasks. This is separate from the maxjobs item in the configuration
file. Before kojid will accept a job, it must pass both the test to ensure the
load average is below capacity and that the current number of jobs it is
already processing is less than maxjobs. However, in today's modern age of quad
core and higher CPUs, a load average of 2 is generally insufficient to fully
utilize hardware.

::

    koji edit-host --capacity=16 kojibuilder1.example.com

The koji-web interface also offers the ability to edit this value to admin
accounts.

Start Kojid
-----------

Once the builder has been added to the database you must start kojid

::

    root@localhost$ systemctl enable kojid --now

Check ``/var/log/kojid.log`` to verify that kojid has started successfully. If
the log does not show any errors then the koji builder should be up and ready.
You can check this by pointing your web browser to the web interface and
clicking on the hosts tab. This will show you a list of builders in the
database and the status of each builder.

Kojira - Dnf|Yum repository creation and maintenance
====================================================

Configuration Files
-------------------

* ``/etc/kojira/kojira.conf`` - Kojira Daemon Configuration

Install kojira
---------------

Install the ``koji-utils`` package::

    # yum install koji-utils

Required Configuration
----------------------

Add the user entry for the kojira user
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

The kojira user requires the ``repo`` permission to function.

::

    kojiadmin@localhost$ koji add-user kojira
    kojiadmin@localhost$ koji grant-permission repo kojira

``/etc/kojira/kojira.conf``
    This needs to point at your koji-hub.

    ::

        ; The URL for the xmlrpc server
        server=http://koji-hub.example.com/kojihub


Additional Notes
^^^^^^^^^^^^^^^^
* Kojira needs read-write access to ``/mnt/koji``.
* There should only be one instance of kojira running at any given time.
* It is not recommended that kojira run on the builders, as builders only
  should require read-only access to ``/mnt/koji``.


.. _auth-config:

Authentication Configuration
----------------------------

/etc/kojira/kojira.conf
^^^^^^^^^^^^^^^^^^^^^^^

**If using SSL,** these settings need to be valid.

::

    ;client certificate
    ; This should reference the kojira certificate we created above
    cert = /etc/pki/koji/kojira.pem

    ;certificate of the CA that issued the HTTP server certificate
    serverca = /etc/pki/koji/koji_ca_cert.crt

**If using Kerberos,** these settings need to be valid.

::

    ;configuration for Kerberos authentication

    ;the kerberos principal to use
    ;principal = kojira@EXAMPLE.COM

    ;location of the keytab
    ;keytab = /etc/kojira/kojira.keytab

``/etc/sysconfig/kojira``
    The local user kojira runs as needs to be able to read and write to
    ``/mnt/koji/repos/``. If the volume that directory resides on is
    root-squashed or otherwise unmodifiable by root, you can set ``RUNAS=`` to
    a user that has the required privileges.

Start Kojira
------------

::

    root@localhost$ service kojira start

Check ``/var/log/kojira/kojira.log`` to verify that kojira has started
successfully.

Bootstrapping the Koji build environment
========================================

For instructions on importing packages and preparing Koji to run builds, see
:doc:`Server Bootstrap <server_bootstrap>`.

For instructions on using External Repos and preparing Koji to run builds, see
:doc:`External Repo Server Bootstrap <external_repo_server_bootstrap>`.

Useful scripts and config files for setting up a Koji instance are available
`here <http://fedora.danny.cz/koji/>`_.

Minutia and Miscellany
======================
Please see :doc:`KojiMisc <misc>` for additional details and notes about
operating a koji server.

.. _dnf: https://fedoraproject.org/wiki/Dnf
.. _yum: https://fedoraproject.org/wiki/Yum
.. _createrepo: http://createrepo.baseurl.org/
.. _mock: https://fedoraproject.org/wiki/Mock
.. _Apache mod_ssl documentation:
    https://httpd.apache.org/docs/trunk/mod/mod_ssl.html#ssloptions 
=============================
Setting RPM Macros for Builds
=============================

The values of RPM macros can have significant effects on the results of RPM builds.
Note that the subject of RPM macros is complicated and goes well beyond Koji.
For the purposes of this document, we assume the reader is familiar with the basic concepts.

Further reading:

* https://rpm-guide.readthedocs.io/en/latest/rpm-guide.html#rpm-macros
* https://rpm.org/user_doc/macros.html
* https://rpm-packaging-guide.github.io/#more-on-macros

When Koji builds RPMs, it does so by running ``rpmbuild`` in a controlled build environment.
Inside that environment, ``rpm`` can pull macro values from multiple sources.

There are two basic ways to set rpm macro values for builds in Koji:

* using a build that places an rpmmacros file in the build environment
* setting ``rpm.macro.`` values for the build tag


Setting rpm macros with a build
===============================

Prior to Koji 1.18, this was the only way to set rpm macros in Koji.
This method is still valid, and in some cases preferred.
However, values set this way can be overridden by ``rpm.macro.*`` values set for the build tag.

In short, this method involves:

* creating an rpm build that places an rpm macros file in the buildroot
* requiring this build to be installed in the build environment

This might be a very simple build that only provides a single rpm macros file.
Such files will be read by ``rpm`` when they are installed into ``/etc/rpm`` or
``/usr/lib/rpm/macros.d/``.

There are many examples of this.
In Fedora, there are numerous packages like ``python-rpm-macros``, ``perl-macros``, and
``systemd-rpm-macros``.
Other packages might package such macro files alongside other content.
The ``ansible`` package is currently an example of this.

*In order for such a build to affect the build environment, it must be installed there.*
First the build needs to be in the build tag, either tagged there directly or indirectly via 
inheritance.
Second, the package needs to be either part of the base buildroot install, or pulled in via
build requirements.

Often, you want to make sure these macros affect *all* builds for a tag.
This means making your macros build part of the base install for the buildroot.
This can be done by adding the rpm name to the ``build`` group for the tag.

::

$ koji add-group-pkg f33-build build my-custom-rpm-macros

If your macro also needs to be available when building source rpms (e.g. ``%dist``), then you'll
also want to add it to the the ``srpm-build`` group.

::

$ koji add-group-pkg f33-build build my-custom-rpm-macros


Setting rpm.macro values
========================

(this feature was added in :doc:`Koji 1.18 <release_notes/release_notes_1.18>`)

As a convenience, Koji will honor any ``rpm.macro.NAME`` values in the "tag extra" settings for
a given build tag.
These values can be set by tag administrators with the ``edit-tag`` command and viewed with
the ``taginfo`` command.
For example, to set the ``dist`` macro value, you could use a command like the following:

::

$ koji edit-tag f33-build -x rpm.macro.dist=.fc33

This will cause Koji to pass this value to ``mock`` when constructing the buildroot.
These values are placed in the mock configuration file.

**Use case**

This feature is best used for macros with simple values that need to be managed by tag administrators.
The canonical example is managing the ``%dist`` macro, but other simple macros would also make sense.

We do not recommend setting complicated macros in this way.
E.g. macros that contain complex expansions, or those that are central to the rpmbuild process.


**Inheritance**

In Koji, the "tag extra" values are inherited.
So, by default, any tag a given build tag inherits from will contribute its settings.
The exception is if the inheritance line has the ``noconfig`` flag set.


**Priority over macros builds**

Koji places these macro values in the ``mock`` configuration file in for the buildroot.
The ``mock`` program places them in the ``.rpmmacros`` file in the build directory, which causes
them to take priority over other macros defined in the build environment.

In short, this method for setting rpm macros "wins".

This can be important when other build tags inherit from yours.
If the child tag has its own macros build, but inherits your ``rpm.macro.*`` setting, then the
inherited value will win.
RPM Signing with Koji
=====================

What is a GPG keypair?
----------------------

A GPG keypair has a public key that you can share with the world and a private key that you keep secret.

Here are some example commands for working with RPM and GPG.

Example of generating a GPG keypair for testing::

    gpg --quick-generate-key security@example.com
    # For testing, simply press "Enter" when prompted for a password.

Exporting your public key::

    gpg --armor --export --output /path/to/my-signing-key.asc

Signing all RPMs in the current directory with this key::

    rpmsign --define "_gpg_name security@example.com" --addsign *.rpm

Inspecting an RPM signature
---------------------------

In order to install a signed RPM on clients, each client must trust (import)
the public GPG key into their RPMDB::

    rpm --import /path/to/my-signing-key.asc

*Example: No GPG signature at all (an unsigned RPM)*::

    rpm -Kv python3-cherrypy-18.6.0-1.fc33.noarch.rpm
    python3-cherrypy-18.6.0-1.fc33.noarch.rpm:
      Header SHA256 digest: OK
      Header SHA1 digest: OK
      Payload SHA256 digest: OK
      MD5 digest: OK

Note there are only "digest" fields here, no "Signature" fields since this RPM
is unsigned.

*Example: A GPG signature that rpmdb DOES trust*::

    rpm -Kv python3-cherrypy-18.4.0-4.fc32.noarch.rpm
    python3-cherrypy-18.4.0-4.fc32.noarch.rpm:
      Header V3 RSA/SHA256 Signature, key ID 12c944d0: OK
      Header SHA256 digest: OK
      Header SHA1 digest: OK
      Payload SHA256 digest: OK
      V3 RSA/SHA256 Signature, key ID 12c944d0: OK
      MD5 digest: OK

*Example: A GPG signature that rpmdb does NOT trust*::

    rpm -Kv python-cherrypy-18.6.0-1.el8.src.rpm
    python-cherrypy-18.6.0-1.el8.src.rpm:
      Header V4 RSA/SHA256 Signature, key ID 782096ac: NOKEY
      Header SHA256 digest: OK
      Header SHA1 digest: OK
      Payload SHA256 digest: OK
      V4 RSA/SHA256 Signature, key ID 782096ac: NOKEY
      MD5 digest: OK

Note the signature is syntatically valid here, but "NOKEY" here means RPMDB
does not trust the GPG key that signed this RPM.

A lower-level command that shows the signature on an RPM file (the
``RSAHEADER`` field piped through RPM's ``pgpsig`` formatter)::

    rpm -q --qf '%{NAME} %{RSAHEADER:pgpsig}\n' -p python-routes-2.5.1-1.el8.src.rpm

Learn more about RPM signatures and digests in `RPM's reference manual
<https://rpm-software-management.github.io/rpm/manual/signatures_digests.html>`_.

Uploading signed RPMs to Koji
-----------------------------

Koji does not sign RPMs. Instead, Koji imports RPMs that are signed with a separate key.

To sign an RPM from Koji, you should make a copy of the file, sign it
with the appropriate rpm command, and import the signature. Note that you
should not simply sign the file directly under /mnt/koji, as this causes an
inconsistency between the filesystem and the database (hence the copy step).

In this example, we download an unsigned build from Koji, then sign it, and
then upload the signed copy with ``koji import-sig``::

    koji download-build --debuginfo bash-5.0.17-2.fc32
    rpmsign --define "_gpg_name security@example.com" --addsign *.rpm
    koji import-sig *.rpm

The ``koji import-sig`` command uploads the signed RPM headers to the Koji
Hub, which stores the headers on disk alongside the main unsigned RPM.
It also writes out a full signed RPM.

Another variant is to import whole signed rpms (e.g. during :doc:`bootstrapping
<server_bootstrap>` via ``koji import`` command.) If such an imported rpm
contains an rpm signature, the import does not automatically write out a signed
copy for that signature (in contrast with ``import-sig``). The primary copy will
be the signed rpm, and the signature will be noted. If a signed copy is desired
(e.g. for generating :doc:`distrepos <exporting_repositories>`), you can use the
koji write-signed-rpm command.

Downloading a signed RPM from Koji
----------------------------------

Specify the ``--key`` option to ``koji download-build``::

    koji download-build --key=3AF362BAB bash-5.0.17-2.fc32

Signing a build with multiple keys
----------------------------------

Currently RPM's file format only allows one single GPG signature per file.

Koji allows users to upload multiple GPG signatures for a single RPM. it
stores each signature alongside the RPM build and splices the signature
headers in to generate full signed RPMs. Here are some use-cases of this
feature:

- Sign a set of RPMs with a "beta" key, and later sign those same RPMs with a
  "main" key.

- Sign the same Fedora RPMs with multiple keys, one per Fedora release.

- Sign the same CentOS RPMs with multiple keys, one per CentOS SIG.

- In Fedora, after the developers stop supporting a Fedora version like "30",
  they can delete the full signed packages, which are many hundreds of GB, and
  just keep the signatures, which are only a few bytes.
  https://lists.fedoraproject.org/archives/list/devel@lists.fedoraproject.org/message/RWILIHQJEKIQM5LAH7UJ7KMRPZEXCKQL/

Creating repos of signed RPMs
-----------------------------

You can put signed RPMs into Yum repos three different ways.

1. Create dist-repos manually with the ``koji dist-repo`` command, that takes
   a GPG key argument.

2. Install and configure the `tag2distrepo
   <https://pagure.io/releng/tag2distrepo>`_ hub plugin to automatically
   export dist-repos for certain tags.

3. Pungi can create signed repos ("composes").

See :doc:`Exporting repositories <exporting_repositories>` for more
information.

How to automate signing?
------------------------

For a small testing environment, you can simply sign RPMs with a GPG key on a
workstation and run ``koji import-sig``. This is not secure and it does not
scale.

See the `Sigil <https://pagure.io/sigul>`_ and `Robosignatory
<https://pagure.io/robosignatory>`_ projects for more advanced workflows.

Koji cryptography best-practices
--------------------------------

- Use HTTPS everywhere (kojihub + kojiweb)
- Understand checksums (md5)
- Understand signatures (GPG)

How do RPM signatures relate to HTTPS?
--------------------------------------

HTTPS is transport-layer security. When you install a package over HTTPS you
verify that:

* The web server is who they say they are
* The information the web server sends is private

As soon as you download that build or copy it to another location, those
security guarantees are lost.

In a release pipeline, you end up copying builds to many locations, and while
it's important to use HTTPS for copying, it's even more important to have a
strong cryptographic signature follow each build.

This means that even if someone or some thing mirrors your build elsewhere,
that signature will go along with the build. In the case of RPMs, the GPG
signatures are actually embedded in the RPMs themselves that we deliver to
users.

Another reason this is important is for image-based artifacts that might use
many RPMs. If you think of cloud images or container images where you're
delivering an image with "preinstalled" RPMs, if you use signed RPMs in the
images you distribute, you're providing an extra layer of security.

How do RPM signatures relate to IMA signing?
--------------------------------------------

IMA stands for `"Integrity Measurement Architecture"
<https://www.redhat.com/en/blog/how-use-linux-kernels-integrity-measurement-architecture>`_.
It's a separate type of signature. RHEL-9 is the first release to have IMA
signing enabled. The change is still `under discussion
<https://fedoraproject.org/wiki/Changes/Signed_RPM_Contents>`_ for Fedora.

IMA does not replace RPM signing. RPM signing is orthogonal to IMA. Packages
can be both RPM-signed and IMA signed at the same time.
Supported Platforms
===================

We're now supporting Linux systems which have at least python 2.7 for
builders and 3.6 for other components. These versions are minimal (so,
everywhere where is 2.7 support it means 2.7+ *and* 3.0+. Currently it
involves all active Fedoras and RHEL/CentOS 7+.

+-----------+-----+-----+---------+-------+-----+-----+
| Component | Hub | Web | Builder | Utils | Lib | CLI |
+===========+=====+=====+=========+=======+=====+=====+
| Python    | 3.6 | 3.6 | 2.7     | 3.6   | 2.7 | 2.7 |
+-----------+-----+-----+---------+-------+-----+-----+
How tag inheritance works
-------------------------

Almost everything in koji is dealing with tag and their inheritance.
What is it good for and how can it be configured?

Every tag handles its own configuration and as an administrator you
can live with just this. But true power of tag structure is hidden in
inheritance. You can compose tags, use parent products from which are
data inherited to new versions or other layered products and more.

Each tag has this set of data:

tag data
   1. architectures - here is the set of architectures which will be
      used in the moment, when given tag is used as a buildroot (in
      other words, when it is used in target definition)
   2. comps groups - similar to architectures, this data are used as
      installation groups, when tag is used as a buildroot. Generally,
      ``srpm-build`` and ``build`` groups are required in almost all
      cases. There could be also some additional groups like
      ``maven-build`` or ``livemedia-build`` for other operations,
      than just building rpms. Groups can be created and edited via
      ``*-group-*`` koji commands.
   3. maven options - If maven builds are enabled in koji environment
      (needs setup on hub's side), ``maven_support`` and
      ``include_all`` are options related to it. First says, that
      maven repositories should be generated and second one limits if
      all tagged versions of some artifact should be present in such
      repository.
package list
   Every tag carries a list of allowed packages, which can be tagged
   there and also owner of such package/tag combination. Note, that
   owner doesn't mean much in respect to who can do what. It is just a
   default recipients of notifications and can be changed in any time.
   Ownership doesn't limit anybody in un/tagging builds for that
   tag/package. This is on the other hand driven by hub policies.
   Package list simply says, what is allowed in tag (even if there is
   no build for given package).
tagged builds list
   This is the last component of tag. Obviously it is a list of builds
   for packages from previous point. It will never happen, that you'll
   see some builds for which is not package listed above.

All these three groups of data can be inherited and propagated through
inheritance chains.

Inheritance options
___________________

Whole inheritance can be edited via CLI's commands
``add-tag-inheritance`` and ``edit-tag-inheritance``. They have same
options which are described with examples here:

Simple ``add-tag-inheritance`` requires two existing tags which will
be linked together in inheritance chain.

::

   $ koji add-tag parent
   $ koji add-tag child
   $ koji add-tag-inheritance child parent
   $ koji list-tag-inheritance child
          child (168)
     ....  └─parent (167)

In the example you can see basic inheritance chain. You see ``child``
tag which is inheriting data from ``parent`` tag. Numbers behind tag
names are numeric ids, which you don't need to care about in normal
situations, but which can be useful in scripting koji. Four dots in
the beginning of line are placeholders for different inheritance
flags. These can be: M, F, I, N which denotes ``maxdepth``,
``pkg_filter``, ``intransitive`` and ``noconfig`` respectively.  All
these options can specified via CLI.

``priority``
    When you're adding new inheritance line to tag, which already has
    some parent, you would like to denote, in which part of
    inheritance chain new parent should appear. Let's continue with
    previous example.

    ::

     $ koji add-tag less-significant-parent
     $ koji add-tag-inheritance child less-significant-parent --priority 100
     $ koji list-tag-inheritance child
          child (168)
     ....  ├─parent (167)
     ....  └─less-significant-parent (169)

    What happened here is, that ``parent`` has default priority 0,
    while new one is priority 100. Lower number wins here. If you
    change your mind, you can always use ``edit-tag-inheritance`` to
    change the priority.

    .. note::
      Good rule of thumb is to not create inheritance without priority
      and use 10's or 100's steps. In such case you shouldn't need to
      update priorities, when adding something in the middle (where you
      can use e.g. priority 15 like in good old Basic times).

``maxdepth``
   For longer inheritance chains you may not want to treat whole
   chain. Let's leave our example and get more real-life situation.

   ::

    $ koji list-tag-inheritance linux3-build
         linux3-build (4380)
    ....  └─linux3-override (4379)
    ....     └─linux3-updates (4373)
    ....        └─linux2 (4368)
    ....           └─linux1 (4350)
    ....              └─linux0 (4250)

    $ koji add-tag linux4-build
    $ koji add-tag-inheritance linux4-build linux3-build --maxdepth 0
    $ koji list-tag-inheritance linux4-build
         linux4-build (4444)
    M...  └─linux3-build (4380)

   This is not, what you would see in Fedora's koji, because Fedora is
   not reusing anything from previous releases and is doing mass
   rebuilds instead, but it could have. In this case, we only want
   packages from ``linux3-build``, but not anything inherited into it.
   ``maxdepth`` does exactly this and strips the rest of inheritance
   chain.

``intransitive``
    Intransitive inheritance links are as what they say. If they are
    used somewhere deeper in inheritance chain, they will be ignored.
    It can be used for ensuring, that something will not be propagated
    by mistake. In combination with ``maxdepth`` it can mean hard stop
    even before ``maxdepth`` is reached.

``noconfig``
    While `normal` inheritance inherits everything - it means tag
    configuration, package list and tagged builds, links with this
    option are used only for propagating list of packages and builds.
    Everything else is ignored (architectures, locks, permissions,
    maven support).

``pkg-filter``
    Package filter is defined as regular expression and limits which
    packages are propagated through this link.
===========================
Using the koji build system
===========================


Using Koji in Fedora
====================

The `Koji Build System <Koji>`__ is Fedora's RPM buildsystem. Packagers
use the koji client to request package builds and get information about
the buildsystem. Koji runs on top of Mock to build RPM packages for
specific architectures and ensure that they build correctly.

Installing Koji
---------------

Installing the Koji CLI
^^^^^^^^^^^^^^^^^^^^^^^

Everything you need to use Koji (and be a Fedora contributor) can be
installed in a single step:

::

    dnf install fedora-packager

fedora-packager provides useful scripts to help maintain and setup your
koji environment. Additionally, it includes dependencies on the Koji
CLI, so it will be installed when you install ``fedora-packager``. The
command is called ``koji`` and is included in the main koji package. By
default the koji tool authenticates to the central server using
Kerberos. However SSL and username/password authentications are
available. You will need to have a valid authentication token to use
many features. However, many of the read-only commands will work without
authentication.

If you run into any problems with Fedora's instance of koji, `here
<https://fedoraproject.org/wiki/Join_the_package_collection_maintainers#Install_the_developer_client_tools>`__
is actual documentation for installing and using developer client tools.

Alternatively, koji CLI is now also available via:

  * `Project releases tarballs <https://pagure.io/koji/releases>`__
    Preferred way is to use your distribution's mechanism instead, as it
    will also contain appropriate configuration files.
  * `PyPi <https://pypi.python.org/pypi/koji>`__ There is only client/API
    part and it is mostly usable for people who wants some more advanced
    client-side scripting in virtualenv's, so API-only access is not
    sufficient for them or who can profit from some utilities in e.g. basic
    ``koji`` library.
  * Actual development version via Pagure's git: ``git clone
    https://pagure.io/koji.git``

Koji Config
^^^^^^^^^^^

The global local client configuration file for koji is
``/etc/koji.conf``. You should not need to change this from the defaults
for building Fedora packages.These will allow you to use the primary
build system as well as secondary arch build systems.

The web interface
-----------------

.. raw:: mediawiki

   {{admon/tip|Optional|The web interface is optional.  You may skip to the
   next section if you like.}}

The primary interface for viewing Koji data is a web application. It is
available at https://koji.fedoraproject.org/koji/ . Most of the interface
is read-only, but with sufficient privileges, you can log in and perform
some additional actions. For example:

-  Cancel a build
-  Resubmit a failed task
-  Setup a notification

Those with admin privileges will find additional actions, such as:

-  Create/Edit/Delete a tag
-  Create/Edit/Delete a target
-  Enable/Disable a build host

The web site utilizes SSL authentication. In order to log in you will
need a valid SSL certificate and your web browser will need to be
configured to trust the SSL cert. Instructions on how to do this are
printed when running ``fedora-packager-setup --with-browser-cert``.

.. raw:: mediawiki

   {{admon/warning|Using the certificate directly downloaded from the FAS web
   interface|If you have generated and downloaded the certificate
   <code>~/.fedora.cert</code> directly from FAS using the form referenced
   above, you need to convert it into a format that the browser can understand
   using the following command:
   <code>openssl pkcs12 -export -in ~/.fedora.cert -CAfile ~/.fedora-upload-ca.cert -out ~/fedora-browser-cert.p12</code>,
   where <code>.fedora-upload-ca.cert</code> can be downloaded from the URL
   referenced above.}}

Installing SSL Certificates in Firefox
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

.. raw:: mediawiki

   {{admon/note|Optional|You only need to check these instructions if you are intending to authenticate with the web interface with Firefox.  Authenticating with the web interface is optional.}}

Once you have created your FAS account, generated your certificate in
the form posted in the link above and ran
``fedora-packager-setup --with-browser-cert``, you will need to import
it into your web browser. You can do this in Firefox by doing the
following:

1. Launch Firefox and click on the **Edit** menu from the toolbar

2. Select **Preferences** in the sub-menu which appears.

3. This should open the **Preferences** window where you can switch to
the **Advanced** section

4. In the **Advanced** section switch to the **Encryption** tab

5. Click on the **View Certificates** button and the Certificates window
will appear

6. Switch to the **Your Certificates** tab and click on the **Import**
button

7. Point to where your Fedora Certificate is located and click **Open**
(fedora-packager-setup will have told you where it was saved and will
have asked you to set a password for the cert)

You should now be able to see your Fedora Certificate listed under
**Your Certificates** and you should be able to authenticate with the
koji web interface.

Installing SSL Certificates in Chromium
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

.. raw:: mediawiki

   {{admon/note|Optional|You only need to check these instructions if you are intending to authenticate with the web interface with Chromium.  Authenticating with the web interface is optional.}}

Chromium uses the NSS Shared DB, you will need the nss-tools package
installed.

::

    pk12util -d sql:$HOME/.pki/nssdb -i fedora-browser-cert.p12

.. _notification-basics:

Notifications
^^^^^^^^^^^^^

Koji supports a limited number of email notifications:

    - build notifications: when builds complete or fail
    - tag notifications: when builds are tagged or untagged

These mails are sent to:

    - the owner of the build in question
    - (for tag notifications) the owner of the package for the tag
    - any user who as subscribed to notifications for that package or tag

Users can manage their notification subscriptions in the web interface.
To do so, they need to be logged in. The main page (Summary) will list
their subscriptions at the bottom. Each entry includes an "edit" and
"delete" link. Below that table is an "Add a notification" link for adding
new notifications.

Starting in Koji version 1.16.0, users can also manage these subscriptions
on the command line. The relevant commands are:

    - add-notification
    - edit-notification
    - list-notifications
    - remove-notification


Building with fedpkg targets
----------------------------

Every push is automatically tagged via git. All you have done to build
the package is to run,

::

    fedpkg build

This will trigger a build request for the branch. Easy!

It is also possible to target a specific koji tag as follows:

::

    fedpkg build --target TARGET

for example, if building on rawhide against a special tag created by
rel-eng for updating API for many packages, e.g. ``dist-f14-python`` you
would use the following:

::

    fedpkg build --target 'dist-f14-python'

Chained builds
^^^^^^^^^^^^^^

.. raw:: mediawiki

   {{Admon/warning | chain-builds only work when building on the devel/ branch (aka rawhide).  To chain-build packages to update a released OS version, [https://fedoraproject.org/wiki/Bodhi/BuildRootOverrides set up an override using bodhi] requesting packages to be included in the proper buildroot.}}

Sometimes you want to make sure than one build succeeded before
launching the next one, for example when you want to rebuild a package
against a just rebuilt dependency. In that case you can use a chain
build with:

``fedpkg chain-build libwidget libgizmo``

The current package is added to the end of the CHAIN list. Colons (:)
can be used in the CHAIN parameter to define groups of packages.
Packages in any single group will be built in parallel and all packages
in a group must build successfully and populate the repository before
the next group will begin building. For example:

``fedpkg chain-build libwidget libaselib : libgizmo :``

will cause libwidget and libaselib to be built in parallel, followed by
libgizmo and then the correct directory package. If no groups are
defined, packages will be built sequentially.

If a build fail, following builds are cancelled but the builds that
already succeeded are pushed to the repository.

Scratch Builds
--------------

Sometimes it is useful to be able to build a package against the
buildroot but without actually including it in the release. This is
called a scratch build. The following section covers using koji directly
as well as the fedpkg tool to do scratch builds. To create a scratch
build from changes you haven't committed, do the following:

::

    rpmbuild -bs foo.spec
    koji build --scratch rawhide foo.srpm

From the latest git commit:

::

    koji build --scratch rawhide 'git url'

Warning: Scratch builds will *not* work correctly if your .spec file
does something different depending on the value of %fedora, %fc9, and so
on. Macro values like these are set by the *builder*, not by koji, so
the value of %fedora will be for whatever created the source RPM, and
*not* what it's being built on. Non-scratch builds get around this by
first re-building the source RPM.

If you have committed the changes to git and you are in the current
branch, you can do a scratch build with fedpkg tool which wraps the koji
command line tool with the appropriate options:

::

    fedpkg scratch-build

if you want to do a scratch build for a specific architecture, you can
type:

::

    fedpkg scratch-build-<archs>

 can be a comma separated list of several architectures.

finally is possible to combine the scratch-build command with a specific
koji tag in the form:

::

    fedpkg scratch-build --target TARGET

fedpkg scratch-build --help or koji build --help for more information.

Build Failures
--------------

If your package fails to build, you will see something like this:

::

    420066 buildArch kernel-2.6.18-1.2739.10.9.el5.jjf.215394.2.src.rpm,
    ia64): open (build-1.example.com) -> FAILED: BuildrootError:
    error building package (arch ia64), mock exited with status 10

You can figure out why the build failed by looking at the log files. If
there is a build.log, start there. Otherwise, look at init.log.

Logs can be found via the web interface in the Task pages for the failed
task. Alternatively the koji client can be used to view the logs via the
``watch-logs`` command. See the help output for more details.

Advanced use of Koji
--------------------

We've tried to make Koji self-documenting wherever possible. The command
line tool will print a list of valid commands and each command supports
--help. For example:

::

    $ koji help

    Koji commands are:
    build                Build a package from source
    cancel-task          Cancel a task
    help                 List available commands
    latest-build         Print the latest builds for a tag
    [...]

::

    $ koji build --help

    usage: koji build [options]  tag URL
    (Specify the --help global option for a list of other help options)

    options:
    -h, --help            show this help message and exit
    --skip-tag            Do not attempt to tag package
    --scratch             Perform a scratch build
    --nowait              Don't wait on build
    [...]

Using koji to generate a mock config to replicate a buildroot
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

koji can be used to replicate a build root for local debugging

::

    koji mock-config --help
    Usage: koji mock-config [options] name
    (Specify the --help global option for a list of other help options)

    Options:
      -h, --help            show this help message and exit
      --arch=ARCH           Specify the arch
      --tag=TAG             Create a mock config for a tag
      --task=TASK           Duplicate the mock config of a previous task
      --buildroot=BUILDROOT
                            Duplicate the mock config for the specified buildroot
                            id
      --mockdir=DIR         Specify mockdir
      --topdir=DIR          Specify topdir
      --topurl=URL          url under which Koji files are accessible
      --distribution=DISTRIBUTION
                            Change the distribution macro
      -o FILE               Output to a file

for example to get the latest buildroot for dist-f12-build run

::

    koji mock-config --tag dist-f12-build --arch=x86_64 --topurl=https://kojipkgs.fedoraproject.org/ dist-f12

you will need to pass in --topurl=https://kojipkgs.fedoraproject.org/ to
any mock-config command to get a working mock-config from fedoras koji.


.. _tuning-mock-per-tag:

Tuning mock's behavior per tag
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

Few options for mock can be configured per-tag. These options are stored in
tag info's *extra* field. Extra values can be checked via `koji taginfo`
command.  Example for forcing `dnf` usage in specific build
environment follows:

::

    koji edit-tag dnf-fedora-tag -x mock.package_manager=dnf


* ``mock.package_manager`` - If this is set, it will override mock's default
  package manager. Typically used with ``yum`` or ``dnf`` values.
* ``mock.new_chroot`` - 0/1 value. If it is set, ``--new-chroot`` or
  `--old-chroot` option is appended to any mock call. If it is not set,
  mock's default behavior is used.
* ``mock.releasever`` - When doing cross-compiles it may be necessary
  to explicitly set the ``releasever`` to be used.
* ``mock.use_bootstrap`` - 0/1 value. If it is set, ``--bootstrap-chroot``
  is appended to the mock init call.  This tells mock to build in two stages,
  using chroot rpm for creating the build chroot. If it is not set, mock's
  default behaviour is used. (Note, that it changed in mock `1.4.1
  <https://github.com/rpm-software-management/mock/wiki/Feature-bootstrap>`_.
  Note, that it is not turn on by default by koji, as it is often not needed and
  it consumes additional resources (larger buildroot, downloading more data).
* ``mock.bootstrap_image`` - set to name of image, which can builder's podman
  download (e.g. ``fedora:32``). See mock's `doc
  <https://github.com/rpm-software-management/mock/wiki/Feature-container-for-bootstrap>`_
  before using this. You could need it, but do it with following
  recommendations:

  - you need to explicitly allow builders to do that (``mock_bootstrap_image =
    True`` in ``kojid.conf``).

  - you need to have builders with `podman <https://podman.io/>`_ installed and
    working.

  - use concrete hashes not potentially moving tags. Otherwise, you can get into
    harder debugging and auditing.

  - builders can consume space during time, no cleanup is made for podman's
    image cache. So, you'll probably want to run something like ``podman rmi
    `podman images -a --quiet``` periodically via cron or use some other
    cache-cleaning mechanism. Even simple task will consume roughly three times
    more space than without bootstrap image (downloaded image + exploded
    bootstrap dir + mock's buildroot itself)

  - be sure, that your podman is configured properly and it downloads images
    only from trusted sources. Note, that this setting effectivelly circumvents
    network isolation *inside* buildroot, as outside DNS, etc. can be spoofed.

  - this option will automatically turn ``mock.use_bootstrap`` (this is how
    it is implemented in mock)
* ``mock.module_setup_commands`` - commands for configuring the modules active
  in a buildroot. Available in `mock 2.4
  <https://github.com/rpm-software-management/mock/wiki/Release-Notes-2.4>`__.
* ``mock.forcearch`` - 0/1 value. If true mock will set the ``forcearch``
  config option to match the target arch of each buildroot.
* ``mock.yum.best`` - 0/1 value. If set yum/dnf will use highest available rpm
  version (see man yum.conf)
* ``mock.yum.module_hotfixes`` - 0/1 value. If set, yum/dnf will use packages
  regardless if they come from modularity repo or not. It makes sense only for
  tags with external repositories. (See dnf `docs
  <https://dnf.readthedocs.io/en/latest/modularity.html#hotfix-repositories>`__)

* `mock signing plugin
  <https://github.com/rpm-software-management/mock/wiki/Plugin-Sign>`__ -
  Options ``mock.plugin_conf.sign_enable``, ``mock.plugin_conf.sign_opts.cmd``
  and ``mock.plugin_conf.sign_opts.opts`` are propagated to mock conf to be used
  by this plugin. Note, that these tools are run outside of the jailed env.
  Note, that this functionality doesn't interfere with koji's standard signing
  commands (``import-sig``, ``write-signed-rpm``, etc.). Note, that rpmsign vs
  gpg must be configured correctly. If it is not it a) can silently ignore
  problems during signing b) can hang forever when e.g. gpg password store is
  not accessible.

You may also specify per-tag environment variables for mock to use.
For example, to set the CC environment variable to clang, you could
do:

::

    koji edit-tag dnf-fedora-tag -x rpm.env.CC=clang


Using Koji to control tasks
^^^^^^^^^^^^^^^^^^^^^^^^^^^

List tasks:

::

    koji list-tasks

List only tasks requested by you:

::

    koji list-tasks --mine

requeue an already-processed task: general syntax is: koji resubmit
[options] taskID

::

    koji resubmit 3

Building a Package with the command-line tool
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

Instead of using the fedpkg target, you can also directly use the
command\_line tool, koji.

To build a package, the syntax is:

::

    $ koji build <build target> <git URL>

For example:

::

    $ koji build dist-f14 'git url'

The koji build command creates a build task in Koji. By default the tool
will wait and print status updates until the build completes. You can
override this with the --nowait option.

.. raw:: html

   </pre>

NOTE: For fedora koji, the git url MUST be based on
pkgs.fedoraproject.org. Other arbitrary git repos cannot be used for
builds.

Koji tags and packages organization
-----------------------------------

Terminology
^^^^^^^^^^^

In Koji, it is sometimes necessary to distinguish between a package in
general, a specific build of a package, and the various rpm files
created by a build. When precision is needed, these terms should be
interpreted as follows:

-  Package: The name of a source rpm. This refers to the package in
   general and not any particular build or subpackage. For example:
   kernel, glibc, etc.
-  Build: A particular build of a package. This refers to the entire
   build: all arches and subpackages. For example: kernel-2.6.9-34.EL,
   glibc-2.3.4-2.19.
-  RPM: A particular rpm. A specific arch and subpackage of a build. For
   example: kernel-2.6.9-34.EL.x86\_64, kernel-devel-2.6.9-34.EL.s390,
   glibc-2.3.4-2.19.i686, glibc-common-2.3.4-2.19.ia64

Tags and targets
^^^^^^^^^^^^^^^^

Koji organizes packages using tags. In Koji a tag is roughly a
collection of packages:

-  Tags support inheritance
-  Each tag has its own list of valid packages (inheritable)
-  Package ownership can be set per-tag (inheritable)
-  When you build you specify a target rather than a tag

A build target specifies where a package should be built and how it
should be tagged afterwards. This allows target names to remain fixed as
tags change through releases.

Koji commands for tags
^^^^^^^^^^^^^^^^^^^^^^

Targets
'''''''

You can get a full list of build targets with the following command:

::

    $ koji list-targets

You can see just a single target with the --name option:

::

    $ koji list-targets --name dist-f14

    Name                           Buildroot                      Destination
    ---------------------------------------------------------------------------------------------
    dist-f14                     dist-f14-build                 dist-f14

This tells you a build for target dist-f14 will use a buildroot with
packages from the tag dist-f14-build and tag the resulting packages as
dist-f14.

Watch out: You probably don't want to build against dist-rawhide. If
Fedora N is the latest one out, to build to the next one, choose
dist-f{N+1}.

Tags
''''

You can get a list of tags with the following command:

::

    $ koji list-tags

Packages
''''''''

As mentioned above, each tag has its own list of packages that may be
placed in the tag. To see that list for a tag, use the list-pkgs
command:

::

    $ koji list-pkgs --tag dist-f14

The first column is the name of the package, the second tells you which
tag the package entry has been inherited from, and the third tells you
the owner of the package.

Latest Builds
'''''''''''''

To see the latest builds for a tag, use the latest-build command:

::

    $ koji latest-build --all dist-f14

The output gives you not only the latest builds, but which tag they have
been inherited from and who built them.

`Category:Package Maintainers <Category:Package Maintainers>`__

Koji XMLRPC API
===============

All features supported by command-line client are also accessible by XMLRPC
API. You can get listing of all available calls, arguments and basic help via
calling `koji list-api` command. This call will also provide you API
extensions provided by plugins in that particular koji instance.

Because of the data Koji routinely deals with, we use the following extensions
to the xmlrpc standard:

    * We use the ``nil`` extension to represent null values (e.g. None in
      Python). Koji's library handles this automatically. If you are using a
      different library, you may need to explicitly enable this (e.g. enabling
      allow_none in Python's own xmlrpc library).
    * We represent large integers with the ``i8`` tag. This standard is borrowed
      from Apache's `ws-xmlrpc <https://ws.apache.org/xmlrpc/types.html>`
      implementation. Python's own xmlrpc library understands this tag, even
      thought it will not emit it.
Koji Utilities
==============

Basic koji is equipped with few handy utilities for maintaining
healthy build environment. They are packaged in ``koji-utils`` rpm and
can be installed as such.

.. _utils-kojira:

Kojira
------

Kojira is stand-alone server which handles buildroot repos. It checks
if any builds were added to buildroots or build tag configuration has
changed. In such case it will trigger ``newRepo`` tasks to get build
repos actual once more. It is worth to create separate usera for
kojira with only ``repo`` permission.

Its usage is straightforward. It is being installed as system service
``kojira``, so standard systemctl commands like ``enable`` ``start``
and ``restart`` simply works.

``/etc/kojira/kojira.conf`` contains basic configuration for this
service. Standard connection options are defined there (keytab,
server, ...) and few options which affects, how kojira works,
especially in relation to throttling in creating ``newRepo`` tasks.

``deleted_repo_lifetime = one week (604800)``
    This time (in seconds) is uses to clean up expired repositories.
    It makes sense, that you don't want only the latest repodata. In
    case of debugging some older build. Even in case you don't want to
    do this, it is recommended to set it at least to few hours in
    case, there is some running build, which is still using these
    older data.

``dist_repo_lifetime = one week (604800)``
    This is similar to previous one. The only difference is that while
    previous is for buildroots, this one is affecting dist repos.

``recent_tasks_lifetime = 600``
    kojira is buffering recent ``newRepo`` finished tasks to avoid
    some race conditions. Generally, there is no reason to change this
    default.

``ignore_tags = ''``
    Space-separated globs for tag names. These tags are simply ignored
    by kojira (but they can still be manually regenerated via ``koji
    regen-repo`` command.

``debuginfo_tags = ''``
    Space-separated globs for tag names. Regenerated repos will have
    separate directory/repodata with corresponding debuginfo RPMs.

``source_tags = ''``
    Space-separated globs for tag names. Regenerated repos will
    contain also corresponding SRPMs.

``separate_source_tags = ''``
    Space-separated globs for tag names. Regenerated repos will have
    separate directory/repodata with corresponding SRPMs.

``ignore_stray_repos = False``
    In some strange cases (someone manually deleted repo via API, but
    not corresponding directories), there could stay some repo
    directories. If this is set to False, kojira will just skip these.
    Otherwise, it will remove them as if they would be normal
    repodata referenced from db.

``max_delete_processes = 4``
    How many threads are used for deleting data on disk.

``max_repo_tasks = 4``
    The largest hub impact is from the first part of the ``newRepo``
    task. Once it is waiting on subtasks (spawned createrepo), that
    part is over. So, it makes sense to limit running ``newRepo``
    tasks to not exhaust hub's capacity.

``max_repo_tasks_maven = 2``
    Maven repo regeneration is ways more resource-demanding than rpm
    ones. So, we've separate limit on this.

``repo_tasks_limit = 10``
    Overall limit on running tasks is set here. It involves all
    ``newRepo`` tasks spawned by kojira and also by other users.

``check_external_repos = false``
    If True, monitor external repos and trigger the appropriate Koji repo
    regenerations when they change.
    Note that you need to have your database set to use UTC, as otherwise
    you can end with weird behaviour. For details see
    https://pagure.io/koji/issue/2159
    
``queue_file = None``
    Writable path could be set here. In such case, kojira will write a
    list of currently monitored tags there with simple statistics in
    every cycle. File would contain information about how long these
    tags are expired and what is the computed score for them. This can
    be used to debug and check in realtime the actual performance.

Garbage Collector
-----------------

There are tons of builds, which will be never used for anything. GC is
caring to get rid of these, so they'll not exhaust disk space. As it
is a sensitive task to not remove something what will be needed in
future, everything is driven by policy with same language as hub's
one.

Note, that GC removes only physical content. Every build will stay in
database, only build artifacts and logs get deleted.

GC runs all of the following actions (if they are not overriden via
``--action``):

``delete``
    Delete builds that have been in the trashcan for long enough.
    Builds satisfying any of following conditions, will be exempted
    from deletion:

      * package is on blacklist ``pkg_filter``
      * they are not tagged with any other tag
      * their signatures are not in ``protected_sig`` or they are
        unknown
      * they are tagged in trashcan for shorter time than
        ``grace_period``

``prune``
    This action goes through all tags and checks tagged builds
    according to ``prune`` policy from config. Policy can result in
    ``keep``, ``untag`` or ``skip`` actions. First two are
    self-evident, last one is similar to ``keep``, but these builds
    are also ignored in tagged builds ordering.

    Prune policy is not run against trashcan tag itself, also locked
    tags are ignored if ``bypass_locks`` is not specified.

    With ``purge`` option, untagged builds can be immediately deleted.

``trash``
    Runs through all builds without any tags (for longer than
    ``delay``) and put them to trashcan (effectively scheduling them
    for deletion after additional ``grace_period`` time).

    Following builds can't be put to trashcan during this action:
      * build was tagged somewhere meanwhile (race condition)
      * build was used inside some build's buildroot. We don't want to
        delete such build, so we have reproducible build.
      * build is part of any image or archive
      * build was untagged later than before ``max_age`` seconds.
      * build has some protected or unknown signature(s) ``protected_sig``

``salvage``
     Untags builds from trashcan, which now have some protected or
     unknown key. (Note, that you can always remove trashcan tag
     from any build - it is normal tag as any other)

Prune Policy
............

Policy is part of config and without it, ``prune`` action will refuse
to work. Best documentation here would be part of example config with
comments.

.. code-block::

  [prune]
  policy =
      # stuff to protect
      # note that tags with master lock engaged are already protected
      tag *-updates :: keep
      hastag no-gc :: skip
      age < 1 day :: skip
      sig fedora-gold :: skip
      sig fedora-test && age < 12 weeks :: keep

      # stuff to chuck semi-rapidly
      tag *-testing *-candidate :: {  # nested rules
          order >= 2 :: untag
          order > 0 && age > 6 weeks :: untag
      } # closing braces must be on a line by themselves (modulo comments/whitespace)
      tag *-candidate && age > 60 weeks :: untag

      # default: keep the last 3
      order > 2 :: untag

GC Options (config/commmand-line)
.................................
``bypass_locks = ''``
    If tag is locked and ``bypass_locks`` is set and GC user has
    sufficient permissions, even locked tags are pruned.

``debug = False``
    Verbose output

``delay = 5 days``
    Time, after which untagged builds can go to trashcan via
    ``trashcan`` action.

``grace_period = 4 weeks``
    How long builds are staying in trashcan before final deletion.

``ignore_tags = ''``
    Tags corresponding to these globs are ignored.

``key_aliases = None``
    Keys are normally defined by their hashes, which could be
    inconvenient while reading configs. This option (pairs of
    hash/name) make it more readable.

``pkg_filter = ''``
    Globs for package names which should be processed.

``purge = False``
    If set, delete packages immediately during pruning action
    (effectively skipping ``delay`` + ``grace_period`` safety period)

``tag_filter = ''``
    If defined, only tags corresponding to these globs are checked.


``trashcan_tag = trashcan``
    Default name for trashcan tag, you can use other tags for testing
    policies, or deploy multiple configuration in cascade-like
    workflows (anyway, not recommended)

``unprotected_keys = ''``
    Set of signing keys, which are treated as in same way as
    "unsigned" packages.


Notification related options
............................
``bcc_addr``
    Blind carbon copy addresses

``cc_addr``
    Carbon copy addresses

``email_domain = fedoraproject.org``
   Append this domain to usernames

``email_template = /etc/koji-gc/email.tpl``
    Simple template which can contain python formatting (via
    ``string.Template``) with ``owner`` (owner name) and ``builds``
    (pre-generated list of builds).

``from_addr = Koji Build System <buildsys@example.com>``
    Sender address

``mail = True``
   Send / don't send e-mail notifications

``smtp_host = None``
   SMTP connection parameters

``smtp_pass = ''``
   SMTP connection parameters

``smtp_user = ''``
   SMTP connection parameters

``weburl = ''``
    Koji base web url. It is used for constructing links in notification messages.

Server connection options
.........................
``cert = /etc/koji-gc/client.crt``
    User certificate for SSL authentication

``keytab = ''``
    Path to keytab for GC authentication

``noauth=False``
    Don't authenticate (e.g. for --test call)

``no_ssl_verify``
    Don't check SSL CA chain. Should be turned off in production.

``password = ''``
    Password for login/password authentication.

``principal = ''``
    Kerberos principal to use for authentication

``runas = ''``
    Run as specified user

``server = ''``
    URL for koji hub

``serverca = /etc/koji-gc/serverca.crt``
    Server certification authority if it is not part of system-wide CAs.

``timeout = 43200``
    GC will quit after 12 hours of trying to connect to the hub.

``user = ''``
    Use specified user for login/password authentication. (It should be never enabled
    in production environment)


General options
...............

``exit_on_lock = False``
   If lock file is present, exit.

``lock_file = /run/user/<uid>/koji-gc.lock``
   Lock file for running GC. It is used to prevent overlapping calls.

``test = False``
    Run in test mode, no build will be deleted


Koji Shadow
-----------

Koji DB Sweeper
---------------
Storage Volumes
===============


Introduction
------------

Since version 1.7.0, Koji has had the ability to store builds on
multiple volumes.

Each build in Koji has a volume field that indicates which volume it is
stored on. The default volume is named ``DEFAULT`` and corresponds to the
original paths under ``/mnt/koji`` that predate this feature.

Additional volumes correspond to paths under
``/mnt/koji/vol/NAME`` (where NAME is the name of the volume). All builds
associated with such a volume will be stored under this directory.
The expectation is that this directory will map to a different file system.


.. hint::
    | If ``koji-1.13.0-1`` is on the DEFAULT volume, its path will be:
    | /mnt/koji/packages/koji/1.13.0/1

    | If ``koji-1.13.0-1`` is on the volume named "test", its path will be:
    | /mnt/koji/vol/test/packages/koji/1.13.0/1


Constructing the path
---------------------

If you are using the Koji python api, things should **just work**.

::

    >>> import koji
    >>> mykoji = koji.get_profile_module('mykoji')
    >>> opts = mykoji.grab_session_options(mykoji.config)
    >>> session = mykoji.ClientSession(mykoji.config.server, opts)
    >>> binfo = session.getBuild('test-1-1')
    >>> binfo['volume_name']
    'DEFAULT'
    >>> mykoji.pathinfo.build(binfo)
    '/mnt/koji/packages/test/1/1'
    >>> binfo = session.getBuild('fake-1.0-21')
    >>> binfo['volume_name']
    'vol3'
    >>> mykoji.pathinfo.build(binfo)
    '/mnt/koji/vol/vol3/packages/fake/1.0/21'

If you are constructing these paths yourself, then you may need to do
a little work.

    * Look for volume information in build data. Hub calls that return build
      data include ``volume_name`` and ``volume_id`` fields in their return.
    * If the volume is ``DEFAULT``, then the path is the "normal" path.
    * Otherwise, you need to insert ``/vol/<volume_name>`` after the
      top directory (normally /mnt/koji).


Symlinks on default volume
--------------------------

For backwards compatibility, Koji maintains symlinks on the default volume
for builds on other volumes.

::

    $ file /mnt/koji/packages/fake/1.0/21
    /mnt/koji/packages/fake/1.0/21: symbolic link to ../../../vol/vol3/packages/fake/1.0/21


Adding a new volume
-------------------

The new volume directory should initially contain a packages/
subdirectory, and the permissions should be the same as the default
packages directory.

Assuming you do use a mount for a vol/NAME directory, you will want to
ensure that the same mounts are created on all systems that interface with
``/mnt/koji``,  such as builders that run createrepo tasks, hosts running
kojira or similar maintenance, and any hosts that rely on the topdir option
rather than the topurl option.

Once you have the directory set up, you can tell Koji about it by
running ``koji add-volume NAME``. This call will fail if the hub can't find
the directory.

Moving builds onto other volumes
--------------------------------

By default, all builds live on the DEFAULT volume.

An admin can move a build to a different volume by using the
``koji set-build-volume`` command, or by using the underlying
``changeBuildVolume`` api call.

Moving a build across volumes will cause kojira to trigger repo
regenerations, if appropriate. When the new volume is not DEFAULT, Koji will
create a relative symlink to the new build directory on the default
volume. Moving builds across volumes may immediately break repos (until
the regen occurs), so use caution.

Consider the following example:

::

    # mypkg-1.1-20 initially on default volume
    $ file /mnt/koji/packages/mypkg/1.1/20
    /mnt/koji/packages/mypkg/1.1/20: directory

    # move it to test volume
    $ koji set-build-volume test mypkg-1.1-20
    $ file /mnt/koji/vol/test/packages/mypkg/1.1/20
    /mnt/koji/vol/test/packages/mypkg/1.1/20: directory

    # original location is now a symlink
    $ file /mnt/koji/packages/mypkg/1.1/20
    /mnt/koji/packages/mypkg/1.1/20: symbolic link to ../../../vol/test/packages/mypkg/1.1/20


Using the volume in policy checks
---------------------------------

Policies involving builds (e.g. gc policy, tag policy), can test a
build's volume with the ``volume`` test. This is a pattern match
test against the volume name.

Setting a volume policy
-----------------------

The Koji 1.14.0 release adds the ability to set a volume policy on the hub.
This policy is used at import time to determine which volume the build should
be assigned to. This provides a systematic way to distribute builds
across multiple volumes without manual intervention.

There is relatively limited data available to the volume policy. Tests that are
expected to work include:

- user based tests (the user performing the build or running the import)
- package based tests (e.g. ``is_new_package`` or ``package``)
- cg match tests
- the buildtag test

The action value for the volume policy should be simply the name of the volume
to use.

The default volume policy is ``all :: DEFAULT``.

If the volume policy contains errors, or does not return a result, then the
DEFAULT volume is used.

For more information about Koji policies see:
:doc:`Defining hub policies <defining_hub_policies>`


CLI commands
------------

``add-volume``
    adds a new volume (directory must already be set up)
``list-volumes``
    prints a list of known volumes
``set-build-volume``
    moves a build to different volume


API calls
---------

``addVolume(name, strict=True)``
    Add a new storage volume in the database

``applyVolumePolicy(build, strict=False)``
    Apply the volume policy to a given build

``changeBuildVolume(build, volume, strict=True)``
    Move a build to a different storage volume

``getVolume(volume, strict=False)``
    Lookup the given volume

``listVolumes()``
    List storage volumes

``removeVolume(volume)``
    Remove unused storage volume from the database
=========================
Example Windows Spec File
=========================

The following is an example ini-format spec file for a :doc:`Windows build <winbuild>` in Koji.

.. code-block:: ini

    [naming]
    ; naming and versioning of the component
    name = qpid-cpp-win
    version = 2.0.0.1
    release = 1
    description = Windows build of qpid-cpp-mrg

    [building]
    ; use os-arch
    platform = w2k8r2-x64

    ; file, directories, and commands that must be available in the build environment
    preinstalled = /cygdrive/c/Program Files/7-Zip/7z.exe
                   /cygdrive/c/Program Files (x86)/CMake 2.8/bin/cmake.exe
                   /cygdrive/c/Python26/python.exe
                   /cygdrive/c/Ruby186/bin/ruby.exe
                   C:\Ruby186\bin\msvcrt-ruby18.dll
                   /cygdrive/c/Program Files (x86)/doxygen/bin/doxygen.exe
                   /cygdrive/c/Program Files (x86)/Microsoft Visual Studio 9.0/Common7/IDE/devenv.exe
                   cpio
                   tar
                   patch
                   powershell

    ; To specify other components you need fetched to be able to build this, fill in a white-space
    ; delimited list of the other components you need by their names. Specific versions are not
    ; yet supported, the latest tagged will always be fetched.
    buildrequires = boost-win
                    qpid-cpp-mrg:type=rpm:arches=src

    ; what does this package provide?
    provides = qpid-cpp-win

    ; what shell are we running the commands below in?
    shell = bash

    ; what should we execute to build it?
    execute = read MAJOR MINOR REVISION BUILD < <(echo $version | tr . " ")
              pushd $boost_win_dir
              mkdir dist
              cd dist
              for z in ../boost-win-*.tar.bz2; do
                  tar xjf $z
              done
              popd
              PATH="/cygdrive/c/Program Files/7-Zip:$PATH"
              PATH="/cygdrive/c/Program Files (x86)/CMake 2.8/bin:$PATH"
              PATH="/cygdrive/c/Python26:$PATH"
              PATH="/cygdrive/c/Ruby186/bin:$PATH"
              PATH="/cygdrive/c/Program Files (x86)/doxygen/bin:$PATH"
              PATH="/cygdrive/c/Program Files (x86)/Microsoft Visual Studio 9.0/Common7/IDE:$PATH"
              export PATH
              # extract the tarball from the qpid-cpp-mrg rpm
              pushd $qpid_cpp_mrg_rpm_dir/src
              7z x qpid-cpp-*.src.rpm
              cpio -idmv < qpid-cpp-*.cpio
              popd
              mkdir source
              cd source
              tar xzf $qpid_cpp_mrg_rpm_dir/src/qpid-cpp-*.tar.gz
              cd qpid-cpp-*
              # apply patches
              for p in ../../*.patch; do
                  patch -p1 --fuzz=0 < $p
              done
              cd cpp
              cat <<EOF >> src/CMakeWinVersions.cmake
              set("winver_FILE_VERSION_N1" "$MAJOR")
              set("winver_FILE_VERSION_N2" "$MINOR")
              set("winver_FILE_VERSION_N3" "$REVISION")
              set("winver_FILE_VERSION_N4" "$BUILD")
              set("winver_PRODUCT_VERSION_N1" "$MAJOR")
              set("winver_PRODUCT_VERSION_N2" "$MINOR")
              set("winver_PRODUCT_VERSION_N3" "$REVISION")
              set("winver_PRODUCT_VERSION_N4" "$BUILD")
              EOF
              powershell -ExecutionPolicy unrestricted -File bld-winsdk.ps1 $(basename $(dirname $PWD)) $(cygpath -wa $boost_win_dir/dist/boost-win-*-32bit) $(cygpath -wa $boost_win_dir/dist/boost-win-*-64bit) $version
              mv ../../x86/qpid-cpp-x86-$version.zip ../../x64/qpid-cpp-x64-$version.zip ../../../

    ; list of files that must be present after the build for the build to be
    ; considered successful, but are not included in the list of build output
    postbuild =

    [files]
    ; all values in this section may be multi-line
    ; output files we're concerned with (specify paths relative to scm root)
    output = qpid-cpp-x86-$version.zip:i386:chk,fre
             qpid-cpp-x64-$version.zip:x86_64:chk,fre

    ; logs we should report
    logs =
====================
Building for Windows
====================

.. toctree::
   :hidden:

   win_spec_example

To perform Windows builds, Koji clones and starts a Windows VM, checks out
sources, and build the sources inside the VM. The VM should have compile tools
already pre-installed. Koji stores the build results in the hub.

Similar to RPM builds, Koji's Windows builds have a unique name, version, and
release field. Windows builds use Koji build targets, and you can tag builds
into Koji tags like any other build. When you build a Windows component, you
can reference other components, and Koji will download those into the build
environment to satisfy build-time dependencies.


Initiating a build
==================

Initiate Windows builds with the ``win-build`` subcommand::

    $ koji win-build <target> <scm-url> <vm-name>

* ``target`` works like build targets for other types of builds. The *build
  tag* for the target determines where Koji will pull build dependencies from,
  and the *destination tag* for the target determines where Koji will tag the
  completed build.

* ``scm-url`` follows the same syntax used for other Koji builds::

     scheme://[user@]host/path/to/repo?path/to/module#revision_or_tag_identifier

  Koji requires a "Windows spec file" (see below).  This spec file can either
  be included in the top directory of the sources or specified separately
  with the ``--winspec`` option.

  ``--winspec`` which allows you to specify a second remote repository that
  contains the windows spec file using the same SCM URL syntax. That way you
  are not forced to keep the spec in with the rest of your sources if you
  don't want to.

  There is also a ``--patches`` option which is just like ``--winspec``,
  except meant for a repository of separate patches, which will be applied to
  the sources before the build is launched. ``--winspec`` and ``--patches``
  may reference the same repo.

* ``vm-name`` is the name of the VM image to use for the build. This name must
  be one of the known images available on the builders.

.. note::
    Koji hub's ``vm`` policy governs access for performing Windows builds.
    Koji's default policy requires the user to have the ``win-admin``
    permission. Different Koji instances may have different policies.  If the
    policy denies you access, you will see an ``ActionNotAllowed`` error.  In
    that case, you will need to file a request with your Koji administators.

The Windows "Spec-File"
=======================

This file controls the build process and is modeled very loosely on RPM's
spec files. The Windows spec file is in ``.ini`` format.

It's probably easiest to start by looking at this :doc:`example spec file
<win_spec_example>`.

``[naming]``
------------

All values in the ``[naming]`` section must be defined.
Koji uses the ``name``, ``version``, and ``release`` to determine the NVR of
the build. As with all NVRs in Koji, this must be a unique value.

``[building]``
--------------

The ``[building]`` section defines preconditions that must be satisfied before
the build will be launched, and the commands used to perform the build.
``platform`` indicates to VM type the build is running in, and should be in
the format of ``osname-arch``.

The ``preinstalled`` value under ``[building]`` lists the files and
directories that must be present for the build to run. Koji administrators
should pre-install these tools into the VM parent image.
If Koji finds any of the the files or directories listed here are missing when
the build runs, it will fail immediately. Files and directories may be listed
in Windows format (``C:\Program Files\...``), full Cygwin paths
(``/bin/...``), or as command names (``tar``, ``unzip``, etc).  If they are
listed as command names, Koji will check the Cygwin ``PATH`` for the command.

The ``buildrequires`` value under ``[building]`` lists other packages that
the build depends on.  As with RPM ``BuildRequires``, the Windows
``buildrequires`` entries should be package names only (no
version information).  Koji will look up the latest build of that package in
the target's build tag and download files from that build into the VM.

Each package listed in ``buildrequires`` can have a colon-delimited list of
options associated with it, which determine which files from the build will be
downloaded.

 * By default, all Windows files associated with the dependent build will be
   downloaded (this is the same as specifying ``type=win``).  In this case,
   comma-separated lists of ``platforms=`` and flags can also be specified, in
   which case the files downloaded into the VM will be limited to those
   associated with one or more of the platform and flag values (more about
   this in the ``[files]`` section below.)

 * If ``type=rpm`` is specified, then all rpms associated with the dependent
   build will be downloaded.  In this case a comma-separated list of
   ``arches=`` may be specified, and only rpms matching one of those arches
   will be downloaded.

 * If ``type=maven`` is specified, then comma-separated lists of
   ``group_ids=``, ``artifact_ids=``, and/or ``versions=`` may be specified,
   and only Maven artifacts matching at least one value in each list will be
   downloaded.

 * In all cases, a ``patterns=`` option may be specified, which is a
   comma-separated list of globs to match against the filenames.  Only files
   matching at least one of the patterns will be downloaded.

After downloading the ``buildrequires`` files, Koji places them in a directory
based on their type sets variables pointing to their locations. The variable
names are based on the dependency name, with some conversions:

 * A leading number is converted to an underscore, and any character that is
   not a letter, number, or underscore is converted to an underscore.

 * If ``type=`` is specified, then ``<type>_dir`` is appended to complete the
   variable name.  Otherwise, just ``_dir`` is appended.

In the :doc:`example spec file <win_spec_example>`:

  * The directory containing the ``boost-win`` files would be
    ``$boost_win_dir``.

  * The directory containing the ``.src.rpm`` from the ``qpid-cpp-mrg`` build
    would be ``$qpid_cpp_mrg_rpm_dir``.  Note the extra ``_rpm`` there,
    because it specified ``type=rpm``.

Koji also sets ``_files`` variables for each dependency. Each ``_files``
variable contains a newline-delimited list of all files downloaded for each
dependency. In the example, Koji would define ``$boost_win_files`` and
``$qpid_cpp_mrg_rpm_files`` variables.

Koji downloads the files to the VM unmodified.  It is up to the build process
to extract them if necessary, and move/copy them to whatever location is
required by the current build.

The ``provides`` value under ``[building]`` is optional, and can be used to
indicate what the build is producing.  It is freeform, but a structure like
``<name>-<version>`` is encouraged.

The ``shell`` value under ``[building]`` indicates how the build script should
be run. Valid values are ``bash``, which will cause the script to be run in
``bash``, and ``cmd.exe``, which will cause the script to be run in Windows
``cmd.exe``.  ``cmd`` is also an alias for ``cmd.exe``.  ``bash`` is the
default if no shell value is present.

The ``execute`` value under ``[building]`` is required, and this is what drives
the build process. This value is multiline, and each line is treated as a
separate command to be execute in the shell specified above.  In addition to
the variables defined for the ``buildrequires``, the following variables will
be available when the script is executed:

  * ``name``: name from the [naming] section
  * ``version``: version from the [naming] section
  * ``release``: release from the [naming] section
  * ``source_dir``: the directory the sources were checked out into
  * ``spec_dir``: the directory the ``.ini`` was checked out into.  If there
    was no separate ``--winspec`` option passed on the command-line, this will
    be the same as ``source_dir``.
  * ``patches_dir``: the directory the patches were checked out into.  If
    there was no ``--patches`` option passed on the command-line, this will be
    undefined.

If using ``bash``, the build script will be executed with ``-x`` and ``-e``,
which will cause all commands to be echoed, and will cause the script to fail
if any commands have a non-zero exit status.  There is no equivalent fail-fast
option for Windows ``cmd.exe``.  If executing the script using ``cmd.exe``, it
is recommend that you frequently check the return value of commands with
this::

    if %ERRORLEVEL% neq 0 exit %ERRORLEVEL%

The script will start in the root directory of the sources that were checked
out (``$source_dir``).  Extra scripts or supplementary files required by the
build may be checked in along with the ``.ini`` file, and will be available
under ``$spec_dir`` (assuming a separate ``--winspec`` option was used).

The ``postbuild`` value is optional, and specifies a list of files and
directories (relative to ``$source_dir``) that must be present for the build
to be considered successful.  If any of them are missing, the build will fail.
Use this to verify that all expected output was generated correctly, in the
case that some commands may have failed silently.

``[files]``
-----------

The ``[files]`` section describes what output Koji should collect after the
build completes. The ``logfiles`` value is optional, but it should be set to
whatever build logs are produced from the build. The syntax for the output
variable is a colon-separated value::

    output = qpid-cpp-x86-$version.zip:i386:chk,fre

* The first token (``qpid-cpp-x86-$version.zip``) is the path to the file you
  want collected as part of your build output. This path is rooted at the
  checkout from the SCM ($source_dir).  The file path relative to
  ``$source_dir`` is retained when output is uploaded to Koji and when it is
  downloaded as a ``buildrequires`` by future builds.  If you don't want a
  long, confusing file path, it may be desirable to copy the build output to
  ``$source_dir`` at the end of your build script.  File globs are not
  allowed, but the ``$name``, ``$version``, and ``$release`` variables will be
  expanded in the file paths and names, using the values from the ``[naming]``
  section.

* The second token is a comma-separated list of platforms (which we haven't
  really standardized on yet, but ``i386`` and ``x86_64`` are logical
  choices).
 
* The last is a comma-separated list of build flags. These fields are purely
  informational, they do not influence future builds at this time, but they do
  make for good housekeeping in the future.  Common flags are ``chk``
  (indicating a debug build) and ``fre`` (indicating an optimized build).  If
  an output file contains both kinds of builds, both may be specified.

The ``logs`` value indicates extra log files generated during the build that
should be tracked.  The contents of these log files will be streamed to the
hub during the build and may be watched in realtime using the "Watch logs"
feature of the web interface, or the ``koji watch-logs`` CLI command.  Koji
will also include these log when storing the build long-term in the hub.


Administration
==============

Windows Build Hosts
-------------------

By default, all ``winbuild`` tasks go to the ``vm`` channel.
The hosts in this channel require special setup.

 * They run the ``kojivmd`` daemon instead of the regular ``kojid`` daemon
 * VM images for builds must be stored in the local image directory

Managing VM Images
------------------

The directory where ``kojivmd`` looks for vm images can be controlled by
setting ``imagedir`` in ``/etc/kojivmd/kojivmd.conf``. The default value is
``/var/lib/libvirt/images``.

These images must be qcow2 images named with a ``.qcow2`` extension. The
basename of the image file is the same name that users refer to in the
``vm-name`` parameter to the ``win-build`` command.
Writing Koji plugins
====================

Depending on what you are trying to do, there are different ways to
write a Koji plugin.

Each is described in this file, by use case.

Adding new task types
---------------------

Koji can do several things, for example build RPMs, or live CDs. Those
are types of tasks which Koji knows about.

If you need to do something which Koji does not know yet how to do, you
could create a Koji Builder plugin.

Such a plugin would minimally look like this:

::

    from koji.tasks import BaseTaskHandler

    class MyTask(BaseTaskHandler):
        Methods = ['mytask']
        _taskWeight = 2.0

        def handler(self, arg1, arg2, kwarg1=None):
            self.logger.debug("Running my task...")

            # Here is where you actually do something

A few explanations on what goes on here:

-  Your task needs to inherit from ``koji.tasks.BaseTaskHandler``
-  Your task must have a ``Methods`` attribute, which is a list of the
   method names your task can handle.
-  You can specify the weight of your task with the ``_taskWeight``
   attribute. The more intensive (CPU, IO, ...) your task is, the higher
   this number should be.
-  The task object has a ``logger`` attribute, which is a Python logger
   with the usual ``debug``, ``info``, ``warning`` and ``error``
   methods. The messages you send with it will end up in the Koji
   Builder logs (``kojid.log``)
-  Your task must have a ``handler()`` method. That is the method Koji
   will call to run your task. It is the method that should actually do
   what you need. It can have as many positional and named arguments as
   you want.

Save your plugin as e.g ``mytask.py``, then install it in the Koji
Builder plugins folder: ``/usr/lib/koji-builder-plugins/``

Finally, edit the Koji Builder config file, ``/etc/kojid/kojid.conf``:

::

    # A space-separated list of plugins to enable
    plugins = mytask

Restart the Koji Builder service, and your plugin will be enabled.

You can try running a task from your new task type with the
command-line:

::

    $ koji make-task mytask arg1 arg2 kwarg1

Exporting new API methods over XMLRPC
-------------------------------------

Koji clients talk to the Koji Hub via an XMLRPC API.

It is sometimes desirable to add to that API, so that clients can
request things Koji does not expose right now.

Such a plugin would minimally look like this:

::

    def mymethod(arg1, arg2, kwarg1=None):
        context.session.assertPerm('admin')
        # Here is where you actually do something

    mymethod.exported = True

A few explanations on what goes on here:

-  Your plugin is just a method, with whatever positional and/or named
   arguments you need.
-  You must export your method by setting its ``exported`` attribute to
   ``True``
-  The ``context.session.assertPerm('admin')`` is how you ensure that only
   the user with administrator privileges can use this call. Read-only
   methods can be (in most cases) public, so such line is not needed.

Save your plugin as e.g ``mymethod.py``, then install it in the Koji Hub
plugins folder: ``/usr/lib/koji-hub-plugins/``

Finally, edit the Koji Hub config file, ``/etc/koji-hub/hub.conf``:

::

    # A space-separated list of plugins to enable
    Plugins = mymethod

Restart the Koji Hub service, and your plugin will be enabled.

You can try calling the new XMLRPC API with the Python client library:

::

    >>> import koji
    >>> session = koji.ClientSession("http://koji/example.org/kojihub")
    >>> session.mymethod(arg1, arg2, kwarg1='some value')

Ensuring the user has the required permissions
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

If you want your new XMLRPC API to require specific permissions from the
user, all you need to do is add the following to your method:

::

    from koji.context import context

    def mymethod(arg1, arg2, kwarg1=None):
        context.session.assertPerm("admin")

        # Here is where you actually do something

    mymethod.exported = True

In the example above, Koji will ensure that the user is an
administrator. You could of course create your own permission, and check
for that.

Running code automatically triggered on events
----------------------------------------------

You might want to run something automatically when something else
happens in Koji.

A typical example is to automatically sign a package right after a build
finished. Another would be to send a notification to a message bus after
any kind of event.

This can be achieved with a plugin, which would look minimally as
follows:

::

    from koji.plugin import callback

    @callback('preTag', 'postTag')
    def mycallback(cbtype, tag, build, user, force=False):
        # Here is where you actually do something

A few explanations on what goes on here:

-  The ``@callback`` decorator allows you to declare which events should
   trigger your function. You can pass as many as you want. For a list
   of supported events, see ``koji/plugins.py``.
-  The arguments of the function depend on the event you subscribed to.
   As a result, you need to know how it will be called by Koji. You
   probably should use ``*kwargs`` to be safe. You can see how callbacks
   are called in the ``hub/kojihub.py`` file, search for calls of the
   ``run_callbacks`` function.

Save your plugin as e.g ``mycallback.py``, then install it in the Koji
Hub plugins folder: ``/usr/lib/koji-hub-plugins``

Finally, edit the Koji Hub config file, ``/etc/koji-hub/hub.conf``:

::

    # A space-separated list of plugins to enable
    Plugins = mycallback

Restart the Koji Hub service, and your plugin will be enabled.

You can try triggering your callback plugin with the command-line. For
example, if you registered a callback for the ``postTag`` event, try
tagging a build:

::

    $ koji tag-build mytag mypkg-1.0-1


List of callbacks
-----------------

hub:

- preBuildStateChange
- preImport
- prePackageListChange
- preRPMSign
- preRepoDone
- preRepoInit
- preTag
- preTaskStateChange
- preUntag
- postBuildStateChange
- postImport
- postPackageListChange
- postRPMSign
- postRepoDone
- postRepoInit
- postTag
- postTaskStateChange
- postUntag

builder:

- preSCMCheckout
- postSCMCheckout
- postCreateDistRepo
- postCreateRepo

.. _plugin-cli-command:

New command for CLI
-------------------

When you add new XMLRPC call or just wanted to do some more complicated
things with API, you can benefit from writing a new command for CLI.

Most simple command would look like this:

::

    from koji.plugin import export_cli

    @export_cli
    def anon_handle_echo(options, session, args):
        "[info] Print arguments"
        usage = "usage: %prog echo <message>"
        parser = OptionParser(usage=usage)
        (opts, args) = parser.parse_args(args)
        print(args[0])

``@export_cli`` is a decorator which registers a new command. The command
name is derived from name of the function. The function name must start with
either ``anon_handle_`` or ``handle_``. The rest of the name becomes the name of
the command.

In the first case, the command will not automatically
authenticate with the hub (though the user can still override
this behavior with ``--force-auth`` option). In the second case, the command
will perform authentication by default (this too can be overridden by the
user with the ``--noauth`` option).

The example above is very simplistic. We recommend that developers also
examine the actual calls included in Koji. The built in commands live in
``koji_cli.commands`` and our standard cli plugins live in ``plugins/cli``.

Koji provides some important functions via in the client cli library
(``koji_cli.lib``) for use by cli commands. Some notable examples are:

 * ``activate_session(session, options)`` - It is needed to authenticate
   against hub. Both parameters are same as those passed to handler.
 * ``watch_tasks(session, tasklist, quiet=False, poll_interval=60)`` - It is
   the same function used e.g. in ``build`` command for waiting for spawned
   tasks.
 * ``list_task_output_all_volumes(session, task_id)`` - wrapper function for
   ``listTaskOutput`` with different versions of hub.

Final command has to be saved in python system-wide library path - e.g. in
``/usr/lib/python3.4/site-packages/koji_cli_plugins``. Filename doesn't matter
as all files in this directory are searched for ``@export_cli`` macros. Note,
that python 3 variant of CLI is looking to different directory than python 2
one.

CLI plugins structure will be extended (made configurable and allowing more
than just adding commands - e.g. own authentication methods, etc.) in future.

Pull requests
-------------

These plugins have to be written in python 2.6+/3.x compatible way. We are
using `six` library to support this, so we will also prefer pull requests
written this way. CLI (and client library) is meant to be fully compatible
with python 3 from koji 1.13.

Tests are also recommended for PR. For example one see
``tests/test_plugins/test_runroot_cli.py``.
=================
Writing Koji Code
=================

Getting Started Hacking on Koji
===============================


This page gives an overview of the Koji code and then describes what
needs to change if you want to add a new type of task. A new task could
be for a new content type, or assembling the results of multiple builds
together, or something else that helps your workflow. New contributors
to Koji should leave this page knowing where to begin and have enough
understanding of Koji's architecture to be able to estimate how much
work is still ahead of them.

Koji is written to support a variety of platforms and python versions,
with some portions of Koji supported over a larger set than others.
When making changes, please be considerate of those
:doc:`supported_platforms`.

Task Flow
=========

A task starts with a user submitting it with the Koji client, which is a
command line interface. This contacts the hub, an apache-based server
application. It leaves a row in the database that represents a "free"
task, one that has not been assigned to a builder. Periodically, the
builders asynchronously ping the hub asking if there are any tasks
available, and at some point one will be given the new task. The hub
marks this in the database, and the builder begins executing the task (a
build).

Upon completion, the builder uploads the results to the hub, including
logs, binaries, environment information, and whatever else the task
handler for the build dictated. The hub moves the results to a permanent
shared storage solution, and marks the task as completed (or failed).
During this whole time, the webUI can be used to check up on progress.
So the flow of work is:

::

    Client -> Hub -> Builder -> Hub

If you wanted to add a new build type or task that was tightly
integrated in Koji's data model, you would need to modify the CLI, Hub,
Builder, and WebUI at a minimum. Alternatively, you could do this with a
plugin, which is far simpler but less flexible.

Tasks' states are following:

* ``FREE`` - Task was created and waits in the queue
* ``OPEN`` - Task was grabbed by some builder and is running now
* ``CLOSED`` - Succesffuly finished task.
* ``CANCELED`` - Task which was either manually cancelled or some sibling task already
  failed, so it would be wasteful to continue with this one so parent task
  will cancel it.
* ``ASSIGNED`` - Task can be manually (admin) assigned to specific builder bypassing channel
  policy. This behaviour can be forbidden via policy, so in some instances this
  could be unreachable state.
* ``FAILED`` - Task failed from some reason (typically build process failed)

.. graphviz::

  digraph task_states {
      "FREE" [color="blue", penwidth=2]
      "OPEN" [shape="box", color="orange", penwidth=2]
      "CLOSED" [shape="box", color="green", penwidth=2]
      "CANCELED" [shape="box", color="yellow", penwidth=2]
      "ASSIGNED" [shape="box", color="purple", penwidth=2]
      "FAILED" [shape="box", color="red", penwidth=2]
      FREE -> OPEN[label="builder picks the task\nand starts it\n[builder]"]
      FREE -> ASSIGNED[label="task is assigned\nto specific builder\n[admin]"]
      OPEN -> CLOSED[label="task is successfully\nfinished\n[builder]"]
      OPEN -> FAILED[label="task fails\n[builder]"]
      OPEN -> CANCELED[label="task is automatically\nor manually canceled\n[builder/owner/admin]"]
      OPEN -> ASSIGNED[label="task is forced to\nrun on specific builder\n[admin]"]
      OPEN -> FREE[label="task is freed\n[admin]"]
      ASSIGNED -> OPEN[label="builder starts\nwork on task\n[builder]"]
      ASSIGNED -> CANCELED[label="task is automatically\nor manually canceled\n[builder/owner/admin]"]
      FAILED -> FREE[label="task is \nresubmitted\n[owner/admin]"]
      CANCELED -> FREE[label="task is \nresubmitted\n[owner/admin]"]
  }

If task is ``OPEN`` it starts with task's starting ``weight`` which is different
for different task types. Every builder has some set ``capacity`` according to
its resources and can accept new task only if ``sum(weight) < capacity``.
Furthermore, task's ``weight`` is further increased based on statistics from
previous runs and current running time, so you can sometimes see that builder's
``load`` is above its ``capacity``.

Tasks which are currently waiting on some of its subtasks have its ``weight``
temporarily ignored and they are effectively sleeping. (``getTaskInfo`` API call
returns all these values).

Note, that cancelling task doesn't immediately stop it. Builder is polling hub
and only in the upcoming call it will acknowledge that some of tasks it is
running was cancelled meanwhile. Only in that point it will kill the
corresponding thread and run cleanup routine. Nevertheless, it is the
implementation detail as cancelled task will not affect anything in the db/filer
as its data get cleaned.

Component Overview
==================

Koji is comprised of several components, this section goes into details
for each one, and what you potentially may need to change. Every
component is written in Python, so you will need to know that language
beyond a beginner level.

Koji-client
-----------

koji-client is a command line interface that provides many hooks into
Koji. It allows the user to query much of the data as well as perform
actions such as adding users and initiating build requests.

Option Handling
~~~~~~~~~~~~~~~

The code is in ``cli/koji``. It uses ``OptionParsers`` extensively with
interspersed arguments disabled. That means these two commands are not
interpreted the same:

::

    $ koji -u admin -p password tag-build some-tag --force some-build
    $ koji tag-build -u admin -p password some-tag --force some-build

The second one will generate an error, because -u and -p are not options
for tag-build, they must show up before that because they are global
options that can be used with any subcommand. There will be two
``OptionParsers`` used with each command. The first is used to pick up
arguments to ``koji`` itself, and the second for the subcommand
specified. When the first one executes (see ``get_options()``) it will
figure out the subcommand and come up with a function name based on it.

The convention is to prepend the word ``handle_`` before it, and change
all hyphens to underscores. If a command does not require an account
with Koji, the function handle will prepended with ``anon_handle_``
instead. The code will dynamically call the derived function handle
which is where the second ``OptionParser`` is used to parse the
remaining options. To have your code log into Koji (you're writing a
handle\_ function), use the ``activate_session`` function. All function
signatures in the client code will get a session object, which is your
interface to the hub.

Profiles
~~~~~~~~

It is possible to run the Koji client with different configuration
profiles so that you can interact with multiple Koji instances easily.
The ``--profile`` option to the Koji command itself enables this. You
should have a ``~/.koji/config`` already, if not just copy from
``/etc/koji.conf`` to get a start. The profile command accepts an
argument that matches a section in that config file. So if your config
file had this:

::

    [Fedora]
    authtype = ssl
    server = https://koji.fedoraproject.org/kojihub
    topdir = /mnt/koji
    weburl = https://koji.fedoraproject.org/koji
    #pkgurl = https://koji.fedoraproject.org/packages
    cert = ~/.fedora.cert
    ca = ~/.fedora-upload-ca.cert
    serverca = ~/.fedora-server-ca.cert

    [MyKoji]
    server = https://koji.mydomain.com/kojihub
    authtype = kerberos
    topdir = /mnt/koji
    weburl = https://koji.mydomain.com/koji
    topurl = https://download.mydomain.com/kojifiles

you could pass Fedora or MyKoji to --profile.

Creating Tasks
~~~~~~~~~~~~~~

Once options are processed and understood, a task needs to be created on
the hub so that a builder can come along and take it. This is
accomplished with the ``makeTask`` method (defined on the Hub, so call
it on the ``session`` object). The name of the task should match the
name given to the task handler in the builder, which is explained later
on.

Be sure to process the channel, priority, background, and watch/nowatch
parameters too, which should be available to most new tasks. They'll be
buried in the first argument to your handler function, which captures
the options passed to the base Koji command.

If the client needs to make locally-available artifacts (config files,
sources, kickstarts) accessible to the builder, it must be uploaded to
the hub. This is the case with uploading SRPMs or kickstarts. You can
easily upload this content with the ``session.uploadWrapper`` method.
You can create progress bars as necessary with this snippet:

::

    if _running_in_bg() or task_opts.noprogress:
      callback = None
    else:
      callback = _progress_callback
    serverdir = unique_path('cli-image')   # create a unique path on the hub
    session.uploadWrapper(somefile, serverdir, callback=callback)

Task Arguments
~~~~~~~~~~~~~~

If you define a new task for Koji, you'll want the task submission
output to have the options ordered usefully. This output is
automatically generated, but sometimes it does not capture the more
important arguments you want displayed.

::

    Created task 10001810
    Watching tasks (this may be safely interrupted)...
    10001810 thing (noarch): free
    10001810 thing (noarch): free -> closed
      0 free  0 open  1 done  0 failed

    10001810 thing (noarch) completed successfully

In this (fake) example, you can see that "noarch" is the only option
being displayed, but maybe you want something more than just the task
architecture displayed, like some other options that were passed in. You
can fix this behavior in ``koji/__init__.py`` in the \_taskLabel
function. Here you can define the string(s) to display when Koji
receives status on a task. That is the return value.

Using multicall
~~~~~~~~~~~~~~~

Koji supports XML-RPC multicalls. Clients can send multiple calls to a hub in
a single request. This is a faster and more efficient way to make many
related calls, and it reduces overhead on the client and server.

The ``ClientSession`` class provides support for this and there are several
examples in the existing client code. Some examples in the cli include:
``edit-host``, ``add-pkg``, ``disable-host``, and ``list-hosts``.

**Using MultiCallSession**

Koji tracks individual muticalls for a session with a ``MultiCallSession``
object. To create one, call your session's ``multicall()`` method. Use this
object like a session, but it will store calls rather than sending
immediately. To execute the calls, call the ``call_all()`` method.

::

    task_ids_to_cancel = [123, 456, 789]
    m = session.multicall()
    for task_id in task_ids_to_cancel:
        m.cancelTask(task_id)
    m.call_all()

Alternatively, you can use this as a context manager. The following is
equivalent::

    task_ids_to_cancel = [123, 456, 789]
    with session.multicall() as m:
        for task_id in task_ids_to_cancel:
            m.cancelTask(task_id)

When the context manager exits, the client sends the multicall to the hub
(implicitly executing ``call_all()``).

Another example, getting a list of tag names from ids::

    with session.multicall() as m:
        tags = [m.getTag(tag_id) for tag_id in my_tag_ids]
    for tag in tags:
        print(tag.result['name'])

Each method you call on a ``MultiCallSession`` object will return a
``VirtualCall`` object that stands in for the result.

Once you send the multicall and the hub executes it, you can access the result
of each call via the ``result`` property on each ``VirtualCall`` object. (You
must execute the call before accessing the ``.result`` property, or
``VirtualCall`` will raise an exception.)

Two parameters affect the behavior of the multicall.

* If the ``strict`` parameter is set to ``True``, the multicall will raise the
  first error it encounters, if any.
* If the ``batch`` parameter is set to a number greater than zero, the
  multicall will spread the calls across multiple multicall batches of at most
  that number.

You may pass these parameters to the ``call_all()`` method, or you may pass
them when you initialize ``MultiCallSession``::

    with session.multicall(strict=True, batch=500) as m:
        builds = [m.getBuild(build_id) for build_id in mylist]


**Deprecated: Using ClientSession.multiCall**

.. note::
   This section describes the old (modal) way to make multicalls in Koji: set
   the ``.multicall`` property to ``True`` and call the ``.multiCall()``
   method on the ``ClientSession`` object. You cannot make other normal calls
   until you complete the multicall with ``.multiCall()``.

   Please switch your code to use the newer ``multicall()`` pattern described
   above. Only use this older-style code pattern if you must support Koji
   versions prior to 1.18.

To use the feature, you first set the ``multicall`` attribute of the session
to ``True``. Once this is done, the session will not immediately process
further calls but will instead store their parameters for later. To tell the
session to process them, use the ``multiCall()`` method (note the
capitalization).

The ``multiCall()`` method returns a list of results, one for each call
in the multicall. Each result with either be:

1. the result of the call wrapped in a singleton list
2. a dictionary representing the error raised by the call

Here is a simple example from the koji-tools package:

::

    session.multicall = True
    for host in hosts:
        session.listChannels(hostID=host['id'])
    for host, [channels] in zip(hosts, session.multiCall(strict=True)):
        host['channels'] = channels

Note that when using multicall for informational calls, it is important
to keep track of which result is which. Here we use the existing hosts
list as a unifying index. Python's ``zip`` function is useful here.
Also note the unpacking of the singletons.

The ``multiCall()`` method supports a few options. Here is its signature:

::

    multiCall(strict=False, batch=None):

If the strict option is set to True, then this method will raise the
first error it encounters, if any.

If the batch option is set to a number greater than zero, the calls
will be spread across multiple multicall batches of at most this
number.

The hub processes multicalls in a *single database transaction*. Note that if
the ``batch`` option is used, then each batch is a separate multicall in the
api and therefore a separate transaction.


Koji-Hub
--------

koji-hub is the center of all Koji operations. It is an XML-RPC server
running under mod\_wsgi in Apache. koji-hub is passive in that it only
receives XML-RPC calls and relies upon the build daemons and other
components to initiate communication. koji-hub is the only component
that has direct access to the database and is one of the two components
that have write access to the file system. If you want to make changes
to the webUI (new pages or themes), you are looking in the wrong
section, there is a separate component for that.

Implementation Details
~~~~~~~~~~~~~~~~~~~~~~

The **hub/kojihub.py** file is where the server-side code lives. If you
need to fix any server problems or want to add any new tasks, you will
need to modify this file. Changes to the database schema will almost
certainly require code changes too. This file gets deployed to
**/usr/share/koji-hub/kojihub.py**, whenever you make changes to that
remember to restart **httpd**. Also there are cases where httpd looks
for an existing .pyc file and takes it as-is, instead of re-compiling it
when the code is changed.

In the code there are two large classes: **RootExports** and
**HostExports**. RootExports exposes methods using XMLRPC for any client
that connects to the server. The Koji CLI makes use of this quite a bit.
If you want to expose a new API to any remote system, add your code
here. The HostExports class does the same thing except it will ensure
the requests are only coming from builders. Attempting to use an API
exposed here with the CLI will fail. If your work requires the builders
to call a new API, you should implement it here. Any other function
defined in this file is inaccessible by remote hosts. It is generally a
good practice to have the exposed APIs do very little work, and pass off
control to internal functions to do the heavy lifting.

Database Interactions
~~~~~~~~~~~~~~~~~~~~~

Database interactions are done with raw query strings, not with any kind
of modern ORM. Consider using context objects from the Koji contexts
library for thread-safe interactions. The database schema is captured in
the **docs** directory in the root of a git clone. A visualization of
the schema is not available at the time of this writing.

If you plan to introduce schema changes, please update both
``schema.sql`` and provide a migration script if necessary.

Troubleshooting
~~~~~~~~~~~~~~~

The hub runs in an Apache service, so you will need to look in Apache
logs for error messages if you are encountering 500 errors or the
service is failing to start. Specifically you want to check in:

-  /var/log/httpd/error\_log
-  /var/log/httpd/ssl\_error\_log

If you need more specific tracebacks and debugging data, consider
changing the debugging setting in **/etc/koji-hub/hub.conf**. Be advised
the hub is very verbose with this setting on, your logs will take up
gigabytes of space within several days.

Kojid
-----

kojid is the build daemon that runs on each of the build machines. Its
primary responsibility is polling for incoming build requests and
handling them accordingly. Essentially kojid asks koji-hub for work.
Koji also has support for tasks other than building. Creating install
images is one example. kojid is responsible for handling these tasks as
well. kojid uses mock for building. It also creates a fresh buildroot
for every build. kojid is written in Python and communicates with
koji-hub via XML-RPC.

Implementation Details
~~~~~~~~~~~~~~~~~~~~~~

The daemon runs as a service on a host that is traditionally not the
same as the hub or webUI. This is a good security practice because the
service runs as root, and executes untrusted code to produce builds on a
regular basis. Keeping the Hub separate limits the damage a malicious
package can do to the build system as a whole. For the same reason, the
filesystem that the hub keeps built software on should be mounted
Read-Only on the build host. It should call APIs on the hub that are
exposed through the ``HostExports`` class in the hub code. Whenever the
builder accepts a task, it forks a process to carry out the build.

An initscript/unit-file is available for kojid, so it can be stopped and
started like a normal service. Remember to do this when you deploy
changes!

TaskHandlers
^^^^^^^^^^^^

All tasks in kojid have a ``TaskHandler`` class that defines what to do
when the task is picked up from the hub. The base class is defined in
``koji/tasks.py`` where a lot of useful utility methods are available.
An example is ``uploadFile``, which is used to upload logs and built
binaries from a completed build to the hub since the shared filesystem
is read only.

The daemon code lives in ``builder/kojid``, which is deployed to
/usr/sbin/kojid. In there you'll notice that each task handler class has
a ``Methods`` member and ``_taskWeight`` member. These must be defined,
and the former is used to match the name of a waiting task (on the hub)
with the task handler code to execute. Each task handler object must
have a ``handler`` method defined, which is the entry point for the
forked process when a builder accepts a task.

Tasks can have subtasks, which is a typical model when a build can be
run on multiple architectures. In this case, developers should write 2
task handlers: one handles the build for exact one architecture, and one
that assembles the results of those tasks into a single build, and sends
status information to the hub. You can think of the latter handler as
the parent task.

All task handler objects have a ``session`` object defined, which is the
interface to use for communications with the hub. So, parent tasks
should kick off child tasks using the session object's subtask method
(which is part of HostExports). It should then call ``self.wait`` with
``all=True`` to wait for the results of the child tasks.

Here's a stub of what a new build task might look like:

::

    class BuildThingTask(BaseTaskHandler):
      Methods = ['thing']
      _taskWeight = 0.5

      def handler(self, a, b, arches, options):
        subtasks = {}
        for arch in arches:
          subtasks[arch] = session.host.subtask(method='thingArch', a, b, arch)
        results = self.wait(subtasks.values(), all=True)
        # parse results and put rows in database
        # put files in their final resting place
        return 'Build successful'

    class BuildThingArchTask(BaseTaskHandler):
      Methods = ['thingArch']
      _taskWeight = 2.0

      def handler(self, a, b, arch):
        # do the build, capture results in a variable
        self.uploadFile('/path/to/some/log')
        self.uploadFile('/path/to/binary/file')
        return result

Source Control Managers
^^^^^^^^^^^^^^^^^^^^^^^

If you your build needs to check out code from a Source Control Manager
(SCM) such as git or subversion, you can use SCM objects defined in
``koji/daemon.py``. They take a specially formed URL as an argument to
the constructor. Here's an example use. The second line is important, it
makes sure the SCM is in the whitelist of SCMs allowed in
``/etc/kojid/kojid.conf`` or in ``build_from_scm`` section of hub policy.

::

    scm = SCM(url)
    scm.assert_allowed(allowed=self.options.allowed_scms,
                       session=self.session,
                       by_config=self.options.allowed_scms_use_config,
                       by_policy=self.options.allowed_scms_use_policy,
                       policy_data={
                           'user_id': self.taskinfo['owner'],
                           'channel': self.session.getChannel(self.taskinfo['channel_id'],
                                                              strict=True)['name'],
                           'scratch': opts.get('scratch')
                       }))
    directory = scm.checkout('/checkout/path', session, uploaddir, logfile)

Checking out takes 4 arguments: where to checkout, a session object
(which is how authentication is handled), a directory to upload the log
to, and a string representing the log file name. Using this method Koji
will checkout (or clone) a remote repository and upload a log of the
standard output to the task results.

Build Root Objects
^^^^^^^^^^^^^^^^^^

It is encouraged to build software in mock chroots if appropriate. That
way Koji can easily track precise details about the environment in which
the build was executed. In ``builder/kojid`` a BuildRoot class is
defined, which provides an interface to execute mock commands. Here's an
example of their use:

::

    broot = BuildRoot(self.session, self.options, build_tag, arch, self.id)

A session object, task options, and a build tag should be passed in
as-is. You should also specify the architecture and the task ID. If you
ever need to pass in specialized options to mock, look in the
ImageTask.makeImgBuildRoot method to see how they are defined and passed
in to the BuildRoot constructor.

Troubleshooting
~~~~~~~~~~~~~~~

The daemon writes a log file to ``/var/log/kojid.log``. Debugging output
can be turned on in ``/etc/kojid/kojid.conf``.

Koji-Web
--------

koji-web is a set of scripts that run in mod\_wsgi and use the Cheetah
templating engine to provide a web interface to Koji. It acts as a
client to koji-hub providing a visual interface to perform a limited
amount of administration. koji-web exposes a lot of information and also
provides a means for certain operations, such as cancelling builds.

The web pages are derived from Cheetah templates, the syntax of which
you can read up on
`here <http://cheetahtemplate.org/users_guide/>`__. These
templates are the ``chtml`` files sitting in ``www/kojiweb``. You'll
notice quickly that these templates are referencing variables, but where
do they come from?

The ``www/kojiweb/index.py`` file provides them. There are several
functions named after the templates they support, and in each one a
dictionary called ``values`` is populated. This is how data is gathered
about the task, build, archive, or whatever the page is about. Take your
time with ``taskinfo.chtml`` in particular, as the conditionals there
have gotten quite long. If you are adding a new task to Koji, you will
need to extend this at a minimum. A new type of build task would require
this, and possibly another that is specific to viewing the archived
information about the build. (taskinfo vs. buildinfo)

If your web page needs to display the contents of a list or dictionary,
use the ``$printMap`` function to help with that. It is often sensible
to define a function that easily prints options and values in a
dictionary. An example of this is in taskinfo.chtml.

::

    #def printOpts($opts)
      #if $opts
      <strong>Options:</strong><br/>
      $printMap($opts, '&nbsp;&nbsp;')
      #end if
    #end def

Finally, if you need to expand the drop-down menus of "method" types
when searching for tasks in the WebUI, you will need to add them to the
``_TASKS`` list in ``www/kojiweb/index.py``. Add values where
appropriate to ``_TOPLEVEL_TASKS`` and ``_PARENT_TASKS`` as well so that
parent-child relationships show up correctly too.

Remember whenever you update a template or index.py, you will need to
deploy and restart apache/httpd!

Troubleshooting
~~~~~~~~~~~~~~~

Like the hub, this component is backed by apache, so you should follow
the same techniques for debugging Koji-Web as
`Koji-Hub <#Troubleshooting>`__.

Kojira
------

kojira is a daemon that keeps the build root repodata updated. It is
responsible for removing redundant build roots and cleaning up after a
build request is completed.

Building and Deploying Changes
==============================

The root of the git clone for Koji code contains a ``Makefile`` that has
a few targets to make building and deployment a little easier. Among
them are:

-  ``tarball``: create a bz2 tarball that could be consumed in an rpm build
-  ``rpm``: create Koji rpms. The NVRs will be defined by the spec file,
   which is also in the same directory. The results will appear in a
   ``noarch`` directory.
-  ``test-rpm``: like rpm, but append the Release field with a date and time
   stamp for easy upgrade-deployment

Writing Koji plugins
====================

.. toctree::
  :hidden:

  writing_a_plugin

There is a separate documentation page :doc:`writing_a_plugin`.

Submitting Changes
==================

To submit code changes for Koji, please file a pull request in Pagure.

https://pagure.io/koji/pull-requests

Here are some guidelines on producing preferable pull requests.

-  Each request should be a coherent whole, e.g. a single feature or bug fix.
   Please do not bundle a series of unrelated changes into a single PR
-  Pull requests in Pagure come from a branch in your personal fork of Koji
   (either in Pagure or a remote git repo). Please use an appropriately named
   branch for this. Do not use the master branch of your fork. Also, please
   be aware that Pagure will automatically update the pull request if you
   modify the source branch
-  Your branch should be based against the current HEAD of the target branch
-  Please adhere to `PEP8 <https://www.python.org/dev/peps/pep-0008/>`__.
   While much of the older code in Koji does not, we try to stick to it
   with new code
-  Code which is imported into CLI or needed for stand-alone API calls must
   run in both 2.6+ and 3.x python versions. We use the python-six library
   for compatibility. The affected files are:

     - ``cli/*``
     - ``koji/__init__.py``
     - ``koji/auth.py``
     - ``koji/tasks.py``
     - ``koji/util.py``
     - ``tests/test_lib/*``
     - ``tests/test_cli/*``

- Check, that unit tests are not broken. Simply run ``make test`` in main
  directory of your branch to check both python2/3 compatible-code. Or you can
  also use ``make test2`` or ``make test3`` target for each of them.

Note that the core development team for Koji is small, so it may take a few
days for someone to reply to your request.

Partial work
------------

Pull requests are for changes that are complete and ready for inclusion, but
sometimes you have partial work that you may want feedback on. Please don't
submit a PR before your code is complete.

The preferred way to request early feedback is to push your changes to a your
own koji fork and then send an email to
`koji-devel AT lists.fedorahosted.org <https://lists.fedorahosted.org/mailman/listinfo/koji-devel>`__
requesting review. This approach is one step short of a PR, making it easy to
upgrade to a PR once the changes are ready.

Unit Tests
==========

Koji comes with a small test suite, that you should always run when making
changes to the code. To do so, just run ``make test`` in your terminal.

You will need to install the following packages to actually run the tests.

 * ``glibc-langpack-en``
 * ``make``
 * ``python3-cheetah``
 * ``python3-coverage``
 * ``python3-dateutil``
 * ``python3-mock``
 * ``python3-multilib``
 * ``python3-pytest``
 * ``python3-psycopg2``
 * ``python3-qpid-proton``
 * ``python3-requests``
 * ``python3-requests-kerberos``
 * ``python3-requests-mock``

Please note that it is currently not supported to use *virtualenv* when hacking
on Koji.

Unit tests are run automatically for any commit in master branch. We use
Fedora's jenkins instance for that. Details are given here: :doc:`Unit tests
in Fedora's Jenkins <configuring_jenkins>`.

Further testing
===============

Currently we automatically build two versions of rpms in Fedora's `Copr
<https://copr.fedorainfracloud.org/>`__. First one is simple "master" branch and
is available `here <https://copr.fedorainfracloud.org/coprs/tkopecek/koji/>`__.
These RPMs are early release candidates before we tag each final release. Second
one lives `here
<https://copr.fedorainfracloud.org/coprs/tkopecek/koji-testing/>`__ and contains
the "master" branch with all the in-progress pull requests that have
"testing-ready" flag. Both repos are built once per four hours if there are new
changes in pagure.

Code Style
==========

We are using ``flake8`` to check the code style. Please refer to ``.flake8`` to
find the PEP8 and extra rules we are following/ignoring.

You will need to install the packages below to run the check.

 * ``python-flake8``
 * ``python-flake8-import-order``

Release process
===============

Merging PRs
-----------

We're not using pagure's merge button as it doesn't do everything we want.
Instead `pg script <https://github.com/mikem23/pagure-tool>`__ is used.

::

    pg pr checkout <PR number>

will checkout given PR and it can be reviewed locally. It can be rebased in this
time or it can be done automatically in the moment of merging:

::

    pg pr merge -r

This will merge the changes, fetch info from pagure, and show you the current
git changelog. The changelog should be looked over for any issues, and it will
require special formatting so pagure will close out the release notes pull
request automatically.

For example, you'll want the 'Merges' and 'Fixes' lines.

The unit tests should be run before pushing if there were any new code changes
since the last tests. After that, it will be ready to push. Test with ``git push
-n`` first before the push to make sure it's correct.


Release Notes
-------------

There should be separate PR with release notes (and version bumps) for every
release.

Pushing the Code
----------------

Once Koji is ready to release, double check that the release notes are the only
open pull request. If that's the case, the release notes are ready to merge.

Once the code is pushed, make sure all the PRs in the release roadmap are
closed, including the release notes PR.

Now the release should be tagged. All Koji releases have been signed, using this
command:

::

    git tag -a -s -m 'Koji 1.18.0' koji-1.18.0

Then the tag must be pushed:

::

    git push origin refs/tags/koji-1.18.0

To create the tarball, run ``make tarball`` from the base koji directory. You
should then copy the tarball to some other directory, just to have a backup. The
tarball should be uploaded through the pagure interface, though the upload can
also be done via ssh.

Once it's uploaded, download it and check the shasum against your local tarball.
If there are no issues, Koji's code release is complete.

Updating the Site
-----------------

The next step is to update the Koji site with the new docs and release notes.
This content is based in a separate repo from the Koji one.

Simple bash script for this repo could be used, ``kdoc``. Using ``kdoc``, run
these commands in the docs repo:

::

    kdoc release-1.18.0:test
    kdoc

``kdoc`` checks out the master branch of the koji git repo, constructs docs from
that, and copies those changes into the doc repo. Once the changes are in place,
commit them and push them to your fork:

::

    git commit -m 'koji 1.18.0 release'
    git diff --stat test
    git push mikem

Check your fork to make sure everything looks good (for example, check against a
previous release), then push the changes with 'git push.'

::

    #!/bin/bash

    # kdoc shell script

    set -x
    set -e

    # TODO more sanity checks

    KDIR=~/cvs/koji
    DOCDIR=~/cvs/koji-docs
    DBRANCH=master
    SBRANCH=master

    branches=$1
    if test -n "$branches"; then
        dst=${branches#*:}
        src=${branches%:*}
        test -n "$src" && SBRANCH=$src
        test -n "$dst" && DBRANCH=$dst
    fi

    cd "$KDIR/docs"
    test -n "$SBRANCH" && git checkout $SBRANCH
    make dirhtml

    cd "$DOCDIR"
    test -n "$DBRANCH" && git checkout $DBRANCH
    rsync -avPH --delete --exclude .git "$KDIR/docs/build/dirhtml/" "$DOCDIR/"
    git status

    echo "Docdir is: $DOCDIR"

Pushing to PyPi
---------------

All releases should also go to PyPi repository, even if we've not publicly
announced, that it is the supported delivery channel.

::

    dnf install twine
    make pypi
    make pypi-upload

Release Email
-------------

The last step is to send out an email to koji-devel@lists.fedorahosted.org. See
previous release emails for how to format the message.

Generally the email should include a link to the release notes, list some
highlights from the release, links to the release roadmap and current roadmap,
and a link to the download.

Required Permissions
--------------------

* Merge permissions for the koji repo
* Merge permissions / access to the docs repo
* Key for signing


.. image:: https://img.shields.io/pypi/v/jsonschema.svg
    :target: https://pypi.python.org/pypi/jsonschema
.. image:: https://travis-ci.org/Julian/jsonschema.svg?branch=master
    :target: https://travis-ci.org/Julian/jsonschema
.. image:: https://img.shields.io/pypi/l/jsonschema.svg
    :target: https://pypi.python.org/pypi/jsonschema

==========
jsonschema
==========

``jsonschema`` is an implementation of `JSON Schema <http://json-schema.org>`_
for Python (supporting 2.7+ including Python 3).

.. code-block:: python

    >>> from jsonschema import validate

    >>> # A sample schema, like what we'd get from json.load()
    >>> schema = {
    ...     "type" : "object",
    ...     "properties" : {
    ...         "price" : {"type" : "number"},
    ...         "name" : {"type" : "string"},
    ...     },
    ... }

    >>> # If no exception is raised by validate(), the instance is valid.
    >>> validate({"name" : "Eggs", "price" : 34.99}, schema)

    >>> validate(
    ...     {"name" : "Eggs", "price" : "Invalid"}, schema
    ... )                                   # doctest: +IGNORE_EXCEPTION_DETAIL
    Traceback (most recent call last):
        ...
    ValidationError: 'Invalid' is not of type 'number'

It can also be used from console:

.. code-block:: bash

    $ jsonschema -i sample.json sample.schema

Features
--------

* Full support for
  `Draft 3 <https://python-jsonschema.readthedocs.io/en/latest/validate/#jsonschema.Draft3Validator>`_
  **and** `Draft 4 <https://python-jsonschema.readthedocs.io/en/latest/validate/#jsonschema.Draft4Validator>`_
  of the schema.

* `Lazy validation <https://python-jsonschema.readthedocs.io/en/latest/validate/#jsonschema.IValidator.iter_errors>`_
  that can iteratively report *all* validation errors.

* Small and extensible

* `Programmatic querying <https://python-jsonschema.readthedocs.io/en/latest/errors/#module-jsonschema>`_
  of which properties or items failed validation.


Release Notes
-------------

Version 2.5.0 is mainly a performance release. The interface for `RefResolver`
was extended to add methods that improve performance on CPython.

Support for custom `RefResolver` objects with the legacy interface should *not*
be affected. If you notice something amiss please file an issue ticket.


Running the Test Suite
----------------------

If you have ``tox`` installed (perhaps via ``pip install tox`` or your
package manager), running``tox`` in the directory of your source checkout will
run ``jsonschema``'s test suite on all of the versions of Python ``jsonschema``
supports. Note that you'll need to have all of those versions installed in
order to run the tests on each of them, otherwise ``tox`` will skip (and fail)
the tests on that version.

Of course you're also free to just run the tests on a single version with your
favorite test runner. The tests live in the ``jsonschema.tests`` package.


Community
---------

There's a `mailing list <https://groups.google.com/forum/#!forum/jsonschema>`_
for this implementation on Google Groups.

Please join, and feel free to send questions there.


Contributing
------------

I'm Julian Berman.

``jsonschema`` is on `GitHub <http://github.com/Julian/jsonschema>`_.

Get in touch, via GitHub or otherwise, if you've got something to contribute,
it'd be most welcome!

You can also generally find me on Freenode (nick: ``tos9``) in various
channels, including ``#python``.

If you feel overwhelmingly grateful, you can woo me with beer money on
`Gittip <https://www.gittip.com/Julian/>`_ or via Google Wallet with the email
in my GitHub profile.
dogpile
=======

Dogpile consists of two subsystems, one building on top of the other.

``dogpile`` provides the concept of a "dogpile lock", a control structure
which allows a single thread of execution to be selected as the "creator" of
some resource, while allowing other threads of execution to refer to the previous
version of this resource as the creation proceeds; if there is no previous
version, then those threads block until the object is available.

``dogpile.cache`` is a caching API which provides a generic interface to
caching backends of any variety, and additionally provides API hooks which
integrate these cache backends with the locking mechanism of ``dogpile``.

Overall, dogpile.cache is intended as a replacement to the `Beaker
<https://pypi.org/project/Beaker/>`_ caching system, the internals of which are
written by the same author.   All the ideas of Beaker which "work" are re-
implemented in dogpile.cache in a more efficient and succinct manner, and all
the cruft (Beaker's internals were first written in 2005) relegated to the
trash heap.

Documentation
-------------

See dogpile.cache's full documentation at
`dogpile.cache documentation <https://dogpilecache.sqlalchemy.org>`_.  The
sections below provide a brief synopsis of the ``dogpile`` packages.

Features
--------

* A succinct API which encourages up-front configuration of pre-defined
  "regions", each one defining a set of caching characteristics including
  storage backend, configuration options, and default expiration time.
* A standard get/set/delete API as well as a function decorator API is
  provided.
* The mechanics of key generation are fully customizable.   The function
  decorator API features a pluggable "key generator" to customize how
  cache keys are made to correspond to function calls, and an optional
  "key mangler" feature provides for pluggable mangling of keys
  (such as encoding, SHA-1 hashing) as desired for each region.
* The dogpile lock, first developed as the core engine behind the Beaker
  caching system, here vastly simplified, improved, and better tested.
  Some key performance
  issues that were intrinsic to Beaker's architecture, particularly that
  values would frequently be "double-fetched" from the cache, have been fixed.
* Backends implement their own version of a "distributed" lock, where the
  "distribution" matches the backend's storage system.  For example, the
  memcached backends allow all clients to coordinate creation of values
  using memcached itself.   The dbm file backend uses a lockfile
  alongside the dbm file.  New backends, such as a Redis-based backend,
  can provide their own locking mechanism appropriate to the storage
  engine.
* Writing new backends or hacking on the existing backends is intended to be
  routine - all that's needed are basic get/set/delete methods. A distributed
  lock tailored towards the backend is an optional addition, else dogpile uses
  a regular thread mutex. New backends can be registered with dogpile.cache
  directly or made available via setuptools entry points.
* Included backends feature three memcached backends (python-memcached, pylibmc,
  bmemcached), a Redis backend, a backend based on Python's
  anydbm, and a plain dictionary backend.
* Space for third party plugins, including one which provides the
  dogpile.cache engine to Mako templates.


The SQLAlchemy Project
----------------------

Dogpile is part of the `SQLAlchemy Project <https://www.sqlalchemy.org>`_ and
adheres to the same standards and conventions as the core project.

Development / Bug reporting / Pull requests
___________________________________________

Please refer to the
`SQLAlchemy Community Guide <https://www.sqlalchemy.org/develop.html>`_ for
guidelines on coding and participating in this project.

Code of Conduct
_______________

Above all, SQLAlchemy places great emphasis on polite, thoughtful, and
constructive communication between users and developers.
Please see our current Code of Conduct at
`Code of Conduct <https://www.sqlalchemy.org/codeofconduct.html>`_.

License
-------

Dogpile is distributed under the `MIT license
<https://opensource.org/licenses/MIT>`_.
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 